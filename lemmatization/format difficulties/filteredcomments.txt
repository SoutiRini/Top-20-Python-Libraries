 "Two-dimensional, size-mutable, potentially heterogeneous tabular data. Data structure also contains labeled axes (rows columns). Arithmetic operations align row column labels. Can thought dict-like container Series objects. The primary pandas data structure. Parameters ---------- data : ndarray (structured homogeneous), Iterable, dict, DataFrame Dict contain Series, arrays, constants, list-like objects. .. versionchanged:: 0.23.0 If data dict, column order follows insertion-order Python 3.6 later. .. versionchanged:: 0.25.0 If data list dicts, column order follows insertion-order Python 3.6 later. index : Index array-like Index use resulting frame. Will default RangeIndex indexing information part input data index provided. columns : Index array-like Column labels use resulting frame. Will default RangeIndex (0, 1, 2, ..., n) column labels provided. dtype : dtype, default None Data type force. Only single dtype allowed. If None, infer. copy : bool, default False Copy data inputs. Only affects DataFrame / 2d ndarray input. See Also -------- DataFrame.from_records : Constructor tuples, also record arrays. DataFrame.from_dict : From dicts Series, arrays, dicts. read_csv read_table read_clipboard Examples -------- Constructing DataFrame dictionary. >>> = {'col1': [1, 2], 'col2': [3, 4]} >>> df = pd.DataFrame(data=d) >>> df col1 col2 0 1 3 1 2 4 Notice inferred dtype int64. >>> df.dtypes col1 int64 col2 int64 dtype: object To enforce single dtype: >>> df = pd.DataFrame(data=d, dtype=np.int8) >>> df.dtypes col1 int8 col2 int8 dtype: object Constructing DataFrame numpy ndarray: >>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), ... columns=['a', 'b', 'c']) >>> df2 b c 0 1 2 3 1 4 5 6 2 7 8 9" "Concatenate pandas objects along particular axis optional set logic along axes. Can also add layer hierarchical indexing concatenation axis, may useful labels (or overlapping) passed axis number. Parameters ---------- objs : sequence mapping Series DataFrame objects If dict passed, sorted keys used `keys` argument, unless passed, case values selected (see below). Any None objects dropped silently unless None case ValueError raised. axis : {0/'index', 1/'columns'}, default 0 The axis concatenate along. join : {'inner', 'outer'}, default 'outer' How handle indexes axis (or axes). ignore_index : bool, default False If True, use index values along concatenation axis. The resulting axis labeled 0, ..., n - 1. This useful concatenating objects concatenation axis meaningful indexing information. Note index values axes still respected join. keys : sequence, default None If multiple levels passed, contain tuples. Construct hierarchical index using passed keys outermost level. levels : list sequences, default None Specific levels (unique values) use constructing MultiIndex. Otherwise inferred keys. names : list, default None Names levels resulting hierarchical index. verify_integrity : bool, default False Check whether new concatenated axis contains duplicates. This expensive relative actual data concatenation. sort : bool, default False Sort non-concatenation axis already aligned `join` 'outer'. This effect ``join='inner'``, already preserves order non-concatenation axis. .. versionadded:: 0.23.0 .. versionchanged:: 1.0.0 Changed sort default. copy : bool, default True If False, copy data unnecessarily. Returns ------- object, type objs When concatenating ``Series`` along index (axis=0), ``Series`` returned. When ``objs`` contains least one ``DataFrame``, ``DataFrame`` returned. When concatenating along columns (axis=1), ``DataFrame`` returned. See Also -------- Series.append : Concatenate Series. DataFrame.append : Concatenate DataFrames. DataFrame.join : Join DataFrames using indexes. DataFrame.merge : Merge DataFrames indexes columns. Notes ----- The keys, levels, names arguments optional. A walkthrough method fits tools combining pandas objects found `here <https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html>`__. Examples -------- Combine two ``Series``. >>> s1 = pd.Series(['a', 'b']) >>> s2 = pd.Series(['c', 'd']) >>> pd.concat([s1, s2]) 0 1 b 0 c 1 dtype: object Clear existing index reset result setting ``ignore_index`` option ``True``. >>> pd.concat([s1, s2], ignore_index=True) 0 1 b 2 c 3 dtype: object Add hierarchical index outermost level data ``keys`` option. >>> pd.concat([s1, s2], keys=['s1', 's2']) s1 0 1 b s2 0 c 1 dtype: object Label index keys create ``names`` option. >>> pd.concat([s1, s2], keys=['s1', 's2'], ... names=['Series name', 'Row ID']) Series name Row ID s1 0 1 b s2 0 c 1 dtype: object Combine two ``DataFrame`` objects identical columns. >>> df1 = pd.DataFrame([['a', 1], ['b', 2]], ... columns=['letter', 'number']) >>> df1 letter number 0 1 1 b 2 >>> df2 = pd.DataFrame([['c', 3], ['d', 4]], ... columns=['letter', 'number']) >>> df2 letter number 0 c 3 1 4 >>> pd.concat([df1, df2]) letter number 0 1 1 b 2 0 c 3 1 4 Combine ``DataFrame`` objects overlapping columns return everything. Columns outside intersection filled ``NaN`` values. >>> df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']], ... columns=['letter', 'number', 'animal']) >>> df3 letter number animal 0 c 3 cat 1 4 dog >>> pd.concat([df1, df3], sort=False) letter number animal 0 1 NaN 1 b 2 NaN 0 c 3 cat 1 4 dog Combine ``DataFrame`` objects overlapping columns return shared passing ``inner`` ``join`` keyword argument. >>> pd.concat([df1, df3], join=""inner"") letter number 0 1 1 b 2 0 c 3 1 4 Combine ``DataFrame`` objects horizontally along x axis passing ``axis=1``. >>> df4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']], ... columns=['animal', 'name']) >>> pd.concat([df1, df4], axis=1) letter number animal name 0 1 bird polly 1 b 2 monkey george Prevent result including duplicate index values ``verify_integrity`` option. >>> df5 = pd.DataFrame([1], index=['a']) >>> df5 0 1 >>> df6 = pd.DataFrame([2], index=['a']) >>> df6 0 2 >>> pd.concat([df5, df6], verify_integrity=True) Traceback (most recent call last): ... ValueError: Indexes overlapping values: ['a']" "Detect missing values array-like object. This function takes scalar array-like object indicates whether values missing (``NaN`` numeric arrays, ``None`` ``NaN`` object arrays, ``NaT`` datetimelike). Parameters ---------- obj : scalar array-like Object check null missing values. Returns ------- bool array-like bool For scalar input, returns scalar boolean. For array input, returns array boolean indicating whether corresponding element missing. See Also -------- notna : Boolean inverse pandas.isna. Series.isna : Detect missing values Series. DataFrame.isna : Detect missing values DataFrame. Index.isna : Detect missing values Index. Examples -------- Scalar arguments (including strings) result scalar boolean. >>> pd.isna('dog') False >>> pd.isna(pd.NA) True >>> pd.isna(np.nan) True ndarrays result ndarray booleans. >>> array = np.array([[1, np.nan, 3], [4, 5, np.nan]]) >>> array array([[ 1., nan, 3.], [ 4., 5., nan]]) >>> pd.isna(array) array([[False, True, False], [False, False, True]]) For indexes, ndarray booleans returned. >>> index = pd.DatetimeIndex([""2017-07-05"", ""2017-07-06"", None, ... ""2017-07-08""]) >>> index DatetimeIndex(['2017-07-05', '2017-07-06', 'NaT', '2017-07-08'], dtype='datetime64[ns]', freq=None) >>> pd.isna(index) array([False, False, True, False]) For Series DataFrame, type returned, containing booleans. >>> df = pd.DataFrame([['ant', 'bee', 'cat'], ['dog', None, 'fly']]) >>> df 0 1 2 0 ant bee cat 1 dog None fly >>> pd.isna(df) 0 1 2 0 False False False 1 False True False >>> pd.isna(df[1]) 0 False 1 True Name: 1, dtype: bool" "Detect non-missing values array-like object. This function takes scalar array-like object indicates whether values valid (not missing, ``NaN`` numeric arrays, ``None`` ``NaN`` object arrays, ``NaT`` datetimelike). Parameters ---------- obj : array-like object value Object check *not* null *non*-missing values. Returns ------- bool array-like bool For scalar input, returns scalar boolean. For array input, returns array boolean indicating whether corresponding element valid. See Also -------- isna : Boolean inverse pandas.notna. Series.notna : Detect valid values Series. DataFrame.notna : Detect valid values DataFrame. Index.notna : Detect valid values Index. Examples -------- Scalar arguments (including strings) result scalar boolean. >>> pd.notna('dog') True >>> pd.notna(pd.NA) False >>> pd.notna(np.nan) False ndarrays result ndarray booleans. >>> array = np.array([[1, np.nan, 3], [4, 5, np.nan]]) >>> array array([[ 1., nan, 3.], [ 4., 5., nan]]) >>> pd.notna(array) array([[ True, False, True], [ True, True, False]]) For indexes, ndarray booleans returned. >>> index = pd.DatetimeIndex([""2017-07-05"", ""2017-07-06"", None, ... ""2017-07-08""]) >>> index DatetimeIndex(['2017-07-05', '2017-07-06', 'NaT', '2017-07-08'], dtype='datetime64[ns]', freq=None) >>> pd.notna(index) array([ True, True, False, True]) For Series DataFrame, type returned, containing booleans. >>> df = pd.DataFrame([['ant', 'bee', 'cat'], ['dog', None, 'fly']]) >>> df 0 1 2 0 ant bee cat 1 dog None fly >>> pd.notna(df) 0 1 2 0 True True True 1 True False True >>> pd.notna(df[1]) 0 True 1 False Name: 1, dtype: bool" "Convert argument datetime. Parameters ---------- arg : int, float, str, datetime, list, tuple, 1-d array, Series DataFrame/dict-like The object convert datetime. errors : {'ignore', 'raise', 'coerce'}, default 'raise' - If 'raise', invalid parsing raise exception. - If 'coerce', invalid parsing set NaT. - If 'ignore', invalid parsing return input. dayfirst : bool, default False Specify date parse order `arg` str list-likes. If True, parses dates day first, eg 10/11/12 parsed 2012-11-10. Warning: dayfirst=True strict, prefer parse day first (this known bug, based dateutil behavior). yearfirst : bool, default False Specify date parse order `arg` str list-likes. - If True parses dates year first, eg 10/11/12 parsed 2010-11-12. - If dayfirst yearfirst True, yearfirst preceded (same dateutil). Warning: yearfirst=True strict, prefer parse year first (this known bug, based dateutil behavior). utc : bool, default None Return UTC DatetimeIndex True (converting tz-aware datetime.datetime objects well). format : str, default None The strftime parse time, eg ""%d/%m/%Y"", note ""%f"" parse way nanoseconds. See strftime documentation information choices: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior. exact : bool, True default Behaves as: - If True, require exact format match. - If False, allow format match anywhere target string. unit : str, default 'ns' The unit arg (D,s,ms,us,ns) denote unit, integer float number. This based origin. Example, unit='ms' origin='unix' (the default), would calculate number milliseconds unix epoch start. infer_datetime_format : bool, default False If True `format` given, attempt infer format datetime strings, inferred, switch faster method parsing them. In cases increase parsing speed ~5-10x. origin : scalar, default 'unix' Define reference date. The numeric values would parsed number units (defined `unit`) since reference date. - If 'unix' (or POSIX) time; origin set 1970-01-01. - If 'julian', unit must 'D', origin set beginning Julian Calendar. Julian day number 0 assigned day starting noon January 1, 4713 BC. - If Timestamp convertible, origin set Timestamp identified origin. cache : bool, default True If True, use cache unique, converted dates apply datetime conversion. May produce significant speed-up parsing duplicate date strings, especially ones timezone offsets. The cache used least 50 values. The presence out-of-bounds values render cache unusable may slow parsing. .. versionadded:: 0.23.0 .. versionchanged:: 0.25.0 - changed default value False True. Returns ------- datetime If parsing succeeded. Return type depends input: - list-like: DatetimeIndex - Series: Series datetime64 dtype - scalar: Timestamp In case possible return designated types (e.g. element input Timestamp.min Timestamp.max) return datetime.datetime type (or corresponding array/Series). See Also -------- DataFrame.astype : Cast argument specified dtype. to_timedelta : Convert argument timedelta. convert_dtypes : Convert dtypes. Examples -------- Assembling datetime multiple columns DataFrame. The keys common abbreviations like ['year', 'month', 'day', 'minute', 'second', 'ms', 'us', 'ns']) plurals >>> df = pd.DataFrame({'year': [2015, 2016], ... 'month': [2, 3], ... 'day': [4, 5]}) >>> pd.to_datetime(df) 0 2015-02-04 1 2016-03-05 dtype: datetime64[ns] If date meet `timestamp limitations <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html #timeseries-timestamp-limits>`_, passing errors='ignore' return original input instead raising exception. Passing errors='coerce' force out-of-bounds date NaT, addition forcing non-dates (or non-parseable dates) NaT. >>> pd.to_datetime('13000101', format='%Y%m%d', errors='ignore') datetime.datetime(1300, 1, 1, 0, 0) >>> pd.to_datetime('13000101', format='%Y%m%d', errors='coerce') NaT Passing infer_datetime_format=True often-times speedup parsing ISO8601 format exactly, regular format. >>> = pd.Series(['3/11/2000', '3/12/2000', '3/13/2000'] * 1000) >>> s.head() 0 3/11/2000 1 3/12/2000 2 3/13/2000 3 3/11/2000 4 3/12/2000 dtype: object >>> %timeit pd.to_datetime(s, infer_datetime_format=True) # doctest: +SKIP 100 loops, best 3: 10.4 ms per loop >>> %timeit pd.to_datetime(s, infer_datetime_format=False) # doctest: +SKIP 1 loop, best 3: 471 ms per loop Using unix epoch time >>> pd.to_datetime(1490195805, unit='s') Timestamp('2017-03-22 15:16:45') >>> pd.to_datetime(1490195805433502912, unit='ns') Timestamp('2017-03-22 15:16:45.433502912') .. warning:: For float arg, precision rounding might happen. To prevent unexpected behavior use fixed-width exact type. Using non-unix epoch origin >>> pd.to_datetime([1, 2, 3], unit='D', ... origin=pd.Timestamp('1960-01-01')) DatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'], \ dtype='datetime64[ns]', freq=None)" "One-dimensional ndarray axis labels (including time series). Labels need unique must hashable type. The object supports integer- label-based indexing provides host methods performing operations involving index. Statistical methods ndarray overridden automatically exclude missing data (currently represented NaN). Operations Series (+, -, /, *, **) align values based associated index values-- need length. The result index sorted union two indexes. Parameters ---------- data : array-like, Iterable, dict, scalar value Contains data stored Series. .. versionchanged:: 0.23.0 If data dict, argument order maintained Python 3.6 later. index : array-like Index (1d) Values must hashable length `data`. Non-unique index values allowed. Will default RangeIndex (0, 1, 2, ..., n) provided. If dict index sequence used, index override keys found dict. dtype : str, numpy.dtype, ExtensionDtype, optional Data type output Series. If specified, inferred `data`. See :ref:`user guide <basics.dtypes>` usages. name : str, optional The name give Series. copy : bool, default False Copy input data." Read comma-separated values (csv) file DataFrame. "Load pickled pandas object (or object) file. .. warning:: Loading pickled data received untrusted sources unsafe. See `here <https://docs.python.org/3/library/pickle.html>`__. Parameters ---------- filepath_or_buffer : str, path object file-like object File path, URL, buffer pickled object loaded from. .. versionchanged:: 1.0.0 Accept URL. URL limited S3 GCS. compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer' If 'infer' 'path_or_url' path-like, detect compression following extensions: '.gz', '.bz2', '.zip', '.xz' (otherwise compression) If 'infer' 'path_or_url' path-like, use None (= decompression). Returns ------- unpickled : type object stored file See Also -------- DataFrame.to_pickle : Pickle (serialize) DataFrame object file. Series.to_pickle : Pickle (serialize) Series object file. read_hdf : Read HDF5 file DataFrame. read_sql : Read SQL query database table DataFrame. read_parquet : Load parquet object, returning DataFrame. Notes ----- read_pickle guaranteed backwards compatible pandas 0.20.3. Examples -------- >>> original_df = pd.DataFrame({""foo"": range(5), ""bar"": range(5, 10)}) >>> original_df foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 >>> pd.to_pickle(original_df, ""./dummy.pkl"") >>> unpickled_df = pd.read_pickle(""./dummy.pkl"") >>> unpickled_df foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 >>> import os >>> os.remove(""./dummy.pkl"")" "Convert JSON string pandas object. Parameters ---------- path_or_buf : valid JSON str, path object file-like object Any valid string path acceptable. The string could URL. Valid URL schemes include http, ftp, s3, file. For file URLs, host expected. A local file could be: ``file://localhost/path/to/table.json``. If want pass path object, pandas accepts ``os.PathLike``. By file-like object, refer objects ``read()`` method, file handler (e.g. via builtin ``open`` function) ``StringIO``. orient : str Indication expected JSON string format. Compatible JSON strings produced ``to_json()`` corresponding orient value. The set possible orients is: - ``'split'`` : dict like ``{index -> [index], columns -> [columns], data -> [values]}`` - ``'records'`` : list like ``[{column -> value}, ... , {column -> value}]`` - ``'index'`` : dict like ``{index -> {column -> value}}`` - ``'columns'`` : dict like ``{column -> {index -> value}}`` - ``'values'`` : values array The allowed default values depend value `typ` parameter. * ``typ == 'series'``, - allowed orients ``{'split','records','index'}`` - default ``'index'`` - The Series index must unique orient ``'index'``. * ``typ == 'frame'``, - allowed orients ``{'split','records','index', 'columns','values', 'table'}`` - default ``'columns'`` - The DataFrame index must unique orients ``'index'`` ``'columns'``. - The DataFrame columns must unique orients ``'index'``, ``'columns'``, ``'records'``. .. versionadded:: 0.23.0 'table' allowed value ``orient`` argument typ : {'frame', 'series'}, default 'frame' The type object recover. dtype : bool dict, default None If True, infer dtypes; dict column dtype, use those; False, infer dtypes all, applies data. For ``orient`` values except ``'table'``, default True. .. versionchanged:: 0.25.0 Not applicable ``orient='table'``. convert_axes : bool, default None Try convert axes proper dtypes. For ``orient`` values except ``'table'``, default True. .. versionchanged:: 0.25.0 Not applicable ``orient='table'``. convert_dates : bool list str, default True List columns parse dates. If True, try parse datelike columns. A column label datelike * ends ``'_at'``, * ends ``'_time'``, * begins ``'timestamp'``, * ``'modified'``, * ``'date'``. keep_default_dates : bool, default True If parsing dates, parse default datelike columns. numpy : bool, default False Direct decoding numpy arrays. Supports numeric data only, non-numeric column index labels supported. Note also JSON ordering MUST term numpy=True. .. deprecated:: 1.0.0 precise_float : bool, default False Set enable usage higher precision (strtod) function decoding string double values. Default (False) use fast less precise builtin functionality. date_unit : str, default None The timestamp unit detect converting dates. The default behaviour try detect correct precision, desired pass one 's', 'ms', 'us' 'ns' force parsing seconds, milliseconds, microseconds nanoseconds respectively. encoding : str, default 'utf-8' The encoding use decode py3 bytes. lines : bool, default False Read file json object per line. chunksize : int, optional Return JsonReader object iteration. See `line-delimited json docs <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#line-delimited-json>`_ information ``chunksize``. This passed `lines=True`. If None, file read memory once. .. versionadded:: 0.21.0 compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer' For on-the-fly decompression on-disk data. If 'infer', use gzip, bz2, zip xz path_or_buf string ending '.gz', '.bz2', '.zip', 'xz', respectively, decompression otherwise. If using 'zip', ZIP file must contain one data file read in. Set None decompression. .. versionadded:: 0.21.0 Returns ------- Series DataFrame The type returned depends value `typ`. See Also -------- DataFrame.to_json : Convert DataFrame JSON string. Series.to_json : Convert Series JSON string. Notes ----- Specific ``orient='table'``, :class:`DataFrame` literal :class:`Index` name `index` gets written :func:`to_json`, subsequent read operation incorrectly set :class:`Index` name ``None``. This `index` also used :func:`DataFrame.to_json` denote missing :class:`Index` name, subsequent :func:`read_json` operation cannot distinguish two. The limitation encountered :class:`MultiIndex` names beginning ``'level_'``. Examples -------- >>> df = pd.DataFrame([['a', 'b'], ['c', 'd']], ... index=['row 1', 'row 2'], ... columns=['col 1', 'col 2']) Encoding/decoding Dataframe using ``'split'`` formatted JSON: >>> df.to_json(orient='split') '{""columns"":[""col 1"",""col 2""], ""index"":[""row 1"",""row 2""], ""data"":[[""a"",""b""],[""c"",""d""]]}' >>> pd.read_json(_, orient='split') col 1 col 2 row 1 b row 2 c Encoding/decoding Dataframe using ``'index'`` formatted JSON: >>> df.to_json(orient='index') '{""row 1"":{""col 1"":""a"",""col 2"":""b""},""row 2"":{""col 1"":""c"",""col 2"":""d""}}' >>> pd.read_json(_, orient='index') col 1 col 2 row 1 b row 2 c Encoding/decoding Dataframe using ``'records'`` formatted JSON. Note index labels preserved encoding. >>> df.to_json(orient='records') '[{""col 1"":""a"",""col 2"":""b""},{""col 1"":""c"",""col 2"":""d""}]' >>> pd.read_json(_, orient='records') col 1 col 2 0 b 1 c Encoding Table Schema >>> df.to_json(orient='table') '{""schema"": {""fields"": [{""name"": ""index"", ""type"": ""string""}, {""name"": ""col 1"", ""type"": ""string""}, {""name"": ""col 2"", ""type"": ""string""}], ""primaryKey"": ""index"", ""pandas_version"": ""0.20.0""}, ""data"": [{""index"": ""row 1"", ""col 1"": ""a"", ""col 2"": ""b""}, {""index"": ""row 2"", ""col 1"": ""c"", ""col 2"": ""d""}]}'" "Type categorical data categories orderedness .. versionchanged:: 0.21.0 Parameters ---------- categories : sequence, optional Must unique, must contain nulls. ordered : bool, default False Attributes ---------- categories ordered Methods ------- None Notes ----- This class useful specifying type ``Categorical`` independent values. See :ref:`categorical.categoricaldtype` more. Examples -------- >>> = CategoricalDtype(categories=['b', 'a'], ordered=True) >>> pd.Series(['a', 'b', 'a', 'c'], dtype=t) 0 1 b 2 3 NaN dtype: category Categories (2, object): [b < a] See Also -------- pandas.Categorical """""" # TODO: Document public vs. private API name = 'category' type = CategoricalDtypeType kind = 'O' str = '|O08' base = np.dtype('O') _metadata = ['categories', 'ordered'] _cache = {} def __init__(self, categories=None, ordered=None): self._finalize(categories, ordered, fastpath=False) @classmethod def _from_fastpath(cls, categories=None, ordered=None): self = cls.__new__(cls) self._finalize(categories, ordered, fastpath=True) return self @classmethod def _from_categorical_dtype(cls, dtype, categories=None, ordered=None): categories ordered None: return dtype categories None: categories = dtype.categories ordered None: ordered = dtype.ordered return cls(categories, ordered) def _finalize(self, categories, ordered, fastpath=False): ordered None: self.validate_ordered(ordered) categories None: categories = self.validate_categories(categories, fastpath=fastpath) self._categories = categories self._ordered = ordered def __setstate__(self, state): self._categories = state.pop('categories', None) self._ordered = state.pop('ordered', False) def __hash__(self): # _hash_categories returns uint64, use negative # space unknown categories avoid conflict self.categories None: self.ordered: return -1 else: return -2 # We *do* want include real self.ordered return int(self._hash_categories(self.categories, self.ordered)) def __eq__(self, other): """""" Rules CDT equality: 1) Any CDT equal string 'category' 2) Any CDT equal CDT categories=None regardless ordered 3) A CDT ordered=True equal another CDT ordered=True identical categories order 4) A CDT ordered={False, None} equal another CDT ordered={False, None} identical categories, order required. There distinction False/None. 5) Any comparison returns False """""" isinstance(other, compat.string_types): return == self.name (hasattr(other, 'ordered') hasattr(other, 'categories')): return False elif self.categories None other.categories None: # We're forced suboptimal corner thanks math # backwards compatibility. We require `CDT(...) == 'category'` # CDTs **including** `CDT(None, ...)`. Therefore, *all* # CDT(., .) = CDT(None, False) *all* # CDT(., .) = CDT(None, True). return True elif self.ordered other.ordered: # At least one ordered=True; equal ordered=True # values categories order. return ((self.ordered == other.ordered) self.categories.equals(other.categories)) else: # Neither ordered=True; equal categories, # order necessary. There distinction # ordered=False ordered=None: CDT(., False) CDT(., None) # equal categories. return hash(self) == hash(other) def __repr__(self): tpl = u'CategoricalDtype(categories={}ordered={})' self.categories None: data = u""None, "" else: data = self.categories._format_data(name=self.__class__.__name__) return tpl.format(data, self.ordered) @staticmethod def _hash_categories(categories, ordered=True): pandas.core.util.hashing import ( hash_array, _combine_hash_arrays, hash_tuples ) len(categories) isinstance(categories[0], tuple): # assumes individual category tuple, our. ATM # I really want support categories # tuples. categories = list(categories) # breaks np.array categories cat_array = hash_tuples(categories) else: categories.dtype == 'O': types = [type(x) x categories] len(set(types)) == 1: # TODO: hash_array handle mixed types. It casts # everything str first, means treat # {'1', '2'} {'1', 2} # find better solution cat_array = np.array([hash(x) x categories]) hashed = hash((tuple(categories), ordered)) return hashed cat_array = hash_array(np.asarray(categories), categorize=False) ordered: cat_array = np.vstack([ cat_array, np.arange(len(cat_array), dtype=cat_array.dtype) ]) else: cat_array = [cat_array] hashed = _combine_hash_arrays(iter(cat_array), num_items=len(cat_array)) len(hashed) == 0: # bug Numpy<1.12 length 0 arrays. Just return correct # value 0 return 0 else: return np.bitwise_xor.reduce(hashed) @classmethod def construct_from_string(cls, string): """""" attempt construct type string, raise TypeError possible """""" try: string == 'category': return cls() except: pass raise TypeError(""cannot construct CategoricalDtype"") @staticmethod def validate_ordered(ordered): """""" Validates valid ordered parameter. If boolean, TypeError raised. Parameters ---------- ordered : object The parameter verified. Raises ------ TypeError If 'ordered' boolean. """""" pandas.core.dtypes.common import is_bool is_bool(ordered): raise TypeError(""'ordered' must either 'True' 'False'"") @staticmethod def validate_categories(categories, fastpath=False): """""" Validates good categories Parameters ---------- categories : array-like fastpath : bool Whether skip nan uniqueness checks Returns ------- categories : Index """""" pandas import Index isinstance(categories, ABCIndexClass): categories = Index(categories, tupleize_cols=False) fastpath: categories.hasnans: raise ValueError('Categorial categories cannot null') categories.is_unique: raise ValueError('Categorical categories must unique') isinstance(categories, ABCCategoricalIndex): categories = categories.categories return categories def update_dtype(self, dtype): """""" Returns CategoricalDtype categories ordered taken dtype specified, otherwise falling back self unspecified Parameters ---------- dtype : CategoricalDtype Returns ------- new_dtype : CategoricalDtype """""" isinstance(dtype, compat.string_types) dtype == 'category': # dtype='category' change anything return self elif self.is_dtype(dtype): msg = ('a CategoricalDtype must passed perform update, ' 'got {dtype!r}').format(dtype=dtype) raise ValueError(msg) # dtype CDT: keep current categories/ordered None new_categories = dtype.categories new_categories None: new_categories = self.categories new_ordered = dtype.ordered new_ordered None: new_ordered = self.ordered return CategoricalDtype(new_categories, new_ordered) @property def categories(self): """""" An ``Index`` containing unique categories allowed." "Convert argument numeric type. The default return dtype `float64` `int64` depending data supplied. Use `downcast` parameter obtain dtypes. Please note precision loss may occur really large numbers passed in. Due internal limitations `ndarray`, numbers smaller `-9223372036854775808` (np.iinfo(np.int64).min) larger `18446744073709551615` (np.iinfo(np.uint64).max) passed in, likely converted float stored `ndarray`. These warnings apply similarly `Series` since internally leverages `ndarray`. Parameters ---------- arg : scalar, list, tuple, 1-d array, Series errors : {'ignore', 'raise', 'coerce'}, default 'raise' - If 'raise', invalid parsing raise exception. - If 'coerce', invalid parsing set NaN. - If 'ignore', invalid parsing return input. downcast : {'integer', 'signed', 'unsigned', 'float'}, default None If None, data successfully cast numerical dtype (or data numeric begin with), downcast resulting data smallest numerical dtype possible according following rules: - 'integer' 'signed': smallest signed int dtype (min.: np.int8) - 'unsigned': smallest unsigned int dtype (min.: np.uint8) - 'float': smallest float dtype (min.: np.float32) As behaviour separate core conversion numeric values, errors raised downcasting surfaced regardless value 'errors' input. In addition, downcasting occur size resulting data's dtype strictly larger dtype cast to, none dtypes checked satisfy specification, downcasting performed data. Returns ------- ret : numeric parsing succeeded. Return type depends input. Series Series, otherwise ndarray. See Also -------- DataFrame.astype : Cast argument specified dtype. to_datetime : Convert argument datetime. to_timedelta : Convert argument timedelta. numpy.ndarray.astype : Cast numpy array specified type. convert_dtypes : Convert dtypes. Examples -------- Take separate series convert numeric, coercing told >>> = pd.Series(['1.0', '2', -3]) >>> pd.to_numeric(s) 0 1.0 1 2.0 2 -3.0 dtype: float64 >>> pd.to_numeric(s, downcast='float') 0 1.0 1 2.0 2 -3.0 dtype: float32 >>> pd.to_numeric(s, downcast='signed') 0 1 1 2 2 -3 dtype: int8 >>> = pd.Series(['apple', '1.0', '2', -3]) >>> pd.to_numeric(s, errors='ignore') 0 apple 1 1.0 2 2 3 -3 dtype: object >>> pd.to_numeric(s, errors='coerce') 0 NaN 1 1.0 2 2.0 3 -3.0 dtype: float64" "Convert categorical variable dummy/indicator variables. Parameters ---------- data : array-like, Series, DataFrame Data get dummy indicators. prefix : str, list str, dict str, default None String append DataFrame column names. Pass list length equal number columns calling get_dummies DataFrame. Alternatively, `prefix` dictionary mapping column names prefixes. prefix_sep : str, default '_' If appending prefix, separator/delimiter use. Or pass list dictionary `prefix`. dummy_na : bool, default False Add column indicate NaNs, False NaNs ignored. columns : list-like, default None Column names DataFrame encoded. If `columns` None columns `object` `category` dtype converted. sparse : bool, default False Whether dummy-encoded columns backed :class:`SparseArray` (True) regular NumPy array (False). drop_first : bool, default False Whether get k-1 dummies k categorical levels removing first level. dtype : dtype, default np.uint8 Data type new columns. Only single dtype allowed. .. versionadded:: 0.23.0 Returns ------- DataFrame Dummy-coded data. See Also -------- Series.str.get_dummies : Convert Series dummy codes. Examples -------- >>> = pd.Series(list('abca')) >>> pd.get_dummies(s) b c 0 1 0 0 1 0 1 0 2 0 0 1 3 1 0 0 >>> s1 = ['a', 'b', np.nan] >>> pd.get_dummies(s1) b 0 1 0 1 0 1 2 0 0 >>> pd.get_dummies(s1, dummy_na=True) b NaN 0 1 0 0 1 0 1 0 2 0 0 1 >>> df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'], ... 'C': [1, 2, 3]}) >>> pd.get_dummies(df, prefix=['col1', 'col2']) C col1_a col1_b col2_a col2_b col2_c 0 1 1 0 0 1 0 1 2 0 1 1 0 0 2 3 1 0 0 0 1 >>> pd.get_dummies(pd.Series(list('abcaa'))) b c 0 1 0 0 1 0 1 0 2 0 0 1 3 1 0 0 4 1 0 0 >>> pd.get_dummies(pd.Series(list('abcaa')), drop_first=True) b c 0 0 0 1 1 0 2 0 1 3 0 0 4 0 0 >>> pd.get_dummies(pd.Series(list('abc')), dtype=float) b c 0 1.0 0.0 0.0 1 0.0 1.0 0.0 2 0.0 0.0 1.0" "Convert structured record ndarray DataFrame. Parameters ---------- data : ndarray (structured dtype), list tuples, dict, DataFrame index : str, list fields, array-like Field array use index, alternately specific set input labels use. exclude : sequence, default None Columns fields exclude. columns : sequence, default None Column names use. If passed data names associated them, argument provides names columns. Otherwise argument indicates order columns result (any names found data become all-NA columns). coerce_float : bool, default False Attempt convert values non-string, non-numeric objects (like decimal.Decimal) floating point, useful SQL result sets. nrows : int, default None Number rows read data iterator. Returns ------- DataFrame" "Quantile-based discretization function. Discretize variable equal-sized buckets based rank based sample quantiles. For example 1000 values 10 quantiles would produce Categorical object indicating quantile membership data point. Parameters ---------- x : 1d ndarray Series q : int list-like int Number quantiles. 10 deciles, 4 quartiles, etc. Alternately array quantiles, e.g. [0, .25, .5, .75, 1.] quartiles. labels : array False, default None Used labels resulting bins. Must length resulting bins. If False, return integer indicators bins. If True, raises error. retbins : bool, optional Whether return (bins, labels) not. Can useful bins given scalar. precision : int, optional The precision store display bins labels. duplicates : {default 'raise', 'drop'}, optional If bin edges unique, raise ValueError drop non-uniques. Returns ------- : Categorical Series array integers labels False The return type (Categorical Series) depends input: Series type category input Series else Categorical. Bins represented categories categorical data returned. bins : ndarray floats Returned `retbins` True. Notes ----- Out bounds values NA resulting Categorical object Examples -------- >>> pd.qcut(range(5), 4) ... # doctest: +ELLIPSIS [(-0.001, 1.0], (-0.001, 1.0], (1.0, 2.0], (2.0, 3.0], (3.0, 4.0]] Categories (4, interval[float64]): [(-0.001, 1.0] < (1.0, 2.0] ... >>> pd.qcut(range(5), 3, labels=[""good"", ""medium"", ""bad""]) ... # doctest: +SKIP [good, good, medium, bad, bad] Categories (3, object): [good < medium < bad] >>> pd.qcut(range(5), 4, labels=False) array([0, 0, 1, 2, 3])"