{}{
    "source file": "abs.py",
    "line number": "28",
    "func name": "make_abs_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do abs.\n\n\n"
}{}{}{}{
    "source file": "activations_test.py",
    "line number": "34",
    "func name": "_ref_softmax",
    "func arg": "(values)",
    "comments": ""
}{
    "source file": "activations.py",
    "line number": "497",
    "func name": "get",
    "func arg": "(identifier)",
    "comments": "Returns function.\n\nArguments: identifier: Function or string\n##### Returns\n* **example**: \n\n* **eback (most recent call last)**: \n\n* **eError**: Unknown activation function\n\n"
}{}{}{
    "source file": "activity.py",
    "line number": "707",
    "func name": "resolve",
    "func arg": "(node, context, parent_scope)",
    "comments": ""
}{}{}{}{}{}{}{}{}{}{
    "source file": "adagrad_test1.py",
    "line number": "51",
    "func name": "sparse_adagrad_update_numpy",
    "func arg": "(param, accum, gindexs, gvalues, lr, epsilon)",
    "comments": ""
}{}{}{}{
    "source file": "adam_test.py",
    "line number": "34",
    "func name": "adam_update_numpy",
    "func arg": "(param, g_t, t, m, v, alpha, beta1, beta2, epsilon)",
    "comments": ""
}{
    "source file": "adam_test1.py",
    "line number": "102",
    "func name": "get_beta_accumulators",
    "func arg": "(opt, dtype)",
    "comments": ""
}{
    "source file": "adam_test2.py",
    "line number": "37",
    "func name": "adam_update_numpy",
    "func arg": "(param, g_t, t, m, v, alpha, beta1, beta2, epsilon)",
    "comments": ""
}{}{}{
    "source file": "adamax_test.py",
    "line number": "72",
    "func name": "get_beta_accumulators",
    "func arg": "(opt, dtype)",
    "comments": ""
}{}{
    "source file": "add_loss_correctness_test.py",
    "line number": "44",
    "func name": "get_ctl_train_step",
    "func arg": "(model)",
    "comments": ""
}{}{
    "source file": "add_n.py",
    "line number": "27",
    "func name": "make_add_n_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests for AddN op.\n\n\n"
}{
    "source file": "adjoint_registrations.py",
    "line number": "133",
    "func name": "_adjoint_householder",
    "func arg": "(householder_operator)",
    "comments": ""
}{}{}{
    "source file": "ag_ctx.py",
    "line number": "76",
    "func name": "_default_control_status_ctx",
    "func arg": "()",
    "comments": ""
}{
    "source file": "ag_logging.py",
    "line number": "145",
    "func name": "warn",
    "func arg": "(msg, **kwargs)",
    "comments": ""
}{}{}{}{
    "source file": "all_reduce.py",
    "line number": "857",
    "func name": "build_shuffle_then_shuffle",
    "func arg": "(input_tensors, first_gather_devices, second_gather_devices, red_op, un_op)",
    "comments": "Construct hybrid of Shuffle within workers, Shuffle across workers.\n\n\n"
}{}{
    "source file": "all_renames_v2.py",
    "line number": "560",
    "func name": "add_contrib_direct_import_support",
    "func arg": "(symbol_dict)",
    "comments": "Add support for `tf.contrib.*` alias `contrib_*.` Updates dict in place.\n\n\n"
}{
    "source file": "all_util.py",
    "line number": "86",
    "func name": "remove_undocumented",
    "func arg": "(module_name, allowed_exception_list, doc_string_modules)",
    "comments": "Removes symbols in a module that are not referenced by a docstring.\n\n\n##### Args\n* **module_name**: the name of the module (usually `__name__`).\n\n* **allowed_exception_list**: a list of names that should not be removed.\n\n* **doc_string_modules**: a list of modules from which to take the docstrings.\n\n##### Returns\n"
}{}{
    "source file": "analytics.py",
    "line number": "21",
    "func name": "track_usage",
    "func arg": "(tool_id, tags)",
    "comments": "No usage tracking for external library.\n\n\n##### Args\n* **tool_id**: A string identifier for tool to be tracked.\n\n* **tags**: list of string tags that will be added to the tracking.\n\n"
}{
    "source file": "analyzer_cli_test.py",
    "line number": "514",
    "func name": "create_analyzer_cli",
    "func arg": "(dump)",
    "comments": "Create an analyzer CLI.\n\n\n##### Args\n* **dump**: A `DebugDumpDir` object to base the analyzer CLI on.\n\n##### Returns\n"
}{
    "source file": "analyzer_cli.py",
    "line number": "1582",
    "func name": "create_analyzer_ui",
    "func arg": "(debug_dump, tensor_filters, ui_type, on_ui_exit, config)",
    "comments": "Create an instance of CursesUI based on a DebugDumpDir object.\n\n\n##### Args\n* **debug_dump**: (debug_data.DebugDumpDir) The debug dump to use.\n\n* **tensor_filters**: (dict) A dict mapping tensor filter name (str) to tensor\n  filter (Callable).\n\n* **ui_type**: (str) requested UI type, e.g., \"curses\", \"readline\".\n\n* **on_ui_exit**: (`Callable`) the callback to be called when the UI exits.\n\n* **config**: A `cli_config.CLIConfig` object.\n\n##### Returns\n"
}{
    "source file": "anf_test.py",
    "line number": "40",
    "func name": "exec_expected_result",
    "func arg": "()",
    "comments": ""
}{
    "source file": "anf.py",
    "line number": "527",
    "func name": "transform",
    "func arg": "(node, ctx, config)",
    "comments": "Converts the given node to A-normal form (ANF).\n\nThe general idea of A-normal form: https://en.wikipedia.org/wiki/A-normal_form\n\nThe specific converters used here are based on Python AST semantics as documented at https://greentreesnakes.readthedocs.io/en/latest/.\n\nWhat exactly should be considered A-normal form for any given programming language is not completely obvious.\n\nThe transformation defined here is therefore configurable as to which syntax to replace with a fresh variable and which to leave be.\n\nThe configuration is intentionally flexible enough to define very precise variable insertion transformations, should that be desired.\n\nThe configuration is a list of syntax rules, each of which is a 2-tuple:\n\n- An `ASTEdgePattern` (which see) defining a type of AST edge, and\n\n- Whether to transform children of such edges. The special object `anf.ANY` may be used as a pattern that matches all edges.\n\nEach replacement directive is one of three possible things:\n\n- The object `anf.REPLACE`, meaning \"Replace this child node with a variable\",\n\n- The object `anf.LEAVE`, meaning \"Do not replace this child node with a variable\", or\n\n- A Python callable.\n\nIf a callable, it is called with the parent node, the field name, and the child node, and must compute a boolean indicating whether to transform the child node or not.\n\nThe callable is free to use whatever context information it chooses.\n\nThe callable may be invoked more than once on the same link, and must produce the same answer each time.\n\nThe syntax rules are tested in order, and the first match governs.\n\nIf no rule matches, the node is not transformed.\n\nThe above rules notwithstanding,\n\n- Variable references are never replaced with (fresh) variables, as that would accomplish nothing.\n\n- The left-hand children of Assign and AugAssign nodes, and the children of Del nodes, are never replaced with variables, as that would break their semantics.\n\n- The right-hand children of Assign nodes are never replaced with variables, as the original assignment would still have to be present in the result to define the new variable.\n\n(That is, there's no point in transforming `x = sin(y)` into `tmp = sin(y); x = tmp`.)\n\n- The right-hand children of AugAssign nodes are never replaced with variables either, but only because the difference from Assign was considered a potential source of confusion (and it would have been slightly awkward in the code to treat the RHS differently than the LHS).\n\n- Various special-purpose AST nodes are not exposed to the configuration, lest the transform produce invalid syntax like, e.g., `tmp = +; x = 1 tmp 2`.\n\nFor example, the configuration ```python [(anf.ASTEdgePattern(anf.ANY, anf.ANY, gast.expr), anf.REPLACE)] ``` gives explicit fresh names to all expressions regardless of context (except as outlined above), whereas ```python [(anf.ASTEdgePattern(gast.If, \"test\", anf.ANY), anf.REPLACE)] ``` only transforms the conditionals of `if` statements (but not, e.g., `while`).\n\nIf no configuration is supplied, the default behavior is to transform all expressions except literal constants, which is defined as a configuration as ```python # For Python 3, and gast library versions before 0.3 literals = (gast.Num, gast.Str, gast.Bytes, gast.NameConstant) [(anf.ASTEdgePattern(anf.ANY, anf.ANY, literals), anf.LEAVE), (anf.ASTEdgePattern(anf.ANY, anf.ANY, gast.expr), anf.REPLACE)] ```\n##### Args\n* **node**: The node to transform.\n\n* **ctx**: transformer.EntityInfo.  TODO(mdan)\n\n* **config**: Optional ANF configuration.  If omitted, ANF replaces all expression\n  expect literal constants.\n\n"
}{}{
    "source file": "anno.py",
    "line number": "153",
    "func name": "dup",
    "func arg": "(node, copy_map, field_name)",
    "comments": "Recursively copies annotations in an AST tree.\n\n\n##### Args\n* **node**: ast.AST\n\n* **copy_map**: Dict[Hashable, Hashable], maps a source anno key to a destination\n    key. All annotations with the source key will be copied to identical\n    annotations with the destination key.\n\n* **field_name**: str\n\n"
}{}{
    "source file": "api_compatibility_test.py",
    "line number": "179",
    "func name": "_FilterGoldenProtoDict",
    "func arg": "(golden_proto_dict, omit_golden_symbols_map)",
    "comments": "Filter out golden proto dict symbols that should be omitted.\n\n\n"
}{
    "source file": "api_template_v1.__init__.py",
    "line number": "142",
    "func name": "_running_from_pip_package",
    "func arg": "()",
    "comments": ""
}{
    "source file": "api_template.__init__.py",
    "line number": "132",
    "func name": "_running_from_pip_package",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "api.py",
    "line number": "879",
    "func name": "to_code",
    "func arg": "(entity, recursive, experimental_optional_features)",
    "comments": "Returns the source code generated by AutoGraph, as a string.\n\nExample usage:\n\n>>> def f(x): ...\n\n if x < 0: ...\n\n\n\n x = -x ...\n\n return x >>> tf.autograph.to_code(f) \"...def tf__f(x):...\"\n\nAlso see: `tf.autograph.to_graph`.\n\nNote: If a function has been decorated with `tf.function`, pass its underlying Python function, rather than the callable that `tf.function creates:\n\n>>> @tf.function ... def f(x): ...\n\n if x < 0: ...\n\n\n\n x = -x ...\n\n return x >>> tf.autograph.to_code(f.python_function) \"...def tf__f(x):...\"\n##### Args\n* **entity**: Python callable or class to convert.\n\n* **recursive**: Whether to recursively convert any functions that the converted\n  function may call.\n\n* **experimental_optional_features**: `None`, a tuple of, or a single\n  `tf.autograph.experimental.Feature` value.\n\n##### Returns\n"
}{}{
    "source file": "app_test.py",
    "line number": "29",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{
    "source file": "app.py",
    "line number": "35",
    "func name": "run",
    "func arg": "(main, argv)",
    "comments": "Runs the program with an optional 'main' function and 'argv' list.\n\n\n"
}{
    "source file": "applications_load_weight_test.py",
    "line number": "78",
    "func name": "_get_elephant",
    "func arg": "(target_size)",
    "comments": ""
}{
    "source file": "applications_test.py",
    "line number": "135",
    "func name": "_get_output_shape",
    "func arg": "(model_fn)",
    "comments": ""
}{
    "source file": "arg_min_max.py",
    "line number": "29",
    "func name": "make_arg_min_max_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do arg_max.\n\n\n"
}{}{}{}{
    "source file": "array_grad.py",
    "line number": "1135",
    "func name": "_BroadcastToGrad",
    "func arg": "(op, grad)",
    "comments": ""
}{}{}{
    "source file": "array_ops.py",
    "line number": "5815",
    "func name": "repeat",
    "func arg": "(input, repeats, axis, name)",
    "comments": "Repeat elements of `input`.\n\nSee also `tf.concat`, `tf.stack`, `tf.tile`.\n##### Args\n* **input**: An `N`-dimensional Tensor.\n\n* **repeats**: An 1-D `int` Tensor. The number of repetitions for each element.\n  repeats is broadcasted to fit the shape of the given axis. `len(repeats)`\n  must equal `input.shape[axis]` if axis is not None.\n\n* **axis**: An int. The axis along which to repeat values. By default (axis=None),\n  use the flattened input array, and return a flat output array.\n\n* **name**: A name for the operation.\n\n##### Returns\n* **ample usage**: \n\n* **f.Tensor**: shape=(8,), dtype=int32,\n\n"
}{}{}{}{
    "source file": "assembler.py",
    "line number": "473",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{}{}{}{}{
    "source file": "asserts.py",
    "line number": "50",
    "func name": "transform",
    "func arg": "(node, ctx)",
    "comments": ""
}{}{
    "source file": "ast_edits.py",
    "line number": "175",
    "func name": "excluded_from_module_rename",
    "func arg": "(module, import_rename_spec)",
    "comments": "Check if this module import should not be renamed.\n\n\n##### Args\n* **module**: (string) module name.\n\n* **import_rename_spec**: ImportRename instance.\n\n##### Returns\n"
}{}{
    "source file": "ast_util.py",
    "line number": "283",
    "func name": "parallel_walk",
    "func arg": "(node, other)",
    "comments": "Walks two ASTs in parallel.\n\nThe two trees must have identical structure.\n##### Args\n* **node**: Union[ast.AST, Iterable[ast.AST]]\n\n* **other**: Union[ast.AST, Iterable[ast.AST]]\n\n* **elds**: \n\n"
}{
    "source file": "async_checkpoint_test.py",
    "line number": "65",
    "func name": "model_fn",
    "func arg": "(features, labels, mode, params)",
    "comments": ""
}{}{
    "source file": "atrous_conv2d_test.py",
    "line number": "34",
    "func name": "_upsample_filters",
    "func arg": "(filters, rate)",
    "comments": "Upsamples the filters by a factor of rate along the spatial dimensions.\n\n\n##### Args\n* **filters**: [h, w, in_depth, out_depth]. Original filters.\n\n* **rate**: An int, specifying the upsampling rate.\n\n##### Returns\n* **filters_up**: [h_up, w_up, in_depth, out_depth]. Upsampled filters with\n  h_up = h + (h - 1) * (rate - 1)\n  w_up = w + (w - 1) * (rate - 1)\n  containing (rate - 1) zeros between consecutive filter values along\n  the filters' spatial dimensions.\n\n"
}{
    "source file": "atrous_convolution_test.py",
    "line number": "36",
    "func name": "upsample_filters",
    "func arg": "(filters, rate)",
    "comments": "Upsamples the filters by a factor of rate along the spatial dimensions.\n\n\n##### Args\n* **filters**: spatial_shape + [in_channels, out_channels]\n  Original filters.\n\n* **rate**: A list of len(spatial_shape) positive ints, specifying the\n  upsampling rate.\n\n##### Returns\n* **filters_up**: output_spatial_shape + [in_channels, out_channels].\n  Upsampled filters with\n  output_spatial_shape[i] = (spatial_shape[i] - 1) * rate[i] + 1\n  containing (rate[i] - 1) zeros between consecutive filter values along\n  spatial dimension i.\n\n"
}{}{}{
    "source file": "audio_microfrontend_op.py",
    "line number": "34",
    "func name": "audio_microfrontend",
    "func arg": "(audio, sample_rate, window_size, window_step, num_channels, upper_band_limit, lower_band_limit, smoothing_bits, even_smoothing, odd_smoothing, min_signal_remaining, enable_pcan, pcan_strength, pcan_offset, gain_bits, enable_log, scale_shift, left_context, right_context, frame_stride, zero_padding, out_scale, out_type)",
    "comments": "Audio Microfrontend Op.\n\nThis Op converts a sequence of audio data into one or more feature vectors containing filterbanks of the input. The conversion process uses a lightweight library to perform:\n\n1. A slicing window function 2. Short-time FFTs 3. Filterbank calculations 4. Noise reduction 5. PCAN Auto Gain Control 6. Logarithmic scaling\n##### Args\n* **audio**: 1D Tensor, int16 audio data in temporal ordering.\n\n* **sample_rate**: Integer, the sample rate of the audio in Hz.\n\n* **window_size**: Integer, length of desired time frames in ms.\n\n* **window_step**: Integer, length of step size for the next frame in ms.\n\n* **num_channels**: Integer, the number of filterbank channels to use.\n\n* **upper_band_limit**: Float, the highest frequency included in the filterbanks.\n\n* **lower_band_limit**: Float, the lowest frequency included in the filterbanks.\n\n* **smoothing_bits**: Int, scale up signal by 2^(smoothing_bits) before reduction.\n\n* **even_smoothing**: Float, smoothing coefficient for even-numbered channels.\n\n* **odd_smoothing**: Float, smoothing coefficient for odd-numbered channels.\n\n* **min_signal_remaining**: Float, fraction of signal to preserve in smoothing.\n\n* **enable_pcan**: Bool, enable PCAN auto gain control.\n\n* **pcan_strength**: Float, gain normalization exponent.\n\n* **pcan_offset**: Float, positive value added in the normalization denominator.\n\n* **gain_bits**: Int, number of fractional bits in the gain.\n\n* **enable_log**: Bool, enable logarithmic scaling of filterbanks.\n\n* **scale_shift**: Integer, scale filterbanks by 2^(scale_shift).\n\n* **left_context**: Integer, number of preceding frames to attach to each frame.\n\n* **right_context**: Integer, number of preceding frames to attach to each frame.\n\n* **frame_stride**: Integer, M frames to skip over, where output[n] = frame[n*M].\n\n* **zero_padding**: Bool, if left/right context is out-of-bounds, attach frame of\n  zeroes. Otherwise, frame[0] or frame[size-1] will be copied.\n\n* **out_scale**: Integer, divide all filterbanks by this number.\n\n* **out_type**: DType, type of the output Tensor, defaults to UINT16.\n\n##### Returns\n* **filterbanks**: 2D Tensor, each row is a time frame, each column is a channel.\n\n"
}{}{
    "source file": "auto_control_deps_utils.py",
    "line number": "155",
    "func name": "_input_index",
    "func arg": "(op, handle)",
    "comments": "Returns the index of `handle` in `op.inputs`.\n\n\n##### Args\n* **op**: Operation.\n\n* **handle**: Resource handle.\n\n##### Returns\n"
}{
    "source file": "auto_control_deps.py",
    "line number": "549",
    "func name": "automatic_control_dependencies",
    "func arg": "(f)",
    "comments": "Wraps f to automatically insert control dependencies.\n\nThe inserted dependencies ensure that: 1. All stateful ops in f run when the result of f runs 2. Updates to the same resources happen in order.\n##### Args\n* **f**: the function to be wrapped.\n\n##### Returns\n"
}{
    "source file": "auto_mixed_precision_test.py",
    "line number": "318",
    "func name": "_example_noninlined_funcdef",
    "func arg": "(features)",
    "comments": "Computes the Swish activation function: `x * sigmoid(x)`.\n\n\n"
}{}{
    "source file": "auto_shard_dataset_test.py",
    "line number": "39",
    "func name": "chunk",
    "func arg": "(l, n)",
    "comments": ""
}{
    "source file": "autocast_variable_test.py",
    "line number": "51",
    "func name": "get_var",
    "func arg": "(val, dtype, name)",
    "comments": ""
}{
    "source file": "autocast_variable.py",
    "line number": "474",
    "func name": "_maybe_wrap",
    "func arg": "(variable, wrap)",
    "comments": "Creates an AutoCastVariable that wraps another variable if applicable.\n\nThis function is used to wrap the return value of AutoCastVariable.assign. Unfortunately MirroredVariable.assign will (incorrectly) return a Mirrored value instead of a MirroredVariable. So we cannot properly wrap it in an AutoCastVariable. We return the original variable in that case.\n##### Args\n* **variable**: A tf.Variable or op.\n\n* **wrap**: A boolean to define whether to wrap the variable in an\n  AutoCastVariable or not.\n\n##### Returns\n"
}{
    "source file": "automatic_outside_compilation_test.py",
    "line number": "133",
    "func name": "mnist_model",
    "func arg": "(input_shape)",
    "comments": "Creates a MNIST model.\n\n\n"
}{}{}{
    "source file": "backend_config.py",
    "line number": "130",
    "func name": "set_image_data_format",
    "func arg": "(data_format)",
    "comments": "Sets the value of the image data format convention.\n\nArguments: data_format: string. `'channels_first'` or `'channels_last'`.\n\nExample: >>> tf.keras.backend.image_data_format() 'channels_last' >>> tf.keras.backend.set_image_data_format('channels_first') >>> tf.keras.backend.image_data_format() 'channels_first' >>> tf.keras.backend.set_image_data_format('channels_last')\n"
}{
    "source file": "backend_test.py",
    "line number": "74",
    "func name": "compare_two_inputs_op_to_numpy",
    "func arg": "(keras_op, np_op, input_shape_a, input_shape_b, dtype, keras_args, keras_kwargs, np_args, np_kwargs)",
    "comments": ""
}{
    "source file": "backend.py",
    "line number": "6275",
    "func name": "maybe_convert_to_ragged",
    "func arg": "(is_ragged_input, output, nested_row_lengths)",
    "comments": "Converts any ragged input back to its initial structure.\n\n\n"
}{}{
    "source file": "backprop_util.py",
    "line number": "25",
    "func name": "IsTrainable",
    "func arg": "(tensor_or_dtype)",
    "comments": ""
}{
    "source file": "backprop.py",
    "line number": "726",
    "func name": "_handle_or_self",
    "func arg": "(x)",
    "comments": "Unwrap resource variable/ndarray to return tensors.\n\n\n"
}{}{}{
    "source file": "base_dir.py",
    "line number": "29",
    "func name": "get_base_dirs_and_prefixes",
    "func arg": "(code_url_prefix)",
    "comments": "Returns the base_dirs and code_prefixes for OSS TensorFlow api gen.\n\n\n"
}{}{}{}{
    "source file": "base_layer_utils.py",
    "line number": "833",
    "func name": "no_ragged_support",
    "func arg": "(inputs, layer_name)",
    "comments": ""
}{}{
    "source file": "base_layer.py",
    "line number": "3232",
    "func name": "_convert_numpy_or_python_types",
    "func arg": "(x)",
    "comments": ""
}{
    "source file": "base_preprocessing_layer_test.py",
    "line number": "125",
    "func name": "get_layer",
    "func arg": "(**kwargs)",
    "comments": ""
}{}{
    "source file": "base_preprocessing_layer.py",
    "line number": "240",
    "func name": "convert_to_list",
    "func arg": "(values, sparse_default_value)",
    "comments": "Convert a TensorLike, CompositeTensor, or ndarray into a Python list.\n\n\n"
}{}{}{}{}{}{
    "source file": "base.py",
    "line number": "581",
    "func name": "_add_elements_to_collection",
    "func arg": "(elements, collection_list)",
    "comments": ""
}{
    "source file": "base1.py",
    "line number": "501",
    "func name": "no_automatic_dependency_tracking_scope",
    "func arg": "(obj)",
    "comments": "A context that disables automatic dependency tracking when assigning attrs.\n\nObjects that inherit from Autotrackable automatically creates dependencies to trackable objects through attribute assignments, and wraps data structures (lists or dicts) with trackable classes. This scope may be used to temporarily disable this behavior. This works similar to the decorator `no_automatic_dependency_tracking`.\n\nExample usage: ``` model = tf.keras.Model() model.arr1 = []\n\n# Creates a ListWrapper object with no_automatic_dependency_tracking_scope(model): model.arr2 = []\n\n# Creates a regular, untracked python list ```\n##### Args\n* **obj**: A trackable object.\n\n* **elds**: \n\n"
}{}{}{}{
    "source file": "basic_definitions.py",
    "line number": "67",
    "func name": "decorated_function",
    "func arg": "(x)",
    "comments": ""
}{}{
    "source file": "basic_loops_test.py",
    "line number": "32",
    "func name": "_test_dir",
    "func arg": "(test_name)",
    "comments": ""
}{
    "source file": "basic_loops.py",
    "line number": "25",
    "func name": "basic_train_loop",
    "func arg": "(supervisor, train_step_fn, args, kwargs, master)",
    "comments": "Basic loop to train a model.\n\nCalls `train_step_fn` in a loop to train a model.\n\nThe function is called as:\n\n```python train_step_fn(session, *args, **kwargs) ```\n\nIt is passed a `tf.compat.v1.Session` in addition to `args` and `kwargs`.\n\nThe function typically runs one training step in the session.\n##### Args\n* **supervisor**: `tf.compat.v1.train.Supervisor` to run the training services.\n\n* **train_step_fn**: Callable to execute one training step.  Called repeatedly as\n  `train_step_fn(session, *args **kwargs)`.\n\n* **args**: Optional positional arguments passed to `train_step_fn`.\n\n* **kwargs**: Optional keyword arguments passed to `train_step_fn`.\n\n* **master**: Master to use to create the training session.  Defaults to `\"\"`\n  which causes the session to be created in the local process.\n\n"
}{}{
    "source file": "basic_session_run_hooks.py",
    "line number": "1083",
    "func name": "_as_graph_element",
    "func arg": "(obj)",
    "comments": "Retrieves Graph element.\n\n\n"
}{
    "source file": "basic_v1.py",
    "line number": "49",
    "func name": "Test",
    "func arg": "()",
    "comments": ""
}{}{}{}{}{
    "source file": "batch_matmul_op_test.py",
    "line number": "195",
    "func name": "_GetBatchMatmulGradientWithBroadcastingTest",
    "func arg": "(dtype, adjoint_a, adjoint_b)",
    "comments": ""
}{}{
    "source file": "batch_norm_benchmark.py",
    "line number": "117",
    "func name": "print_difference",
    "func arg": "(mode, t1, t2)",
    "comments": "Print the difference in timing between two runs.\n\n\n"
}{
    "source file": "batch_ops_test.py",
    "line number": "37",
    "func name": "delayed_plus1",
    "func arg": "(x)",
    "comments": "Sleeps for 100ms then returns x+1.\n\n\n"
}{
    "source file": "batch_ops.py",
    "line number": "32",
    "func name": "batch_function",
    "func arg": "(num_batch_threads, max_batch_size, batch_timeout_micros, allowed_batch_sizes, max_enqueued_batches, autograph)",
    "comments": "Batches the computation done by the decorated function.\n\nSo, for example, in the following code\n\n```python @batch_function(1, 2, 3) def layer(a): return tf.matmul(a, a)\n\nb = layer(w) ```\n\nif more than one session.run call is simultaneously trying to compute `b` the values of `w` will be gathered, non-deterministically concatenated along the first axis, and only one thread will run the computation. See the documentation of the `Batch` op for more details.\n\nAssumes that all arguments of the decorated function are Tensors which will be batched along their first dimension.\n\nSparseTensor is not supported. The return value of the decorated function must be a Tensor or a list/tuple of Tensors.\n##### Args\n* **num_batch_threads**: Number of scheduling threads for processing batches\n of work. Determines the number of batches processed in parallel.\n\n* **max_batch_size**: Batch sizes will never be bigger than this.\n\n* **batch_timeout_micros**: Maximum number of microseconds to wait before\n outputting an incomplete batch.\n\n* **allowed_batch_sizes**: Optional list of allowed batch sizes. If left empty,\n does nothing. Otherwise, supplies a list of batch sizes, causing the op\n to pad batches up to one of those sizes. The entries must increase\n monotonically, and the final entry must equal max_batch_size.\n\n* **max_enqueued_batches**: The maximum depth of the batch queue. Defaults to 10.\n\n* **autograph**: Whether to use autograph to compile python and eager style code\n for efficient graph-mode execution.\n\n##### Returns\n"
}{
    "source file": "batch_scatter_ops_test.py",
    "line number": "36",
    "func name": "_NumpyUpdate",
    "func arg": "(ref, indices, updates)",
    "comments": ""
}{}{
    "source file": "batch_to_space_nd.py",
    "line number": "28",
    "func name": "make_batch_to_space_nd_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do batch_to_space_nd.\n\n\n"
}{
    "source file": "batching.py",
    "line number": "269",
    "func name": "unbatch",
    "func arg": "()",
    "comments": "Splits elements of a dataset into multiple elements on the batch dimension.\n\nFor example, if elements of the dataset are shaped `[B, a0, a1, ...]`, where `B` may vary for each input element, then for each element in the dataset, the unbatched dataset will contain `B` consecutive elements of shape `[a0, a1, ...]`.\n\n```python # NOTE: The following example uses `{ ... }` to represent the contents # of a dataset. a = { ['a', 'b', 'c'], ['a', 'b'], ['a', 'b', 'c', 'd'] }\n\na.unbatch() == { 'a', 'b', 'c', 'a', 'b', 'a', 'b', 'c', 'd'} ```\n##### Returns\n"
}{}{}{}{}{}{
    "source file": "benchmark_util.py",
    "line number": "47",
    "func name": "measure_performance",
    "func arg": "(model_fn, x, y, epoch, batch_size, run_iters, optimizer, loss, metrics, verbose, num_gpus, distribution_strategy)",
    "comments": "Run models and measure the performance.\n\nArguments: model_fn: Model function to be benchmarked. x: Input data. See `x` in the `fit()` method of `keras.Model`. y: Target data. See `y` in the `fit()` method of `keras.Model`. epoch: Integer. Number of epochs to train the model. If unspecified, `epoch` will default to 2. batch_size: Integer. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. run_iters: Integer. Number of iterations to run the performance measurement. If unspecified, `run_iters` will default to 4. optimizer: String (name of optimizer) or optimizer instance. See `tf.keras.optimizers`. loss: String (name of objective function), objective function or `tf.keras.losses.Loss` instance. See `tf.keras.losses`. metrics: Lists of metrics to be evaluated by the model during training. See `metrics` in the `compile()` method of\n\n`keras.Model`. verbose: 0, 1, 2. Verbosity mode. See `verbose` in the `fit()` method of `keras.Model`. If unspecified, `verbose` will default to 0. num_gpus: Number of GPUs to run the model. distribution_strategy: Distribution strategies. It could be `multi_worker_mirrored`, `one_device`, `mirrored`. If unspecified, `distribution_strategy` will default to 'off'. Note that, `TPU` and `parameter_server` are not supported yet.\n##### Returns\n* **ise**: \n\n* **ValueError**: If `x` is none or if `optimizer` is not provided or\n\n"
}{
    "source file": "benchmark.py",
    "line number": "459",
    "func name": "benchmarks_main",
    "func arg": "(true_main, argv)",
    "comments": "Run benchmarks as declared in argv.\n\n\n##### Args\n* **true_main**: True main function to run if benchmarks are not requested.\n\n* **argv**: the command line arguments (if None, uses sys.argv).\n\n"
}{}{
    "source file": "benchmarks_test.py",
    "line number": "95",
    "func name": "run_benchmark",
    "func arg": "(func, num_iters, execution_mode)",
    "comments": ""
}{
    "source file": "benchmarks_test1.py",
    "line number": "69",
    "func name": "_save_checkpoint",
    "func arg": "()",
    "comments": ""
}{
    "source file": "bernoulli_test.py",
    "line number": "54",
    "func name": "entropy",
    "func arg": "(p)",
    "comments": ""
}{
    "source file": "bernoulli.py",
    "line number": "170",
    "func name": "_kl_bernoulli_bernoulli",
    "func arg": "(a, b, name)",
    "comments": "Calculate the batched KL divergence KL(a || b) with a and b Bernoulli.\n\n\n##### Args\n* **a**: instance of a Bernoulli distribution object.\n\n* **b**: instance of a Bernoulli distribution object.\n\n* **name**: (optional) Name to use for created operations.\n  default is \"kl_bernoulli_bernoulli\".\n\n##### Returns\n"
}{
    "source file": "beta_test.py",
    "line number": "36",
    "func name": "try_import",
    "func arg": "(name)",
    "comments": ""
}{
    "source file": "beta.py",
    "line number": "383",
    "func name": "_kl_beta_beta",
    "func arg": "(d1, d2, name)",
    "comments": "Calculate the batchwise KL divergence KL(d1 || d2) with d1 and d2 Beta.\n\n\n##### Args\n* **d1**: instance of a Beta distribution object.\n\n* **d2**: instance of a Beta distribution object.\n\n* **name**: (optional) Name to use for created operations.\n  default is \"kl_beta_beta\".\n\n##### Returns\n"
}{}{
    "source file": "bfloat16_test.py",
    "line number": "35",
    "func name": "numpy_assert_allclose",
    "func arg": "(a, b, **kwargs)",
    "comments": ""
}{
    "source file": "bfloat16_test1.py",
    "line number": "35",
    "func name": "float_values",
    "func arg": "()",
    "comments": "Returns values that should round trip exactly to float and back.\n\n\n"
}{}{
    "source file": "bfloat16.py",
    "line number": "73",
    "func name": "bfloat16_scope",
    "func arg": "(name)",
    "comments": "Scope class for bfloat16 variables so that the model uses custom getter.\n\nThis enables variables to be read as bfloat16 type when using get_variable.\n"
}{}{}{}{}{}{}{}{
    "source file": "bijector_test_util.py",
    "line number": "163",
    "func name": "assert_bijective_and_finite",
    "func arg": "(bijector, x, y, event_ndims, atol, rtol, sess)",
    "comments": "Assert that forward/inverse (along with jacobians) are inverses and finite.\n\nIt is recommended to use x and y values that are very very close to the edge of the Bijector's domain.\n##### Args\n* **bijector**: A Bijector instance.\n\n* **x**: np.array of values in the domain of bijector.forward.\n\n* **y**: np.array of values in the domain of bijector.inverse.\n\n* **event_ndims**: Integer describing the number of event dimensions this bijector\n  operates on.\n\n* **atol**: Absolute tolerance.\n\n* **rtol**: Relative tolerance.\n\n* **sess**: TensorFlow session.  Defaults to the default session.\n\n"
}{}{}{
    "source file": "binary_op.py",
    "line number": "307",
    "func name": "make_squared_difference_tests",
    "func arg": "(options)",
    "comments": ""
}{}{}{}{}{
    "source file": "bincount_ops.py",
    "line number": "502",
    "func name": "validate_ragged_weights",
    "func arg": "(values, weights, dtype)",
    "comments": "Validates the passed weight tensor or creates an empty one.\n\n\n"
}{}{}{}{}{}{}{
    "source file": "boston_housing.py",
    "line number": "28",
    "func name": "load_data",
    "func arg": "(path, test_split, seed)",
    "comments": "Loads the Boston Housing dataset.\n\nThis is a dataset taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nSamples contain 13 attributes of houses at different locations around the Boston suburbs in the late 1970s. Targets are the median values of the houses at a location (in k$).\n\nThe attributes themselves are defined in the [StatLib website](http://lib.stat.cmu.edu/datasets/boston).\n\nArguments: path: path where to cache the dataset locally (relative to `~/.keras/datasets`). test_split: fraction of the data to reserve as test set. seed: Random seed for shuffling the data before computing the test split.\n##### Returns\n* **Tuple of Numpy arrays**: `(x_train, y_train), (x_test, y_test)`.\n\n* ****x_train, x_test****: numpy arrays with shape `(num_samples, 13)`\n  containing either the training samples (for x_train),\n  or test samples (for y_train).\n\n* ****y_train, y_test****: numpy arrays of shape `(num_samples,)` containing the\n  target scalars. The targets are float scalars typically between 10 and\n  50 that represent the home prices in k$.\n\n"
}{}{
    "source file": "break_statements.py",
    "line number": "183",
    "func name": "transform",
    "func arg": "(node, ctx)",
    "comments": ""
}{}{
    "source file": "bucket_by_sequence_length_test.py",
    "line number": "66",
    "func name": "_get_record_shape",
    "func arg": "(sparse)",
    "comments": ""
}{}{}{}{
    "source file": "build_java_api_docs.py",
    "line number": "53",
    "func name": "main",
    "func arg": "(unused_argv)",
    "comments": ""
}{
    "source file": "build_java_api_docs1.py",
    "line number": "60",
    "func name": "main",
    "func arg": "(unused_argv)",
    "comments": ""
}{
    "source file": "build_py_api_docs.py",
    "line number": "55",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "builder_impl.py",
    "line number": "795",
    "func name": "_add_op_to_signature_def_map",
    "func arg": "(signature_def_map, op, key)",
    "comments": ""
}{}{}{
    "source file": "c_api_util.py",
    "line number": "238",
    "func name": "new_tf_operations",
    "func arg": "(graph)",
    "comments": "Generator that yields newly-added TF_Operations in `graph`.\n\nSpecifically, yields TF_Operations that don't have associated Operations in `graph`. This is useful for processing nodes added by the C API.\n##### Args\n* **graph**: Graph\n\n* **elds**: \n\n"
}{}{}{}{}{}{}{}{}{
    "source file": "call_trees.py",
    "line number": "211",
    "func name": "transform",
    "func arg": "(node, ctx)",
    "comments": "Transform function call to the compiled counterparts.\n\n\n##### Args\n* **node**: AST\n\n* **ctx**: EntityContext\n\n##### Returns\n* **A tuple (node, new_names)**: node\n\n"
}{
    "source file": "callbacks_test.py",
    "line number": "1730",
    "func name": "list_summaries",
    "func arg": "(logdir)",
    "comments": "Read all summaries under the logdir into a `_SummaryFile`.\n\n\n##### Args\n* **logdir**: A path to a directory that contains zero or more event\n  files, either as direct children or in transitive subdirectories.\n  Summaries in these events must only contain old-style scalars,\n  images, and histograms. Non-summary events, like `graph_def`s, are\n  ignored.\n\n##### Returns\n"
}{}{}{
    "source file": "callbacks.py",
    "line number": "187",
    "func name": "make_logs",
    "func arg": "(model, logs, outputs, mode, prefix)",
    "comments": "Computes logs for sending to `on_batch_end` methods.\n\n\n"
}{}{}{}{
    "source file": "candidate_sampling_ops.py",
    "line number": "350",
    "func name": "compute_accidental_hits",
    "func arg": "(true_classes, sampled_candidates, num_true, seed, name)",
    "comments": "Compute the position ids in `sampled_candidates` matching `true_classes`.\n\nIn Candidate Sampling, this operation facilitates virtually removing sampled classes which happen to match target classes.\n\nThis is done in Sampled Softmax and Sampled Logistic.\n\nSee our [Candidate Sampling Algorithms Reference](http://www.tensorflow.org/extras/candidate_sampling.pdf).\n\nWe presuppose that the `sampled_candidates` are unique.\n\nWe call it an 'accidental hit' when one of the target classes matches one of the sampled classes.\n\nThis operation reports accidental hits as triples `(index, id, weight)`, where `index` represents the row number in `true_classes`, `id` represents the position in `sampled_candidates`, and weight is `-FLOAT_MAX`.\n\nThe result of this op should be passed through a `sparse_to_dense` operation, then added to the logits of the sampled classes. This removes the contradictory effect of accidentally sampling the true target classes as noise classes for the same example.\n##### Args\n* **true_classes**: A `Tensor` of type `int64` and shape `[batch_size,\n  num_true]`. The target classes.\n\n* **sampled_candidates**: A tensor of type `int64` and shape `[num_sampled]`.\n  The sampled_candidates output of CandidateSampler.\n\n* **num_true**: An `int`.  The number of target classes per training example.\n\n* **seed**: An `int`. An operation-specific seed. Default is 0.\n\n* **name**: A name for the operation (optional).\n\n##### Returns\n* **indices**: A `Tensor` of type `int32` and shape `[num_accidental_hits]`.\n  Values indicate rows in `true_classes`.\n\n* **ids**: A `Tensor` of type `int64` and shape `[num_accidental_hits]`.\n  Values indicate positions in `sampled_candidates`.\n\n* **weights**: A `Tensor` of type `float` and shape `[num_accidental_hits]`.\n  Each value is `-FLOAT_MAX`.\n\n"
}{
    "source file": "capture_tpu_profile.py",
    "line number": "139",
    "func name": "main",
    "func arg": "(unused_argv)",
    "comments": ""
}{
    "source file": "captured_data_to_wav.py",
    "line number": "28",
    "func name": "new_data_to_array",
    "func arg": "(fn)",
    "comments": ""
}{
    "source file": "cardinality_test.py",
    "line number": "31",
    "func name": "_test_combinations",
    "func arg": "()",
    "comments": ""
}{
    "source file": "cardinality.py",
    "line number": "72",
    "func name": "assert_cardinality",
    "func arg": "(expected_cardinality)",
    "comments": "Asserts the cardinality of the input dataset.\n\nNOTE: The following assumes that \"examples.tfrecord\" contains 42 records.\n\n>>> dataset = tf.data.TFRecordDataset(\"examples.tfrecord\") >>> cardinality = tf.data.experimental.cardinality(dataset) >>> print((cardinality == tf.data.experimental.UNKNOWN_CARDINALITY).numpy()) True >>> dataset = dataset.apply(tf.data.experimental.assert_cardinality(42)) >>> print(tf.data.experimental.cardinality(dataset).numpy()) 42\n##### Args\n* **expected_cardinality**: The expected cardinality of the input dataset.\n\n##### Returns\n"
}{}{}{}{
    "source file": "cast.py",
    "line number": "27",
    "func name": "make_cast_tests",
    "func arg": "(options)",
    "comments": "Generate examples for cast.\n\n\n"
}{}{
    "source file": "categorical_test.py",
    "line number": "40",
    "func name": "make_categorical",
    "func arg": "(batch_shape, num_classes, dtype)",
    "comments": ""
}{
    "source file": "categorical.py",
    "line number": "329",
    "func name": "_kl_categorical_categorical",
    "func arg": "(a, b, name)",
    "comments": "Calculate the batched KL divergence KL(a || b) with a and b Categorical.\n\n\n##### Args\n* **a**: instance of a Categorical distribution object.\n\n* **b**: instance of a Categorical distribution object.\n\n* **name**: (optional) Name to use for created operations.\n  default is \"kl_categorical_categorical\".\n\n##### Returns\n"
}{
    "source file": "category_crossing_benchmark.py",
    "line number": "43",
    "func name": "int_gen",
    "func arg": "()",
    "comments": ""
}{
    "source file": "category_crossing_distribution_test.py",
    "line number": "36",
    "func name": "batch_wrapper",
    "func arg": "(dataset, batch_size, distribution, repeat)",
    "comments": ""
}{}{}{}{
    "source file": "category_encoding_distribution_test.py",
    "line number": "36",
    "func name": "batch_wrapper",
    "func arg": "(dataset, batch_size, distribution, repeat)",
    "comments": ""
}{
    "source file": "category_encoding_test.py",
    "line number": "43",
    "func name": "get_layer_class",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "ceil.py",
    "line number": "27",
    "func name": "make_ceil_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do ceil.\n\n\n"
}{}{}{
    "source file": "cfg.py",
    "line number": "964",
    "func name": "build",
    "func arg": "(node)",
    "comments": ""
}{
    "source file": "check_futures_test.py",
    "line number": "88",
    "func name": "main",
    "func arg": "()",
    "comments": ""
}{
    "source file": "check_load_py_test.py",
    "line number": "44",
    "func name": "main",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "check_numerics_callback.py",
    "line number": "448",
    "func name": "disable_check_numerics",
    "func arg": "()",
    "comments": "Disable the eager/graph unified numerics checking mechanism.\n\nThis method can be used after a call to `tf.debugging.enable_check_numerics()` to disable the numerics-checking mechanism that catches infinity and NaN values output by ops executed eagerly or in tf.function-compiled graphs.\n\nThis method is idempotent. Calling it multiple times has the same effect as calling it once.\n\nThis method takes effect only on the thread in which it is called.\n"
}{}{
    "source file": "check_ops.py",
    "line number": "2340",
    "func name": "_ensure_shape_grad",
    "func arg": "(op, grad)",
    "comments": ""
}{
    "source file": "checkpoint_input_pipeline_hook_test.py",
    "line number": "40",
    "func name": "_test_combinations",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "checkpoint_management.py",
    "line number": "493",
    "func name": "meta_graph_filename",
    "func arg": "(checkpoint_filename, meta_graph_suffix)",
    "comments": "Returns the meta graph filename.\n\n\n##### Args\n* **checkpoint_filename**: Name of the checkpoint file.\n\n* **meta_graph_suffix**: Suffix for `MetaGraphDef` file. Defaults to 'meta'.\n\n##### Returns\n"
}{}{}{
    "source file": "checkpoint_ops.py",
    "line number": "419",
    "func name": "_load_embedding_initializer",
    "func arg": "(ckpt_path, embedding_tensor_name, new_vocab_size, embedding_dim, old_vocab_file, new_vocab_file, old_vocab_size, num_oov_buckets, initializer, max_rows_in_memory)",
    "comments": "Returns a variable initializer for loading pre-trained embeddings.\n\nWrapper around `load_and_remap_matrix_initializer()` specialized for loading embedding weights and remapping according to the provided vocab files. See docs for `load_and_remap_matrix_initializer()` for more details.\n\nNOTE: Only for use with div-partitioned variables / vocabularies.\n##### Args\n* **ckpt_path**: Path to the TensorFlow checkpoint (version 2, `TensorBundle`)\n  from which the old matrix `Tensor` will be loaded.\n\n* **embedding_tensor_name**: Name of the 2-D `Tensor` to load from checkpoint.\n\n* **new_vocab_size**: Number of entries in the new vocab.\n\n* **embedding_dim**: `int` specifying the dimension of the embedding vectors from\n  the checkpoint. Must match the number of columns in the old embedding\n  matrix.\n\n* **old_vocab_file**: A scalar `Tensor` of type `string` containing the\n  path to the old vocabulary file.\n\n* **new_vocab_file**: A scalar `Tensor` of type `string` containing the\n  path to the new vocabulary file.\n\n* **old_vocab_size**: The number of entries to consider in the old vocabulary.\n  With the default value of -1, the entire old row vocabulary file will be\n  used.  Otherwise, only the first `old_vocab_size` entries will be\n  considered for remapping.Must be smaller than the length of\n  `old_row_vocab_file`.\n\n* **num_oov_buckets**: `int` specifying the number of out-of-vocabulary\n  buckets to use. Must be >= 0.\n\n* **initializer**: Initializer function that accepts a 1-D tensor as the arg to\n  specify the shape of the returned tensor. If `None`, defaults to using\n  `truncated_normal_initializer()`.\n\n* **max_rows_in_memory**: `int` specifying the maximum number of rows to load from\n  the checkpoint at once. If less than or equal to 0, the entire matrix will\n  be loaded into memory. Setting this arg trades increased disk reads for\n  lower memory usage.\n\n##### Returns\n"
}{}{}{
    "source file": "checkpoint_utils_test.py",
    "line number": "42",
    "func name": "_create_checkpoints",
    "func arg": "(sess, checkpoint_dir)",
    "comments": ""
}{
    "source file": "checkpoint_utils_test1.py",
    "line number": "63",
    "func name": "_create_partition_checkpoints",
    "func arg": "(sess, checkpoint_dir)",
    "comments": ""
}{
    "source file": "checkpoint_utils.py",
    "line number": "481",
    "func name": "_collect_partitioned_variable",
    "func arg": "(name, all_vars)",
    "comments": "Returns list of `tf.Variable` that comprise the partitioned variable.\n\n\n"
}{}{}{}{
    "source file": "cholesky_op_test1.py",
    "line number": "80",
    "func name": "TriAngInvCompositeGrad",
    "func arg": "(l, grad)",
    "comments": ""
}{
    "source file": "cholesky_registrations.py",
    "line number": "93",
    "func name": "_cholesky_kronecker",
    "func arg": "(kronecker_operator)",
    "comments": ""
}{}{}{}{}{}{}{
    "source file": "cifar.py",
    "line number": "26",
    "func name": "load_batch",
    "func arg": "(fpath, label_key)",
    "comments": "Internal utility for parsing CIFAR data.\n\nArguments: fpath: path the file to parse. label_key: key for label data in the retrieve dictionary.\n##### Returns\n"
}{
    "source file": "cifar10.py",
    "line number": "32",
    "func name": "load_data",
    "func arg": "()",
    "comments": "Loads [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n\nThis is a dataset of 50,000 32x32 color training images and 10,000 test images, labeled over 10 categories. See more info at the [CIFAR homepage](https://www.cs.toronto.edu/~kriz/cifar.html).\n##### Returns\n* **Tuple of Numpy arrays**: `(x_train, y_train), (x_test, y_test)`.\n\n* ****x_train, x_test****: uint8 arrays of RGB image data with shape\n  `(num_samples, 3, 32, 32)` if `tf.keras.backend.image_data_format()` is\n  `'channels_first'`, or `(num_samples, 32, 32, 3)` if the data format\n  is `'channels_last'`.\n\n* ****y_train, y_test****: uint8 arrays of category labels\n  (integers in range 0-9) each with shape (num_samples, 1).\n\n"
}{
    "source file": "cifar100.py",
    "line number": "32",
    "func name": "load_data",
    "func arg": "(label_mode)",
    "comments": "Loads [CIFAR100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n\nThis is a dataset of 50,000 32x32 color training images and 10,000 test images, labeled over 100 fine-grained classes that are grouped into 20 coarse-grained classes. See more info at the [CIFAR homepage](https://www.cs.toronto.edu/~kriz/cifar.html).\n\nArguments: label_mode: one of \"fine\", \"coarse\". If it is \"fine\" the category labels are the fine-grained labels, if it is \"coarse\" the output labels are the coarse-grained superclasses.\n##### Returns\n* **Tuple of Numpy arrays**: `(x_train, y_train), (x_test, y_test)`.\n\n* ****x_train, x_test****: uint8 arrays of RGB image data with shape\n  `(num_samples, 3, 32, 32)` if `tf.keras.backend.image_data_format()` is\n  `'channels_first'`, or `(num_samples, 32, 32, 3)` if the data format\n  is `'channels_last'`.\n\n* ****y_train, y_test****: uint8 arrays of category labels with shape\n  (num_samples, 1).\n\n"
}{}{}{}{
    "source file": "cli_shared.py",
    "line number": "434",
    "func name": "get_error_intro",
    "func arg": "(tf_error)",
    "comments": "Generate formatted intro for TensorFlow run-time error.\n\n\n##### Args\n* **tf_error**: (errors.OpError) TensorFlow run-time error object.\n\n##### Returns\n"
}{
    "source file": "cli_test_utils.py",
    "line number": "48",
    "func name": "assert_array_lines_close",
    "func arg": "(test, expected_array, array_lines)",
    "comments": "Assert that the array value represented by lines is close to expected.\n\nNote that the shape of the array represented by the `array_lines` is ignored.\n##### Args\n* **test**: An instance of TensorFlowTestCase.\n\n* **expected_array**: Expected value of the array.\n\n* **array_lines**: A list of strings representing the array.\n  E.g., \"array([[ 1.0, 2.0 ], [ 3.0, 4.0 ]])\"\n  Assumes that values are separated by commas, parentheses, brackets, \"|\"\n  characters and whitespace.\n\n"
}{}{
    "source file": "client_test.py",
    "line number": "43",
    "func name": "mock_request_compute_metadata",
    "func arg": "(path)",
    "comments": ""
}{
    "source file": "client.py",
    "line number": "106",
    "func name": "_as_text",
    "func arg": "(s)",
    "comments": ""
}{}{}{
    "source file": "clip_ops.py",
    "line number": "389",
    "func name": "clip_by_average_norm",
    "func arg": "(t, clip_norm, name)",
    "comments": "Clips tensor values to a maximum average L2-norm.\n\nGiven a tensor `t`, and a maximum clip value `clip_norm`, this operation normalizes `t` so that its average L2-norm is less than or equal to `clip_norm`. Specifically, if the average L2-norm is already less than or equal to `clip_norm`, then `t` is not modified. If the average L2-norm is greater than `clip_norm`, then this operation returns a tensor of the same type and shape as `t` with its values set to:\n\n`t * clip_norm / l2norm_avg(t)`\n\nIn this case, the average L2-norm of the output tensor is `clip_norm`.\n\nThis operation is typically used to clip gradients before applying them with an optimizer.\n##### Args\n* **t**: A `Tensor`.\n\n* **clip_norm**: A 0-D (scalar) `Tensor` > 0. A maximum clipping value.\n\n* **name**: A name for the operation (optional).\n\n##### Returns\n"
}{}{
    "source file": "cluster_resolver.py",
    "line number": "42",
    "func name": "get_accelerator_devices",
    "func arg": "(master, config_proto)",
    "comments": "Returns accelerator devices given a master and a configuration.\n\n\n"
}{}{
    "source file": "cluster.py",
    "line number": "115",
    "func name": "Provision",
    "func arg": "(allow_soft_placement, disable_detailed_stats, disable_timeline, devices)",
    "comments": ""
}{}{}{}{}{
    "source file": "codegen.py",
    "line number": "233",
    "func name": "generate_random_functiondef",
    "func arg": "()",
    "comments": ""
}{
    "source file": "collective_all_reduce_strategy_test.py",
    "line number": "59",
    "func name": "create_test_objects",
    "func arg": "(cluster_spec, task_type, task_id, num_gpus)",
    "comments": ""
}{
    "source file": "collective_all_reduce_strategy_test1.py",
    "line number": "54",
    "func name": "create_test_objects",
    "func arg": "(cluster_spec, task_type, task_id, num_gpus)",
    "comments": ""
}{}{}{}{}{}{
    "source file": "collective_ops.py",
    "line number": "174",
    "func name": "broadcast_recv",
    "func arg": "(shape, dtype, group_size, group_key, instance_key, communication_hint, timeout)",
    "comments": "Receives a broadcasts tensor, across devices.\n\n\n##### Args\n* **shape**: Shape of the tensor to be received.\n\n* **dtype**: Type of the tensor to be received.\n\n* **group_size**: one plus the number of receiving tensors, i.e. the total\n  number of devices participating.  Each tensor must reside on a\n  different device.\n\n* **group_key**: an integer identifying the group of devices.\n\n* **instance_key**: an integer identifying the participating group of Ops.\n\n* **communication_hint**: preferred collective communication.  The implementation\n  may fall back to another mechanism.  Options include `auto`, `ring`, and\n  `nccl`.\n\n* **timeout**: If set to a non zero, set a completion timeout to detect staleness.\n  If the timer goes off, a DeadlineExceededError is raised.\n  The timeout value in seconds. This feature is experimental.\n\n##### Returns\n"
}{}{}{}{
    "source file": "combinations.py",
    "line number": "474",
    "func name": "_multi_worker_session",
    "func arg": "(kwargs)",
    "comments": "Returns a context manager that enters a session that is configured for the MultiWorkerMirroredStrategy.\n\n\n##### Args\n* **kwargs**: a dict. Keyword arguments passed to the test.\n\n##### Returns\n"
}{}{
    "source file": "combinations2.py",
    "line number": "64",
    "func name": "keras_tensor_combinations",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "command_parser.py",
    "line number": "494",
    "func name": "get_print_tensor_argparser",
    "func arg": "(description)",
    "comments": "Get an ArgumentParser for a command that prints tensor values.\n\nExamples of such commands include print_tensor and print_feed.\n##### Args\n* **description**: Description of the ArgumentParser.\n\n##### Returns\n"
}{}{
    "source file": "common_shapes.py",
    "line number": "89",
    "func name": "broadcast_shape",
    "func arg": "(shape_x, shape_y)",
    "comments": "Returns the broadcasted shape between `shape_x` and `shape_y`.\n\n\n##### Args\n* **shape_x**: A `TensorShape`\n\n* **shape_y**: A `TensorShape`\n\n##### Returns\n"
}{}{
    "source file": "common_v1.py",
    "line number": "49",
    "func name": "do_test",
    "func arg": "(create_signature, canonicalize, show_debug_info)",
    "comments": "Runs test.\n\n1. Performs absl and tf \"main\"-like initialization that must run before almost anything else. 2. Converts signature_def_map to SavedModel V1 3. Converts SavedModel V1 to MLIR 4. Prints the textual MLIR to stdout (it is expected that the caller will have FileCheck checks in its file to check this output).\n\nThis is only for use by the MLIR SavedModel importer tests.\n##### Args\n* **create_signature**: A functor that return signature_def_map, init_op and\n  assets_collection. signature_def_map is a map from string key to\n  signature_def. The key will be used as function name in the resulting\n  MLIR.\n\n* **canonicalize**: If true, canonicalizer will be run on the resulting MLIR.\n\n* **show_debug_info**: If true, shows debug locations in the resulting MLIR.\n\n"
}{
    "source file": "common1.py",
    "line number": "43",
    "func name": "do_test",
    "func arg": "(create_module_fn, exported_names, show_debug_info)",
    "comments": "Runs test.\n\n1. Performs absl and tf \"main\"-like initialization that must run before almost anything else. 2. Converts `tf.Module` to SavedModel 3. Converts SavedModel to MLIR 4. Prints the textual MLIR to stdout (it is expected that the caller will have FileCheck checks in its file to check this output).\n\nThis is only for use by the MLIR SavedModel importer tests.\n##### Args\n* **create_module_fn**: A callable taking no arguments, which returns the\n  `tf.Module` to be converted and printed.\n\n* **exported_names**: A set of exported names for the MLIR converter (default is\n  \"export all\").\n\n* **show_debug_info**: If true, shows debug locations in the resulting MLIR.\n\n"
}{
    "source file": "common2.py",
    "line number": "74",
    "func name": "get_run_key",
    "func arg": "(feed_dict, fetches)",
    "comments": "Summarize the names of feeds and fetches as a RunKey JSON string.\n\n\n##### Args\n* **feed_dict**: The feed_dict given to the `Session.run()` call.\n\n* **fetches**: The fetches from the `Session.run()` call.\n\n##### Returns\n"
}{
    "source file": "compare_1k.py",
    "line number": "63",
    "func name": "to_float",
    "func arg": "(x, n)",
    "comments": ""
}{}{
    "source file": "compare_test.py",
    "line number": "34",
    "func name": "LargePbs",
    "func arg": "()",
    "comments": "Converts ASCII string Large PBs to messages.\n\n\n"
}{
    "source file": "compare.py",
    "line number": "203",
    "func name": "ProtoEq",
    "func arg": "(a, b)",
    "comments": "Compares two proto2 objects for equality.\n\nRecurses into nested messages. Uses list (not set) semantics for comparing repeated fields, ie duplicates and order matter.\n##### Args\n* **a**: A proto2 message or a primitive.\n\n* **b**: A proto2 message or a primitive.\n\n##### Returns\n"
}{}{
    "source file": "compat_checker.py",
    "line number": "110",
    "func name": "_get_func_name",
    "func arg": "()",
    "comments": "Get the name of current function.\n\n\n##### Returns\n"
}{
    "source file": "compat_internal.py",
    "line number": "24",
    "func name": "path_to_str",
    "func arg": "(path)",
    "comments": "Returns the file system path representation of a `PathLike` object, else as it is.\n\n\n##### Args\n* **path**: An object that can be converted to path representation.\n\n##### Returns\n"
}{}{}{}{
    "source file": "compat_util.py",
    "line number": "34",
    "func name": "deprecated_py2_support",
    "func arg": "(module_name)",
    "comments": "Swaps calling module with a Py2-specific implementation. Noop in Py3.\n\n\n"
}{
    "source file": "compat.py",
    "line number": "131",
    "func name": "forward_compatibility_horizon",
    "func arg": "(year, month, day)",
    "comments": "Context manager for testing forward compatibility of generated graphs.\n\nSee [Version compatibility](https://tensorflow.org/guide/version_compat#backward_forward).\n\nTo ensure forward compatibility of generated graphs (see `forward_compatible`) with older binaries, new features can be gated with:\n\n```python if compat.forward_compatible(year=2018, month=08, date=01): generate_graph_with_new_features() else: generate_graph_so_older_binaries_can_consume_it() ```\n\nHowever, when adding new features, one may want to unittest it before the forward compatibility window expires. This context manager enables such tests. For example:\n\n```python from tensorflow.python.compat import compat\n\ndef testMyNewFeature(self): with compat.forward_compatibility_horizon(2018, 08, 02): # Test that generate_graph_with_new_features() has an effect ```\n##### Args\n* **year**: A year (e.g., 2018). Must be an `int`.\n\n* **month**: A month (1 <= month <= 12) in year. Must be an `int`.\n\n* **day**: A day (1 <= day <= 31, or 30, or 29, or 28) in month. Must be an\n  `int`.\n\n* **elds**: \n\n"
}{
    "source file": "compat1.py",
    "line number": "183",
    "func name": "path_to_bytes",
    "func arg": "(path)",
    "comments": "Converts input which is a `PathLike` object to `bytes`.\n\nConverts from any python constant representation of a `PathLike` object or `str` to bytes.\n##### Args\n* **path**: An object that can be converted to path representation.\n\n##### Returns\n* **age**: \n\n"
}{}{
    "source file": "compile_utils.py",
    "line number": "630",
    "func name": "apply_mask",
    "func arg": "(y_p, sw, mask)",
    "comments": "Applies any mask on predictions to sample weights.\n\n\n"
}{}{
    "source file": "composite_tensor_support_test.py",
    "line number": "308",
    "func name": "prepare_inputs",
    "func arg": "(data, use_dict, use_dataset, action, input_name)",
    "comments": ""
}{}{}{
    "source file": "composite_tensor_utils.py",
    "line number": "113",
    "func name": "append_composite_tensor",
    "func arg": "(target, to_append)",
    "comments": "Helper function to append composite tensors to each other in the 0 axis.\n\nIn order to support batching within a fit/evaluate/predict call, we need to be able to aggregate within a CompositeTensor. Unfortunately, the CT API currently does not make this easy\n\n- especially in V1 mode, where we're working with CompositeTensor Value objects that have no connection with the CompositeTensors that created them.\n\nArguments: target: CompositeTensor or CompositeTensor value object that will be appended to. to_append: CompositeTensor or CompositeTensor value object to append to. 'target'.\n##### Returns\n"
}{
    "source file": "composite_tensor.py",
    "line number": "94",
    "func name": "replace_composites_with_components",
    "func arg": "(structure)",
    "comments": "Recursively replaces CompositeTensors with their components.\n\n\n##### Args\n* **structure**: A `nest`-compatible structure, possibly containing composite\n  tensors.\n\n##### Returns\n"
}{
    "source file": "compression_ops_test.py",
    "line number": "31",
    "func name": "_test_objects",
    "func arg": "()",
    "comments": ""
}{
    "source file": "compression_ops.py",
    "line number": "39",
    "func name": "uncompress",
    "func arg": "(element, output_spec)",
    "comments": "Uncompress a compressed dataset element.\n\n\n##### Args\n* **element**: A scalar variant tensor to uncompress. The element should have been\n  created by calling `compress`.\n\n* **output_spec**: A nested structure of `tf.TypeSpec` representing the type(s) of\n  the uncompressed element.\n\n##### Returns\n"
}{
    "source file": "concat_benchmark.py",
    "line number": "35",
    "func name": "build_graph",
    "func arg": "(device, input_shape, variable, num_inputs, axis, grad)",
    "comments": "Build a graph containing a sequence of concat operations.\n\n\n##### Args\n* **device**: string, the device to run on.\n\n* **input_shape**: shape of the input tensors.\n\n* **variable**: whether or not to randomize the input shape\n\n* **num_inputs**: the number of inputs to concat\n\n* **axis**: axis to be concat'ed\n\n* **grad**: if True compute the gradient\n\n##### Returns\n"
}{}{}{
    "source file": "concat.py",
    "line number": "27",
    "func name": "make_concat_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do concatenation.\n\n\n"
}{}{}{}{
    "source file": "concrete_function_error.py",
    "line number": "64",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{}{
    "source file": "cond_v2_test.py",
    "line number": "1537",
    "func name": "_has_node_with_op",
    "func arg": "(run_metadata, op_type)",
    "comments": "Whether any node in `run_metadata.partition_graphs` matches `op_type`.\n\n\n"
}{
    "source file": "cond_v2.py",
    "line number": "1146",
    "func name": "_set_read_only_resource_inputs_attr",
    "func arg": "(op, branch_graphs)",
    "comments": "Sets the list of resource inputs which are read-only.\n\nThis is used by AutomaticControlDependencies.\n##### Args\n* **op**: If or Case Operation.\n\n* **branch_graphs**: List of branch FuncGraphs.\n\n"
}{}{}{
    "source file": "conditional_expressions_test1.py",
    "line number": "29",
    "func name": "_basic_expr",
    "func arg": "(cond)",
    "comments": ""
}{
    "source file": "conditional_expressions.py",
    "line number": "48",
    "func name": "transform",
    "func arg": "(node, ctx)",
    "comments": ""
}{
    "source file": "conditional_expressions1.py",
    "line number": "55",
    "func name": "_py_if_exp",
    "func arg": "(cond, if_true, if_false)",
    "comments": ""
}{
    "source file": "config_detector.py",
    "line number": "657",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{}{
    "source file": "config_test.py",
    "line number": "42",
    "func name": "reset_eager",
    "func arg": "(fn)",
    "comments": ""
}{}{
    "source file": "config1.py",
    "line number": "729",
    "func name": "disable_mlir_graph_optimization",
    "func arg": "()",
    "comments": "Disables experimental MLIR-Based TensorFlow Compiler Optimizations.\n\n\n"
}{}{
    "source file": "confusion_matrix.py",
    "line number": "209",
    "func name": "confusion_matrix_v1",
    "func arg": "(labels, predictions, num_classes, dtype, name, weights)",
    "comments": "Computes the confusion matrix from predictions and labels.\n\nThe matrix columns represent the prediction labels and the rows represent the real labels. The confusion matrix is always a 2-D array of shape `[n, n]`, where `n` is the number of valid labels for a given classification task. Both prediction and labels must be 1-D arrays of the same shape in order for this function to work.\n\nIf `num_classes` is `None`, then `num_classes` will be set to one plus the maximum value in either predictions or labels. Class labels are expected to start at 0. For example, if `num_classes` is 3, then the possible labels would be `[0, 1, 2]`.\n\nIf `weights` is not `None`, then each prediction contributes its corresponding weight to the total value of the confusion matrix cell.\n\nFor example:\n\n```python tf.math.confusion_matrix([1, 2, 4], [2, 2, 4]) ==> [[0 0 0 0 0] [0 0 1 0 0] [0 0 1 0 0] [0 0 0 0 0] [0 0 0 0 1]] ```\n\nNote that the possible labels are assumed to be `[0, 1, 2, 3, 4]`, resulting in a 5x5 confusion matrix.\n##### Args\n* **labels**: 1-D `Tensor` of real labels for the classification task.\n\n* **predictions**: 1-D `Tensor` of predictions for a given classification.\n\n* **num_classes**: The possible number of labels the classification task can have.\n  If this value is not provided, it will be calculated using both\n  predictions and labels array.\n\n* **dtype**: Data type of the confusion matrix.\n\n* **name**: Scope name.\n\n* **weights**: An optional `Tensor` whose shape matches `predictions`.\n\n##### Returns\n"
}{
    "source file": "conjugate_gradient_test.py",
    "line number": "41",
    "func name": "_get_conjugate_gradient_test",
    "func arg": "(dtype_, use_static_shape_, shape_)",
    "comments": ""
}{
    "source file": "conjugate_gradient.py",
    "line number": "36",
    "func name": "conjugate_gradient",
    "func arg": "(operator, rhs, preconditioner, x, tol, max_iter, name)",
    "comments": "Conjugate gradient solver.\n\nSolves a linear system of equations `A*x = rhs` for self-adjoint, positive definite matrix `A` and right-hand side vector `rhs`, using an iterative, matrix-free algorithm where the action of the matrix A is represented by `operator`. The iteration terminates when either the number of iterations exceeds `max_iter` or when the residual norm has been reduced to `tol` times its initial value, i.e. \\\\(||rhs\n\n- A x_k|| <= tol ||rhs||\\\\).\n##### Args\n* **operator**: A `LinearOperator` that is self-adjoint and positive definite.\n\n* **rhs**: A possibly batched vector of shape `[..., N]` containing the right-hand\n  size vector.\n\n* **preconditioner**: A `LinearOperator` that approximates the inverse of `A`.\n  An efficient preconditioner could dramatically improve the rate of\n  convergence. If `preconditioner` represents matrix `M`(`M` approximates\n  `A^{-1}`), the algorithm uses `preconditioner.apply(x)` to estimate\n  `A^{-1}x`. For this to be useful, the cost of applying `M` should be\n  much lower than computing `A^{-1}` directly.\n\n* **x**: A possibly batched vector of shape `[..., N]` containing the initial\n  guess for the solution.\n\n* **tol**: A float scalar convergence tolerance.\n\n* **max_iter**: An integer giving the maximum number of iterations.\n\n* **name**: A name scope for the operation.\n\n##### Returns\n* **output**: A namedtuple representing the final state with fields\n\n"
}{}{}{}{}{}{
    "source file": "constant_op.py",
    "line number": "381",
    "func name": "_dimension_tensor_conversion_function",
    "func arg": "(d, dtype, name, as_ref)",
    "comments": "Function to convert Dimension to Tensor.\n\n\n"
}{
    "source file": "constant.py",
    "line number": "31",
    "func name": "make_constant_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do constant ops.\n\n\n"
}{}{}{
    "source file": "constraints_test.py",
    "line number": "42",
    "func name": "get_example_kernel",
    "func arg": "(width)",
    "comments": ""
}{
    "source file": "constraints.py",
    "line number": "300",
    "func name": "get",
    "func arg": "(identifier)",
    "comments": ""
}{}{
    "source file": "context_managers.py",
    "line number": "27",
    "func name": "control_dependency_on_returns",
    "func arg": "(return_value)",
    "comments": "Create a TF control dependency on the return values of a function.\n\nIf the function had no return value, a no-op context is returned.\n##### Args\n* **return_value**: The return value to set as control dependency.\n\n##### Returns\n"
}{}{
    "source file": "context.py",
    "line number": "2402",
    "func name": "_tmp_in_graph_mode",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "continue_statements.py",
    "line number": "163",
    "func name": "transform",
    "func arg": "(node, ctx)",
    "comments": ""
}{
    "source file": "control_flow_deprecated_py2.py",
    "line number": "630",
    "func name": "transform",
    "func arg": "(node, ctx)",
    "comments": ""
}{
    "source file": "control_flow_deprecated_py2(1).py",
    "line number": "1152",
    "func name": "_py_if_stmt",
    "func arg": "(cond, body, orelse)",
    "comments": "Overload of if_stmt that executes a Python if statement.\n\n\n"
}{
    "source file": "control_flow_duplicate_v1.py",
    "line number": "42",
    "func name": "Test",
    "func arg": "()",
    "comments": ""
}{
    "source file": "control_flow_grad.py",
    "line number": "243",
    "func name": "_LoopCondGrad",
    "func arg": "(_)",
    "comments": "Stop backprop for the predicate of a while loop.\n\n\n"
}{}{
    "source file": "control_flow_ops_py_test.py",
    "line number": "154",
    "func name": "tf_function_in_tf2",
    "func arg": "(f)",
    "comments": ""
}{
    "source file": "control_flow_ops_test.py",
    "line number": "567",
    "func name": "_raw_nested_shape",
    "func arg": "(nested_shape)",
    "comments": ""
}{
    "source file": "control_flow_ops_test2.py",
    "line number": "1298",
    "func name": "create_dynamic_lstm",
    "func arg": "(cell_fn, batch_size, state_size, max_steps)",
    "comments": ""
}{
    "source file": "control_flow_ops.py",
    "line number": "3691",
    "func name": "from_control_flow_context_def",
    "func arg": "(context_def, import_scope)",
    "comments": "Deserializes `context_def` into the appropriate ControlFlowContext.\n\n\n##### Args\n* **context_def**: ControlFlowContextDef proto\n\n* **import_scope**: Optional `string`. Name scope to add.\n\n##### Returns\n"
}{
    "source file": "control_flow_ops2.py",
    "line number": "336",
    "func name": "vectorized_map",
    "func arg": "(fn, elems, fallback_to_while_loop)",
    "comments": "Parallel map on the list of tensors unpacked from `elems` on dimension 0.\n\nThis method works similar to `tf.map_fn` but is optimized to run much faster, possibly with a much larger memory footprint. The speedups are obtained by vectorization (see [Auto-Vectorizing TensorFlow Graphs: Jacobians, Auto-Batching and Beyond](https://arxiv.org/pdf/1903.04243.pdf)). The idea behind vectorization is to semantically launch all the invocations of `fn` in parallel and fuse corresponding operations across all these invocations. This fusion is done statically at graph generation time and the generated code is often similar in performance to a manually fused version.\n\nBecause `tf.vectorized_map` fully parallelizes the batch, this method will generally be significantly faster than using `tf.map_fn`, especially in eager mode. However this is an experimental feature and currently has a lot of limitations:\n\n- There should be no data dependency between the different semantic invocations of `fn`, i.e. it should be safe to map the elements of the inputs in any order.\n\n- Stateful kernels may mostly not be supported since these often imply a data dependency. We do support a limited set of such stateful kernels though (like RandomFoo, Variable operations like reads, etc).\n\n- `fn` has limited support for control flow operations.\n\n- `fn` should return nested structure of Tensors or Operations. However if an Operation is returned, it should have zero outputs.\n\n- The shape and dtype of any intermediate or output tensors in the computation of `fn` should not depend on the input to `fn`.\n\nExamples: ```python def outer_product(a): return tf.tensordot(a, a, 0)\n\nbatch_size = 100 a = tf.ones((batch_size, 32, 32)) c = tf.vectorized_map(outer_product, a) assert c.shape == (batch_size, 32, 32, 32, 32) ```\n\n```python # Computing per-example gradients\n\nbatch_size = 10 num_features = 32 layer = tf.keras.layers.Dense(1)\n\ndef model_fn(arg): with tf.GradientTape() as g: inp, label = arg inp = tf.expand_dims(inp, 0) label = tf.expand_dims(label, 0) prediction = layer(inp) loss = tf.nn.l2_loss(label\n\n- prediction) return g.gradient(loss, (layer.kernel, layer.bias))\n\ninputs = tf.random.uniform([batch_size, num_features]) labels = tf.random.uniform([batch_size, 1]) per_example_gradients = tf.vectorized_map(model_fn, (inputs, labels)) assert per_example_gradients[0].shape == (batch_size, num_features, 1) assert per_example_gradients[1].shape == (batch_size, 1) ```\n##### Args\n* **fn**: The callable to be performed. It accepts one argument, which will have\n  the same (possibly nested) structure as `elems`, and returns a possibly\n  nested structure of Tensors and Operations, which may be different than\n  the structure of `elems`.\n\n* **elems**: A tensor or (possibly nested) sequence of tensors, each of which will\n  be unpacked along their first dimension. The nested sequence of the\n  resulting slices will be mapped over by `fn`.\n\n* **fallback_to_while_loop**: If true, on failing to vectorize an operation,\n  the unsupported op is wrapped in a tf.while_loop to execute the map\n  iterations. Note that this fallback only happens for unsupported ops and\n  other parts of `fn` are still vectorized. If false, on encountering an\n  unsupported op, a ValueError is thrown. Note that the fallbacks can result\n  in slowdowns since vectorization often yields speedup of one to two orders\n  of magnitude.\n\n##### Returns\n"
}{
    "source file": "control_flow_state.py",
    "line number": "834",
    "func name": "ZerosLike",
    "func arg": "(op, index)",
    "comments": "Create zeros_like for the specified output of an op.\n\n\n"
}{}{}{}{
    "source file": "control_flow_upgrade_legacy_v1.py",
    "line number": "34",
    "func name": "Test",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "control_flow_util_v2.py",
    "line number": "293",
    "func name": "get_func_graph",
    "func arg": "(op, input_shapes, func_name)",
    "comments": "Generates and returns a FuncGraph for the given op and input_shapes.\n\n\n"
}{
    "source file": "control_flow_util.py",
    "line number": "366",
    "func name": "GetWhileContext",
    "func arg": "(op)",
    "comments": "Get the WhileContext to which this op belongs.\n\n\n"
}{}{}{}{}{
    "source file": "control_flow_v2_toggles.py",
    "line number": "75",
    "func name": "output_all_intermediates",
    "func arg": "(state)",
    "comments": "Whether to output all intermediates from functional control flow ops.\n\nThe \"default\" behavior to is to output all intermediates when using v2 control flow inside Keras models in graph mode (possibly inside Estimators). This is needed to support taking gradients of v2 control flow. In graph mode, Keras can sometimes freeze the forward graph before the gradient computation which does not work for v2 control flow since it requires updating the forward ops to output the needed intermediates. We work around this by proactively outputting the needed intermediates when building the forward pass itself. Ideally any such extra tensors should be pruned out at runtime. However, if for any reason this doesn't work for you or if you have an inference-only model you can turn this behavior off using `tf.compat.v1.experimental.output_all_intermediates(False)`.\n\nIf with the default behavior you are still seeing errors of the form \"Connecting to invalid output X of source node Y which has Z outputs\" try setting `tf.compat.v1.experimental.output_all_intermediates(True)` and please file an issue at https://github.com/tensorflow/tensorflow/issues.\n##### Args\n* **state**: True, False or None. None restores the default behavior.\n\n"
}{
    "source file": "control_flow.py",
    "line number": "402",
    "func name": "transform",
    "func arg": "(node, ctx)",
    "comments": ""
}{
    "source file": "control_flow1.py",
    "line number": "1011",
    "func name": "_py_if_stmt",
    "func arg": "(cond, body, orelse)",
    "comments": "Overload of if_stmt that executes a Python if statement.\n\n\n"
}{}{
    "source file": "conv_activation.py",
    "line number": "151",
    "func name": "make_conv_relu1_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do conv_relu1.\n\n\n"
}{}{
    "source file": "conv_ops_3d_test.py",
    "line number": "38",
    "func name": "GetTestConfigs",
    "func arg": "()",
    "comments": "Get all the valid tests configs to run.\n\n\n##### Returns\n"
}{
    "source file": "conv_ops_test.py",
    "line number": "3194",
    "func name": "GetInceptionBackFilterTest",
    "func arg": "(input_size, filter_size, output_size, strides, padding, gpu_only)",
    "comments": ""
}{
    "source file": "conv_to_depthwiseconv_with_shared_weights.py",
    "line number": "28",
    "func name": "make_conv_to_depthwiseconv_with_shared_weights_tests",
    "func arg": "(options)",
    "comments": "Make a test where 2 Conv ops shared the same constant weight tensor.\n\n\n"
}{
    "source file": "conv_utils_test.py",
    "line number": "30",
    "func name": "_get_const_output_shape",
    "func arg": "(input_shape, dim)",
    "comments": ""
}{
    "source file": "conv_utils.py",
    "line number": "468",
    "func name": "conv_output_shape",
    "func arg": "(input_shape, kernel_shape, strides, padding)",
    "comments": "Return the output shape of an N-D convolution.\n\nForces dimensions where input is empty (size 0) to remain empty.\n##### Args\n* **input_shape**: tuple of size N\n\n* **kernel_shape**: tuple of size N, spatial shape of the convolutional kernel /\n  receptive field.\n\n* **strides**: tuple of size N, strides along each spatial dimension.\n\n* **padding**: type of padding, string `\"same\"` or `\"valid\"`.\n  `\"valid\"` means no padding. `\"same\"` results in padding evenly to \n  the left/right or up/down of the input such that output has the same \n  height/width dimension as the input.\n\n##### Returns\n* **tuple of size N**: `(d_out1, ..., d_outN)`, spatial shape of the output.\n\n"
}{
    "source file": "conv_with_shared_weights.py",
    "line number": "28",
    "func name": "make_conv_with_shared_weights_tests",
    "func arg": "(options)",
    "comments": "Make a test where 2 Conv ops shared the same constant weight tensor.\n\n\n"
}{
    "source file": "conv.py",
    "line number": "28",
    "func name": "make_conv_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do convolution.\n\n\n"
}{}{}{}{
    "source file": "conv2d_benchmark.py",
    "line number": "44",
    "func name": "build_graph",
    "func arg": "(device, dtype, data_format, input_shape, filter_shape, strides, padding, num_iters, warmup_iters)",
    "comments": "builds a graph containing a sequence of conv2d operations.\n\n\n##### Args\n* **device**: String, the device to run on.\n\n* **dtype**: Data type for the convolution.\n\n* **data_format**: A string from\n\n* **input_shape**: Shape of the input tensor.\n\n* **filter_shape**: Shape of the filter tensor.\n\n* **strides**: A list of ints. 1-D of length 4. The stride of sliding\n         window for each dimension of input.\n\n* **padding**: A string from\n\n* **num_iters**: number of iterations to run conv2d.\n\n* **warmup_iters**: number of iterations for warmup runs.\n\n##### Returns\n"
}{}{
    "source file": "conv2d_test1.py",
    "line number": "66",
    "func name": "build_graph",
    "func arg": "(inp, dtype, num_filters, data_format, kernel_sizes, dilation_rates, padding)",
    "comments": ""
}{}{
    "source file": "conv2d_transpose.py",
    "line number": "28",
    "func name": "make_conv2d_transpose_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do transpose_conv.\n\n\n"
}{}{}{}{}{
    "source file": "conversion.py",
    "line number": "229",
    "func name": "cache_allowlisted",
    "func arg": "(entity, options)",
    "comments": ""
}{
    "source file": "convert_binary_to_cc_source.py",
    "line number": "155",
    "func name": "main",
    "func arg": "()",
    "comments": ""
}{
    "source file": "convert_file_to_c_source.py",
    "line number": "101",
    "func name": "main",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "convert_image_to_csv.py",
    "line number": "110",
    "func name": "main",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "convert_saved_model.py",
    "line number": "155",
    "func name": "freeze_saved_model",
    "func arg": "(saved_model_dir, input_arrays, input_shapes, output_arrays, tag_set, signature_key)",
    "comments": "Converts a SavedModel to a frozen graph.\n\n\n##### Args\n* **saved_model_dir**: SavedModel directory to convert.\n\n* **input_arrays**: List of input tensors to freeze graph with. Uses input arrays\n  from SignatureDef when none are provided.\n\n* **input_shapes**: Dict of strings representing input tensor names to list of\n  integers representing input shapes (e.g., {\"foo\"\n\n* **output_arrays**: List of output tensors to freeze graph with. Uses output\n  arrays from SignatureDef when none are provided.\n\n* **tag_set**: Set of tags identifying the MetaGraphDef within the SavedModel to\n  analyze. All tags in the tag set must be present.\n\n* **signature_key**: Key identifying SignatureDef containing inputs and outputs.\n\n##### Returns\n* **frozen_graph_def**: Frozen GraphDef.\n\n* **in_tensors**: List of input tensors for the graph.\n\n* **out_tensors**: List of output tensors for the graph.\n\n* **graph**: `Graph` object.\n\n"
}{}{}{}{}{
    "source file": "convert_to_constants.py",
    "line number": "1113",
    "func name": "convert_variables_to_constants_from_session_graph",
    "func arg": "(session, graph_def, output_node_names, variable_names_allowlist, variable_names_blacklist)",
    "comments": "Replaces all the variables in a graph with constants of the same values.\n\nThis function works similarly to convert_variables_to_constants_v2, but it retrieves the constant values from a Session instead of from a ConcreteFunction. This is useful when converting graphs generated from TensorFlow V1, where ConcreteFunctions are not available. This also differs from graph_util.convert_variables_to_constants in that it supports resource variables when V2 control flow constructions are present.\n##### Args\n* **session**: Active TensorFlow session containing the variables.\n\n* **graph_def**: A GraphDef to convert.\n\n* **output_node_names**: List of name strings for the result nodes of the graph.\n\n* **variable_names_allowlist**: The set of variable names to convert (by default,\n  all variables are converted).\n\n* **variable_names_blacklist**: The set of variable names to omit converting to\n  constants.\n\n##### Returns\n"
}{}{
    "source file": "convert.py",
    "line number": "580",
    "func name": "toco_convert",
    "func arg": "(input_data, input_tensors, output_tensors, **kwargs)",
    "comments": "Convert a model using TOCO.\n\nTypically this function is used to convert from TensorFlow GraphDef to TFLite. Conversion can be customized by providing arguments that are forwarded to `build_toco_convert_protos` (see documentation for details). This function has been deprecated. Please use `lite.TFLiteConverter` instead.\n##### Args\n* **input_data**: Input data (i.e. often `sess.graph_def`),\n\n* **input_tensors**: List of input tensors. Type and shape are computed using\n  `foo.shape` and `foo.dtype`.\n\n* **output_tensors**: List of output tensors (only .name is used from this).\n\n* ***args**: See `build_toco_convert_protos`,\n\n* ****kwargs**: See `build_toco_convert_protos`.\n\n##### Returns\n"
}{
    "source file": "convert1.py",
    "line number": "38",
    "func name": "partial_shape_to_tensor",
    "func arg": "(shape_like)",
    "comments": "Returns a `tf.Tensor` that represents the given shape.\n\n\n##### Args\n* **shape_like**: A value that can be converted to a `tf.TensorShape` or a\n  `tf.Tensor`.\n\n##### Returns\n"
}{}{
    "source file": "converter_testing.py",
    "line number": "47",
    "func name": "is_inside_generated_code",
    "func arg": "()",
    "comments": "Tests whether the caller is generated code. Implementation-specific.\n\n\n"
}{}{}{}{}{}{}{}{
    "source file": "convolutional1.py",
    "line number": "1407",
    "func name": "conv3d_transpose",
    "func arg": "(inputs, filters, kernel_size, strides, padding, data_format, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)",
    "comments": "Functional interface for transposed 3D convolution layer.\n\nArguments: inputs: Input tensor. filters: Integer, the dimensionality of the output space (i.e. the number of filters in the convolution). kernel_size: A tuple or list of 3 positive integers specifying the spatial dimensions of the filters. Can be a single integer to specify the same value for all spatial dimensions. strides: A tuple or list of 3 positive integers specifying the strides of the convolution. Can be a single integer to specify the same value for all spatial dimensions. padding: one of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no padding. `\"same\"` results in padding evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input. data_format: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, depth, height, width, channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, depth, height, width)`. activation: Activation function. Set it to None to maintain a linear activation. use_bias: Boolean, whether the layer uses a bias. kernel_initializer: An initializer for the convolution kernel. bias_initializer: An initializer for the bias vector. If None, the default initializer will be used. kernel_regularizer: Optional regularizer for the convolution kernel. bias_regularizer: Optional regularizer for the bias vector. activity_regularizer: Optional regularizer function for the output. kernel_constraint: Optional projection function to be applied to the kernel after being updated by an `Optimizer` (e.g. used to implement norm constraints or value constraints for layer weights). The function must take as input the unprojected variable and must return the projected variable (which must have the same shape). Constraints are not safe to use when doing asynchronous distributed training. bias_constraint: Optional projection function to be applied to the bias after being updated by an `Optimizer`. trainable: Boolean, if `True` also add variables to the graph collection `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`). name: A string, the name of the layer. reuse: Boolean, whether to reuse the weights of a previous layer by the same name.\n##### Returns\n"
}{}{
    "source file": "coordinator_test.py",
    "line number": "337",
    "func name": "_StopAt0",
    "func arg": "(coord, n)",
    "comments": ""
}{}{
    "source file": "copy_binary.py",
    "line number": "94",
    "func name": "main",
    "func arg": "()",
    "comments": "This script copies binaries.\n\nRequirements: filename: The path to the whl file AND new_py_ver: Create a nightly tag with current date\n"
}{}{
    "source file": "core_test.py",
    "line number": "69",
    "func name": "configure_virtual_cpus",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "core_test2.py",
    "line number": "379",
    "func name": "_get_variable_dict_from_varstore",
    "func arg": "()",
    "comments": ""
}{
    "source file": "core.py",
    "line number": "28",
    "func name": "_status_to_exception",
    "func arg": "(code, message)",
    "comments": ""
}{}{
    "source file": "core2.py",
    "line number": "30",
    "func name": "dense",
    "func arg": "(inputs, kernel, bias, activation, dtype)",
    "comments": "Densely connected NN layer op.\n\nArguments: inputs: `tf.Tensor` or `tf.SparseTensor`. Inputs to operation. kernel: `tf.Variable`. Matrix kernel. bias: (Optional) `tf.Variable`. Bias to add to outputs. activation: (Optional) 1-argument callable. Activation function to apply to outputs. dtype: (Optional) `tf.DType`. Dtype to cast `inputs` to.\n##### Returns\n"
}{
    "source file": "core3.py",
    "line number": "304",
    "func name": "flatten",
    "func arg": "(inputs, name, data_format)",
    "comments": "Flattens an input tensor while preserving the batch axis (axis 0).\n\nArguments: inputs: Tensor input. name: The name of the layer (string). data_format: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, height, width, channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, height, width)`.\n##### Returns\n* **amples**: \n\n"
}{}{}{
    "source file": "correctness_test.py",
    "line number": "43",
    "func name": "multi_input_functional",
    "func arg": "()",
    "comments": "Functional Model that adds its inputs and then adds a bias.\n\n\n"
}{
    "source file": "cos.py",
    "line number": "28",
    "func name": "make_cos_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do cos.\n\n\n"
}{}{
    "source file": "cost_analyzer_tool.py",
    "line number": "80",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "cost_analyzer.py",
    "line number": "52",
    "func name": "GenerateMemoryReport",
    "func arg": "(metagraph, detailed_report, cluster)",
    "comments": "Analyze the peak memory usage for the provided metagraph.\n\n\n##### Args\n* **metagraph**: A TensorFlow MetaGraphDef.\n\n* **detailed_report**: print the live tensors in addition to the peak memory\n  usage.\n\n* **cluster**: Analyze the memory using the specified cluster, or the local\n  machine if no cluster was specified.\n\n##### Returns\n"
}{}{
    "source file": "counter.py",
    "line number": "59",
    "func name": "CounterV1",
    "func arg": "(start, step, dtype)",
    "comments": ""
}{
    "source file": "create_constants.py",
    "line number": "44",
    "func name": "to_h",
    "func arg": "(_, varname, directory)",
    "comments": "Writes a header file for the table values.\n\n\n"
}{
    "source file": "create_python_api_test.py",
    "line number": "35",
    "func name": "deprecated_test_op",
    "func arg": "()",
    "comments": ""
}{
    "source file": "create_python_api.py",
    "line number": "695",
    "func name": "main",
    "func arg": "()",
    "comments": ""
}{
    "source file": "critical_section_ops.py",
    "line number": "89",
    "func name": "_push_critical_section_stack",
    "func arg": "(signature)",
    "comments": "Push a CriticalSection._signature to the thread-local stack.\n\nIf the signature is already on the stack, raise an error because it means we're trying to execute inside the same locked CriticalSection, which will create a deadlock.\n##### Args\n* **signature**: Tuple of the type `CriticalSection._signature`.  Uniquely\n  identifies a CriticalSection by its `shared_name`, `container`,\n  and device.\n\n* **elds**: \n\n"
}{}{
    "source file": "cross_device_ops_test.py",
    "line number": "109",
    "func name": "_make_mirrored_indexed_slices",
    "func arg": "(devices, values, indices, dense_shape)",
    "comments": ""
}{
    "source file": "cross_device_ops.py",
    "line number": "1167",
    "func name": "choose_the_best",
    "func arg": "(devices, session_config)",
    "comments": "Find the best CrossDeviceOps locally given a `tf.compat.v1.ConfigProto`.\n\n\n##### Args\n* **devices**: a list of devices passed to `tf.distribute.Strategy`.\n\n* **session_config**: a `tf.compat.v1.ConfigProto` or `None`. If `None`, it will\n  make decision based on all logical devices.\n\n##### Returns\n"
}{}{
    "source file": "cross_device_utils.py",
    "line number": "972",
    "func name": "_control_input",
    "func arg": "(devices, control_inputs, idx)",
    "comments": "Returns the `idx`-th item in control_inputs to be used in ops.control_dependencies.\n\nThis is a helper function for building collective ops.\n##### Args\n* **devices**: a list of device strings the collective run on.\n\n* **control_inputs**: a list or None.\n\n* **idx**: the index into `inputs` and `control_inputs`.\n\n##### Returns\n"
}{}{
    "source file": "csr_sparse_matrix_dense_mat_mul_grad_test.py",
    "line number": "42",
    "func name": "_add_test",
    "func arg": "(test, op_name, testcase_name, fn)",
    "comments": ""
}{
    "source file": "csr_sparse_matrix_grad_test.py",
    "line number": "41",
    "func name": "_add_test",
    "func arg": "(test, op_name, testcase_name, fn)",
    "comments": ""
}{
    "source file": "csr_sparse_matrix_ops_test.py",
    "line number": "63",
    "func name": "twist_matrix",
    "func arg": "(matrix, permutation_indices)",
    "comments": "Permute the rows and columns of a 2D or (batched) 3D Tensor.\n\n\n"
}{
    "source file": "csr_sparse_matrix_sparse_mat_mul_grad_test.py",
    "line number": "42",
    "func name": "_add_test",
    "func arg": "(test, op_name, testcase_name, fn)",
    "comments": ""
}{}{}{}{}{
    "source file": "ctc_decoder_ops_test.py",
    "line number": "41",
    "func name": "flatten",
    "func arg": "(list_of_lists)",
    "comments": "Flatten one level of nesting.\n\n\n"
}{
    "source file": "ctc_loss_op_test.py",
    "line number": "943",
    "func name": "_ctc_loss_v3",
    "func arg": "(labels, logits, label_length, logit_length, use_gpu)",
    "comments": ""
}{
    "source file": "ctc_ops.py",
    "line number": "1429",
    "func name": "_get_dim",
    "func arg": "(tensor, i)",
    "comments": "Get value of tensor shape[i] preferring static value if available.\n\n\n"
}{
    "source file": "ctl_correctness_test.py",
    "line number": "162",
    "func name": "iteration_outside_func",
    "func arg": "(initial_weights, dataset, optimizer_fn, iteration_type, strategy, sync_batchnorm)",
    "comments": "Helper function to test iterating over data outside a tf.function.\n\n\n"
}{
    "source file": "cuda_compute_capability.py",
    "line number": "237",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{}{}{}{}{}{}{}{
    "source file": "cudnn_rnn_grad.py",
    "line number": "77",
    "func name": "_cudnn_rnn_backwardv3",
    "func arg": "(op)",
    "comments": "Gradients for the CudnnRNNV3 op.\n\n\n"
}{}{
    "source file": "curses_ui_test.py",
    "line number": "43",
    "func name": "codes_to_string",
    "func arg": "(cmd_code)",
    "comments": ""
}{
    "source file": "curses_ui.py",
    "line number": "51",
    "func name": "_get_command_from_line_attr_segs",
    "func arg": "(mouse_x, attr_segs)",
    "comments": "Attempt to extract command from the attribute segments of a line.\n\n\n##### Args\n* **mouse_x**: (int) x coordinate of the mouse event.\n\n* **attr_segs**: (list) The list of attribute segments of a line from a\n  RichTextLines object.\n\n##### Returns\n* **(str or None) If a command exists**: the command as a str; otherwise, None.\n\n"
}{}{}{}{
    "source file": "custom_gradient.py",
    "line number": "547",
    "func name": "grad_pass_through",
    "func arg": "(f)",
    "comments": "Creates a grad-pass-through op with the forward behavior provided in f.\n\nUse this function to wrap any op, maintaining its behavior in the forward pass, but replacing the original op in the backward graph with an identity. For example:\n\n```python x = tf.Variable(1.0, name=\"x\") z = tf.Variable(3.0, name=\"z\")\n\nwith tf.GradientTape() as tape: # y will evaluate to 9.0 y = tf.grad_pass_through(x.assign)(z**2) # grads will evaluate to 6.0 grads = tape.gradient(y, z) ```\n\nAnother example is a 'differentiable' moving average approximation, where gradients are allowed to flow into the last value fed to the moving average, but the moving average is still used for the forward pass:\n\n```python x = ... # Some scalar value # A moving average object, we don't need to know how this is implemented moving_average = MovingAverage() with backprop.GradientTape() as tape: # mavg_x will evaluate to the current running average value mavg_x = tf.grad_pass_through(moving_average)(x) grads = tape.gradient(mavg_x, x) # grads will evaluate to 1.0 ```\n##### Args\n* **f**: function `f(*x)` that returns a `Tensor` or nested structure of `Tensor`\n  outputs.\n\n##### Returns\n"
}{
    "source file": "custom_training_loop_gradient_test.py",
    "line number": "34",
    "func name": "get_dataset_from_tensor_slices",
    "func arg": "(inp_array)",
    "comments": ""
}{
    "source file": "custom_training_loop_input_test.py",
    "line number": "43",
    "func name": "get_dataset_from_tensor_slices",
    "func arg": "(inp_array)",
    "comments": ""
}{}{}{}{
    "source file": "custom_training_loop_test.py",
    "line number": "115",
    "func name": "add_metric_step",
    "func arg": "(defun)",
    "comments": ""
}{
    "source file": "cwise_ops_binary_test.py",
    "line number": "61",
    "func name": "_default_tolerance",
    "func arg": "(dtype)",
    "comments": "Returns a sensible default tolerance for comparing results of a given type.\n\n\n##### Args\n* **dtype**: A datatype.\n\n"
}{
    "source file": "cwise_ops_test.py",
    "line number": "68",
    "func name": "_default_tolerance",
    "func arg": "(dtype)",
    "comments": "Returns a sensible default tolerance for comparing results of a given type.\n\n\n##### Args\n* **dtype**: A datatype.\n\n"
}{
    "source file": "cwise_ops_unary_test.py",
    "line number": "58",
    "func name": "_default_tolerance",
    "func arg": "(dtype)",
    "comments": "Returns a sensible default tolerance for comparing results of a given type.\n\n\n##### Args\n* **dtype**: A datatype.\n\n"
}{}{}{
    "source file": "data_adapter_test.py",
    "line number": "65",
    "func name": "fail_on_convert",
    "func arg": "(x, **kwargs)",
    "comments": ""
}{
    "source file": "data_adapter.py",
    "line number": "1554",
    "func name": "_is_distributed_dataset",
    "func arg": "(ds)",
    "comments": ""
}{}{
    "source file": "data_augmentation.py",
    "line number": "43",
    "func name": "augment_data",
    "func arg": "(original_data, original_label)",
    "comments": "Perform data augmentation.\n\n\n"
}{
    "source file": "data_flow_grad.py",
    "line number": "50",
    "func name": "_DynamicStitchGrads",
    "func arg": "(op, grad)",
    "comments": "Gradients for DynamicStitch and ParallelDynamicStitch.\n\n\n"
}{
    "source file": "data_flow_ops.py",
    "line number": "613",
    "func name": "_shared_name",
    "func arg": "(shared_name)",
    "comments": ""
}{}{}{}{}{
    "source file": "data_prepare.py",
    "line number": "143",
    "func name": "write_data",
    "func arg": "(data_to_write, path)",
    "comments": ""
}{
    "source file": "data_service_ops_test.py",
    "line number": "43",
    "func name": "_make_distributed_dataset",
    "func arg": "(dataset, address, job_name)",
    "comments": "Creates a distributed dataset with a short task refresh interval.\n\n\n"
}{
    "source file": "data_service_ops.py",
    "line number": "260",
    "func name": "distribute",
    "func arg": "(processing_mode, service, job_name, max_outstanding_requests)",
    "comments": "A transformation that moves dataset processing to the tf.data service.\n\nWhen you iterate over a dataset containing the `distribute` transformation, the tf.data service creates a \"job\" which produces data for the dataset iteration.\n\nThe `processing_mode` argument controls what data is produced by a tf.data service job. Currently, the only supported mode is \"parallel_epochs\".\n\nprocessing_mode=\"parallel_epochs\" means that multiple tf.data workers will iterate through the dataset in parallel, each producing all elements of the dataset. For example, if the dataset contains {0, 1, 2}, every tf.data worker used for execution will produce {0, 1, 2}. If there are 3 workers, the job will produce the elements {0, 0, 0, 1, 1, 1, 2, 2, 2} (though not necessarily in that order). To account for this, it is recommended to randomly shuffle your dataset, so that different tf.data workers will iterate through the dataset in different orders.\n\nIn the future, there will be additional processing modes. For example, a \"one_epoch\" mode which partitions the dataset across the tf.data workers, so that the consumers see each element of the dataset only once.\n\n``` dataset = tf.data.Dataset.range(5) dataset = dataset.map(lambda x: x*x) dataset = dataset.apply( tf.data.experimental.service.distribute(\"parallel_epochs\", \"grpc://dataservice:5000\")) dataset = dataset.map(lambda x: x+1)\n\nfor element in dataset: print(element)\n\n# prints { 1, 2, 5, 10, 17 } ```\n\nIn the above example, the first two lines (before the call to `distribute`) will be executed on tf.data workers, and the elements provided over RPC. The remaining transformations (after the call to `distribute`) will be executed locally.\n\nThe `job_name` argument allows jobs to be shared across multiple datasets. Instead of each dataset creating its own job, all datasets with the same `job_name` will consume from the same job. A new job will be created for each iteration of the dataset (with each repetition of `Dataset.repeat` counting as a new iteration). Suppose two training workers (in either a single client or multi-client setup) iterate over the below dataset, and there is a single tf.data worker:\n\n``` range5_dataset = tf.data.Dataset.range(5) dataset = range5_dataset.apply(tf.data.experimental.service.distribute( \"parallel_epochs\", \"grpc://dataservice:5000\", job_name=\"my_job_name\")) for iteration in range(3): print(list(dataset)) ```\n\nThe elements of each job will be split between the two processes, with elements being consumed by the processes on a first-come first-served basis. One possible result is that process 1 prints\n\n``` [0, 2, 4] [0, 1, 3] [1] ```\n\nand process 2 prints\n\n``` [1, 3] [2, 4] [0, 2, 3, 4] ```\n\nJob names must not be re-used across different training jobs within the lifetime of the tf.data service. In general, the tf.data service is expected to live for the duration of a single training job. To use the tf.data service with multiple training jobs, make sure to use different job names to avoid conflicts. For example, suppose a training job calls `distribute` with `job_name=\"job\"` and reads until end of input. If another independent job connects to the same tf.data service and tries to read from `job_name=\"job\"`, it will immediately receive end of input, without getting any data.\n\n**Keras and Distribution Strategies**\n\nThe dataset produced by the `distribute` transformation can be passed to Keras' `Model.fit` or Distribution Strategy's `tf.distribute.Strategy.experimental_distribute_dataset` like any other `tf.data.Dataset`. We recommend setting a `job_name` on the call to `distribute` so that if there are multiple workers, they read data from the same job. Note that the autosharding normally performed by `experimental_distribute_dataset` will be disabled when setting a `job_name`, since sharing the job already results in splitting data across the workers. When using a shared job, data will be dynamically balanced across workers, so that they reach end of input about the same time. This results in better worker utilization than with autosharding, where each worker processes an independent set of files, and some workers may run out of data earlier than others.\n##### Args\n* **processing_mode**: A string specifying the policy for how data should be\n  processed by tf.data workers. Currently, the only supported value is\n  \"parallel_epochs\".\n\n* **service**: A string indicating how to connect to the tf.data service. The\n  string should be in the format protocol\n\n* **job_name**: (Optional.) The name of the job. This argument makes it possible\n  for multiple datasets to share the same job. The default behavior is that\n  the dataset creates anonymous, exclusively owned jobs.\n\n* **max_outstanding_requests**: (Optional.) A limit on how many elements may be\n  requested at the same time. You can use this option to control the amount\n  of memory used, since `distribute` won't use more than `element_size` *\n  `max_outstanding_requests` of memory.\n\n##### Returns\n* **Dataset**: A `Dataset` of the elements produced by the data service.\n\n"
}{}{
    "source file": "data_split_person.py",
    "line number": "41",
    "func name": "person_split",
    "func arg": "(whole_data, train_names, valid_names, test_names)",
    "comments": "Split data by person.\n\n\n"
}{}{
    "source file": "data_split.py",
    "line number": "51",
    "func name": "split_data",
    "func arg": "(data, train_ratio, valid_ratio)",
    "comments": "Splits data into train, validation and test according to ratio.\n\n\n"
}{}{}{
    "source file": "data_structures.py",
    "line number": "348",
    "func name": "_py_list_stack",
    "func arg": "(list_, opts)",
    "comments": "Overload of list_stack that executes a Python list append.\n\n\n"
}{
    "source file": "data_structures1.py",
    "line number": "1070",
    "func name": "_set_tuple_item",
    "func arg": "(list_object, index_string, value)",
    "comments": ""
}{
    "source file": "data_utils_test.py",
    "line number": "122",
    "func name": "create_generator_from_sequence_pcs",
    "func arg": "(ds)",
    "comments": ""
}{
    "source file": "data_utils.py",
    "line number": "930",
    "func name": "next_sample",
    "func arg": "(uid)",
    "comments": "Gets the next value from the generator `uid`.\n\nTo allow multiple generators to be used at the same time, we use `uid` to get a specific one. A single generator would cause the validation to overwrite the training generator.\n\nArguments: uid: int, generator identifier\n##### Returns\n"
}{}{
    "source file": "dataset_ops.py",
    "line number": "4575",
    "func name": "_resource_resolver",
    "func arg": "(op, resource_reads, resource_writes)",
    "comments": "Updates resource inputs for tf.data ops with indirect dependencies.\n\n\n"
}{
    "source file": "dataset_serialization_test_base.py",
    "line number": "41",
    "func name": "remove_variants",
    "func arg": "(get_next_op)",
    "comments": "Remove variants from a nest structure, so sess.run will execute.\n\n\n"
}{}{}{
    "source file": "dataset_utils.py",
    "line number": "201",
    "func name": "check_validation_split_arg",
    "func arg": "(validation_split, subset, shuffle, seed)",
    "comments": "Raise errors in case of invalid argument values.\n\n\n"
}{
    "source file": "dataset.py",
    "line number": "120",
    "func name": "test",
    "func arg": "(directory)",
    "comments": "tf.data.Dataset object for MNIST test data.\n\n\n"
}{}{}{
    "source file": "datasets.py",
    "line number": "50",
    "func name": "StreamingFilesDataset",
    "func arg": "(files, filetype, file_reader_job, worker_job, num_epochs, filename_shuffle_buffer_size, num_parallel_reads, batch_transfer_size, sloppy)",
    "comments": "StreamingFilesDataset constructs a dataset to stream from workers (GCE VM).\n\nBecause Cloud TPUs are allocated over the network, a Cloud TPU cannot read files local to your GCE VM. In order to train using files stored on your local VM (e.g. on local SSD for extreme performance), use the StreamingFilesDataset helper to generate a dataset to feed your Cloud TPU with files from your GCE VM.\n\nThe resulting dataset may return an OutOfRangeError if there are no files found as a result of the fileglob expansion.\n\nNote: StreamingFilesDataset assumes that the session is using a TPUClusterResolver and has therefore a worker and a coordinator job. File loading will be done on the coordinator job.\n##### Args\n* **files**: A string glob to match files, or a `tf.data.Dataset` generating file\n  names.\n\n* **filetype**: A string (one of 'tfrecord', or 'textline') or a single-argument\n  TensorFlow function that when given a filename returns a dataset.\n\n* **file_reader_job**: An optional string that corresponds to the job that should\n  perform the file reads.\n\n* **worker_job**: An optional string that corresponds to the job that should\n  process the tensors (i.e. your GPU or TPU worker).\n\n* **num_epochs**: The number of epochs through the training set that should be\n  generated. By default, it will repeat infinitely.\n\n* **filename_shuffle_buffer_size**: An optional integer whose value controls the\n  shuffling of the file names. If you would like to read from the files in\n  the same order, set to 0 or False.\n\n* **num_parallel_reads**: An optional integer controlling the number of files to\n  read from concurrently. (Set to 1 for no parallelism.)\n\n* **batch_transfer_size**: An optional integer controlling the batching used to\n  amortize the remote function invocation overhead. Set to a very large\n  number to increase throughput. Set to a very small number to reduce memory\n  consumption. Set to False to skip batching.\n\n* **sloppy**: (Optional.) If `False`, read input data while maintaining a\n  deterministic order. (This may have significant performance impacts.)\n  sloppy defaults to\n\n##### Returns\n"
}{
    "source file": "dct_ops_test.py",
    "line number": "123",
    "func name": "_np_dct4",
    "func arg": "(signals, n, norm)",
    "comments": "Computes the DCT-IV manually with NumPy.\n\n\n"
}{
    "source file": "dct_ops.py",
    "line number": "187",
    "func name": "idct",
    "func arg": "(input, type, n, axis, norm, name)",
    "comments": "Computes the 1D [Inverse Discrete Cosine Transform (DCT)][idct] of `input`.\n\nCurrently Types I, II, III, IV are supported. Type III is the inverse of Type II, and vice versa.\n\nNote that you must re-normalize by 1/(2n) to obtain an inverse if `norm` is not `'ortho'`. That is: `signal == idct(dct(signal)) * 0.5 / signal.shape[-1]`. When `norm='ortho'`, we have: `signal == idct(dct(signal, norm='ortho'), norm='ortho')`.\n\n@compatibility(scipy) Equivalent to [scipy.fftpack.idct] (https://docs.scipy.org/doc/scipy-1.4.0/reference/generated/scipy.fftpack.idct.html) for Type-I, Type-II, Type-III and Type-IV DCT. @end_compatibility\n##### Args\n* **input**: A `[..., samples]` `float32`/`float64` `Tensor` containing the\n  signals to take the DCT of.\n\n* **type**: The IDCT type to perform. Must be 1, 2, 3 or 4.\n\n* **n**: For future expansion. The length of the transform. Must be `None`.\n\n* **axis**: For future expansion. The axis to compute the DCT along. Must be `-1`.\n\n* **norm**: The normalization to apply. `None` for no normalization or `'ortho'`\n  for orthonormal normalization.\n\n* **name**: An optional name for the operation.\n\n##### Returns\n"
}{}{
    "source file": "debug_data.py",
    "line number": "257",
    "func name": "device_path_to_device_name",
    "func arg": "(device_dir)",
    "comments": "Parse device name from device path.\n\n\n##### Args\n* **device_dir**: (str) a directory name for the device.\n\n##### Returns\n"
}{
    "source file": "debug_errors.py",
    "line number": "32",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{}{}{
    "source file": "debug_events_reader.py",
    "line number": "891",
    "func name": "_execution_from_debug_event_proto",
    "func arg": "(debug_event, locator)",
    "comments": "Convert a DebugEvent proto into an Execution data object.\n\n\n"
}{}{}{
    "source file": "debug_fibonacci_v2.py",
    "line number": "33",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "debug_fibonacci.py",
    "line number": "34",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{}{
    "source file": "debug_gradients.py",
    "line number": "372",
    "func name": "gradient_values_from_dump",
    "func arg": "(grad_debugger, x_tensor, dump)",
    "comments": "Find gradient values from a `DebugDumpDir` object.\n\n\n##### Args\n* **grad_debugger**: the `tf_debug.GradientsDebugger` instance to be used.\n\n* **x_tensor**: (`tf.Tensor`, `tf.Variable` or `str`) The x-tensor object or its\n  name. x-tensor refers to the independent `tf.Tensor`, i.e., the tensor\n  on the denominator of the differentiation.\n\n* **dump**: A `tfdbg.DebugDumpDir` object.\n\n##### Returns\n* **If this `GradientsDebugger` instance has the gradient tensor of `x_tensor`\n  registered**: a list of `numpy.ndarray` representing the value of the\n  gradient tensor from `dump`. The list could be empty, if the gradient\n  tensor is not executed in the `tf.Session.run()` call that generated\n  the `dump`. The list could also contain multiple values of the gradient\n  tensor, e.g., if gradient tensor is computed repeatedly in a\n  `tf.while_loop` during the run that generated the `dump`.\n\n"
}{}{}{
    "source file": "debug_graphs.py",
    "line number": "481",
    "func name": "reconstruct_non_debug_graph_def",
    "func arg": "(debug_graph_def)",
    "comments": "Reconstruct original (non-debugger-decorated) partition GraphDef.\n\nThis method strips the input `tf.compat.v1.GraphDef` of the Copy* and Debug*-type nodes inserted by the debugger.\n\nThe reconstructed partition graph is identical to the original (i.e., non-debugger-decorated) partition graph except in the following respects: 1) The exact names of the runtime-inserted internal nodes may differ. These include _Send, _Recv, _HostSend, _HostRecv, _Retval ops. 2) As a consequence of 1, the nodes that receive input directly from such send- and recv-type ops will have different input names. 3) The parallel_iteration attribute of while-loop Enter ops are set to 1.\n##### Args\n* **debug_graph_def**: The debugger-decorated `tf.compat.v1.GraphDef`, with the\n  debugger-inserted Copy* and Debug* nodes.\n\n##### Returns\n"
}{
    "source file": "debug_grappler_test.py",
    "line number": "37",
    "func name": "_grappler_enabled_session_config",
    "func arg": "()",
    "comments": "Constructs a Session config proto that explicitly enables Grappler.\n\n\n##### Returns\n"
}{}{
    "source file": "debug_keras.py",
    "line number": "33",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "debug_mnist_v1.py",
    "line number": "112",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "debug_mnist_v2.py",
    "line number": "125",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "debug_service_pb2_grpc.py",
    "line number": "91",
    "func name": "add_EventListenerServicer_to_server",
    "func arg": "(servicer, server)",
    "comments": ""
}{
    "source file": "debug_tflearn_iris.py",
    "line number": "33",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{}{
    "source file": "debug_utils.py",
    "line number": "202",
    "func name": "watch_graph_with_blacklists",
    "func arg": "(run_options, graph, debug_ops, debug_urls, node_name_regex_blacklist, op_type_regex_blacklist, tensor_dtype_regex_blacklist, tolerate_debug_op_creation_failures, global_step, reset_disk_byte_usage)",
    "comments": "Add debug tensor watches, blacklisting nodes and op types.\n\nThis is similar to `watch_graph()`, but the node names and op types are blacklisted, instead of allowlisted.\n\nN.B.: 1. Under certain circumstances, the `Tensor` may not get actually watched (e.g., if the node of the `Tensor` is constant-folded during runtime). 2. For debugging purposes, the `parallel_iteration` attribute of all `tf.while_loop`s in the graph are set to 1 to prevent any node from being executed multiple times concurrently. This change does not affect subsequent non-debugged runs of the same `tf.while_loop`s.\n##### Args\n* **run_options**: An instance of `config_pb2.RunOptions` to be modified.\n\n* **graph**: An instance of `ops.Graph`.\n\n* **debug_ops**: (`str` or `list` of `str`) name(s) of the debug op(s) to use.\n  See the documentation of `watch_graph` for more details.\n\n* **debug_urls**: URL(s) to send debug values to, e.g.,\n  `file\n\n* **node_name_regex_blacklist**: Regular-expression blacklist for node_name.\n  This should be a string, e.g., `\"(weight_[0-9]+|bias_.*)\"`.\n\n* **op_type_regex_blacklist**: Regular-expression blacklist for the op type of\n  nodes, e.g., `\"(Variable|Add)\"`.\n  If both node_name_regex_blacklist and op_type_regex_blacklist\n  are set, the two filtering operations will occur in a logical `OR`\n  relation. In other words, a node will be excluded if it hits either of\n  the two blacklists; a node will be included if and only if it hits\n  neither of the blacklists.\n\n* **tensor_dtype_regex_blacklist**: Regular-expression blacklist for Tensor\n  data type, e.g., `\"^int.*\"`.\n  This blacklist operates in logical `OR` relations to the two allowlists\n  above.\n\n* **tolerate_debug_op_creation_failures**: (`bool`) whether debug op creation\n  failures (e.g., due to dtype incompatibility) are to be tolerated by not\n  throwing exceptions.\n\n* **global_step**: (`int`) Optional global_step count for this debug tensor\n  watch.\n\n* **reset_disk_byte_usage**: (`bool`) whether to reset the tracked disk byte\n  usage to zero (default\n\n"
}{}{}{
    "source file": "debugger_cli_common.py",
    "line number": "434",
    "func name": "wrap_rich_text_lines",
    "func arg": "(inp, cols)",
    "comments": "Wrap RichTextLines according to maximum number of columns.\n\nProduces a new RichTextLines object with the text lines, font_attr_segs and annotations properly wrapped. This ought to be used sparingly, as in most cases, command handlers producing RichTextLines outputs should know the screen/panel width via the screen_info kwarg and should produce properly length-limited lines in the output accordingly.\n##### Args\n* **inp**: Input RichTextLines object.\n\n* **cols**: Number of columns, as an int.\n\n##### Returns\n* **2) A list of new (wrapped) line index. For example, if the original input\n  consists of three lines and only the second line is wrapped, and it's\n  wrapped into two lines, this return value will be**: [0, 1, 3].\n\n"
}{}{}{}{}{}{}{}{}{}{
    "source file": "decorator_utils_test.py",
    "line number": "29",
    "func name": "_test_function",
    "func arg": "(unused_arg)",
    "comments": ""
}{
    "source file": "decorator_utils.py",
    "line number": "117",
    "func name": "validate_callable",
    "func arg": "(func, decorator_name)",
    "comments": ""
}{
    "source file": "decorators.py",
    "line number": "41",
    "func name": "functional_decorator",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "def_function_test.py",
    "line number": "50",
    "func name": "undecorated_function",
    "func arg": "(x)",
    "comments": ""
}{}{}{
    "source file": "def_function.py",
    "line number": "1206",
    "func name": "function",
    "func arg": "(func, input_signature, autograph, experimental_implements, experimental_autograph_options, experimental_relax_shapes, experimental_compile)",
    "comments": "Compiles a function into a callable TensorFlow graph.\n\n`tf.function` constructs a callable that executes a TensorFlow graph (`tf.Graph`) created by trace-compiling the TensorFlow operations in `func`, effectively executing `func` as a TensorFlow graph.\n\nExample usage:\n\n>>> @tf.function ... def f(x, y): ...\n\n return x ** 2 + y >>> x = tf.constant([2, 3]) >>> y = tf.constant([3, -2]) >>> f(x, y) <tf.Tensor: ... numpy=array([7, 7], ...)>\n\n_Features_\n\n`func` may use data-dependent control flow, including `if`, `for`, `while` `break`, `continue` and `return` statements:\n\n>>> @tf.function ... def f(x): ...\n\n if tf.reduce_sum(x) > 0: ...\n\n\n\n return x * x ...\n\n else: ...\n\n\n\n return -x // 2 >>> f(tf.constant(-2)) <tf.Tensor: ... numpy=1>\n\n`func`'s closure may include `tf.Tensor` and `tf.Variable` objects:\n\n>>> @tf.function ... def f(): ...\n\n return x ** 2 + y >>> x = tf.constant([-2, -3]) >>> y = tf.Variable([3, -2]) >>> f() <tf.Tensor: ... numpy=array([7, 7], ...)>\n\n`func` may also use ops with side effects, such as `tf.print`, `tf.Variable` and others:\n\n>>> v = tf.Variable(1) >>> @tf.function ... def f(x): ...\n\n for i in tf.range(x): ...\n\n\n\n v.assign_add(i) >>> f(3) >>> v <tf.Variable ... numpy=4>\n\nImportant: Any Python side-effects (appending to a list, printing with `print`, etc) will only happen once, when `func` is traced. To have side-effects executed into your `tf.function` they need to be written as TF ops:\n\n>>> l = [] >>> @tf.function ... def f(x): ...\n\n for i in x: ...\n\n\n\n l.append(i + 1)\n\n\n\n# Caution! Will only happen once when tracing >>> f(tf.constant([1, 2, 3])) >>> l [<tf.Tensor ...>]\n\nInstead, use TensorFlow collections like `tf.TensorArray`:\n\n>>> @tf.function ... def f(x): ...\n\n ta = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True) ...\n\n for i in range(len(x)): ...\n\n\n\n ta = ta.write(i, x[i] + 1) ...\n\n return ta.stack() >>> f(tf.constant([1, 2, 3])) <tf.Tensor: ..., numpy=array([2, 3, 4], ...)>\n\n_`tf.function` is polymorphic_\n\nInternally, `tf.function` can build more than one graph, to support arguments with different data types or shapes, since TensorFlow can build more efficient graphs that are specialized on shapes and dtypes. `tf.function` also treats any pure Python value as opaque objects, and builds a separate graph for each set of Python arguments that it encounters.\n\nTo obtain an individual graph, use the `get_concrete_function` method of the callable created by `tf.function`. It can be called with the same arguments as `func` and returns a special `tf.Graph` object:\n\n>>> @tf.function ... def f(x): ...\n\n return x + 1 >>> isinstance(f.get_concrete_function(1).graph, tf.Graph) True\n\nCaution: Passing python scalars or lists as arguments to `tf.function` will always build a new graph. To avoid this, pass numeric arguments as Tensors whenever possible:\n\n>>> @tf.function ... def f(x): ...\n\n return tf.abs(x) >>> f1 = f.get_concrete_function(1) >>> f2 = f.get_concrete_function(2)\n\n# Slow\n\n- builds new graph >>> f1 is f2 False >>> f1 = f.get_concrete_function(tf.constant(1)) >>> f2 = f.get_concrete_function(tf.constant(2))\n\n# Fast\n\n- reuses f1 >>> f1 is f2 True\n\nPython numerical arguments should only be used when they take few distinct values, such as hyperparameters like the number of layers in a neural network.\n\n_Input signatures_\n\nFor Tensor arguments, `tf.function` instantiates a separate graph for every unique set of input shapes and datatypes. The example below creates two separate graphs, each specialized to a different shape:\n\n>>> @tf.function ... def f(x): ...\n\n return x + 1 >>> vector = tf.constant([1.0, 1.0]) >>> matrix = tf.constant([[3.0]]) >>> f.get_concrete_function(vector) is f.get_concrete_function(matrix) False\n\nAn \"input signature\" can be optionally provided to `tf.function` to control the graphs traced. The input signature specifies the shape and type of each Tensor argument to the function using a `tf.TensorSpec` object. More general shapes can be used. This is useful to avoid creating multiple graphs when Tensors have dynamic shapes. It also restricts the shape and datatype of Tensors that can be used:\n\n>>> @tf.function( ...\n\n\n\n input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)]) ... def f(x): ...\n\n return x + 1 >>> vector = tf.constant([1.0, 1.0]) >>> matrix = tf.constant([[3.0]]) >>> f.get_concrete_function(vector) is f.get_concrete_function(matrix) True\n\n_Variables may only be created once_\n\n`tf.function` only allows creating new `tf.Variable` objects when it is called for the first time:\n\n>>> class MyModule(tf.Module): ...\n\n def __init__(self): ...\n\n\n\n self.v = None ... ...\n\n @tf.function ...\n\n def __call__(self, x): ...\n\n\n\n if self.v is None: ...\n\n\n\n\n\n self.v = tf.Variable(tf.ones_like(x)) ...\n\n\n\n return self.v * x\n\nIn general, it is recommended to create stateful objects like `tf.Variable` outside of `tf.function` and passing them as arguments.\n##### Args\n* **func**: the function to be compiled. If `func` is None, `tf.function` returns\n  a decorator that can be invoked with a single argument - `func`. In other\n  words, `tf.function(input_signature=...)(func)` is equivalent to\n  `tf.function(func, input_signature=...)`. The former can be used as\n  decorator.\n\n* **input_signature**: A possibly nested sequence of `tf.TensorSpec` objects\n  specifying the shapes and dtypes of the Tensors that will be supplied to\n  this function. If `None`, a separate function is instantiated for each\n  inferred input signature.  If input_signature is specified, every input to\n  `func` must be a `Tensor`, and `func` cannot accept `**kwargs`.\n\n* **autograph**: Whether autograph should be applied on `func` before tracing a\n  graph. Data-dependent control flow requires `autograph=True`. For more\n  information, see the [tf.function and AutoGraph guide](\n  https\n\n* **experimental_implements**: If provided, contains a name of a \"known\" function\n  this implements. For example \"mycompany.my_recurrent_cell\".\n  This is stored as an attribute in inference function,\n  which can then be detected when processing serialized function.\n  See [standardizing composite ops](https\n\n* **experimental_autograph_options**: Optional tuple of\n  `tf.autograph.experimental.Feature` values.\n\n* **experimental_relax_shapes**: When True, `tf.function` may generate fewer,\n  graphs that are less specialized on input shapes.\n\n* **experimental_compile**: If True, the function is always compiled by\n  [XLA](https\n\n##### Returns\n"
}{
    "source file": "default_gradient.py",
    "line number": "68",
    "func name": "supports_default_grad",
    "func arg": "(t)",
    "comments": "Whether tensor `t` supports creating a default gradient.\n\nThis function assumes that `t` is of a trainable type.\n##### Args\n* **t**: Tensor\n\n##### Returns\n"
}{
    "source file": "deferred_sequential_test.py",
    "line number": "206",
    "func name": "get_model",
    "func arg": "()",
    "comments": ""
}{
    "source file": "defun_export.py",
    "line number": "33",
    "func name": "test_defun",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "dense_attention.py",
    "line number": "498",
    "func name": "_merge_masks",
    "func arg": "(x, y)",
    "comments": ""
}{
    "source file": "dense_features_test.py",
    "line number": "46",
    "func name": "_initialized_session",
    "func arg": "(config)",
    "comments": ""
}{
    "source file": "dense_features_v2_test.py",
    "line number": "42",
    "func name": "_initialized_session",
    "func arg": "(config)",
    "comments": ""
}{}{}{
    "source file": "dense_layer_test.py",
    "line number": "45",
    "func name": "InLabels",
    "func arg": "(labels, substr)",
    "comments": "Returns true iff one of the labels contains substr.\n\n\n"
}{
    "source file": "dense_to_ragged_batch_test.py",
    "line number": "98",
    "func name": "_to_list",
    "func arg": "(v)",
    "comments": ""
}{}{}{}{}{
    "source file": "densenet.py",
    "line number": "366",
    "func name": "decode_predictions",
    "func arg": "(preds, top)",
    "comments": ""
}{
    "source file": "deploy_mnist_cnn.py",
    "line number": "47",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{}{}{
    "source file": "deprecation.py",
    "line number": "610",
    "func name": "silence",
    "func arg": "()",
    "comments": "Temporarily silence deprecation warnings.\n\n\n"
}{
    "source file": "depth_to_space.py",
    "line number": "27",
    "func name": "make_depth_to_space_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do depth_to_space.\n\n\n"
}{}{
    "source file": "depthwise_conv_op_test.py",
    "line number": "117",
    "func name": "CheckGradConfigsToTest",
    "func arg": "()",
    "comments": "Iterator for different convolution shapes, strides and paddings.\n\ncompute_gradient_error() is very expensive. So the configs should be relatively small.\n\nYields: Tuple (input_size, filter_size, out_size, stride, padding), the depthwise convolution parameters.\n"
}{
    "source file": "depthwise_conv_op_test1.py",
    "line number": "234",
    "func name": "CheckGradConfigsToTestExplicit",
    "func arg": "()",
    "comments": "Iterator for different convolution shapes, strides and explicit paddings.\n\ncompute_gradient_error() is very expensive. So the configs should be relatively small.\n##### Returns\n"
}{
    "source file": "depthwiseconv.py",
    "line number": "28",
    "func name": "make_depthwiseconv_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do convolution.\n\n\n"
}{}{}{}{}{
    "source file": "device_assignment.py",
    "line number": "316",
    "func name": "device_assignment",
    "func arg": "(topology, computation_shape, computation_stride, num_replicas)",
    "comments": "Computes a device_assignment of a computation across a TPU topology.\n\nAttempts to choose a compact grid of cores for locality.\n##### Args\n* **topology**: A `Topology` object that describes the TPU cluster topology.\n  To obtain a TPU topology, evaluate the `Tensor` returned by\n  `initialize_system` using `Session.run`. Either a serialized\n  `TopologyProto` or a `Topology` object may be passed. Note\n\n* **computation_shape**: A rank 1 int32 numpy array with size equal to the\n  topology rank, describing the shape of the computation's block of cores.\n  If None, the `computation_shape` is `[1] * topology_rank`.\n\n* **computation_stride**: A rank 1 int32 numpy array of size `topology_rank`,\n  describing the inter-core spacing of the `computation_shape` cores in the\n  TPU topology. If None, the `computation_stride` is `[1] * topology_rank`.\n\n* **num_replicas**: The number of computation replicas to run. The replicas will\n  be packed into the free spaces of the topology.\n\n##### Returns\n"
}{
    "source file": "device_compatibility_check_test.py",
    "line number": "29",
    "func name": "device_details",
    "func arg": "(device_name, compute_capability)",
    "comments": ""
}{
    "source file": "device_compatibility_check.py",
    "line number": "135",
    "func name": "log_device_compatibility_check",
    "func arg": "(policy_name)",
    "comments": "Logs a compatibility check if the devices support the policy.\n\nCurrently only logs for the policy mixed_float16. A log is shown only the first time this function is called.\n##### Args\n* **policy_name**: The name of the dtype policy.\n\n"
}{
    "source file": "device_context.py",
    "line number": "21",
    "func name": "enclosing_tpu_context",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "device_lib.py",
    "line number": "25",
    "func name": "list_local_devices",
    "func arg": "(session_config)",
    "comments": "List the available devices available in the local process.\n\n\n##### Args\n* **session_config**: a session config proto or None to use the default config.\n\n##### Returns\n"
}{}{}{
    "source file": "device_setter.py",
    "line number": "137",
    "func name": "replica_device_setter",
    "func arg": "(ps_tasks, ps_device, worker_device, merge_devices, cluster, ps_ops, ps_strategy)",
    "comments": "Return a `device function` to use when building a Graph for replicas.\n\nDevice Functions are used in `with tf.device(device_function):` statement to automatically assign devices to `Operation` objects as they are constructed, Device constraints are added from the inner-most context first, working outwards. The merging behavior adds constraints to fields that are yet unset by a more inner context. Currently the fields are (job, task, cpu/gpu).\n\nIf `cluster` is `None`, and `ps_tasks` is 0, the returned function is a no-op. Otherwise, the value of `ps_tasks` is derived from `cluster`.\n\nBy default, only Variable ops are placed on ps tasks, and the placement strategy is round-robin over all ps tasks. A custom `ps_strategy` may be used to do more intelligent placement, such as `tf.contrib.training.GreedyLoadBalancingStrategy`.\n\nFor example,\n\n```python # To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker # jobs on hosts worker0, worker1 and worker2. cluster_spec = { \"ps\": [\"ps0:2222\", \"ps1:2222\"], \"worker\": [\"worker0:2222\", \"worker1:2222\", \"worker2:2222\"]} with tf.device(tf.compat.v1.train.replica_device_setter(cluster=cluster_spec)): # Build your graph v1 = tf.Variable(...)\n\n# assigned to /job:ps/task:0 v2 = tf.Variable(...)\n\n# assigned to /job:ps/task:1 v3 = tf.Variable(...)\n\n# assigned to /job:ps/task:0 # Run compute ```\n##### Args\n* **ps_tasks**: Number of tasks in the `ps` job.  Ignored if `cluster` is\n  provided.\n\n* **ps_device**: String.  Device of the `ps` job.  If empty no `ps` job is used.\n  Defaults to `ps`.\n\n* **worker_device**: String.  Device of the `worker` job.  If empty no `worker`\n  job is used.\n\n* **merge_devices**: `Boolean`. If `True`, merges or only sets a device if the\n  device constraint is completely unset. merges device specification rather\n  than overriding them.\n\n* **cluster**: `ClusterDef` proto or `ClusterSpec`.\n\n* **ps_ops**: List of strings representing `Operation` types that need to be\n  placed on `ps` devices.  If `None`, defaults to `STANDARD_PS_OPS`.\n\n* **ps_strategy**: A callable invoked for every ps `Operation` (i.e. matched by\n  `ps_ops`), that takes the `Operation` and returns the ps task index to\n  use.  If `None`, defaults to a round-robin strategy across all `ps`\n  devices.\n\n##### Returns\n"
}{}{
    "source file": "device_spec.py",
    "line number": "42",
    "func name": "_as_device_str_or_none",
    "func arg": "(device_type)",
    "comments": ""
}{}{}{
    "source file": "device_util.py",
    "line number": "130",
    "func name": "local_devices_from_num_gpus",
    "func arg": "(num_gpus)",
    "comments": "Returns device strings for local GPUs or CPU.\n\n\n"
}{
    "source file": "device.py",
    "line number": "67",
    "func name": "merge_device",
    "func arg": "(spec)",
    "comments": "Returns a device function that merges devices specifications.\n\nThis can be used to merge partial specifications of devices. The innermost setting for a device field takes precedence. For example:\n\nwith tf.device(merge_device(\"/device:GPU:0\")) # Nodes created here have device \"/device:GPU:0\" with tf.device(merge_device(\"/job:worker\")): # Nodes created here have device \"/job:worker/device:GPU:0\" with tf.device(merge_device(\"/device:CPU:0\")): # Nodes created here have device \"/job:worker/device:CPU:0\" with tf.device(merge_device(\"/job:ps\")): # Nodes created here have device \"/job:ps/device:CPU:0\"\n##### Args\n* **spec**: A `DeviceSpec` or a device spec string (partially) describing the\n  device that should be used for all nodes created in the scope of\n  the returned device function's with block.\n\n##### Returns\n"
}{
    "source file": "diag_op_test.py",
    "line number": "323",
    "func name": "all_tests",
    "func arg": "(align)",
    "comments": ""
}{}{}{
    "source file": "directives1.py",
    "line number": "50",
    "func name": "set_loop_options",
    "func arg": "(parallel_iterations, swap_memory, maximum_iterations, shape_invariants)",
    "comments": "Specifies additional arguments to be passed to the enclosing while_loop.\n\nThe parameters apply to and only to the immediately enclosing loop. It only has effect if the loop is staged as a TF while_loop; otherwise the parameters have no effect.\n\nUsage:\n\n>>> @tf.function(autograph=True) ... def f(): ...\n\n n = 0 ...\n\n for i in tf.range(10): ...\n\n\n\n tf.autograph.experimental.set_loop_options(maximum_iterations=3) ...\n\n\n\n n += 1 ...\n\n return n\n\n>>> @tf.function(autograph=True) ... def f(): ...\n\n v = tf.constant((0,)) ...\n\n for i in tf.range(3): ...\n\n\n\n tf.autograph.experimental.set_loop_options( ...\n\n\n\n\n\n\n\n shape_invariants=[(v, tf.TensorShape([None]))] ...\n\n\n\n ) ...\n\n\n\n v = tf.concat((v, [i]), 0) ...\n\n return v\n\nAlso see tf.while_loop.\n##### Args\n* **parallel_iterations**: The maximum number of iterations allowed to run in\n    parallel at any given time. Note that this does not guarantee parallel\n    execution.\n\n* **swap_memory**: Whether to store intermediate values needed for\n    gradients on the CPU instead of GPU.\n\n* **maximum_iterations**: Allows limiting the total number of iterations executed\n    by the loop.\n\n* **shape_invariants**: Allows controlling the argument with the same name passed\n    to tf.while_loop. Unlike tf.while_loop, this is a list of\n    `(tensor, shape)` pairs.\n\n"
}{
    "source file": "directives2.py",
    "line number": "180",
    "func name": "transform",
    "func arg": "(node, ctx)",
    "comments": ""
}{}{}{
    "source file": "dirichlet_test.py",
    "line number": "35",
    "func name": "try_import",
    "func arg": "(name)",
    "comments": ""
}{
    "source file": "dirichlet.py",
    "line number": "339",
    "func name": "_kl_dirichlet_dirichlet",
    "func arg": "(d1, d2, name)",
    "comments": "Batchwise KL divergence KL(d1 || d2) with d1 and d2 Dirichlet.\n\n\n##### Args\n* **d1**: instance of a Dirichlet distribution object.\n\n* **d2**: instance of a Dirichlet distribution object.\n\n* **name**: (optional) Name to use for created operations.\n  default is \"kl_dirichlet_dirichlet\".\n\n##### Returns\n"
}{}{}{}{}{}{}{
    "source file": "dispatch_test.py",
    "line number": "45",
    "func name": "test_op",
    "func arg": "(x, y, z)",
    "comments": "A fake op for testing dispatch of Python ops.\n\n\n"
}{
    "source file": "dispatch.py",
    "line number": "196",
    "func name": "add_dispatch_support",
    "func arg": "(target)",
    "comments": "Decorator that adds a dispatch handling wrapper to an op.\n\n\n"
}{}{}{
    "source file": "distribute_coordinator_context.py",
    "line number": "26",
    "func name": "get_current_worker_context",
    "func arg": "()",
    "comments": "Returns the current task context.\n\n\n"
}{
    "source file": "distribute_coordinator_test.py",
    "line number": "77",
    "func name": "_strip_protocol",
    "func arg": "(target)",
    "comments": ""
}{
    "source file": "distribute_coordinator.py",
    "line number": "631",
    "func name": "run_distribute_coordinator",
    "func arg": "(worker_fn, strategy, eval_fn, eval_strategy, mode, cluster_spec, task_type, task_id, session_config, rpc_layer)",
    "comments": "Runs the coordinator for distributed TensorFlow.\n\nThis function runs a split coordinator for distributed TensorFlow in its default mode, i.e the STANDALONE_CLIENT mode. Given a `cluster_spec` specifying server addresses and their roles in a cluster, this coordinator will figure out how to set them up, give the underlying function the right targets for master sessions via a scope object and coordinate their training. The cluster consisting of standard servers needs to be brought up either with the standard server binary or with a binary running distribute coordinator with `task_type` set to non-client type which will then turn into standard servers.\n\nIn addition to be the distribute coordinator, this is also the source of configurations for each job in the distributed training. As there are multiple ways to configure a distributed TensorFlow cluster, its context object provides these configurations so that users or higher-level APIs don't have to figure out the configuration for each job by themselves.\n\nIn the between-graph replicated training, this coordinator will create multiple threads and each calls the `worker_fn` which is supposed to create its own graph and connect to one worker master given by its context object. In the in-graph replicated training, it has only one thread calling this `worker_fn`.\n\nAnother mode is the INDEPENDENT_WORKER mode where each server runs a distribute coordinator which will start a standard server and optionally runs `worker_fn` depending whether it is between-graph training or in-graph replicated training.\n\nThe `strategy` object is expected to be a DistributionStrategy object which has implemented methods needed by distributed coordinator such as `configure(session_config, cluster_spec, task_type, task_id)` which configures the strategy object for a specific task and `experimental_should_init` property which instructs the distribute coordinator whether to run init ops for a task. The distribute coordinator will make a copy of the `strategy` object, call its `configure` method and pass it to `worker_fn` as an argument.\n\nThe `worker_fn` defines the training logic and is called under its own worker context which can be accessed to via `get_current_worker_context`. A worker context provides access to configurations for each task, e.g. the task_type, task_id, master target and so on. Since `worker_fn` will be called in a thread and possibly multiple times, caller should be careful when it accesses global data. For example, it is unsafe to define flags in a `worker_fn` or to define different environment variables for different `worker_fn`s.\n\nThe `worker_fn` for the between-graph replication is defined as if there is only one worker corresponding to the `worker_fn` and possibly ps jobs. For example, when training with parameter servers, it assigns variables to parameter servers and all other operations to that worker. In the in-graph replication case, the `worker_fn` has to define operations for all worker jobs. Using a distribution strategy can simplify the `worker_fn` by not having to worry about the replication and device assignment of variables and operations.\n\nThis method is intended to be invoked by high-level APIs so that users don't have to explicitly call it to run this coordinator. For those who don't use high-level APIs, to change a program to use this coordinator, wrap everything in a the program after global data definitions such as commandline flag definition into the `worker_fn` and get task-specific configurations from the worker context.\n\nThe `cluster_spec` can be either passed by the argument or parsed from the \"TF_CONFIG\" environment variable. Example of a TF_CONFIG: ``` cluster = {'chief': ['host0:2222'], 'ps': ['host1:2222', 'host2:2222'], 'worker': ['host3:2222', 'host4:2222', 'host5:2222']} os.environ['TF_CONFIG'] = json.dumps({'cluster': cluster}) ```\n\nIf `cluster_spec` is not given in any format, it becomes local training and this coordinator will connect to a local session.\n\nFor evaluation, if \"evaluator\" exists in the cluster_spec, a separate thread will be created to call `eval_fn` with its `task_type` set to \"evaluator\". If `eval_fn` is not defined, fall back to `worker_fn`. This implies that evaluation will be done on a single machine if there is an \"evaluator\" task. If \"evaluator\" doesn't exist in the cluster_spec, it entirely depends on the `worker_fn` for how to do evaluation.\n##### Args\n* **worker_fn**: the function to be called. The function should accept a\n  `strategy` object and will be given access to a context object via a\n  context manager scope.\n\n* **strategy**: a DistributionStrategy object specifying whether it should\n  run between-graph replicated training or not, whether to run init ops,\n  etc. This object will also be configured given `session_config`,\n  `cluster_spec`, `task_type` and `task_id`.\n\n* **eval_fn**: optional function for \"evaluator\" task. If `eval_fn` is not passed\n  in but a \"evaluator\" task is found in the `cluster_spec`, the `worker_fn`\n  will be used for this task.\n\n* **eval_strategy**: optional DistributionStrategy object for \"evaluator\" task.\n\n* **mode**: in which mode this distribute coordinator runs.\n\n* **cluster_spec**: a dict, ClusterDef or ClusterSpec specifying servers and roles\n  in a cluster. If not set or empty, fall back to local training.\n\n* **task_type**: the current task type, optional if this is a client.\n\n* **task_id**: the current task id, optional if this is a client.\n\n* **session_config**: an optional `tf.compat.v1.ConfigProto` object which will be\n  passed to `strategy`'s `configure` method and used to create a session.\n\n* **rpc_layer**: optional string, the protocol for RPC, e.g. \"grpc\".\n\n##### Returns\n"
}{
    "source file": "distribute_lib_test.py",
    "line number": "140",
    "func name": "_run_in_and_out_of_scope",
    "func arg": "(unbound_test_method)",
    "comments": ""
}{
    "source file": "distribute_lib.py",
    "line number": "3263",
    "func name": "_from_proto_fn",
    "func arg": "(v, import_scope)",
    "comments": ""
}{}{
    "source file": "distribute_strategy_test.py",
    "line number": "2336",
    "func name": "_functional_with_layer_reuse",
    "func arg": "(input_shape, num_classes, l1, l2)",
    "comments": ""
}{
    "source file": "distribute_utils_test.py",
    "line number": "38",
    "func name": "_nested_value",
    "func arg": "(d)",
    "comments": ""
}{
    "source file": "distribute_utils.py",
    "line number": "248",
    "func name": "create_mirrored_variable",
    "func arg": "(strategy, real_mirrored_creator, mirrored_cls, sync_on_read_cls, **kwargs)",
    "comments": ""
}{
    "source file": "distribute.py",
    "line number": "132",
    "func name": "replicate",
    "func arg": "(dataset, devices)",
    "comments": "A transformation that replicates `dataset` onto a list of devices.\n\n\n##### Args\n* **dataset**: A `tf.data.Dataset` object.\n\n* **devices**: A list of devices to replicate the dataset on.\n\n##### Returns\n"
}{}{}{}{
    "source file": "distributed_file_utils.py",
    "line number": "142",
    "func name": "remove_temp_dir_with_filepath",
    "func arg": "(filepath, strategy)",
    "comments": "Removes the temp path for file after writing is finished.\n\n\n##### Args\n* **filepath**: Original filepath that would be used without distribution.\n\n* **strategy**: The tf.distribute strategy object currently used.\n\n"
}{}{
    "source file": "distributed_training_utils.py",
    "line number": "1195",
    "func name": "concat_along_batch_dimension",
    "func arg": "(outputs)",
    "comments": "Concats prediction outputs along the batch dimension.\n\n\n"
}{
    "source file": "distribution_strategy_context.py",
    "line number": "342",
    "func name": "_get_default_replica_mode",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "distribution.py",
    "line number": "132",
    "func name": "_convert_to_tensor",
    "func arg": "(value, name, preferred_dtype)",
    "comments": "Converts to tensor avoiding an eager bug that loses float precision.\n\n\n"
}{}{}{}{
    "source file": "dlpack_test.py",
    "line number": "44",
    "func name": "GetNamedTestParameters",
    "func arg": "()",
    "comments": ""
}{
    "source file": "dlpack.py",
    "line number": "49",
    "func name": "from_dlpack",
    "func arg": "(dlcapsule)",
    "comments": "Returns the Tensorflow eager tensor.\n\nThe returned tensor uses the memory shared by dlpack capsules from other framework.\n\n```python a = tf.experimental.dlpack.from_dlpack(dlcapsule) # `a` uses the memory shared by dlpack ```\n##### Args\n* **dlcapsule**: A PyCapsule named as dltensor\n\n##### Returns\n"
}{}{}{}{}{
    "source file": "doc_controls.py",
    "line number": "264",
    "func name": "should_skip_class_attr",
    "func arg": "(cls, name)",
    "comments": "Returns true if docs should be skipped for this class attribute.\n\n\n##### Args\n* **cls**: The class the attribute belongs to.\n\n* **name**: The name of the attribute.\n\n##### Returns\n"
}{}{}{
    "source file": "doc_srcs.py",
    "line number": "79",
    "func name": "get_doc_sources",
    "func arg": "(api_name)",
    "comments": "Get a map from module to a DocSource object.\n\n\n##### Args\n* **api_name**: API you want to generate (e.g. `tensorflow` or `estimator`).\n\n##### Returns\n"
}{
    "source file": "doc_typealias.py",
    "line number": "24",
    "func name": "document",
    "func arg": "(obj, doc)",
    "comments": "Adds a docstring to typealias by overriding the `__doc__` attribute.\n\nNote: Overriding `__doc__` is only possible after python 3.7.\n##### Args\n* **obj**: Typealias object that needs to be documented.\n\n* **doc**: Docstring of the typealias. It should follow the standard pystyle\n  docstring rules.\n\n"
}{}{
    "source file": "dtypes_test.py",
    "line number": "30",
    "func name": "_is_numeric_dtype_enum",
    "func arg": "(datatype_enum)",
    "comments": ""
}{
    "source file": "dtypes.py",
    "line number": "607",
    "func name": "as_dtype",
    "func arg": "(type_value)",
    "comments": "Converts the given `type_value` to a `DType`.\n\nNote: `DType` values are interned. When passed a new `DType` object, `as_dtype` always returns the interned value.\n##### Args\n* **type_value**: A value that can be converted to a `tf.DType` object. This may\n  currently be a `tf.DType` object, a [`DataType`\n  enum](https\n\n##### Returns\n"
}{}{}{
    "source file": "dumping_callback.py",
    "line number": "878",
    "func name": "disable_dump_debug_info",
    "func arg": "()",
    "comments": "Disable the currently-enabled debugging dumping.\n\nIf the `enable_dump_debug_info()` method under the same Python namespace has been invoked before, calling this method disables it. If no call to `enable_dump_debug_info()` has been made, calling this method is a no-op. Calling this method more than once is idempotent.\n"
}{}{}{
    "source file": "duplicate_method_names_v1.py",
    "line number": "37",
    "func name": "Test",
    "func arg": "()",
    "comments": ""
}{}{}{}{}{}{}{
    "source file": "eager_microbenchmarks_test.py",
    "line number": "30",
    "func name": "_run_benchmark",
    "func arg": "(func, num_iters, execution_mode)",
    "comments": ""
}{
    "source file": "eager_test.py",
    "line number": "772",
    "func name": "multiple_tpus",
    "func arg": "()",
    "comments": ""
}{
    "source file": "edit_distance_op_test.py",
    "line number": "30",
    "func name": "ConstantOf",
    "func arg": "(x)",
    "comments": ""
}{}{
    "source file": "efficientnet_weight_update_util.py",
    "line number": "300",
    "func name": "check_match",
    "func arg": "(keras_block, tf_block, keras_weight_names, tf_weight_names, model_name_tf)",
    "comments": "Check if the weights in h5 and ckpt match.\n\nwe match each name from keras_weight_names that is in keras_block and check if there is 1-1 correspondence to names from tf_weight_names that is in tf_block\n##### Args\n* **keras_block**: str, the block name for keras implementation (e.g. 'block1a')\n\n* **tf_block**: str, the block name for tf implementation (e.g. 'blocks_0')\n\n* **keras_weight_names**: list of str, weight names in keras implementation\n\n* **tf_weight_names**: list of str, weight names in tf implementation\n\n* **model_name_tf**: str, the name of model in ckpt.\n\n"
}{
    "source file": "efficientnet.py",
    "line number": "741",
    "func name": "decode_predictions",
    "func arg": "(preds, top)",
    "comments": ""
}{
    "source file": "eig_op_test.py",
    "line number": "196",
    "func name": "_GetEigGradTest",
    "func arg": "(dtype_, shape_, compute_v_)",
    "comments": ""
}{}{
    "source file": "einsum_dense.py",
    "line number": "240",
    "func name": "_analyze_split_string",
    "func arg": "(split_string, bias_axes, input_shape, output_shape, left_elided)",
    "comments": "Analyze an pre-split einsum string to find the weight shape.\n\n\n"
}{}{}{
    "source file": "elementwise.py",
    "line number": "81",
    "func name": "make_square_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do square.\n\n\n"
}{
    "source file": "elu.py",
    "line number": "28",
    "func name": "make_elu_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do (float) tf.nn.elu.\n\n\n"
}{
    "source file": "embedding_lookup.py",
    "line number": "27",
    "func name": "make_embedding_lookup_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do gather.\n\n\n"
}{
    "source file": "embedding_ops_test.py",
    "line number": "188",
    "func name": "_EmbeddingResult",
    "func arg": "(params, id_vals, num_shards, vocab_size, partition_strategy, weight_vals)",
    "comments": ""
}{
    "source file": "embedding_ops.py",
    "line number": "1002",
    "func name": "_prune_invalid_weights",
    "func arg": "(sparse_ids, sparse_weights)",
    "comments": "Prune invalid weights (< 0) from the input ids and weights.\n\n\n"
}{}{}{}{}{}{
    "source file": "enumerate_ops.py",
    "line number": "26",
    "func name": "enumerate_dataset",
    "func arg": "(start)",
    "comments": "A transformation that enumerates the elements of a dataset.\n\nIt is similar to python's `enumerate`. For example:\n\n```python # NOTE: The following examples use `{ ... }` to represent the # contents of a dataset. a = { 1, 2, 3 } b = { (7, 8), (9, 10) }\n\n# The nested structure of the `datasets` argument determines the # structure of elements in the resulting dataset. a.apply(tf.data.experimental.enumerate_dataset(start=5)) => { (5, 1), (6, 2), (7, 3) } b.apply(tf.data.experimental.enumerate_dataset()) => { (0, (7, 8)), (1, (9, 10)) } ```\n##### Args\n* **start**: A `tf.int64` scalar `tf.Tensor`, representing the start value for\n  enumeration.\n\n##### Returns\n"
}{}{
    "source file": "equal.py",
    "line number": "27",
    "func name": "make_equal_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do equal.\n\n\n"
}{}{
    "source file": "error_interpolation_test.py",
    "line number": "48",
    "func name": "_modify_op_stack_with_filenames",
    "func arg": "(op, num_user_frames, user_filename, num_inner_tf_frames)",
    "comments": "Replace op._traceback with a new traceback using special filenames.\n\n\n"
}{
    "source file": "error_interpolation.py",
    "line number": "487",
    "func name": "interpolate",
    "func arg": "(error_message, graph)",
    "comments": "Interpolates an error message.\n\nThe error message can contain tags of the form `{{type name}}` which will be replaced. For example: \"{{node <name>}}\" would get expanded to: \"node <name>(defined at <path>)\".\n##### Args\n* **error_message**: A string to interpolate.\n\n* **graph**: ops.Graph object containing all nodes referenced in the error\n    message.\n\n##### Returns\n"
}{
    "source file": "error_ops.py",
    "line number": "26",
    "func name": "ignore_errors",
    "func arg": "()",
    "comments": "Creates a `Dataset` from another `Dataset` and silently ignores any errors.\n\nUse this transformation to produce a dataset that contains the same elements as the input, but silently drops any elements that caused an error. For example:\n\n```python dataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.])\n\n# Computing `tf.debugging.check_numerics(1. / 0.)` will raise an InvalidArgumentError. dataset = dataset.map(lambda x: tf.debugging.check_numerics(1. / x, \"error\"))\n\n# Using `ignore_errors()` will drop the element that causes an error. dataset = dataset.apply(tf.data.experimental.ignore_errors())\n\n# ==> {1., 0.5, 0.2} ```\n##### Returns\n"
}{}{
    "source file": "error_utils.py",
    "line number": "34",
    "func name": "_stack_trace_inside_mapped_code",
    "func arg": "(tb, source_map, converter_filename)",
    "comments": "Summarizes inner traceback frames up to the call to a given function.\n\nThis functions locates the innermost (i.e. most recent) frame that corresponds to code that can be mapped by source_map originated from, and returns a translated stack trace ending at that frame. If no such frame is found, the entire stack trace is summarized.\n\nFor example, the following code:\n\ndef f(): for i in tf.range(1): z = y + i\n\n# z only defined here\n\nWould generate this traceback:\n\n<converted code> ag__.for_stmt(...) <for_stmt> return _known_len_tf_for_stmt(iter_, extra_test, body, init_state) <_known_len_tf_for_stmt> _disallow_undefs_into_loop(*init_state) <_disallow_undefs_into_loop> raise ...\n\nWhich is then processed into:\n\n<f> for i in tf.range(1): <for_stmt> return _known_len_tf_for_stmt(iter_, extra_test, body, init_state) <_known_len_tf_for_stmt> _disallow_undefs_into_loop(*init_state) <_disallow_undefs_into_loop> raise ...\n##### Args\n* **tb**: traceback.FrameSummary, The traceback corresponding to an error.\n  Typically, the output of traceback.Summary.extract(capture_locals=True).\n\n* **source_map**: Dict[LineLocation, OriginInfo], a source map as created by\n  origin_info.create_source_map.\n\n* **converter_filename**: str, the file path of the converted module. Call frames\n  corresponding to this module are elided and their preceding frames are\n  marked as allowlisted. Note that frames enclosing converted code are\n  dropped using a different mechanism.\n\n##### Returns\n"
}{
    "source file": "errors_impl.py",
    "line number": "527",
    "func name": "_make_specific_exception",
    "func arg": "(node_def, op, message, error_code)",
    "comments": ""
}{}{}{}{}{
    "source file": "estimator_training.py",
    "line number": "344",
    "func name": "estimator_evaluate",
    "func arg": "(estimator, evaluate_distributed_fn, hooks)",
    "comments": "Run distribute coordinator for Estimator's `evaluate` method.\n\n\n"
}{}{
    "source file": "evaluation_test.py",
    "line number": "51",
    "func name": "local_variable",
    "func arg": "(init_value, name)",
    "comments": ""
}{
    "source file": "evaluation.py",
    "line number": "172",
    "func name": "_evaluate_once",
    "func arg": "(checkpoint_path, master, scaffold, eval_ops, feed_dict, final_ops, final_ops_feed_dict, hooks, config)",
    "comments": "Evaluates the model at the given checkpoint path.\n\nDuring a single evaluation, the `eval_ops` is run until the session is interrupted or requested to finish. This is typically requested via a `tf.contrib.training.StopAfterNEvalsHook` which results in `eval_ops` running the requested number of times.\n\nOptionally, a user can pass in `final_ops`, a single `Tensor`, a list of `Tensors` or a dictionary from names to `Tensors`. The `final_ops` is evaluated a single time after `eval_ops` has finished running and the fetched values of `final_ops` are returned. If `final_ops` is left as `None`, then `None` is returned.\n\nOne may also consider using a `tf.contrib.training.SummaryAtEndHook` to record summaries after the `eval_ops` have run. If `eval_ops` is `None`, the summaries run immediately after the model checkpoint has been restored.\n\nNote that `evaluate_once` creates a local variable used to track the number of evaluations run via `tf.contrib.training.get_or_create_eval_step`. Consequently, if a custom local init op is provided via a `scaffold`, the caller should ensure that the local init op also initializes the eval step.\n##### Args\n* **checkpoint_path**: The path to a checkpoint to use for evaluation.\n\n* **master**: The BNS address of the TensorFlow master.\n\n* **scaffold**: An tf.compat.v1.train.Scaffold instance for initializing variables\n  and restoring variables. Note that `scaffold.init_fn` is used by the\n  function to restore the checkpoint. If you supply a custom init_fn, then\n  it must also take care of restoring the model from its checkpoint.\n\n* **eval_ops**: A single `Tensor`, a list of `Tensors` or a dictionary of names to\n  `Tensors`, which is run until the session is requested to stop, commonly\n  done by a `tf.contrib.training.StopAfterNEvalsHook`.\n\n* **feed_dict**: The feed dictionary to use when executing the `eval_ops`.\n\n* **final_ops**: A single `Tensor`, a list of `Tensors` or a dictionary of names\n  to `Tensors`.\n\n* **final_ops_feed_dict**: A feed dictionary to use when evaluating `final_ops`.\n\n* **hooks**: List of `tf.estimator.SessionRunHook` callbacks which are run inside\n  the evaluation loop.\n\n* **config**: An instance of `tf.compat.v1.ConfigProto` that will be used to\n  configure the `Session`. If left as `None`, the default will be used.\n\n##### Returns\n"
}{}{
    "source file": "evaluator.py",
    "line number": "34",
    "func name": "_parse_debug_tensor_name",
    "func arg": "(debug_tensor_name)",
    "comments": "Parse a debug tensor name in a to-be-evaluated expression.\n\n\n##### Args\n* **debug_tensor_name**: name of the debug tensor, with or without\n  device name as a prefix, with or without debug op, with or\n  without '[<exec_index>]' as a suffix.\n  E.g., without device name prefix, without debug op suffix\n\n##### Returns\n* **device_name**: If device name prefix exists, the device name; otherwise,\n  `None`.\n\n* **node_name**: Name of the node.\n\n* **output_slot**: Output slot index as an `int`.\n\n* **debug_op**: If the debug op suffix exists, the debug op name; otherwise,\n  `None`.\n\n* **exec_index**: Execution index (applicable to cases in which a debug tensor\n  is computed multiple times in a `tf.Session.run` call, e.g., due to\n  `tf.while_loop`). If the exec_index suffix does not exist, this value\n  defaults to `0`.\n\n"
}{}{}{}{}{
    "source file": "example_parser_configuration.py",
    "line number": "135",
    "func name": "_extract_from_parse_example_v2",
    "func arg": "(parse_example_op, sess)",
    "comments": "Extract ExampleParserConfig from ParseExampleV2 op.\n\n\n"
}{}{
    "source file": "exceptions.py",
    "line number": "83",
    "func name": "_py_assert_stmt",
    "func arg": "(expression1, expression2)",
    "comments": "Overload of assert_stmt that executes a Python assert statement.\n\n\n"
}{
    "source file": "execute.py",
    "line number": "287",
    "func name": "args_to_mixed_eager_tensors",
    "func arg": "(lists, ctx)",
    "comments": "Converts a list of same-length lists of values to eager tensors.\n\n\n"
}{
    "source file": "executor.py",
    "line number": "76",
    "func name": "new_executor",
    "func arg": "(enable_async)",
    "comments": ""
}{
    "source file": "exp.py",
    "line number": "27",
    "func name": "make_exp_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do exp.\n\n\n"
}{
    "source file": "expand_dims.py",
    "line number": "28",
    "func name": "make_expand_dims_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do expand_dims.\n\n\n"
}{}{
    "source file": "exponential_test.py",
    "line number": "34",
    "func name": "try_import",
    "func arg": "(name)",
    "comments": ""
}{}{}{
    "source file": "export_mnist_cnn.py",
    "line number": "163",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{}{}{}{
    "source file": "export_rnn_cell.py",
    "line number": "32",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{
    "source file": "export_simple_text_embedding.py",
    "line number": "96",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{}{
    "source file": "export_text_rnn_model.py",
    "line number": "170",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{
    "source file": "export_utils.py",
    "line number": "328",
    "func name": "_maybe_add_default_serving_output",
    "func arg": "(export_outputs)",
    "comments": "Add a default serving output to the export_outputs if not present.\n\n\n##### Args\n* **export_outputs**: Describes the output signatures to be exported to\n  `SavedModel` and used during serving. Should be a dict.\n\n##### Returns\n"
}{}{}{}{}{}{}{}{}{
    "source file": "eye.py",
    "line number": "28",
    "func name": "make_eye_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests for tf.eye op.\n\n\n"
}{}{}{}{
    "source file": "fashion_mnist.py",
    "line number": "31",
    "func name": "load_data",
    "func arg": "()",
    "comments": "Loads the Fashion-MNIST dataset.\n\nThis is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The class labels are:\n\n| Label | Description | |:-----:|-------------| |\n\n 0\n\n | T-shirt/top | |\n\n 1\n\n | Trouser\n\n\n\n | |\n\n 2\n\n | Pullover\n\n\n\n| |\n\n 3\n\n | Dress\n\n\n\n\n\n | |\n\n 4\n\n | Coat\n\n\n\n\n\n\n\n| |\n\n 5\n\n | Sandal\n\n\n\n\n\n| |\n\n 6\n\n | Shirt\n\n\n\n\n\n | |\n\n 7\n\n | Sneaker\n\n\n\n | |\n\n 8\n\n | Bag\n\n\n\n\n\n\n\n | |\n\n 9\n\n | Ankle boot\n\n|\n##### Returns\n* **Tuple of Numpy arrays**: `(x_train, y_train), (x_test, y_test)`.\n\n* ****x_train, x_test****: uint8 arrays of grayscale image data with shape\n  (num_samples, 28, 28).\n\n* ****y_train, y_test****: uint8 arrays of labels (integers in range 0-9)\n  with shape (num_samples,).\n\n* **nse**: \n\n* **https**: //github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE).\n\n"
}{}{
    "source file": "feature_column_test.py",
    "line number": "3344",
    "func name": "_assert_sparse_tensor_value",
    "func arg": "(test_case, expected, actual)",
    "comments": ""
}{
    "source file": "feature_column_test1.py",
    "line number": "37",
    "func name": "_initialized_session",
    "func arg": "()",
    "comments": ""
}{
    "source file": "feature_column_v2_test.py",
    "line number": "2983",
    "func name": "_assert_sparse_tensor_value",
    "func arg": "(test_case, expected, actual)",
    "comments": ""
}{
    "source file": "feature_column_v2_test1.py",
    "line number": "40",
    "func name": "_initialized_session",
    "func arg": "()",
    "comments": ""
}{
    "source file": "feature_column_v2.py",
    "line number": "4430",
    "func name": "_sanitize_column_name_for_variable_scope",
    "func arg": "(name)",
    "comments": "Sanitizes user-provided feature names for use as variable scopes.\n\n\n"
}{
    "source file": "feature_column_v2(1).py",
    "line number": "870",
    "func name": "_check_invalid_cases",
    "func arg": "(embedding_lookup_device)",
    "comments": "Checks for invalid embedding_lookup_device configurations.\n\n\n"
}{
    "source file": "feature_column.py",
    "line number": "3227",
    "func name": "_verify_static_batch_size_equality",
    "func arg": "(tensors, columns)",
    "comments": "Validates that the first dim (batch size) of all tensors are equal or None.\n\n\n##### Args\n* **tensors**: list of tensors to check.\n\n* **columns**: list of feature columns matching tensors. Will be used for error\n  messaging.\n\n"
}{
    "source file": "feature_column1.py",
    "line number": "678",
    "func name": "split_sequence_columns",
    "func arg": "(feature_columns)",
    "comments": "Split a list of _TPUEmbeddingColumn into sequence and non-sequence columns.\n\nFor use in a TPUEstimator model_fn function. E.g.\n\ndef model_fn(features): sequence_columns, feature_columns = ( tf.tpu.feature_column.split_sequence_columns(feature_columns)) input = tf.feature_column.input_layer( features=features, feature_columns=feature_columns) sequence_features, sequence_lengths = ( tf.contrib.feature_column.sequence_input_layer( features=features, feature_columns=sequence_columns))\n##### Args\n* **feature_columns**: A list of _TPUEmbeddingColumns to split.\n\n##### Returns\n"
}{}{}{}{}{
    "source file": "fft_ops.py",
    "line number": "419",
    "func name": "ifftshift",
    "func arg": "(x, axes, name)",
    "comments": "The inverse of fftshift.\n\nAlthough identical for even-length x, the functions differ by one sample for odd-length x.\n\n@compatibility(numpy) Equivalent to numpy.fft.ifftshift. https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.ifftshift.html @end_compatibility\n\nFor example:\n\n```python x = tf.signal.ifftshift([[ 0.,\n\n1.,\n\n2.],[ 3.,\n\n4., -4.],[-3., -2., -1.]]) x.numpy() # array([[ 4., -4.,\n\n3.],[-2., -1., -3.],[ 1.,\n\n2.,\n\n0.]]) ```\n##### Args\n* **x**: `Tensor`, input tensor.\n\n* **axes**: `int` or shape `tuple` Axes over which to calculate. Defaults to None,\n  which shifts all axes.\n\n* **name**: An optional name for the operation.\n\n##### Returns\n"
}{
    "source file": "fft_test.py",
    "line number": "45",
    "func name": "to_32bit",
    "func arg": "(x)",
    "comments": ""
}{}{}{}{
    "source file": "file_io.py",
    "line number": "842",
    "func name": "file_crc32",
    "func arg": "(filename, block_size)",
    "comments": "Get the crc32 of the passed file.\n\nThe crc32 of a file can be used for error checking; two files with the same crc32 are considered equivalent. Note that the entire file must be read to produce the crc32.\n##### Args\n* **filename**: string, path to a file\n\n* **block_size**: Integer, process the files by reading blocks of `block_size`\n  bytes. Use -1 to read the file as once.\n\n##### Returns\n"
}{
    "source file": "file_name_test.py",
    "line number": "31",
    "func name": "main",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "fill.py",
    "line number": "28",
    "func name": "make_fill_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do fill.\n\n\n"
}{}{}{
    "source file": "filter_fusion_test.py",
    "line number": "34",
    "func name": "_test_combinations",
    "func arg": "()",
    "comments": ""
}{
    "source file": "filter_test.py",
    "line number": "33",
    "func name": "_test_combinations",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "fix_arduino_subfolders.py",
    "line number": "124",
    "func name": "parse_args",
    "func arg": "()",
    "comments": "Converts the raw arguments into accessible flags.\n\n\n"
}{}{}{}{
    "source file": "flags.py",
    "line number": "44",
    "func name": "_wrap_define_function",
    "func arg": "(original_function)",
    "comments": "Wraps absl.flags's define functions so tf.flags accepts old names.\n\n\n"
}{}{}{}{
    "source file": "flatbuffer_utils.py",
    "line number": "130",
    "func name": "randomize_weights",
    "func arg": "(model, random_seed)",
    "comments": "Randomize weights in a model.\n\n\n##### Args\n* **model**: The model in which to randomize weights.\n\n* **random_seed**: The input to the random number generator (default value is 0).\n\n"
}{
    "source file": "floor.py",
    "line number": "27",
    "func name": "make_floor_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do floor.\n\n\n"
}{
    "source file": "flops_registry.py",
    "line number": "441",
    "func name": "_add_n_flops",
    "func arg": "(graph, node)",
    "comments": "Compute flops for AddN operation.\n\n\n"
}{}{
    "source file": "forwardprop_test.py",
    "line number": "944",
    "func name": "_fprop_cond",
    "func arg": "(k, y)",
    "comments": ""
}{
    "source file": "forwardprop_test1.py",
    "line number": "124",
    "func name": "_test_gradients",
    "func arg": "(testcase, f, primals, order, delta, rtol, atol)",
    "comments": "Tests forward/backward jacobians of `f`'s [0, `order`)-order gradients.\n\n\n"
}{
    "source file": "forwardprop_util.py",
    "line number": "61",
    "func name": "push_forwardprop_state",
    "func arg": "()",
    "comments": "Temporarily push or pop transient state for accumulators in the active set.\n\nAllows an accumulator which is currently processing an operation to temporarily reset its state. This is useful when building forwardprop versions of functions, where an accumulator will trigger function building and then must process captured symbolic tensors while building it. Without pushing and popping, accumulators ignore operations executed as a direct result of their own jvp computations.\n\nYields: None (used for its side effect).\n"
}{
    "source file": "forwardprop.py",
    "line number": "201",
    "func name": "_jvp_dispatch",
    "func arg": "(op_name, attr_tuple, inputs, outputs, tangents, use_batch)",
    "comments": "Determine which forwardprop function to call.\n\n\n"
}{}{}{}{
    "source file": "framework_test.py",
    "line number": "411",
    "func name": "_is_public_method_name",
    "func arg": "(method_name)",
    "comments": ""
}{
    "source file": "framework.py",
    "line number": "119",
    "func name": "_check_type",
    "func arg": "(obj, expected_types)",
    "comments": "Check if an object is of the expected type.\n\n\n##### Args\n* **obj**: The object being checked.\n\n* **expected_types**: (`type` or an iterable of `type`s) The expected `type`(s)\n  of obj.\n\n"
}{}{
    "source file": "freeze_graph.py",
    "line number": "381",
    "func name": "run_main",
    "func arg": "()",
    "comments": "Main function of freeze_graph.\n\n\n"
}{}{
    "source file": "freeze.py",
    "line number": "211",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{}{}{}{}{}{}{}{}{}{}{
    "source file": "fully_connected_feed.py",
    "line number": "218",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "fully_connected.py",
    "line number": "28",
    "func name": "make_fully_connected_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do fully_connected.\n\n\n"
}{
    "source file": "func_graph.py",
    "line number": "1284",
    "func name": "override_func_graph_name_scope",
    "func arg": "(func_graph, name_scope)",
    "comments": ""
}{}{}{
    "source file": "function_def_to_graph.py",
    "line number": "258",
    "func name": "_get_num_args",
    "func arg": "(arg_def, node_def)",
    "comments": ""
}{}{
    "source file": "function_deserialization.py",
    "line number": "488",
    "func name": "_clean_function_name",
    "func arg": "(name)",
    "comments": "Vanity function to keep the function names comprehensible.\n\n\n"
}{}{
    "source file": "function_serialization.py",
    "line number": "110",
    "func name": "wrap_cached_variables",
    "func arg": "(concrete_function)",
    "comments": "Wraps the concrete function if it uses cached read tensors.\n\nThis function creates a new concrete function that captures variables instead of the cached read tensors.\n##### Args\n* **concrete_function**: A Concrete function that maybe captures cached read\n  tensors.\n\n##### Returns\n"
}{}{
    "source file": "function_test1.py",
    "line number": "111",
    "func name": "_spec_for_value",
    "func arg": "(value)",
    "comments": "Returns the (nested) TypeSpec for a value.\n\n\n"
}{
    "source file": "function_test2.py",
    "line number": "57",
    "func name": "_OptimizerOptions",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "function_utils_test.py",
    "line number": "27",
    "func name": "silly_example_function",
    "func arg": "()",
    "comments": ""
}{
    "source file": "function_utils.py",
    "line number": "125",
    "func name": "get_disabled_rewriter_config",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "function_wrappers.py",
    "line number": "114",
    "func name": "with_function_scope",
    "func arg": "(thunk, scope_name, options)",
    "comments": "Inline version of the FunctionScope context manager.\n\n\n"
}{
    "source file": "function.py",
    "line number": "3849",
    "func name": "_contains_type_spec",
    "func arg": "(value)",
    "comments": ""
}{
    "source file": "function1.py",
    "line number": "1346",
    "func name": "function_def_from_tf_function",
    "func arg": "(c_func)",
    "comments": "Converts a SWIG-wrapped TF_Function* to a FunctionDef proto.\n\n\n"
}{
    "source file": "functional_ops_test.py",
    "line number": "50",
    "func name": "simple_scoped_fn",
    "func arg": "(a, x)",
    "comments": "Simple function: (a, x) -> 2(x+a), but with \"2\" as a variable in scope.\n\n\n"
}{}{
    "source file": "functional_ops.py",
    "line number": "1217",
    "func name": "_set_read_only_resource_inputs_attr",
    "func arg": "(op, func_graph)",
    "comments": "Sets the list of resource inputs which are read-only.\n\nThis is used by AutomaticControlDependencies.\n##### Args\n* **op**: PartitionedCall Operation.\n\n* **func_graph**: FuncGraph.\n\n"
}{}{
    "source file": "functional_saver.py",
    "line number": "117",
    "func name": "sharded_filename",
    "func arg": "(filename_tensor, shard, num_shards)",
    "comments": "Append sharding information to a filename.\n\n\n##### Args\n* **filename_tensor**: A string tensor.\n\n* **shard**: Integer.  The shard for the filename.\n\n* **num_shards**: An int Tensor for the number of shards.\n\n##### Returns\n"
}{}{
    "source file": "functional.py",
    "line number": "1244",
    "func name": "get_network_config",
    "func arg": "(network, serialize_layer_fn)",
    "comments": "Builds the config, which consists of the node graph and serialized layers.\n\n\n##### Args\n* **network**: A Network object.\n\n* **serialize_layer_fn**: Function used to serialize layers.\n\n##### Returns\n"
}{}{}{
    "source file": "functions.py",
    "line number": "134",
    "func name": "transform",
    "func arg": "(node, ctx)",
    "comments": ""
}{
    "source file": "fused_batch_norm.py",
    "line number": "27",
    "func name": "make_fused_batch_norm_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do fused_batch_norm.\n\n\n"
}{}{}{
    "source file": "gamma_test.py",
    "line number": "36",
    "func name": "try_import",
    "func arg": "(name)",
    "comments": ""
}{
    "source file": "gamma.py",
    "line number": "318",
    "func name": "_kl_gamma_gamma",
    "func arg": "(g0, g1, name)",
    "comments": "Calculate the batched KL divergence KL(g0 || g1) with g0 and g1 Gamma.\n\n\n##### Args\n* **g0**: instance of a Gamma distribution object.\n\n* **g1**: instance of a Gamma distribution object.\n\n* **name**: (optional) Name to use for created operations.\n  Default is \"kl_gamma_gamma\".\n\n##### Returns\n* **kl_gamma_gamma**: `Tensor`. The batchwise KL(g0 || g1).\n\n"
}{}{
    "source file": "gast_util.py",
    "line number": "57",
    "func name": "_is_ellipsis_gast_3",
    "func arg": "(node)",
    "comments": ""
}{}{}{
    "source file": "gather_nd.py",
    "line number": "27",
    "func name": "make_gather_nd_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do gather_nd.\n\n\n"
}{
    "source file": "gather_op_test.py",
    "line number": "43",
    "func name": "_to_str_elements",
    "func arg": "(values)",
    "comments": "Converts the inner list elements to strings.\n\n\n"
}{}{
    "source file": "gather_with_constant.py",
    "line number": "28",
    "func name": "make_gather_with_constant_tests",
    "func arg": "(options)",
    "comments": "Make a set of test which feed a constant to gather toco.\n\n\n"
}{
    "source file": "gather.py",
    "line number": "27",
    "func name": "make_gather_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do gather.\n\n\n"
}{}{}{}{
    "source file": "gcs_smoke.py",
    "line number": "192",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{
    "source file": "gen_build_info.py",
    "line number": "32",
    "func name": "write_build_info",
    "func arg": "(filename, key_value_list)",
    "comments": "Writes a Python that describes the build.\n\n\n##### Args\n* **filename**: filename to write to.\n\n* **key_value_list**: A list of \"key=value\" strings that will be added to the\n  module's \"build_info\" dictionary as additional entries.\n\n"
}{
    "source file": "gen_git_source.py",
    "line number": "274",
    "func name": "raw_generate",
    "func arg": "(output_file, source_dir, git_tag_override)",
    "comments": "Simple generator used for cmake/make build systems.\n\nThis does not create any symlinks. It requires the build system to build unconditionally.\n##### Args\n* **output_file**: Output filename for the version info cc\n\n* **source_dir**: Base path of the source code\n\n* **git_tag_override**: Override the value for the git tag. This is useful for\n  releases where we want to build the release before the git tag is\n  created.\n\n"
}{}{
    "source file": "gen_html.py",
    "line number": "208",
    "func name": "gen_conversion_log_html",
    "func arg": "(conversion_log_dir, quantization_enabled, tflite_graph_path)",
    "comments": "Generates an HTML report about the conversion process.\n\n\n##### Args\n* **conversion_log_dir**: A string specifying the file directory of the conversion\n  logs. It's required that before calling this function, the\n  `conversion_log_dir`\n  already contains the following files\n\n* **quantization_enabled**: A boolean, passed from the tflite converter to\n  indicate whether post-training quantization is enabled during conversion.\n\n* **tflite_graph_path**: A string, the filepath to the converted TFLite model.\n\n"
}{
    "source file": "gen_tftrt_model.py",
    "line number": "90",
    "func name": "GenerateModelV1",
    "func arg": "(tf_saved_model_dir, tftrt_saved_model_dir)",
    "comments": "Generate and convert a model using TFv1 API.\n\n\n"
}{
    "source file": "generate_examples_lib.py",
    "line number": "294",
    "func name": "generate_multi_set_examples",
    "func arg": "(options, test_sets)",
    "comments": "Generate examples for test sets.\n\n\n##### Args\n* **options**: Options containing information to generate examples.\n\n* **test_sets**: List of the name of test sets to generate examples.\n\n"
}{
    "source file": "generate_examples_report.py",
    "line number": "32",
    "func name": "make_report_table",
    "func arg": "(fp, title, reports)",
    "comments": "Make an HTML report of the success/failure reports.\n\n\n##### Args\n* **fp**: File-like object in which to put the html.\n\n* **title**: \"Title of the zip file this pertains to.\"\n\n* **reports**: a list of conversion attempts. (report_args, report_vals) i.e.\n  ({\"shape\"\n\n"
}{
    "source file": "generate_examples.py",
    "line number": "100",
    "func name": "main",
    "func arg": "(unused_args)",
    "comments": ""
}{
    "source file": "generate_keil_project.py",
    "line number": "82",
    "func name": "parse_args",
    "func arg": "()",
    "comments": "Converts the raw arguments into accessible flags.\n\n\n"
}{
    "source file": "generate_lib.py",
    "line number": "421",
    "func name": "replace_refs",
    "func arg": "(src_dir, output_dir, reference_resolver, file_pattern, api_docs_relpath)",
    "comments": "Fix @{} references in all files under `src_dir` matching `file_pattern`.\n\nA matching directory structure, with the modified files is written to `output_dir`.\n\n`{\"__init__.py\",\"OWNERS\",\"README.txt\"}` are skipped.\n\nFiles not matching `file_pattern` (using `fnmatch`) are copied with no change.\n\nAlso, files in the `api_guides/python` directory get explicit ids set on all heading-2s to ensure back-links work.\n##### Args\n* **src_dir**: The directory to convert files from.\n\n* **output_dir**: The root directory to write the resulting files to.\n\n* **reference_resolver**: A `parser.ReferenceResolver` to make the replacements.\n\n* **file_pattern**: Only replace references in files matching file_patters,\n  using fnmatch. Non-matching files are copied unchanged.\n\n* **api_docs_relpath**: Relative-path string to the api_docs, from the src_dir.\n\n"
}{
    "source file": "generate_saved_models.py",
    "line number": "77",
    "func name": "main",
    "func arg": "(args)",
    "comments": ""
}{}{
    "source file": "generate_streaming_test_wav.py",
    "line number": "86",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "generate_test_models.py",
    "line number": "74",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{
    "source file": "generate_v2_renames_map.py",
    "line number": "191",
    "func name": "main",
    "func arg": "(unused_argv)",
    "comments": ""
}{
    "source file": "generate_v2_reorders_map.py",
    "line number": "137",
    "func name": "main",
    "func arg": "(unused_argv)",
    "comments": ""
}{}{}{
    "source file": "generate2.py",
    "line number": "283",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{}{
    "source file": "generic_utils.py",
    "line number": "797",
    "func name": "populate_dict_with_module_objects",
    "func arg": "(target_dict, modules, obj_filter)",
    "comments": ""
}{}{}{}{
    "source file": "get_layer_policy.py",
    "line number": "29",
    "func name": "get_layer_policy",
    "func arg": "(layer)",
    "comments": "Returns the dtype policy of a layer.\n\n\n##### Args\n* **layer**: A `tf.keras.layers.Layer`.\n\n##### Returns\n"
}{}{
    "source file": "get_single_element.py",
    "line number": "27",
    "func name": "get_single_element",
    "func arg": "(dataset)",
    "comments": "Returns the single element in `dataset` as a nested structure of tensors.\n\nThis function enables you to use a `tf.data.Dataset` in a stateless \"tensor-in tensor-out\" expression, without creating an iterator. This can be useful when your preprocessing transformations are expressed as a `Dataset`, and you want to use the transformation at serving time.\n\nFor example:\n\n```python def preprocessing_fn(input_str): # ... return image, label\n\ninput_batch = ...\n\n# input batch of BATCH_SIZE elements dataset = (tf.data.Dataset.from_tensor_slices(input_batch) .map(preprocessing_fn, num_parallel_calls=BATCH_SIZE) .batch(BATCH_SIZE))\n\nimage_batch, label_batch = tf.data.experimental.get_single_element(dataset) ```\n##### Args\n* **dataset**: A `tf.data.Dataset` object containing a single element.\n\n##### Returns\n"
}{}{
    "source file": "global_batch_norm.py",
    "line number": "27",
    "func name": "make_global_batch_norm_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do batch_norm_with_global_normalization.\n\n\n"
}{
    "source file": "googletest.py",
    "line number": "111",
    "func name": "StatefulSessionAvailable",
    "func arg": "()",
    "comments": ""
}{
    "source file": "gpu_info_lib.py",
    "line number": "167",
    "func name": "gather_gpu_devices",
    "func arg": "()",
    "comments": "Gather gpu device info.\n\n\n##### Returns\n"
}{
    "source file": "gpu_util.py",
    "line number": "35",
    "func name": "compute_capability_from_device_desc",
    "func arg": "(device_attrs)",
    "comments": "Returns the GpuInfo given a DeviceAttributes proto.\n\n\n##### Args\n* **device_attrs**: A DeviceAttributes proto.\n\n##### Returns\n"
}{
    "source file": "gradient_checker_test.py",
    "line number": "43",
    "func name": "_nan_grad",
    "func arg": "(unused_op, grad)",
    "comments": "A gradient that returns NaN.\n\n\n"
}{
    "source file": "gradient_checker_v2_test.py",
    "line number": "41",
    "func name": "_random_complex",
    "func arg": "(shape, dtype)",
    "comments": ""
}{
    "source file": "gradient_checker_v2.py",
    "line number": "335",
    "func name": "max_error",
    "func arg": "(grad1, grad2)",
    "comments": "Computes maximum elementwise gap.\n\nComputes the maximum elementwise gap between two lists of tensors of the same shape.\n##### Args\n* **grad1**: a lists of tensors.\n\n* **grad2**: a lists of tensors with the same shape as grad1.\n\n##### Returns\n"
}{
    "source file": "gradient_checker.py",
    "line number": "354",
    "func name": "compute_gradient_error",
    "func arg": "(x, x_shape, y, y_shape, x_init_value, delta, init_targets, extra_feed_dict)",
    "comments": "Computes the gradient error.\n\nComputes the maximum error for dy/dx between the computed Jacobian and the numerically estimated Jacobian.\n\nThis function will modify the tensors passed in as it adds more operations and hence changing the consumers of the operations of the input tensors.\n\nThis function adds operations to the current session. To compute the error using a particular device, such as a GPU, use the standard methods for setting a device (e.g. using with sess.graph.device() or setting a device function in the session constructor).\n##### Args\n* **x**: a tensor or list of tensors\n\n* **x_shape**: the dimensions of x as a tuple or an array of ints. If x is a list,\n\n* **y**: a tensor\n\n* **y_shape**: the dimensions of y as a tuple or an array of ints.\n\n* **x_init_value**: (optional) a numpy array of the same shape as \"x\"\n  representing the initial value of x. If x is a list, this should be a list\n  of numpy arrays.  If this is none, the function will pick a random tensor\n  as the initial value.\n\n* **delta**: (optional) the amount of perturbation.\n\n* **init_targets**: list of targets to run to initialize model params.\n\n* **extra_feed_dict**: dict that allows fixing specified tensor values\n  during the Jacobian calculation.\n\n##### Returns\n"
}{
    "source file": "gradient_checkpoint_test.py",
    "line number": "111",
    "func name": "_train_with_recompute",
    "func arg": "(n_steps)",
    "comments": "Trains a single large model with gradient checkpointing using tf.recompute_grad.\n\n\n"
}{}{}{}{}{}{}{
    "source file": "gradient_input_output_exclusions.py",
    "line number": "374",
    "func name": "main",
    "func arg": "(output_file)",
    "comments": ""
}{
    "source file": "gradients_impl.py",
    "line number": "444",
    "func name": "HessiansV2",
    "func arg": "(ys, xs, gate_gradients, aggregation_method, name)",
    "comments": "Constructs the Hessian of sum of `ys` with respect to `x` in `xs`.\n\n`hessians()` adds ops to the graph to output the Hessian matrix of `ys` with respect to `xs`.\n\nIt returns a list of `Tensor` of length `len(xs)` where each tensor is the Hessian of `sum(ys)`.\n\nThe Hessian is a matrix of second-order partial derivatives of a scalar tensor (see https://en.wikipedia.org/wiki/Hessian_matrix for more details).\n##### Args\n* **ys**: A `Tensor` or list of tensors to be differentiated.\n\n* **xs**: A `Tensor` or list of tensors to be used for differentiation.\n\n* **gate_gradients**: See `gradients()` documentation for details.\n\n* **aggregation_method**: See `gradients()` documentation for details.\n\n* **name**: Optional name to use for grouping all the gradient ops together.\n  defaults to 'hessians'.\n\n##### Returns\n"
}{}{}{
    "source file": "gradients_test2.py",
    "line number": "332",
    "func name": "create_fc_per_eg_jacobians",
    "func arg": "(batch_size, activation_size, num_layers)",
    "comments": ""
}{
    "source file": "gradients_util.py",
    "line number": "925",
    "func name": "_AggregatedGrads",
    "func arg": "(grads, op, gradient_uid, loop_state, aggregation_method)",
    "comments": "Get the aggregated gradients for op.\n\n\n##### Args\n* **grads**: The map of memoized gradients.\n\n* **op**: The op to get gradients for.\n\n* **gradient_uid**: A unique identifier within the graph indicating\n  which invocation of gradients is being executed. Used to cluster\n  ops for compilation.\n\n* **loop_state**: An object for maintaining the state of the while loops in the\n            graph. It is of type ControlFlowState. None if the graph\n            contains no while loops.\n\n* **aggregation_method**: Specifies the method used to combine gradient terms.\n  Accepted values are constants defined in the class `AggregationMethod`.\n\n##### Returns\n"
}{}{
    "source file": "gradients2.py",
    "line number": "83",
    "func name": "batch_jacobian",
    "func arg": "(output, inp, use_pfor, parallel_iterations)",
    "comments": "Computes and stacks jacobians of `output[i,...]` w.r.t. `input[i,...]`.\n\ne.g. x = tf.constant([[1, 2], [3, 4]], dtype=tf.float32) y = x * x jacobian = batch_jacobian(y, x) # => [[[2,\n\n0], [0,\n\n4]], [[6,\n\n0], [0,\n\n8]]]\n##### Args\n* **output**: A tensor with shape [b, y1, ..., y_n]. `output[i,...]` should\n  only depend on `inp[i,...]`.\n\n* **inp**: A tensor with shape [b, x1, ..., x_m]\n\n* **use_pfor**: If true, uses pfor for computing the Jacobian. Else uses a\n  tf.while_loop.\n\n* **parallel_iterations**: A knob to control how many iterations are vectorized\n  and dispatched in parallel. The default value of None, when use_pfor is\n  true, corresponds to vectorizing all the iterations. When use_pfor is\n  false, the default value of None corresponds to parallel_iterations=10.\n  This knob can be used to control the total memory usage.\n\n##### Returns\n"
}{
    "source file": "graph_analyzer.py",
    "line number": "32",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "graph_building_benchmark.py",
    "line number": "44",
    "func name": "run_benchmark",
    "func arg": "(func, num_iters)",
    "comments": ""
}{
    "source file": "graph_io.py",
    "line number": "31",
    "func name": "write_graph",
    "func arg": "(graph_or_graph_def, logdir, name, as_text)",
    "comments": "Writes a graph proto to a file.\n\nThe graph is written as a text proto unless `as_text` is `False`.\n\n```python v = tf.Variable(0, name='my_variable') sess = tf.compat.v1.Session() tf.io.write_graph(sess.graph_def, '/tmp/my-model', 'train.pbtxt') ```\n\nor\n\n```python v = tf.Variable(0, name='my_variable') sess = tf.compat.v1.Session() tf.io.write_graph(sess.graph, '/tmp/my-model', 'train.pbtxt') ```\n##### Args\n* **graph_or_graph_def**: A `Graph` or a `GraphDef` protocol buffer.\n\n* **logdir**: Directory where to write the graph. This can refer to remote\n  filesystems, such as Google Cloud Storage (GCS).\n\n* **name**: Filename for the graph.\n\n* **as_text**: If `True`, writes the graph as an ASCII proto.\n\n##### Returns\n"
}{}{
    "source file": "graph_only_ops.py",
    "line number": "29",
    "func name": "graph_placeholder",
    "func arg": "(dtype, shape, name)",
    "comments": "Graph-only version of tf.compat.v1.placeholder(), for internal use only.\n\n\n"
}{
    "source file": "graph_to_function_def.py",
    "line number": "122",
    "func name": "graph_to_function_def",
    "func arg": "(graph, operations, inputs, outputs, out_names)",
    "comments": "Returns `graph` as a `FunctionDef` protocol buffer.\n\nThis method creates a [`FunctionDef`]( https://www.tensorflow.org/code/tensorflow/core/framework/function.proto) protocol buffer that contains all the ops in `operations`.\n\nThe operations become the body of the function.\n\nThe arguments `inputs` and `outputs` will be listed as the inputs and outputs tensors of the function.\n\nThey must be lists of tensors present in the graph.\n\nThe lists can optionally be empty.\n##### Args\n* **graph**: Graph.\n\n* **operations**: the operations to put in the function. Must be a subset of\n the operations in the graph.\n\n* **inputs**: List of tensors. Inputs to the function.\n\n* **outputs**: List of tensors. Outputs of the function.\n\n* **out_names**: Optional list of string names for the outputs.\n\n##### Returns\n"
}{
    "source file": "graph_util_impl.py",
    "line number": "291",
    "func name": "remove_training_nodes",
    "func arg": "(input_graph, protected_nodes)",
    "comments": "Prunes out nodes that aren't needed for inference.\n\nThere are nodes like Identity and CheckNumerics that are only useful during training, and can be removed in graphs that will be used for nothing but inference. Here we identify and remove them, returning an equivalent graph. To be specific, CheckNumerics nodes are always removed, and Identity nodes that aren't involved in control edges are spliced out so that their input and outputs are directly connected.\n##### Args\n* **input_graph**: Model to analyze and prune.\n\n* **protected_nodes**: An optional list of names of nodes to be kept\n  unconditionally. This is for example useful to preserve Identity output\n  nodes.\n\n##### Returns\n"
}{
    "source file": "graph_util_test1.py",
    "line number": "36",
    "func name": "test_device_func_pin_variable_to_cpu",
    "func arg": "(op)",
    "comments": ""
}{}{}{
    "source file": "graph_view.py",
    "line number": "89",
    "func name": "_serialize_slot_variables",
    "func arg": "(trackable_objects, node_ids, object_names)",
    "comments": "Gather and name slot variables.\n\n\n"
}{}{
    "source file": "greater_equal.py",
    "line number": "27",
    "func name": "make_greater_equal_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do greater_equal.\n\n\n"
}{
    "source file": "greater.py",
    "line number": "27",
    "func name": "make_greater_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do greater.\n\n\n"
}{}{}{}{}{
    "source file": "grouping.py",
    "line number": "128",
    "func name": "bucket_by_sequence_length",
    "func arg": "(element_length_func, bucket_boundaries, bucket_batch_sizes, padded_shapes, padding_values, pad_to_bucket_boundary, no_padding, drop_remainder)",
    "comments": "A transformation that buckets elements in a `Dataset` by length.\n\nElements of the `Dataset` are grouped together by length and then are padded and batched.\n\nThis is useful for sequence tasks in which the elements have variable length. Grouping together elements that have similar lengths reduces the total fraction of padding in a batch which increases training step efficiency.\n##### Args\n* **element_length_func**: function from element in `Dataset` to `tf.int32`,\n  determines the length of the element, which will determine the bucket it\n  goes into.\n\n* **bucket_boundaries**: `list<int>`, upper length boundaries of the buckets.\n\n* **bucket_batch_sizes**: `list<int>`, batch size per bucket. Length should be\n  `len(bucket_boundaries) + 1`.\n\n* **padded_shapes**: Nested structure of `tf.TensorShape` to pass to\n  `tf.data.Dataset.padded_batch`. If not provided, will use\n  `dataset.output_shapes`, which will result in variable length dimensions\n  being padded out to the maximum length in each batch.\n\n* **padding_values**: Values to pad with, passed to\n  `tf.data.Dataset.padded_batch`. Defaults to padding with 0.\n\n* **pad_to_bucket_boundary**: bool, if `False`, will pad dimensions with unknown\n  size to maximum length in batch. If `True`, will pad dimensions with\n  unknown size to bucket boundary minus 1 (i.e., the maximum length in each\n  bucket), and caller must ensure that the source `Dataset` does not contain\n  any elements with length longer than `max(bucket_boundaries)`.\n\n* **no_padding**: `bool`, indicates whether to pad the batch features (features\n  need to be either of type `tf.sparse.SparseTensor` or of same shape).\n\n* **drop_remainder**: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n  whether the last batch should be dropped in the case it has fewer than\n  `batch_size` elements; the default behavior is not to drop the smaller\n  batch.\n\n##### Returns\n"
}{
    "source file": "grpc_debug_server.py",
    "line number": "41",
    "func name": "_state_change",
    "func arg": "(new_state, node_name, output_slot, debug_op)",
    "comments": ""
}{
    "source file": "grpc_debug_test_server.py",
    "line number": "428",
    "func name": "_poll_server_till_success",
    "func arg": "(max_attempts, sleep_per_poll_sec, debug_server_url, dump_dir, server, gpu_memory_fraction)",
    "comments": "Poll server until success or exceeding max polling count.\n\n\n##### Args\n* **max_attempts**: (int) How many times to poll at maximum\n\n* **sleep_per_poll_sec**: (float) How many seconds to sleep for after each\n  unsuccessful poll.\n\n* **debug_server_url**: (str) gRPC URL to the debug server.\n\n* **dump_dir**: (str) Dump directory to look for files in. If None, will directly\n  check data from the server object.\n\n* **server**: The server object.\n\n* **gpu_memory_fraction**: (float) Fraction of GPU memory to be\n  allocated for the Session used in server polling.\n\n##### Returns\n"
}{}{
    "source file": "grpc_tensorflow_server.py",
    "line number": "91",
    "func name": "main",
    "func arg": "(unused_args)",
    "comments": ""
}{
    "source file": "grpc_wrapper.py",
    "line number": "154",
    "func name": "register_signal_handler",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "hardswish.py",
    "line number": "47",
    "func name": "make_hardswish_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do hardswish.\n\n\n"
}{
    "source file": "hash_table_asset_v1.py",
    "line number": "49",
    "func name": "test",
    "func arg": "()",
    "comments": ""
}{
    "source file": "hash_table_v1.py",
    "line number": "60",
    "func name": "Test",
    "func arg": "()",
    "comments": ""
}{
    "source file": "hashing_benchmark.py",
    "line number": "45",
    "func name": "word_gen",
    "func arg": "()",
    "comments": ""
}{}{}{}{
    "source file": "hdf5_format_test.py",
    "line number": "866",
    "func name": "_make_sequential_input_shape",
    "func arg": "(input_size, output_size)",
    "comments": ""
}{
    "source file": "hdf5_format.py",
    "line number": "862",
    "func name": "_legacy_weights",
    "func arg": "(layer)",
    "comments": "DO NOT USE.\n\nFor legacy reason, the layer.weights was in the order of [self.trainable_weights + self.non_trainable_weights], and this order was used for preserving the weights in h5 format. The new order of layer.weights are the same as layer.get_weights() which is more intuitive for user. To keep supporting the existing saved h5 file, this method should be used to save/load weights. In future version, we will delete this method and introduce a breaking change for h5 and stay with the new order for weights.\n##### Args\n* **layer**: a `tf.keras.Model` or `tf.keras.layers.Layer` instance.\n\n##### Returns\n"
}{}{}{
    "source file": "histogram_ops.py",
    "line number": "104",
    "func name": "histogram_fixed_width",
    "func arg": "(values, value_range, nbins, dtype, name)",
    "comments": "Return histogram of values.\n\nGiven the tensor `values`, this operation returns a rank 1 histogram counting the number of entries in `values` that fell into every bin.\n\nThe bins are equal width and determined by the arguments `value_range` and `nbins`.\n##### Args\n* **values**: Numeric `Tensor`.\n\n* **value_range**: Shape [2] `Tensor` of same `dtype` as `values`.\n  values <= value_range[0] will be mapped to hist[0],\n  values >= value_range[1] will be mapped to hist[-1].\n\n* **nbins**: Scalar `int32 Tensor`.  Number of histogram bins.\n\n* **dtype**: dtype for returned histogram.\n\n* **name**: A name for this operation (defaults to 'histogram_fixed_width').\n\n##### Returns\n"
}{
    "source file": "hoist_random_uniform_test.py",
    "line number": "38",
    "func name": "_test_combinations",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "hvp_test.py",
    "line number": "67",
    "func name": "_back_over_back_hvp",
    "func arg": "(model, images, labels, vector)",
    "comments": ""
}{}{}{}{}{}{
    "source file": "identity.py",
    "line number": "29",
    "func name": "make_identity_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do identity.\n\n\n"
}{}{}{}{
    "source file": "image_dataset.py",
    "line number": "227",
    "func name": "path_to_image",
    "func arg": "(path, image_size, num_channels, interpolation)",
    "comments": ""
}{}{
    "source file": "image_grad.py",
    "line number": "171",
    "func name": "_RGBToHSVGrad",
    "func arg": "(op, grad)",
    "comments": "The gradients for `rgb_to_hsv` operation.\n\nThis function is a piecewise continuous function as defined here: https://en.wikipedia.org/wiki/HSL_and_HSV#From_RGB We perform the multivariate derivative and compute all partial derivatives separately before adding them in the end. Formulas are given before each partial derivative calculation.\n##### Args\n* **op**: The `rgb_to_hsv` `Operation` that we are differentiating.\n\n* **grad**: Gradient with respect to the output of the `rgb_to_hsv` op.\n\n##### Returns\n"
}{
    "source file": "image_ops_impl.py",
    "line number": "5044",
    "func name": "generate_bounding_box_proposals",
    "func arg": "(scores, bbox_deltas, image_info, anchors, nms_threshold, pre_nms_topn, min_size, post_nms_topn, name)",
    "comments": "Generate bounding box proposals from encoded bounding boxes.\n\n\n##### Returns\n* **rois**: Region of interest boxes sorted by their scores.\n\n* **roi_probabilities**: scores of the ROI boxes in the ROIs' tensor.\n\n"
}{
    "source file": "image_ops_test.py",
    "line number": "40",
    "func name": "_generate_numpy_random_rgb",
    "func arg": "(shape)",
    "comments": ""
}{
    "source file": "image_ops_test1.py",
    "line number": "3792",
    "func name": "_SimpleColorRamp",
    "func arg": "()",
    "comments": "Build a simple color ramp RGB image.\n\n\n"
}{
    "source file": "image_ops.py",
    "line number": "243",
    "func name": "_image_projective_transform_grad",
    "func arg": "(op, grad)",
    "comments": "Computes the gradient for ImageProjectiveTransform.\n\n\n"
}{
    "source file": "image_preproc_benchmark.py",
    "line number": "78",
    "func name": "image_augmentation",
    "func arg": "(inputs, batch_size)",
    "comments": "image augmentation.\n\n\n"
}{}{
    "source file": "image_preprocessing_test.py",
    "line number": "131",
    "func name": "get_numpy_center_crop",
    "func arg": "(images, expected_height, expected_width)",
    "comments": ""
}{
    "source file": "image_preprocessing.py",
    "line number": "1347",
    "func name": "get_interpolation",
    "func arg": "(interpolation)",
    "comments": ""
}{
    "source file": "image_test.py",
    "line number": "40",
    "func name": "_generate_test_images",
    "func arg": "()",
    "comments": ""
}{
    "source file": "image.py",
    "line number": "266",
    "func name": "load_img",
    "func arg": "(path, grayscale, color_mode, target_size, interpolation)",
    "comments": "Loads an image into PIL format.\n\nUsage:\n\n``` image = tf.keras.preprocessing.image.load_img(image_path) input_arr = keras.preprocessing.image.img_to_array(image) input_arr = np.array([input_arr])\n\n# Convert single image to a batch. predictions = model.predict(input_arr) ```\n\nArguments: path: Path to image file. grayscale: DEPRECATED use `color_mode=\"grayscale\"`. color_mode: One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\". The desired image format. target_size: Either `None` (default to original size) or tuple of ints `(img_height, img_width)`. interpolation: Interpolation method used to resample the image if the target size is different from that of the loaded image. Supported methods are \"nearest\", \"bilinear\", and \"bicubic\". If PIL version 1.1.3 or newer is installed, \"lanczos\" is also supported. If PIL version 3.4.0 or newer is installed, \"box\" and \"hamming\" are also supported. By default, \"nearest\" is used.\n##### Returns\n"
}{}{
    "source file": "imagenet_utils.py",
    "line number": "411",
    "func name": "validate_activation",
    "func arg": "(classifier_activation, weights)",
    "comments": "validates that the classifer_activation is compatible with the weights.\n\n\n##### Args\n* **classifier_activation**: str or callable activation function\n\n* **weights**: The pretrained weights to load.\n\n"
}{
    "source file": "imdb.py",
    "line number": "166",
    "func name": "get_word_index",
    "func arg": "(path)",
    "comments": "Retrieves a dict mapping words to their index in the IMDB dataset.\n\nArguments: path: where to cache the data (relative to `~/.keras/dataset`).\n##### Returns\n"
}{
    "source file": "imperative_grad.py",
    "line number": "33",
    "func name": "imperative_grad",
    "func arg": "(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)",
    "comments": "Computes gradients from the imperatively defined tape on top of the stack.\n\nWorks by filtering the tape, computing how many downstream usages are of each tensor and entry, and repeatedly applying backward functions until we have gradients for all sources.\n##### Args\n* **tape**: the gradient tape which stores the trace.\n\n* **target**: either a Tensor or list of Tensors to be differentiated.\n\n* **sources**: list of Tensors for which we want gradients\n\n* **output_gradients**: if not None, a list of gradient provided for each Target,\n or None if we are to use the target's computed downstream gradient.\n\n* **sources_raw**: if not None, a list of the source python objects from which the\n sources were generated. Should have the same length as sources. Only needs\n to be populated if unconnected_gradients is 'zero'.\n\n* **unconnected_gradients**: determines the value returned if the target and\n sources are unconnected. When 'none' the value returned is None wheras when\n 'zero' a zero tensor in the same shape as the sources is returned.\n\n##### Returns\n"
}{
    "source file": "import_pb_to_tensorboard.py",
    "line number": "67",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{}{
    "source file": "importer.py",
    "line number": "415",
    "func name": "_import_graph_def_internal",
    "func arg": "(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)",
    "comments": "Imports the graph from `graph_def` into the current default `Graph`.\n\nThis function provides a way to import a serialized TensorFlow [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto) protocol buffer, and extract individual objects in the `GraphDef` as `tf.Tensor` and `tf.Operation` objects. Once extracted, these objects are placed into the current default `Graph`. See `tf.Graph.as_graph_def` for a way to create a `GraphDef` proto.\n##### Args\n* **graph_def**: A `GraphDef` proto containing operations to be imported into the\n  default graph.\n\n* **input_map**: A dictionary mapping input names (as strings) in `graph_def` to\n  `Tensor` objects. The values of the named input tensors in the imported\n  graph will be re-mapped to the respective `Tensor` values.\n\n* **return_elements**: A list of strings containing operation names in `graph_def`\n  that will be returned as `Operation` objects; and/or tensor names in\n  `graph_def` that will be returned as `Tensor` objects.\n\n* **validate_colocation_constraints**: Whether to validate colocation constraints.\n\n* **name**: (Optional.) A prefix that will be prepended to the names in\n  `graph_def`. Note that this does not apply to imported function names.\n  Defaults to `\"import\"`.\n\n* **producer_op_list**: (Optional.) An `OpList` proto with the (possibly stripped)\n  list of `OpDef`s used by the producer of the graph. If provided,\n  unrecognized attrs for ops in `graph_def` that have their default value\n  according to `producer_op_list` will be removed. This will allow some more\n  `GraphDef`s produced by later binaries to be accepted by earlier binaries.\n\n##### Returns\n"
}{}{}{
    "source file": "inception_resnet_v2.py",
    "line number": "387",
    "func name": "decode_predictions",
    "func arg": "(preds, top)",
    "comments": ""
}{
    "source file": "inception_v3.py",
    "line number": "413",
    "func name": "decode_predictions",
    "func arg": "(preds, top)",
    "comments": ""
}{
    "source file": "index_lookup_adapt_benchmark.py",
    "line number": "50",
    "func name": "get_top_k",
    "func arg": "(dataset, k)",
    "comments": "Python implementation of vocabulary building using a defaultdict.\n\n\n"
}{
    "source file": "index_lookup_distribution_test.py",
    "line number": "37",
    "func name": "get_layer_class",
    "func arg": "()",
    "comments": ""
}{
    "source file": "index_lookup_test.py",
    "line number": "55",
    "func name": "_get_end_to_end_test_cases",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "indexed_slices.py",
    "line number": "394",
    "func name": "_indexed_slices_to_tensor",
    "func arg": "(value, dtype, name, as_ref)",
    "comments": "Converts an IndexedSlices object `value` to a Tensor.\n\nNOTE(mrry): This function is potentially expensive.\n##### Args\n* **value**: An ops.IndexedSlices object.\n\n* **dtype**: The dtype of the Tensor to be returned.\n\n* **name**: Optional name to use for the returned Tensor.\n\n* **as_ref**: True if a ref is requested.\n\n##### Returns\n"
}{
    "source file": "init_ops_test.py",
    "line number": "92",
    "func name": "_init_sampler",
    "func arg": "(tc, init, num)",
    "comments": "Returns a func to generate a random tensor of shape [num].\n\n\n##### Args\n* **tc**: An instance of TensorFlowTestCase.\n\n* **init**: An Initializer that generates a tensor of a given shape\n\n* **num**: Size of 1D tensor to create.\n\n##### Returns\n"
}{}{}{
    "source file": "init_ops_v2.py",
    "line number": "997",
    "func name": "_assert_float_dtype",
    "func arg": "(dtype)",
    "comments": "Validate and return floating point type based on `dtype`.\n\n`dtype` must be a floating point type.\n##### Args\n* **dtype**: The data type to validate.\n\n##### Returns\n"
}{
    "source file": "init_ops.py",
    "line number": "1428",
    "func name": "_assert_float_dtype",
    "func arg": "(dtype)",
    "comments": "Validate and return floating point type based on `dtype`.\n\n`dtype` must be a floating point type.\n##### Args\n* **dtype**: The data type to validate.\n\n##### Returns\n"
}{}{}{}{
    "source file": "initializers_v2.py",
    "line number": "747",
    "func name": "_get_dtype",
    "func arg": "(dtype)",
    "comments": ""
}{}{}{
    "source file": "inplace_ops.py",
    "line number": "228",
    "func name": "inplace_sub",
    "func arg": "(x, i, v)",
    "comments": "Applies an inplace sub on input x at index i with value v.\n\nNote that this function is not actually inplace\n\n- it allocates a copy of x.\n\nThe utility is not avoiding memory copies but rather specifying a sparse update.\n\nIf i is None, x and v must be the same shape. Computes y = x; y -= v; If i is a scalar, x has a rank 1 higher than v's. Computes y = x; y[i, :] -= v; Otherwise, x and v must have the same rank. Computes y = x; y[i, :] -= v;\n##### Args\n* **x**: A Tensor.\n\n* **i**: None, a scalar or a vector.\n\n* **v**: A Tensor.\n\n"
}{}{
    "source file": "input_data1.py",
    "line number": "160",
    "func name": "get_features_range",
    "func arg": "(model_settings)",
    "comments": "Returns the expected min/max for generated features.\n\n\n##### Args\n* **model_settings**: Information about the current model being trained.\n\n##### Returns\n"
}{
    "source file": "input_data2.py",
    "line number": "266",
    "func name": "read_data_sets",
    "func arg": "(train_dir, fake_data, one_hot, dtype, reshape, validation_size, seed, source_url)",
    "comments": ""
}{
    "source file": "input_layer.py",
    "line number": "211",
    "func name": "Input",
    "func arg": "(shape, batch_size, name, dtype, sparse, tensor, ragged, **kwargs)",
    "comments": "`Input()` is used to instantiate a Keras tensor.\n\nA Keras tensor is a TensorFlow symbolic tensor object, which we augment with certain attributes that allow us to build a Keras model just by knowing the inputs and outputs of the model.\n\nFor instance, if `a`, `b` and `c` are Keras tensors, it becomes possible to do: `model = Model(input=[a, b], output=c)`\n\nArguments: shape: A shape tuple (integers), not including the batch size. For instance, `shape=(32,)` indicates that the expected input will be batches of 32-dimensional vectors. Elements of this tuple can be None; 'None' elements represent dimensions where the shape is not known. batch_size: optional static batch size (integer). name: An optional name string for the layer. Should be unique in a model (do not reuse the same name twice). It will be autogenerated if it isn't provided. dtype: The data type expected by the input, as a string (`float32`, `float64`, `int32`...) sparse: A boolean specifying whether the placeholder to be created is sparse. Only one of 'ragged' and 'sparse' can be True. Note that, if `sparse` is False, sparse tensors can still be passed into the input\n\n- they will be densified with a default value of 0. tensor: Optional existing tensor to wrap into the `Input` layer. If set, the layer will not create a placeholder tensor. ragged: A boolean specifying whether the placeholder to be created is ragged. Only one of 'ragged' and 'sparse' can be True. In this case, values of 'None' in the 'shape' argument represent ragged dimensions. For more information about RaggedTensors, see [this guide](https://www.tensorflow.org/guide/ragged_tensors). **kwargs: deprecated arguments support. Supports `batch_shape` and `batch_input_shape`.\n##### Returns\n* **ample**: \n\n* **nsorFlow ops, as such**: \n\n"
}{}{
    "source file": "input_lib_type_spec_test.py",
    "line number": "409",
    "func name": "_check_type_spec_structure",
    "func arg": "(x)",
    "comments": "Verifies that `x` has the same structure as its `TypeSpec`.\n\n\n"
}{
    "source file": "input_lib.py",
    "line number": "1928",
    "func name": "_replace_per_replica_spec",
    "func arg": "(spec, i)",
    "comments": "If `spec` is a `PerReplicaSpec`, then return its `i`th value_spec.\n\n\n"
}{}{
    "source file": "input_ops.py",
    "line number": "68",
    "func name": "_clone_helper",
    "func arg": "(op_to_clone, variant_tensor_ops)",
    "comments": "Helper method that recursively clones `op_to_clone`.\n\n\n##### Args\n* **op_to_clone**: The op we want to clone.\n\n* **variant_tensor_ops**: A list of ops that we have to clone along the way.\n\n##### Returns\n"
}{}{
    "source file": "input_spec.py",
    "line number": "237",
    "func name": "to_tensor_spec",
    "func arg": "(input_spec, default_dtype)",
    "comments": "Converts a Keras InputSpec object to a TensorSpec.\n\n\n"
}{}{
    "source file": "input.py",
    "line number": "1517",
    "func name": "maybe_shuffle_batch_join",
    "func arg": "(tensors_list, batch_size, capacity, min_after_dequeue, keep_input, seed, enqueue_many, shapes, allow_smaller_final_batch, shared_name, name)",
    "comments": "Create batches by randomly shuffling conditionally-enqueued tensors.\n\nSee docstring in `shuffle_batch_join` for more details.\n##### Args\n* **tensors_list**: A list of tuples or dictionaries of tensors to enqueue.\n\n* **batch_size**: An integer. The new batch size pulled from the queue.\n\n* **capacity**: An integer. The maximum number of elements in the queue.\n\n* **min_after_dequeue**: Minimum number elements in the queue after a\n  dequeue, used to ensure a level of mixing of elements.\n\n* **keep_input**: A `bool` Tensor.  This tensor controls whether the input is\n  added to the queue or not.  If it is a scalar and evaluates `True`, then\n  `tensors` are all added to the queue. If it is a vector and `enqueue_many`\n  is `True`, then each example is added to the queue only if the\n  corresponding value in `keep_input` is `True`. This tensor essentially\n  acts as a filtering mechanism.\n\n* **seed**: Seed for the random shuffling within the queue.\n\n* **enqueue_many**: Whether each tensor in `tensor_list_list` is a single\n  example.\n\n* **shapes**: (Optional) The shapes for each example.  Defaults to the\n  inferred shapes for `tensors_list[i]`.\n\n* **allow_smaller_final_batch**: (Optional) Boolean. If `True`, allow the final\n  batch to be smaller if there are insufficient items left in the queue.\n\n* **shared_name**: (optional). If set, this queue will be shared under the given\n  name across multiple sessions.\n\n* **name**: (Optional) A name for the operations.\n\n##### Returns\n"
}{}{
    "source file": "inspect_checkpoint.py",
    "line number": "148",
    "func name": "main",
    "func arg": "(unused_argv)",
    "comments": ""
}{
    "source file": "inspect_utils_test.py",
    "line number": "93",
    "func name": "free_factory",
    "func arg": "()",
    "comments": ""
}{
    "source file": "inspect_utils.py",
    "line number": "339",
    "func name": "getfutureimports",
    "func arg": "(entity)",
    "comments": "Detects what future imports are necessary to safely execute entity source.\n\n\n##### Args\n* **entity**: Any object\n\n##### Returns\n"
}{}{
    "source file": "integer_lookup_test.py",
    "line number": "55",
    "func name": "_get_end_to_end_test_cases",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "integration_scripts.py",
    "line number": "62",
    "func name": "MaybeRunScriptInstead",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "interleave_ops.py",
    "line number": "280",
    "func name": "choose_from_datasets_v1",
    "func arg": "(datasets, choice_dataset)",
    "comments": ""
}{
    "source file": "interleave_test.py",
    "line number": "92",
    "func name": "_repeat",
    "func arg": "(values, count)",
    "comments": "Produces a list of lists suitable for testing interleave.\n\n\n##### Args\n* **values**: for each element `x` the result contains `[x] * x`\n\n* **count**: determines how many times to repeat `[x] * x` in the result\n\n##### Returns\n"
}{}{}{
    "source file": "interpreter.py",
    "line number": "131",
    "func name": "load_delegate",
    "func arg": "(library, options)",
    "comments": "Returns loaded Delegate object.\n\n\n##### Args\n* **library**: Name of shared library containing the\n  [TfLiteDelegate](https\n\n* **options**: Dictionary of options that are required to load the delegate. All\n  keys and values in the dictionary should be convertible to str. Consult\n  the documentation of the specific delegate for required and legal options.\n  (default None)\n\n##### Returns\n"
}{}{
    "source file": "inverse_registrations.py",
    "line number": "224",
    "func name": "_inverse_householder",
    "func arg": "(householder_operator)",
    "comments": ""
}{}{
    "source file": "io_ops.py",
    "line number": "75",
    "func name": "_restore_slice",
    "func arg": "(file_pattern, tensor_name, shape_and_slice, tensor_type, name, preferred_shard)",
    "comments": "Restore a tensor slice from a set of files with a given pattern.\n\nExample usage: RestoreSlice(\"/foo/bar-?????-of-?????\", \"w\", \"10 10 0,2:-\", DT_FLOAT)\n##### Args\n* **file_pattern**: the file pattern used to match a set of checkpoint files.\n\n* **tensor_name**: the name of the tensor to restore.\n\n* **shape_and_slice**: the shape-and-slice spec of the slice.\n\n* **tensor_type**: the type of the tensor to restore.\n\n* **name**: string.  Optional name for the op.\n\n* **preferred_shard**: Int. Optional shard to open first in the checkpoint file.\n\n##### Returns\n"
}{}{
    "source file": "io_utils_test.py",
    "line number": "40",
    "func name": "create_dataset",
    "func arg": "(h5_path)",
    "comments": ""
}{
    "source file": "io_utils.py",
    "line number": "236",
    "func name": "ask_to_proceed_with_overwrite",
    "func arg": "(filepath)",
    "comments": "Produces a prompt asking about overwriting a file.\n\nArguments: filepath: the path to the file to be overwritten.\n##### Returns\n"
}{
    "source file": "io1.py",
    "line number": "146",
    "func name": "load",
    "func arg": "(path, element_spec, compression, reader_func)",
    "comments": "Loads a previously saved dataset.\n\nExample usage:\n\n>>> import tempfile >>> path = os.path.join(tempfile.gettempdir(), \"saved_data\") >>> # Save a dataset >>> dataset = tf.data.Dataset.range(2) >>> tf.data.experimental.save(dataset, path) >>> new_dataset = tf.data.experimental.load(path, ...\n\n\n\n tf.TensorSpec(shape=(), dtype=tf.int64)) >>> for elem in new_dataset: ...\n\n print(elem) tf.Tensor(0, shape=(), dtype=int64) tf.Tensor(1, shape=(), dtype=int64)\n\n Note that to load a previously saved dataset, you need to specify `element_spec` -- a type signature of the elements of the saved dataset, which can be obtained via `tf.data.Dataset.element_spec`. This requirement exists so that shape inference of the loaded dataset does not need to perform I/O.\n\nIf the default option of sharding the saved dataset was used, the element order of the saved dataset will be preserved when loading it.\n\nThe `reader_func` argument can be used to specify a custom order in which elements should be loaded from the individual shards. The `reader_func` is expected to take a single argument -- a dataset of datasets, each containing elements of one of the shards -- and return a dataset of elements. For example, the order of shards can be shuffled when loading them as follows:\n\n```python def custom_reader_func(datasets): datasets = datasets.shuffle(NUM_SHARDS) return datasets.interleave(lambda x: x, num_parallel_calls=AUTOTUNE)\n\ndataset = tf.data.experimental.load( path=\"/path/to/data\", ..., reader_func=custom_reader_func) ```\n##### Args\n* **path**: Required. A path pointing to a previously saved dataset.\n\n* **element_spec**: Required. A nested structure of `tf.TypeSpec` objects matching\n  the structure of an element of the saved dataset and specifying the type\n  of individual element components.\n\n* **compression**: Optional. The algorithm to use to decompress the data when\n  reading it. Supported options are `GZIP` and `NONE`. Defaults to `NONE`.\n\n* **reader_func**: Optional. A function to control how to read data from shards.\n  If present, the function will be traced and executed as graph computation.\n\n##### Returns\n"
}{
    "source file": "ipynb.py",
    "line number": "155",
    "func name": "_update_notebook",
    "func arg": "(original_notebook, original_raw_lines, updated_code_lines)",
    "comments": "Updates notebook, once migration is done.\n\n\n"
}{}{
    "source file": "is_mlir_bridge_test_true.py",
    "line number": "28",
    "func name": "is_mlir_bridge_enabled",
    "func arg": "()",
    "comments": "Returns true to if MLIR bridge should be enabled for tests.\n\n\n"
}{
    "source file": "is_tfrt_test_true.py",
    "line number": "28",
    "func name": "is_tfrt_enabled",
    "func arg": "()",
    "comments": "Returns true to state TFRT should be enabled for Tensorflow tests.\n\n\n"
}{
    "source file": "is_xla_test_true.py",
    "line number": "27",
    "func name": "is_xla_enabled",
    "func arg": "()",
    "comments": "Returns true to state XLA should be enabled for Tensorflow tests.\n\n\n"
}{}{}{}{
    "source file": "iterator_ops.py",
    "line number": "49",
    "func name": "make_saveable_from_iterator",
    "func arg": "(iterator, external_state_policy)",
    "comments": "Returns a SaveableObject for saving/restoring iterator state using Saver.\n\n\n##### Args\n* **iterator**: Iterator.\n\n* **external_state_policy**: A string that identifies how to handle input\n  pipelines that depend on external state. Possible values are\n  'ignore'\n\n##### Returns\n"
}{
    "source file": "iterator_ops1.py",
    "line number": "939",
    "func name": "get_next_as_optional",
    "func arg": "(iterator)",
    "comments": "Returns a `tf.experimental.Optional` with the next element of the iterator.\n\nIf the iterator has reached the end of the sequence, the returned `tf.experimental.Optional` will have no value.\n##### Args\n* **iterator**: A `tf.data.Iterator`.\n\n##### Returns\n"
}{}{
    "source file": "jit_test.py",
    "line number": "82",
    "func name": "MetadataHasXlaRunOp",
    "func arg": "(run_metadata)",
    "comments": "Returns true if there are XlaRun kernels in run_metadata's timeline.\n\n\n"
}{
    "source file": "jit_test1.py",
    "line number": "39",
    "func name": "enable_jit_nonstateful",
    "func arg": "(node_def)",
    "comments": ""
}{
    "source file": "jit.py",
    "line number": "42",
    "func name": "experimental_jit_scope",
    "func arg": "(compile_ops, separate_compiled_gradients)",
    "comments": "Enable or disable JIT compilation of operators within the scope.\n\nNOTE: This is an experimental feature.\n\nThe compilation is a hint and only supported on a best-effort basis.\n\nExample usage:\n\n```python with tf.xla.experimental.jit_scope(): c = tf.matmul(a, b)\n\n# compiled with tf.xla.experimental.jit_scope(compile_ops=False): d = tf.matmul(a, c)\n\n# not compiled with tf.xla.experimental.jit_scope( compile_ops=lambda node_def: 'matmul' in node_def.op.lower()): e = tf.matmul(a, b) + d\n\n# matmul is compiled, the addition is not. ```\n\nExample of `separate_compiled_gradients`:\n\n```python # In the example below, the computations for f, g and h will all be compiled # in separate scopes. with tf.xla.experimental.jit_scope( separate_compiled_gradients=True): f = tf.matmul(a, b) g = tf.gradients([f], [a, b], name='mygrads1') h = tf.gradients([f], [a, b], name='mygrads2') ```\n##### Args\n* **compile_ops**: Whether to enable or disable compilation in the scope.\n  Either a Python bool, or a callable that accepts the parameter\n  `node_def` and returns a python bool.\n\n* **separate_compiled_gradients**: If true put each gradient subgraph into a\n  separate compilation scope. This gives fine-grained control over which\n  portions of the graph will be compiled as a single unit. Compiling\n  gradients separately may yield better performance for some graphs.\n  The scope is named based on the scope of the forward computation as well\n  as the name of the gradients. As a result, the gradients will be compiled\n  in a scope that is separate from both the forward computation, and from\n  other gradients.\n\n"
}{}{
    "source file": "json_utils.py",
    "line number": "63",
    "func name": "_decode_helper",
    "func arg": "(obj)",
    "comments": ""
}{
    "source file": "keras_correctness_test_base.py",
    "line number": "356",
    "func name": "should_skip_tpu_with_eager",
    "func arg": "(distribution)",
    "comments": ""
}{}{
    "source file": "keras_dnn_correctness_test.py",
    "line number": "45",
    "func name": "is_default_strategy",
    "func arg": "(strategy)",
    "comments": ""
}{}{}{}{
    "source file": "keras_metrics_test.py",
    "line number": "84",
    "func name": "tpu_combinations",
    "func arg": "()",
    "comments": ""
}{
    "source file": "keras_optimizer_v2_test.py",
    "line number": "138",
    "func name": "_replica_id",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "keras_parameterized.py",
    "line number": "439",
    "func name": "_test_or_class_decorator",
    "func arg": "(test_or_class, single_method_decorator)",
    "comments": "Decorate a test or class with a decorator intended for one method.\n\nIf the test_or_class is a class: This will apply the decorator to all test methods in the class.\n\nIf the test_or_class is an iterable of already-parameterized test cases: This will apply the decorator to all the cases, and then flatten the resulting cross-product of test cases. This allows stacking the Keras parameterized decorators w/ each other, and to apply them to test methods that have already been marked with an absl parameterized decorator.\n\nOtherwise, treat the obj as a single method and apply the decorator directly.\n##### Args\n* **test_or_class**: A test method (that may have already been decorated with a\n  parameterized decorator, or a test class that extends\n  keras_parameterized.TestCase\n\n* **single_method_decorator**: A parameterized decorator intended for a single test method.\n\n##### Returns\n"
}{
    "source file": "keras_premade_models_test.py",
    "line number": "53",
    "func name": "get_dataset",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "keras_stateful_lstm_model_correctness_test.py",
    "line number": "38",
    "func name": "test_combinations_for_stateful_embedding_model",
    "func arg": "()",
    "comments": ""
}{
    "source file": "keras_tensor.py",
    "line number": "295",
    "func name": "keras_tensor_from_tensor",
    "func arg": "(x)",
    "comments": "Convert a traced (composite)tensor to a representative KerasTensor.\n\n\n"
}{
    "source file": "keras_test.py",
    "line number": "107",
    "func name": "create_central_storage_strategy",
    "func arg": "()",
    "comments": "Create a CentralStorageStrategy, using a GPU if it is available.\n\n\n"
}{}{
    "source file": "keras.py",
    "line number": "27",
    "func name": "mnist_model",
    "func arg": "()",
    "comments": "Creates a MNIST model.\n\n\n"
}{}{
    "source file": "kernelized_test.py",
    "line number": "57",
    "func name": "_exact_laplacian",
    "func arg": "(stddev)",
    "comments": ""
}{
    "source file": "kernelized_utils_test.py",
    "line number": "35",
    "func name": "_exact_laplacian",
    "func arg": "(stddev)",
    "comments": ""
}{
    "source file": "kernelized_utils.py",
    "line number": "90",
    "func name": "exact_laplacian_kernel",
    "func arg": "(x, y, stddev)",
    "comments": "Computes exact Laplacian kernel value(s) for tensors x and y using stddev.\n\nThe Laplacian kernel for vectors u, v is defined as follows: K(u, v) = exp(-||u-v|| / stddev) where the norm is the l1-norm. x, y can be either vectors or matrices. If they are vectors, they must have the same dimension. If they are matrices, they must have the same number of columns. In the latter case, the method returns (as a matrix) K(u, v) values for all pairs (u, v) where u is a row from x and v is a row from y.\n##### Args\n* **x**: a tensor of rank 1 or 2. It's shape should be either [dim] or [m, dim].\n\n* **y**: a tensor of rank 1 or 2. It's shape should be either [dim] or [n, dim].\n\n* **stddev**: The width of the Gaussian kernel.\n\n##### Returns\n"
}{
    "source file": "kernelized.py",
    "line number": "273",
    "func name": "_get_default_scale",
    "func arg": "(initializer, input_dim)",
    "comments": ""
}{}{
    "source file": "kernels.py",
    "line number": "36",
    "func name": "get_registered_kernels_for_op",
    "func arg": "(name)",
    "comments": "Returns a KernelList proto of registered kernels for a given op.\n\n\n##### Args\n* **name**: A string representing the name of the op whose kernels to retrieve.\n\n"
}{}{
    "source file": "keyword_args.py",
    "line number": "27",
    "func name": "keyword_args_only",
    "func arg": "(func)",
    "comments": "Decorator for marking specific function accepting keyword args only.\n\nThis decorator raises a `ValueError` if the input `func` is called with any non-keyword args. This prevents the caller from providing the arguments in wrong order.\n##### Args\n* **func**: The function or method needed to be decorated.\n\n##### Returns\n"
}{
    "source file": "kubernetes_cluster_resolver_test.py",
    "line number": "47",
    "func name": "_create_pod_list",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "kullback_leibler.py",
    "line number": "132",
    "func name": "cross_entropy",
    "func arg": "(ref, other, allow_nan_stats, name)",
    "comments": "Computes the (Shannon) cross entropy.\n\nDenote two distributions by `P` (`ref`) and `Q` (`other`). Assuming `P, Q` are absolutely continuous with respect to one another and permit densities `p(x) dr(x)` and `q(x) dr(x)`, (Shanon) cross entropy is defined as:\n\n```none H[P, Q] = E_p[-log q(X)] = -int_F p(x) log q(x) dr(x) ```\n\nwhere `F` denotes the support of the random variable `X ~ P`.\n##### Args\n* **ref**: `tfd.Distribution` instance.\n\n* **other**: `tfd.Distribution` instance.\n\n* **allow_nan_stats**: Python `bool`, default `True`. When `True`,\n  statistics (e.g., mean, mode, variance) use the value \"`NaN`\" to\n  indicate the result is undefined. When `False`, an exception is raised\n  if one or more of the statistic's batch members are undefined.\n\n* **name**: Python `str` prepended to names of ops created by this function.\n\n##### Returns\n* **cross_entropy**: `ref.dtype` `Tensor` with shape `[B1, ..., Bn]`\n  representing `n` different calculations of (Shanon) cross entropy.\n\n"
}{
    "source file": "l2norm_shared_epsilon.py",
    "line number": "28",
    "func name": "make_l2norm_shared_epsilon_tests",
    "func arg": "(options)",
    "comments": "Regression test for a bug (b/122651451).\n\n\n"
}{
    "source file": "l2norm.py",
    "line number": "28",
    "func name": "make_l2norm_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do l2norm.\n\n\n"
}{
    "source file": "label_image1.py",
    "line number": "65",
    "func name": "load_labels",
    "func arg": "(label_file)",
    "comments": ""
}{
    "source file": "label_image2.py",
    "line number": "29",
    "func name": "load_labels",
    "func arg": "(filename)",
    "comments": ""
}{
    "source file": "label_wav_dir.py",
    "line number": "101",
    "func name": "main",
    "func arg": "(_)",
    "comments": "Entry point for script, converts flags to arguments.\n\n\n"
}{}{
    "source file": "label_wav.py",
    "line number": "98",
    "func name": "main",
    "func arg": "(_)",
    "comments": "Entry point for script, converts flags to arguments.\n\n\n"
}{
    "source file": "laplace_test.py",
    "line number": "35",
    "func name": "try_import",
    "func arg": "(name)",
    "comments": ""
}{}{}{}{
    "source file": "layer_correctness_test.py",
    "line number": "49",
    "func name": "create_mirrored_strategy",
    "func arg": "()",
    "comments": ""
}{
    "source file": "layer_serialization.py",
    "line number": "113",
    "func name": "get_config",
    "func arg": "(obj)",
    "comments": ""
}{
    "source file": "layer_utils.py",
    "line number": "399",
    "func name": "is_builtin_layer",
    "func arg": "(layer)",
    "comments": ""
}{
    "source file": "layer_utils1.py",
    "line number": "277",
    "func name": "gather_non_trainable_weights",
    "func arg": "(trainable, sub_layers, extra_variables)",
    "comments": "Lists the non-trainable weights for an object with sub-layers.\n\n\n##### Args\n* **trainable**: Whether the object collecting the variables is trainable.\n\n* **sub_layers**: A flat list of Layer objects owned by this object, to collect\n  variables from.\n\n* **extra_variables**: Any extra variables to include. Their `.trainable` property\n  is used to categorize them.\n\n##### Returns\n"
}{}{}{
    "source file": "layout_optimizer_test.py",
    "line number": "199",
    "func name": "_is_permute",
    "func arg": "(node)",
    "comments": ""
}{}{
    "source file": "leaky_relu.py",
    "line number": "28",
    "func name": "make_leaky_relu_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do LeakyRelu.\n\n\n"
}{}{
    "source file": "learning_rate_schedule_test.py",
    "line number": "37",
    "func name": "_maybe_serialized",
    "func arg": "(lr_decay, serialize_and_deserialize)",
    "comments": ""
}{
    "source file": "learning_rate_schedule.py",
    "line number": "1002",
    "func name": "deserialize",
    "func arg": "(config, custom_objects)",
    "comments": ""
}{}{
    "source file": "legacy_learning_rate_decay.py",
    "line number": "682",
    "func name": "noisy_linear_cosine_decay",
    "func arg": "(learning_rate, global_step, decay_steps, initial_variance, variance_decay, num_periods, alpha, beta, name)",
    "comments": "Applies noisy linear cosine decay to the learning rate.\n\nNote that linear cosine decay is more aggressive than cosine decay and larger initial learning rates can typically be used.\n\nWhen training a model, it is often recommended to lower the learning rate as the training progresses.\n\nThis function applies a noisy linear cosine decay function to a provided initial learning rate. It requires a `global_step` value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.\n\nThe function returns the decayed learning rate.\n\nIt is computed as: ```python global_step = min(global_step, decay_steps) linear_decay = (decay_steps\n\n- global_step) / decay_steps) cosine_decay = 0.5 * ( 1 + cos(pi * 2 * num_periods * global_step / decay_steps)) decayed = (alpha + linear_decay + eps_t) * cosine_decay + beta decayed_learning_rate = learning_rate * decayed ``` where eps_t is 0-centered gaussian noise with variance initial_variance / (1 + global_step) ** variance_decay\n\nExample usage: ```python decay_steps = 1000 lr_decayed = noisy_linear_cosine_decay( learning_rate, global_step, decay_steps) ```\n##### Args\n* **learning_rate**: A scalar `float32` or `float64` Tensor or a Python number.\n  The initial learning rate.\n\n* **global_step**: A scalar `int32` or `int64` `Tensor` or a Python number. Global\n  step to use for the decay computation.\n\n* **decay_steps**: A scalar `int32` or `int64` `Tensor` or a Python number. Number\n  of steps to decay over.\n\n* **initial_variance**: initial variance for the noise. See computation above.\n\n* **variance_decay**: decay for the noise's variance. See computation above.\n\n* **num_periods**: Number of periods in the cosine part of the decay. See\n  computation above.\n\n* **alpha**: See computation above.\n\n* **beta**: See computation above.\n\n* **name**: String.  Optional name of the operation.  Defaults to\n  'NoisyLinearCosineDecay'.\n\n##### Returns\n"
}{
    "source file": "legacy_rnn_test.py",
    "line number": "373",
    "func name": "get_test_data",
    "func arg": "(train_samples, test_samples, input_shape, num_classes)",
    "comments": ""
}{}{
    "source file": "less_equal.py",
    "line number": "27",
    "func name": "make_less_equal_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do less_equal.\n\n\n"
}{
    "source file": "less.py",
    "line number": "27",
    "func name": "make_less_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do less.\n\n\n"
}{}{
    "source file": "lift_to_graph.py",
    "line number": "205",
    "func name": "lift_to_graph",
    "func arg": "(tensors, graph, sources, disallowed_placeholders, add_sources, handle_captures, base_graph, op_map)",
    "comments": "Copies the tensor and all its inputs recursively to the outer graph.\n\n\n##### Args\n* **tensors**: The Tensors to lift.\n\n* **graph**: The graph to lift to.\n\n* **sources**: Optional sequence of nodes to start from. If omitted the whole\n  subgraph which feeds into `init_tensor` is lifted.\n\n* **disallowed_placeholders**: An optional set of ops which may not appear in the\n  lifted graph. Defaults to all placeholders.\n\n* **add_sources**: A boolean indicating whether placeholders which are not in\n  sources should be allowed.\n\n* **handle_captures**: A boolean indicating whether to re-capture s in the new\n  graph or simply create a vanilla placeholder.\n\n* **base_graph**: The graph from which to lift ops. This will be inferred if not\n  specified.\n\n* **op_map**: A map contains all the existing nodes that have been lifted to the\n  destination graph, so they won't be lifted and copied again.\n\n##### Returns\n"
}{
    "source file": "linalg_grad_test.py",
    "line number": "135",
    "func name": "_GetBandedTriangularSolveGradientTest",
    "func arg": "(functor_, dtype_, shape_, float32_tol_fudge, **kwargs_)",
    "comments": ""
}{
    "source file": "linalg_grad.py",
    "line number": "1017",
    "func name": "_MatmulExtractingThreeDiagonals",
    "func arg": "(x, y_tr)",
    "comments": "Multiplies matrices and extracts three diagonals from the product.\n\nWith sizes M x K and K x M, this function takes O(MK) time and O(M) space, while using math_ops.matmul, and then extracting the diagonals would take O(M^2 K) time and O(M^2) space.\n##### Args\n* **x**: first matrix\n\n* **y_tr**: second matrix transposed\n\n##### Returns\n"
}{
    "source file": "linalg_impl.py",
    "line number": "1177",
    "func name": "_lu_solve_assertions",
    "func arg": "(lower_upper, perm, rhs, validate_args)",
    "comments": "Returns list of assertions related to `lu_solve` assumptions.\n\n\n"
}{
    "source file": "linalg_ops_impl.py",
    "line number": "33",
    "func name": "eye",
    "func arg": "(num_rows, num_columns, batch_shape, dtype, name)",
    "comments": "Construct an identity matrix, or a batch of matrices.\n\nSee `linalg_ops.eye`.\n"
}{
    "source file": "linalg_ops_test.py",
    "line number": "415",
    "func name": "make_tensor_hiding_attributes",
    "func arg": "(value, hide_shape, hide_value)",
    "comments": ""
}{
    "source file": "linalg_ops.py",
    "line number": "632",
    "func name": "norm",
    "func arg": "(tensor, ord, axis, keepdims, name, keep_dims)",
    "comments": "Computes the norm of vectors, matrices, and tensors.\n\nThis function can compute several different vector norms (the 1-norm, the Euclidean or 2-norm, the inf-norm, and in general the p-norm for p > 0) and matrix norms (Frobenius, 1-norm, 2-norm and inf-norm).\n##### Args\n* **tensor**: `Tensor` of types `float32`, `float64`, `complex64`, `complex128`\n\n* **ord**: Order of the norm. Supported values are 'fro', 'euclidean',\n  `1`, `2`, `np.inf` and any positive real number yielding the corresponding\n  p-norm. Default is 'euclidean' which is equivalent to Frobenius norm if\n  `tensor` is a matrix and equivalent to 2-norm for vectors.\n  Some restrictions apply\n\n* **axis**: If `axis` is `None` (the default), the input is considered a vector\n  and a single vector norm is computed over the entire set of values in the\n  tensor, i.e. `norm(tensor, ord=ord)` is equivalent to\n  `norm(reshape(tensor, [-1]), ord=ord)`.\n  If `axis` is a Python integer, the input is considered a batch of vectors,\n  and `axis` determines the axis in `tensor` over which to compute vector\n  norms.\n  If `axis` is a 2-tuple of Python integers it is considered a batch of\n  matrices and `axis` determines the axes in `tensor` over which to compute\n  a matrix norm.\n  Negative indices are supported. Example\n\n* **keepdims**: If True, the axis indicated in `axis` are kept with size 1.\n  Otherwise, the dimensions in `axis` are removed from the output shape.\n\n* **name**: The name of the op.\n\n* **keep_dims**: Deprecated alias for `keepdims`.\n\n##### Returns\n* **output**: A `Tensor` of the same type as tensor, containing the vector or\n  matrix norms. If `keepdims` is True then the rank of output is equal to\n  the rank of `tensor`. Otherwise, if `axis` is none the output is a scalar,\n  if `axis` is an integer, the rank of `output` is one less than the rank\n  of `tensor`, if `axis` is a 2-tuple the rank of `output` is two less\n  than the rank of `tensor`.\n\n"
}{}{}{
    "source file": "linear_operator_addition.py",
    "line number": "410",
    "func name": "_type",
    "func arg": "(operator)",
    "comments": "Returns the type name constant (e.g. _TRIL) for operator.\n\n\n"
}{}{}{}{
    "source file": "linear_operator_algebra.py",
    "line number": "172",
    "func name": "inverse",
    "func arg": "(lin_op_a, name)",
    "comments": "Get the Inverse associated to lin_op_a.\n\n\n##### Args\n* **lin_op_a**: The LinearOperator to decompose.\n\n* **name**: Name to use for this operation.\n\n##### Returns\n"
}{
    "source file": "linear_operator_block_diag_test.py",
    "line number": "38",
    "func name": "_block_diag_dense",
    "func arg": "(expected_shape, blocks)",
    "comments": "Convert a list of blocks, into a dense block diagonal matrix.\n\n\n"
}{}{
    "source file": "linear_operator_block_lower_triangular_test.py",
    "line number": "38",
    "func name": "_block_lower_triangular_dense",
    "func arg": "(expected_shape, blocks)",
    "comments": "Convert a list of blocks into a dense blockwise lower-triangular matrix.\n\n\n"
}{}{}{
    "source file": "linear_operator_circulant.py",
    "line number": "1088",
    "func name": "_to_complex",
    "func arg": "(x)",
    "comments": ""
}{}{}{}{}{}{}{}{}{}{}{}{}{
    "source file": "linear_operator_kronecker_test.py",
    "line number": "38",
    "func name": "_kronecker_dense",
    "func arg": "(factors)",
    "comments": "Convert a list of factors, into a dense Kronecker product.\n\n\n"
}{
    "source file": "linear_operator_kronecker.py",
    "line number": "53",
    "func name": "_rotate_last_dim",
    "func arg": "(x, rotate_right)",
    "comments": "Rotate the last dimension either left or right.\n\n\n"
}{}{}{}{}{}{}{
    "source file": "linear_operator_test_util.py",
    "line number": "1087",
    "func name": "random_normal_correlated_columns",
    "func arg": "(shape, mean, stddev, dtype, eps, seed)",
    "comments": "Batch matrix with (possibly complex) Gaussian entries and correlated cols.\n\n\n##### Args\n* **shape**: Python list of integers.\n  Shape of the returned tensor.  Must be at least length two.\n\n* **mean**: `Tensor` giving mean of normal to sample from.\n\n* **stddev**: `Tensor` giving stdev of normal to sample from.\n\n* **dtype**: `TensorFlow` `dtype` or numpy dtype\n\n* **eps**: Distance each column is perturbed from the low-dimensional subspace.\n\n* **seed**: Python integer seed for the RNG.\n\n##### Returns\n"
}{}{}{
    "source file": "linear_operator_toeplitz.py",
    "line number": "274",
    "func name": "_to_complex",
    "func arg": "(x)",
    "comments": ""
}{}{}{}{
    "source file": "linear_operator_util.py",
    "line number": "566",
    "func name": "split_arg_into_blocks",
    "func arg": "(block_dims, block_dims_fn, arg, axis)",
    "comments": "Split `x` into blocks matching `operators`'s `domain_dimension`.\n\nSpecifically, if we have a blockwise lower-triangular matrix, with block sizes along the diagonal `[M_j, M_j] j = 0,1,2..J`,\n\nthis method splits `arg` on `axis` into `J` tensors, whose shape at `axis` is `M_j`.\n##### Args\n* **block_dims**: Iterable of `TensorShapes`.\n\n* **block_dims_fn**: Callable returning an iterable of `Tensor`s.\n\n* **arg**: `Tensor`. `arg` is split into `J` tensors.\n\n* **axis**: Python `Integer` representing the axis to split `arg` on.\n\n##### Returns\n"
}{}{}{
    "source file": "linear_operator.py",
    "line number": "1219",
    "func name": "_trace",
    "func arg": "(x, name)",
    "comments": ""
}{}{}{}{}{}{
    "source file": "list_comprehensions.py",
    "line number": "81",
    "func name": "transform",
    "func arg": "(node, ctx)",
    "comments": ""
}{}{}{}{
    "source file": "list_ops.py",
    "line number": "330",
    "func name": "_build_element_shape",
    "func arg": "(shape)",
    "comments": "Converts shape to a format understood by list_ops for element_shape.\n\nIf `shape` is already a `Tensor` it is returned as-is. We do not perform a type check here.\n\nIf shape is None or a TensorShape with unknown rank, -1 is returned.\n\nIf shape is a scalar, an int32 tensor with empty list is returned. Note we do directly return an empty list since ops.convert_to_tensor would conver it to a float32 which is not a valid type for element_shape.\n\nIf shape is a sequence of dims, None's in the list are replaced with -1. We do not check the dtype of the other dims.\n##### Args\n* **shape**: Could be None, Tensor, TensorShape or a list of dims (each dim could\n  be a None, scalar or Tensor).\n\n##### Returns\n"
}{}{}{}{
    "source file": "lists.py",
    "line number": "239",
    "func name": "transform",
    "func arg": "(node, ctx)",
    "comments": ""
}{}{}{}{}{}{}{}{}{
    "source file": "liveness.py",
    "line number": "206",
    "func name": "resolve",
    "func arg": "(node, source_info, graphs, include_annotations)",
    "comments": "Resolves the live symbols at the exit of control flow statements.\n\n\n##### Args\n* **node**: ast.AST\n\n* **source_info**: transformer.SourceInfo\n\n* **graphs**: Dict[ast.FunctionDef, cfg.Graph]\n\n* **include_annotations**: Bool, whether type annotations should be included in\n  the analysis.\n\n##### Returns\n"
}{
    "source file": "load_library.py",
    "line number": "124",
    "func name": "load_library",
    "func arg": "(library_location)",
    "comments": "Loads a TensorFlow plugin.\n\n\"library_location\" can be a path to a specific shared object, or a folder. If it is a folder, all shared objects that are named \"libtfkernel*\" will be loaded. When the library is loaded, kernels registered in the library via the `REGISTER_*` macros are made available in the TensorFlow process.\n##### Args\n* **library_location**: Path to the plugin or the folder of plugins.\n  Relative or absolute filesystem path to a dynamic library file or folder.\n\n##### Returns\n"
}{}{
    "source file": "load_test.py",
    "line number": "68",
    "func name": "cycle",
    "func arg": "(obj, cycles, signatures)",
    "comments": ""
}{}{
    "source file": "load_v1_in_v2.py",
    "line number": "260",
    "func name": "load",
    "func arg": "(export_dir, tags)",
    "comments": "Load a v1-style SavedModel as an object.\n\n\n"
}{
    "source file": "load.py",
    "line number": "1035",
    "func name": "_get_keras_attr",
    "func arg": "(layer)",
    "comments": ""
}{
    "source file": "load1.py",
    "line number": "606",
    "func name": "load_internal",
    "func arg": "(export_dir, tags, options, loader_cls)",
    "comments": "Loader implementation.\n\n\n"
}{
    "source file": "loader_deprecated_py2.py",
    "line number": "58",
    "func name": "load_ast",
    "func arg": "(nodes, indentation, include_source_map, delete_on_exit)",
    "comments": "Loads the given AST as a Python module.\n\nCompiling the AST code this way ensures that the source code is readable by e.g. `pdb` or `inspect`.\n##### Args\n* **nodes**: Union[ast.AST, Iterable[ast.AST]], the code to compile, as an AST\n  object.\n\n* **indentation**: Text, the string to use for indentation.\n\n* **include_source_map**: bool, whether return a source map.\n\n* **delete_on_exit**: bool, whether to delete the temporary file used for\n  compilation on exit.\n\n##### Returns\n* **Tuple[module, Text, Dict[LineLocation, OriginInfo]], containing**: \n\n"
}{
    "source file": "loader_impl.py",
    "line number": "275",
    "func name": "load",
    "func arg": "(sess, tags, export_dir, import_scope, **saver_kwargs)",
    "comments": "Loads the model from a SavedModel as specified by tags.\n\n\n##### Args\n* **sess**: The TensorFlow session to restore the variables.\n\n* **tags**: Set of string tags to identify the required MetaGraphDef. These should\n    correspond to the tags used when saving the variables using the\n    SavedModel `save()` API.\n\n* **export_dir**: Directory in which the SavedModel protocol buffer and variables\n    to be loaded are located.\n\n* **import_scope**: Optional `string` -- if specified, prepend this string\n    followed by '/' to all loaded tensor names. This scope is applied to\n    tensor instances loaded into the passed session, but it is *not* written\n    through to the static `MetaGraphDef` protocol buffer that is returned.\n\n* ****saver_kwargs**: Optional keyword arguments passed through to Saver.\n\n##### Returns\n"
}{}{
    "source file": "loader_test1.py",
    "line number": "48",
    "func name": "build_graph_helper",
    "func arg": "()",
    "comments": ""
}{
    "source file": "loader.py",
    "line number": "70",
    "func name": "load_ast",
    "func arg": "(nodes, indentation, include_source_map, delete_on_exit)",
    "comments": "Loads the given AST as a Python module.\n\nCompiling the AST code this way ensures that the source code is readable by e.g. `pdb` or `inspect`.\n##### Args\n* **nodes**: Union[ast.AST, Iterable[ast.AST]], the code to compile, as an AST\n  object.\n\n* **indentation**: Text, the string to use for indentation.\n\n* **include_source_map**: bool, whether return a source map.\n\n* **delete_on_exit**: bool, whether to delete the temporary file used for\n  compilation on exit.\n\n##### Returns\n* **Tuple[module, Text, Dict[LineLocation, OriginInfo]], containing**: \n\n"
}{}{}{}{
    "source file": "local_response_norm.py",
    "line number": "28",
    "func name": "make_local_response_norm_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do local_response_norm.\n\n\n"
}{
    "source file": "local_test.py",
    "line number": "529",
    "func name": "copy_model_weights",
    "func arg": "(model_from, model_to)",
    "comments": ""
}{
    "source file": "local.py",
    "line number": "821",
    "func name": "make_2d",
    "func arg": "(tensor, split_dim)",
    "comments": "Reshapes an N-dimensional tensor into a 2D tensor.\n\nDimensions before (excluding) and after (including) `split_dim` are grouped together.\n\nArguments: tensor: a tensor of shape `(d0, ..., d(N-1))`. split_dim: an integer from 1 to N-1, index of the dimension to group dimensions before (excluding) and after (including).\n##### Returns\n"
}{}{}{}{
    "source file": "log_softmax.py",
    "line number": "27",
    "func name": "make_log_softmax_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do log_softmax.\n\n\n"
}{}{}{
    "source file": "logging_ops.py",
    "line number": "635",
    "func name": "scalar_summary",
    "func arg": "(tags, values, collections, name)",
    "comments": "Outputs a `Summary` protocol buffer with scalar values.\n\nThis ops is deprecated. Please switch to tf.summary.scalar. For an explanation of why this op was deprecated, and information on how to migrate, look ['here'](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/deprecated/__init__.py)\n\nThe input `tags` and `values` must have the same shape.\n\nThe generated summary has a summary value for each tag-value pair in `tags` and `values`.\n##### Args\n* **tags**: A `string` `Tensor`.  Tags for the summaries.\n\n* **values**: A real numeric Tensor.  Values for the summaries.\n\n* **collections**: Optional list of graph collections keys. The new summary op is\n  added to these collections. Defaults to `[GraphKeys.SUMMARIES]`.\n\n* **name**: A name for the operation (optional).\n\n##### Returns\n"
}{}{
    "source file": "logic.py",
    "line number": "77",
    "func name": "make_logical_xor_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do logical_xor, test logical_not as well.\n\n\n"
}{}{
    "source file": "logical_expressions.py",
    "line number": "135",
    "func name": "transform",
    "func arg": "(node, ctx)",
    "comments": ""
}{}{
    "source file": "logical.py",
    "line number": "98",
    "func name": "not_eq",
    "func arg": "(a, b)",
    "comments": "Functional form of \"not-equal\".\n\n\n"
}{}{
    "source file": "lookup_ops.py",
    "line number": "1601",
    "func name": "index_to_string_table_from_tensor",
    "func arg": "(vocabulary_list, default_value, name)",
    "comments": "Returns a lookup table that maps a `Tensor` of indices into strings.\n\nThis operation constructs a lookup table to map int64 indices into string values. The mapping is initialized from a string `vocabulary_list` 1-D `Tensor` where each element is a value and the corresponding index within the tensor is the key.\n\nAny input which does not have a corresponding index in 'vocabulary_list' (an out-of-vocabulary entry) is assigned the `default_value`\n\nThe underlying table must be initialized by calling `session.run(tf.compat.v1.tables_initializer())` or `session.run(table.init())` once.\n\nElements in `vocabulary_list` cannot have duplicates, otherwise when executing the table initializer op, it will throw a `FailedPreconditionError`.\n\nSample Usages:\n\n```python vocabulary_list = tf.constant([\"emerson\", \"lake\", \"palmer\"]) indices = tf.constant([1, 5], tf.int64) table = tf.lookup.index_to_string_table_from_tensor( vocabulary_list, default_value=\"UNKNOWN\") values = table.lookup(indices) ... tf.compat.v1.tables_initializer().run()\n\nvalues.eval() ==> [\"lake\", \"UNKNOWN\"] ```\n##### Args\n* **vocabulary_list**: A 1-D string `Tensor` that specifies the strings to map\n  from indices.\n\n* **default_value**: The value to use for out-of-vocabulary indices.\n\n* **name**: A name for this op (optional).\n\n##### Returns\n"
}{}{
    "source file": "loss_scale_benchmark.py",
    "line number": "38",
    "func name": "_get_strategy",
    "func arg": "(num_gpus)",
    "comments": ""
}{
    "source file": "loss_scale_optimizer_test.py",
    "line number": "55",
    "func name": "create_mirrored_strategy",
    "func arg": "()",
    "comments": ""
}{
    "source file": "loss_scale_optimizer_test1.py",
    "line number": "69",
    "func name": "create_identity_with_grad_check_fn",
    "func arg": "(expected_gradient, expected_dtype)",
    "comments": "Returns a function that asserts it's gradient has a certain value.\n\nThis serves as a hook to assert intermediate gradients have a certain value. This returns an identity function. The identity's gradient function is also the identity function, except it asserts that the gradient equals `expected_gradient` and has dtype `expected_dtype`.\n##### Args\n* **expected_gradient**: The gradient function asserts that the gradient is this\n  value.\n\n* **expected_dtype**: The gradient function asserts the gradient has this dtype.\n\n##### Returns\n"
}{
    "source file": "loss_scale_optimizer.py",
    "line number": "603",
    "func name": "strategy_supports_loss_scaling",
    "func arg": "()",
    "comments": "Returns True if the current Strategy supports loss scaling.\n\n\n"
}{}{
    "source file": "loss_scale_test.py",
    "line number": "101",
    "func name": "_get_example_iter",
    "func arg": "(inputs)",
    "comments": ""
}{
    "source file": "loss_scale.py",
    "line number": "46",
    "func name": "get",
    "func arg": "(identifier)",
    "comments": ""
}{
    "source file": "loss_scale1.py",
    "line number": "421",
    "func name": "get",
    "func arg": "(identifier)",
    "comments": "Get a loss scale object.\n\n\n"
}{
    "source file": "loss_scaling_gradient_tape_test.py",
    "line number": "44",
    "func name": "create_mirrored_strategy",
    "func arg": "()",
    "comments": ""
}{
    "source file": "loss_scaling_gradient_tape.py",
    "line number": "204",
    "func name": "_compute_gradients_until_finite",
    "func arg": "(distribution, loss_scale_gradient_tapes, loss_scale, target, sources, output_gradients, unconnected_gradients)",
    "comments": "Compute gradients and update the loss scale until the gradients are finite.\n\nThis must be called in a cross-replica context.\n\nThis is a function instead of a method of LossScaleGradientTape, as the `self` parameter would be meaningless. There is one LossScaleGradientTape per replica, but this function is called once total (not per replica), so there cannot be a singular `self` parameter.\n##### Args\n* **distribution**: The distribution strategy in effect.\n\n* **loss_scale_gradient_tapes**: A PerReplica value of LossScaleGradientTapes.\n  Contains the LossScaleGradientTape of each replica.\n\n* **loss_scale**: The loss scale to use to scale the loss and unscale the\n  gradient.\n\n* **target**: a list or nested structure of Tensors or Variables to be\n  differentiated.\n\n* **sources**: a list or nested structure of Tensors or Variables. `target` will\n  be differentiated against elements in `sources`.\n\n* **output_gradients**: Passed to GradientTape.gradient\n\n* **unconnected_gradients**: Pass to GradientTape.gradient.\n\n##### Returns\n"
}{
    "source file": "losses_impl.py",
    "line number": "847",
    "func name": "sparse_softmax_cross_entropy",
    "func arg": "(labels, logits, weights, scope, loss_collection, reduction)",
    "comments": "Cross-entropy loss using `tf.nn.sparse_softmax_cross_entropy_with_logits`.\n\n`weights` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `weights` is a tensor of shape `[batch_size]`, then the loss weights apply to each corresponding sample.\n##### Args\n* **labels**: `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of\n  `labels` and result) and dtype `int32` or `int64`. Each entry in `labels`\n  must be an index in `[0, num_classes)`. Other values will raise an\n  exception when this op is run on CPU, and return `NaN` for corresponding\n  loss and gradient rows on GPU.\n\n* **logits**: Unscaled log probabilities of shape\n  `[d_0, d_1, ..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or\n  `float64`.\n\n* **weights**: Coefficients for the loss. This must be scalar or broadcastable to\n  `labels` (i.e. same rank and each dimension is either 1 or the same).\n\n* **scope**: the scope for the operations performed in computing the loss.\n\n* **loss_collection**: collection to which the loss will be added.\n\n* **reduction**: Type of reduction to apply to loss.\n\n##### Returns\n"
}{
    "source file": "losses_serialization_test.py",
    "line number": "59",
    "func name": "_get_multi_io_model",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "losses_utils.py",
    "line number": "281",
    "func name": "cast_losses_to_common_dtype",
    "func arg": "(losses)",
    "comments": "Cast a list of losses to a common dtype.\n\nIf any loss is floating-point, they will all be casted to the most-precise floating-point loss. Otherwise the losses are not casted. We also skip casting losses if there are any complex losses.\n##### Args\n* **losses**: A list of losses.\n\n##### Returns\n"
}{
    "source file": "losses.py",
    "line number": "1854",
    "func name": "get",
    "func arg": "(identifier)",
    "comments": "Retrieves a Keras loss as a `function`/`Loss` class instance.\n\nThe `identifier` may be the string name of a loss function or `Loss` class.\n\n>>> loss = tf.keras.losses.get(\"categorical_crossentropy\") >>> type(loss) <class 'function'> >>> loss = tf.keras.losses.get(\"CategoricalCrossentropy\") >>> type(loss) <class '...tensorflow.python.keras.losses.CategoricalCrossentropy'>\n\nYou can also specify `config` of the loss to this function by passing dict containing `class_name` and `config` as an identifier. Also note that the `class_name` must map to a `Loss` class\n\n>>> identifier = {\"class_name\": \"CategoricalCrossentropy\", ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n \"config\": {\"from_logits\": True}} >>> loss = tf.keras.losses.get(identifier) >>> type(loss) <class '...tensorflow.python.keras.losses.CategoricalCrossentropy'>\n\nArguments: identifier: A loss identifier. One of None or string name of a loss function/class or loss configuration dictionary or a loss function or a loss class instance\n##### Returns\n"
}{}{}{}{}{
    "source file": "lstm_test.py",
    "line number": "51",
    "func name": "_Clip",
    "func arg": "(x)",
    "comments": ""
}{}{}{
    "source file": "lstm1.py",
    "line number": "140",
    "func name": "BuildLSTMLayer",
    "func arg": "(batch_size, seq_length, num_inputs, num_nodes)",
    "comments": "Builds a single LSTM layer with random weights and inputs.\n\n\n##### Args\n* **batch_size**: Inputs are fed in batches of this size.\n\n* **seq_length**: The sequence length to unroll the LSTM layer.\n\n* **num_inputs**: Dimension of inputs that are fed into each LSTM cell.\n\n* **num_nodes**: The number of nodes in each LSTM cell.\n\n##### Returns\n"
}{
    "source file": "lstm2.py",
    "line number": "29",
    "func name": "make_lstm_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do basic Lstm cell.\n\n\n"
}{}{
    "source file": "main_op_impl.py",
    "line number": "57",
    "func name": "main_op_with_restore",
    "func arg": "(restore_op_name)",
    "comments": "Returns a main op to init variables, tables and restore the graph.\n\n\n##### Args\n* **restore_op_name**: Name of the op to use to restore the graph.\n\n##### Returns\n"
}{}{}{}{
    "source file": "make_test_graphs.py",
    "line number": "223",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{}{
    "source file": "manip_grad.py",
    "line number": "26",
    "func name": "_RollGrad",
    "func arg": "(op, grad)",
    "comments": ""
}{}{}{
    "source file": "manip_ops.py",
    "line number": "31",
    "func name": "roll",
    "func arg": "(input, shift, axis, name)",
    "comments": ""
}{}{}{}{}{
    "source file": "map_and_filter_fusion_test.py",
    "line number": "34",
    "func name": "_test_combinations",
    "func arg": "()",
    "comments": ""
}{}{}{}{
    "source file": "map_defun_op_test.py",
    "line number": "44",
    "func name": "_test_combinations",
    "func arg": "()",
    "comments": ""
}{
    "source file": "map_defun.py",
    "line number": "26",
    "func name": "map_defun",
    "func arg": "(fn, elems, output_dtypes, output_shapes, max_intra_op_parallelism)",
    "comments": "Map a function on the list of tensors unpacked from `elems` on dimension 0.\n\n\n##### Args\n* **fn**: A function (`function.defun`) that takes a list of tensors and returns\n  another list of tensors. The output list has the same types as\n  output_dtypes. The elements of the output list have the same dimension 0\n  as `elems`, and the remaining dimensions correspond to those of\n  `fn_output_shapes`.\n\n* **elems**: A list of tensors.\n\n* **output_dtypes**: A list of dtypes corresponding to the output types of the\n  function.\n\n* **output_shapes**: A list of `TensorShape`s corresponding to the output shapes\n  from each invocation of the function on slices of inputs.\n\n* **max_intra_op_parallelism**: An integer. If positive, sets the max parallelism\n  limit of each function call to this.\n\n##### Returns\n"
}{
    "source file": "map_fn_test.py",
    "line number": "41",
    "func name": "simple_scoped_fn",
    "func arg": "(a, x)",
    "comments": "Simple function: (a, x) -> 2(x+a), but with \"2\" as a variable in scope.\n\n\n"
}{
    "source file": "map_fn.py",
    "line number": "628",
    "func name": "map_fn_v2",
    "func arg": "(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name, fn_output_signature)",
    "comments": "Transform `elems` by applying `fn` to each element unstacked on axis 0.\n\n\n"
}{
    "source file": "map_fusion_test.py",
    "line number": "34",
    "func name": "_test_combinations",
    "func arg": "()",
    "comments": ""
}{
    "source file": "map_parallelization_test.py",
    "line number": "37",
    "func name": "_test_combinations",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "map_test.py",
    "line number": "119",
    "func name": "_make_coordinated_sloppy_dataset",
    "func arg": "(apply_map, num_elements, num_parallel_calls)",
    "comments": "Produces a dataset iterator and events to control the order of elements.\n\n\n##### Args\n* **apply_map**: method that applies the `map` transformation\n\n* **num_elements**: the number of input elements\n\n* **num_parallel_calls**: the degree of map parallelism\n\n##### Returns\n"
}{
    "source file": "map_vectorization_benchmark.py",
    "line number": "57",
    "func name": "_generate_parse_single_example_test_case",
    "func arg": "()",
    "comments": "Generates a `parse_single_example()` test case.\n\n\n"
}{
    "source file": "map_vectorization_test.py",
    "line number": "150",
    "func name": "_binary_real_test_combinations",
    "func arg": "()",
    "comments": ""
}{}{}{}{}{}{
    "source file": "math_grad.py",
    "line number": "1985",
    "func name": "_NextAfterGrad",
    "func arg": "(op, grad)",
    "comments": "Returns gradient of nextafter(x1, x2) with respect to x1 and x2.\n\n\n"
}{}{}{
    "source file": "math_ops.py",
    "line number": "4857",
    "func name": "rsqrt",
    "func arg": "(x, name)",
    "comments": "Computes reciprocal of square root of x element-wise.\n\nFor example:\n\n>>> x = tf.constant([2., 0., -2.]) >>> tf.math.rsqrt(x) <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.707, inf, nan], dtype=float32)>\n##### Args\n* **x**: A `tf.Tensor`. Must be one of the following types\n\n* **name**: A name for the operation (optional).\n\n##### Returns\n"
}{}{
    "source file": "matmul_benchmark.py",
    "line number": "35",
    "func name": "build_graph",
    "func arg": "(device, n, m, k, transpose_a, transpose_b, dtype)",
    "comments": "Build a graph containing a sequence of matmul operations.\n\n\n##### Args\n* **device**: String, the device to run on.\n\n* **n**: tensor A's first dimension size.\n\n* **m**: tensor A's second dimension size.\n\n* **k**: tensor B's second dimension size.\n\n* **transpose_a**: boolean value to show if tensor A is transposed.\n\n* **transpose_b**: boolean value to show if tensor B is transposed.\n\n* **dtype**: numpy data type of the input tensor.\n\n##### Returns\n"
}{
    "source file": "matmul_op_test.py",
    "line number": "115",
    "func name": "_GetMatMulGradientTest",
    "func arg": "(a_np_, b_np_, use_static_shape_, **kwargs_)",
    "comments": ""
}{
    "source file": "matmul_registrations.py",
    "line number": "208",
    "func name": "_matmul_linear_operator_circulant_circulant",
    "func arg": "(linop_a, linop_b)",
    "comments": ""
}{
    "source file": "matrix_band_part_op_test.py",
    "line number": "76",
    "func name": "_GetMatrixBandPartGradTest",
    "func arg": "(dtype_, batch_shape_, shape_)",
    "comments": ""
}{}{
    "source file": "matrix_diag_ops_test.py",
    "line number": "329",
    "func name": "all_tests",
    "func arg": "(align)",
    "comments": ""
}{
    "source file": "matrix_diag.py",
    "line number": "27",
    "func name": "make_matrix_diag_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests for tf.linalg.diag op.\n\n\n"
}{
    "source file": "matrix_exponential_op_test.py",
    "line number": "227",
    "func name": "_TestL1Norms",
    "func arg": "(dtype, shape, scale)",
    "comments": ""
}{}{}{}{
    "source file": "matrix_set_diag.py",
    "line number": "27",
    "func name": "make_matrix_set_diag_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests for tf.linalg.set_diag op.\n\n\n"
}{
    "source file": "matrix_solve_ls_op_test.py",
    "line number": "233",
    "func name": "_GetLargeMatrixSolveLsOpTests",
    "func arg": "(dtype, use_placeholder, fast, l2_regularizer)",
    "comments": ""
}{}{}{}{
    "source file": "matrix_triangular_solve_op_test.py",
    "line number": "36",
    "func name": "MakePlaceholder",
    "func arg": "(x)",
    "comments": ""
}{}{
    "source file": "maximum.py",
    "line number": "27",
    "func name": "make_maximum_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do maximum.\n\n\n"
}{
    "source file": "mel_ops_test.py",
    "line number": "57",
    "func name": "spectrogram_to_mel_matrix",
    "func arg": "(num_mel_bins, num_spectrogram_bins, audio_sample_rate, lower_edge_hertz, upper_edge_hertz, unused_dtype)",
    "comments": "Return a matrix that can post-multiply spectrogram rows to make mel.\n\nCopied from https://github.com/tensorflow/models/blob/master/research/audioset/mel_features.py.\n##### Args\n* **num_mel_bins**: How many bands in the resulting mel spectrum.  This is\n  the number of columns in the output matrix.\n\n* **num_spectrogram_bins**: How many bins there are in the source spectrogram\n  data, which is understood to be fft_size/2 + 1, i.e. the spectrogram\n  only contains the nonredundant FFT bins.\n\n* **audio_sample_rate**: Samples per second of the audio at the input to the\n  spectrogram. We need this to figure out the actual frequencies for\n  each spectrogram bin, which dictates how they are mapped into mel.\n\n* **lower_edge_hertz**: Lower bound on the frequencies to be included in the mel\n  spectrum.  This corresponds to the lower edge of the lowest triangular\n  band.\n\n* **upper_edge_hertz**: The desired top edge of the highest frequency band.\n\n##### Returns\n"
}{
    "source file": "mel_ops.py",
    "line number": "95",
    "func name": "linear_to_mel_weight_matrix",
    "func arg": "(num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz, upper_edge_hertz, dtype, name)",
    "comments": "Returns a matrix to warp linear scale spectrograms to the [mel scale][mel].\n\n\n##### Args\n* **num_mel_bins**: Python int. How many bands in the resulting mel spectrum.\n\n* **num_spectrogram_bins**: An integer `Tensor`. How many bins there are in the\n  source spectrogram data, which is understood to be `fft_size // 2 + 1`,\n  i.e. the spectrogram only contains the nonredundant FFT bins.\n\n* **sample_rate**: An integer or float `Tensor`. Samples per second of the input\n  signal used to create the spectrogram. Used to figure out the frequencies\n  corresponding to each spectrogram bin, which dictates how they are mapped\n  into the mel scale.\n\n* **lower_edge_hertz**: Python float. Lower bound on the frequencies to be\n  included in the mel spectrum. This corresponds to the lower edge of the\n  lowest triangular band.\n\n* **upper_edge_hertz**: Python float. The desired top edge of the highest\n  frequency band.\n\n* **dtype**: The `DType` of the result matrix. Must be a floating point type.\n\n* **name**: An optional name for the operation.\n\n##### Returns\n"
}{}{}{}{
    "source file": "memory_checker.py",
    "line number": "32",
    "func name": "_get_test_name_best_effort",
    "func arg": "()",
    "comments": "If available, return the current test name. Otherwise, `None`.\n\n\n"
}{}{}{
    "source file": "memory_test_util.py",
    "line number": "78",
    "func name": "memory_profiler_is_available",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "memory.py",
    "line number": "24",
    "func name": "dismantle_ordered_dict",
    "func arg": "(ordered_dict)",
    "comments": "Remove reference cycle in OrderedDict `ordered_dict`.\n\nHelpful for making sure the garbage collector doesn't need to run after using an OrderedDict.\n##### Args\n* **ordered_dict**: A `OrderedDict` object to destroy. This object is unusable\n  after this function runs.\n\n"
}{
    "source file": "merge_arduino_zips.py",
    "line number": "39",
    "func name": "parse_args",
    "func arg": "()",
    "comments": "Converts the raw arguments into accessible flags.\n\n\n"
}{}{
    "source file": "merge.py",
    "line number": "935",
    "func name": "dot",
    "func arg": "(inputs, axes, normalize, **kwargs)",
    "comments": "Functional interface to the `Dot` layer.\n\nArguments: inputs: A list of input tensors (at least 2). axes: Integer or tuple of integers, axis or axes along which to take the dot product. normalize: Whether to L2-normalize samples along the dot product axis before taking the dot product. If set to True, then the output of the dot product is the cosine proximity between the two samples. **kwargs: Standard layer keyword arguments.\n##### Returns\n"
}{}{
    "source file": "meta_graph_test.py",
    "line number": "54",
    "func name": "_TestDir",
    "func arg": "(test_name)",
    "comments": ""
}{
    "source file": "meta_graph.py",
    "line number": "1068",
    "func name": "copy_scoped_meta_graph",
    "func arg": "(from_scope, to_scope, from_graph, to_graph)",
    "comments": "Copies a sub-meta_graph from one scope to another.\n\n\n##### Args\n* **from_scope**: `String` name scope containing the subgraph to be copied.\n\n* **to_scope**: `String` name scope under which the copied subgraph will reside.\n\n* **from_graph**: Optional `Graph` from which to copy the subgraph. If `None`, the\n  default graph is use.\n\n* **to_graph**: Optional `Graph` to which to copy the subgraph. If `None`, the\n  default graph is used.\n\n##### Returns\n"
}{}{}{}{}{}{
    "source file": "metrics_correctness_test.py",
    "line number": "48",
    "func name": "custom_generator_multi_io",
    "func arg": "(sample_weights)",
    "comments": ""
}{}{
    "source file": "metrics_impl.py",
    "line number": "3632",
    "func name": "specificity_at_sensitivity",
    "func arg": "(labels, predictions, sensitivity, weights, num_thresholds, metrics_collections, updates_collections, name)",
    "comments": "Computes the specificity at a given sensitivity.\n\nThe `specificity_at_sensitivity` function creates four local variables, `true_positives`, `true_negatives`, `false_positives` and `false_negatives` that are used to compute the specificity at the given sensitivity value. The threshold for the given sensitivity value is computed and used to evaluate the corresponding specificity.\n\nFor estimation of the metric over a stream of data, the function creates an `update_op` operation that updates these variables and returns the `specificity`. `update_op` increments the `true_positives`, `true_negatives`, `false_positives` and `false_negatives` counts with the weight of each case found in the `predictions` and `labels`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\nFor additional information about specificity and sensitivity, see the following: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n##### Args\n* **labels**: The ground truth values, a `Tensor` whose dimensions must match\n  `predictions`. Will be cast to `bool`.\n\n* **predictions**: A floating point `Tensor` of arbitrary shape and whose values\n  are in the range `[0, 1]`.\n\n* **sensitivity**: A scalar value in range `[0, 1]`.\n\n* **weights**: Optional `Tensor` whose rank is either 0, or the same rank as\n  `labels`, and must be broadcastable to `labels` (i.e., all dimensions must\n  be either `1`, or the same as the corresponding `labels` dimension).\n\n* **num_thresholds**: The number of thresholds to use for matching the given\n  sensitivity.\n\n* **metrics_collections**: An optional list of collections that `specificity`\n  should be added to.\n\n* **updates_collections**: An optional list of collections that `update_op` should\n  be added to.\n\n* **name**: An optional variable_scope name.\n\n##### Returns\n* **specificity**: A scalar `Tensor` representing the specificity at the given\n  `sensitivity` value.\n\n* **update_op**: An operation that increments the `true_positives`,\n  `true_negatives`, `false_positives` and `false_negatives` variables\n  appropriately and whose value matches `specificity`.\n\n"
}{
    "source file": "metrics_serialization_test.py",
    "line number": "56",
    "func name": "_get_multi_io_model",
    "func arg": "()",
    "comments": ""
}{
    "source file": "metrics_test.py",
    "line number": "2066",
    "func name": "_get_model",
    "func arg": "(compile_metrics)",
    "comments": ""
}{
    "source file": "metrics_test1.py",
    "line number": "3396",
    "func name": "_reweight",
    "func arg": "(predictions, labels, weights)",
    "comments": ""
}{}{
    "source file": "metrics_utils.py",
    "line number": "479",
    "func name": "ragged_assert_compatible_and_get_flat_values",
    "func arg": "(values, mask)",
    "comments": "If ragged, it checks the compatibility and then returns the flat_values.\n\nNote: If two tensors are dense, it does not check their compatibility. Note: Although two ragged tensors with different ragged ranks could have identical overall rank and dimension sizes and hence be compatible, we do not support those cases.\n##### Args\n* **values**: A list of potentially ragged tensor of the same ragged_rank.\n\n* **mask**: A potentially ragged tensor of the same ragged_rank as elements in\n  Values.\n\n##### Returns\n"
}{
    "source file": "metrics_v1_test.py",
    "line number": "86",
    "func name": "tpu_combinations",
    "func arg": "()",
    "comments": ""
}{
    "source file": "metrics.py",
    "line number": "3497",
    "func name": "is_built_in",
    "func arg": "(cls)",
    "comments": ""
}{}{}{
    "source file": "mfcc_ops.py",
    "line number": "31",
    "func name": "mfccs_from_log_mel_spectrograms",
    "func arg": "(log_mel_spectrograms, name)",
    "comments": "Computes [MFCCs][mfcc] of `log_mel_spectrograms`.\n\nImplemented with GPU-compatible ops and supports gradients.\n\n[Mel-Frequency Cepstral Coefficient (MFCC)][mfcc] calculation consists of taking the DCT-II of a log-magnitude mel-scale spectrogram. [HTK][htk]'s MFCCs use a particular scaling of the DCT-II which is almost orthogonal normalization. We follow this convention.\n\nAll `num_mel_bins` MFCCs are returned and it is up to the caller to select a subset of the MFCCs based on their application. For example, it is typical to only use the first few for speech recognition, as this results in an approximately pitch-invariant representation of the signal.\n\nFor example:\n\n```python batch_size, num_samples, sample_rate = 32, 32000, 16000.0 # A Tensor of [batch_size, num_samples] mono PCM samples in the range [-1, 1]. pcm = tf.random.normal([batch_size, num_samples], dtype=tf.float32)\n\n# A 1024-point STFT with frames of 64 ms and 75% overlap. stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256, fft_length=1024) spectrograms = tf.abs(stfts)\n\n# Warp the linear scale spectrograms into the mel-scale. num_spectrogram_bins = stfts.shape[-1].value lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80 linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix( num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz, upper_edge_hertz) mel_spectrograms = tf.tensordot( spectrograms, linear_to_mel_weight_matrix, 1) mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate( linear_to_mel_weight_matrix.shape[-1:]))\n\n# Compute a stabilized log to get log-magnitude mel-scale spectrograms. log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n\n# Compute MFCCs from log_mel_spectrograms and take the first 13. mfccs = tf.signal.mfccs_from_log_mel_spectrograms( log_mel_spectrograms)[..., :13] ```\n##### Args\n* **log_mel_spectrograms**: A `[..., num_mel_bins]` `float32`/`float64` `Tensor`\n  of log-magnitude mel-scale spectrograms.\n\n* **name**: An optional name for the operation.\n\n##### Returns\n"
}{}{
    "source file": "minimum.py",
    "line number": "27",
    "func name": "make_minimum_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do minimum.\n\n\n"
}{
    "source file": "mirror_pad.py",
    "line number": "28",
    "func name": "make_mirror_pad_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do mirror_pad.\n\n\n"
}{}{
    "source file": "mirrored_function_strategy.py",
    "line number": "158",
    "func name": "_unwrap_tensors",
    "func arg": "(maybe_wrapped)",
    "comments": ""
}{
    "source file": "mirrored_run.py",
    "line number": "128",
    "func name": "_call_for_each_replica",
    "func arg": "(distribution, fn, args, kwargs)",
    "comments": "Run `fn` in separate threads, once per replica/worker device.\n\n\n##### Args\n* **distribution**: the DistributionStrategy object.\n\n* **fn**: function to run (will be run once per replica, each in its own thread).\n\n* **args**: positional arguments for `fn`\n\n* **kwargs**: keyword arguments for `fn`.\n\n##### Returns\n"
}{
    "source file": "mirrored_strategy_test.py",
    "line number": "1434",
    "func name": "_replica_id_as_int",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "mirrored_strategy.py",
    "line number": "178",
    "func name": "all_devices",
    "func arg": "()",
    "comments": ""
}{
    "source file": "mirrored_variable_test.py",
    "line number": "50",
    "func name": "_mimic_two_cpus",
    "func arg": "()",
    "comments": ""
}{
    "source file": "mirrored_variable_test1.py",
    "line number": "33",
    "func name": "_mimic_two_cpus",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "misc.py",
    "line number": "55",
    "func name": "get_range_len",
    "func arg": "(start, limit, delta)",
    "comments": ""
}{}{}{}{
    "source file": "mixed_precision.py",
    "line number": "376",
    "func name": "disable_mixed_precision_graph_rewrite_v1",
    "func arg": "()",
    "comments": "Disables the mixed precision graph rewrite.\n\nAfter this is called, the mixed precision graph rewrite will no longer run for new Sessions, and so float32 operations will no longer be converted to float16 in such Sessions. However, any existing Sessions will continue to have the graph rewrite enabled if they were created after `enable_mixed_precision_graph_rewrite` was called but before `disable_mixed_precision_graph_rewrite` was called.\n\nThis does not undo the effects of loss scaling. Any optimizers wrapped with a LossScaleOptimizer will continue to do loss scaling, although this loss scaling will no longer be useful if the optimizer is used in new Sessions, as the graph rewrite no longer converts the graph to use float16.\n\nThis function is useful for unit testing. A unit tests can test using the mixed precision graph rewrite, then disable it so future unit tests continue using float32. If this is done, unit tests should not share a single session, as `enable_mixed_precision_graph_rewrite` and `disable_mixed_precision_graph_rewrite` have no effect on existing sessions.\n"
}{}{
    "source file": "mlir_gen.py",
    "line number": "444",
    "func name": "mlir_gen_from_source",
    "func arg": "(source, src_file)",
    "comments": "Parse a function as either a string or from a supplied file path and return a TFProgram.\n\n\n"
}{}{
    "source file": "mlir.py",
    "line number": "26",
    "func name": "convert_graph_def",
    "func arg": "(graph_def, pass_pipeline)",
    "comments": "Import a GraphDef and convert it to a textual MLIR module.\n\n\n##### Args\n* **graph_def**: An object of type graph_pb2.GraphDef or a textual proto\n  representation of a valid GraphDef.\n\n* **pass_pipeline**: A textual description of an MLIR Pass Pipeline to run on the\n  module, see MLIR documentation for the\n  [textual pass pipeline syntax](https\n\n##### Returns\n"
}{
    "source file": "mnist_softmax_xla.py",
    "line number": "34",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "mnist_tflite.py",
    "line number": "72",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "mnist_util.py",
    "line number": "49",
    "func name": "_prepare_label",
    "func arg": "(y)",
    "comments": "Conerts labels to one-hot encoding.\n\n\n"
}{
    "source file": "mnist_with_summaries.py",
    "line number": "185",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "mnist.py",
    "line number": "28",
    "func name": "load_data",
    "func arg": "(path)",
    "comments": "Loads the [MNIST dataset](http://yann.lecun.com/exdb/mnist/).\n\nThis is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. More info can be found at the [MNIST homepage](http://yann.lecun.com/exdb/mnist/).\n\n Arguments: path: path where to cache the dataset locally (relative to `~/.keras/datasets`).\n##### Returns\n* **Tuple of Numpy arrays**: `(x_train, y_train), (x_test, y_test)`.\n\n* ****x_train, x_test****: uint8 arrays of grayscale image data with shapes\n  (num_samples, 28, 28).\n\n* ****y_train, y_test****: uint8 arrays of digit labels (integers in range 0-9)\n  with shapes (num_samples,).\n\n* **nse**: \n\n* **https**: //creativecommons.org/licenses/by-sa/3.0/)\n\n"
}{
    "source file": "mnist1.py",
    "line number": "254",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "mnist2.py",
    "line number": "130",
    "func name": "evaluation",
    "func arg": "(logits, labels)",
    "comments": "Evaluate the quality of the logits at predicting the label.\n\n\n##### Args\n* **logits**: Logits tensor, float - [batch_size, NUM_CLASSES].\n\n* **labels**: Labels tensor, int32 - [batch_size], with values in the\n  range [0, NUM_CLASSES).\n\n##### Returns\n"
}{}{
    "source file": "mobilenet_v2.py",
    "line number": "506",
    "func name": "decode_predictions",
    "func arg": "(preds, top)",
    "comments": ""
}{
    "source file": "mobilenet.py",
    "line number": "449",
    "func name": "decode_predictions",
    "func arg": "(preds, top)",
    "comments": ""
}{}{}{
    "source file": "mode_keys1.py",
    "line number": "64",
    "func name": "is_train",
    "func arg": "(mode)",
    "comments": ""
}{}{}{
    "source file": "model_analyzer_testlib.py",
    "line number": "114",
    "func name": "CheckAndRemoveDoc",
    "func arg": "(profile)",
    "comments": ""
}{
    "source file": "model_analyzer.py",
    "line number": "24",
    "func name": "GenerateModelReport",
    "func arg": "(metagraph, assume_valid_feeds, debug)",
    "comments": "Report what's known statically about each node in the provided metagraph.\n\n\n##### Args\n* **metagraph**: A TensorFlow MetaGraphDef.\n\n* **assume_valid_feeds**: If True, assume that the shape of the fed nodes is valid\n\n* **debug**: Add some information useful for debugging.\n\n##### Returns\n"
}{
    "source file": "model_analyzer1.py",
    "line number": "385",
    "func name": "advise",
    "func arg": "(graph, run_meta, options)",
    "comments": "Auto profile and advise.\n\nBuilds profiles and automatically check anomalies of various aspects. For more details: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/README.md\n##### Args\n* **graph**: tf.Graph. If None and eager execution is not enabled, use\n    default graph.\n\n* **run_meta**: optional tensorflow.RunMetadata proto. It is necessary to\n    to support run time information profiling, such as time and memory.\n\n* **options**: see ALL_ADVICE example above. Default checks everything.\n\n"
}{}{
    "source file": "model_architectures.py",
    "line number": "292",
    "func name": "get_models",
    "func arg": "(exclude_models)",
    "comments": "Get all models excluding the specificed ones.\n\n\n"
}{}{}{
    "source file": "model_components_benchmarks_test.py",
    "line number": "86",
    "func name": "run_benchmark",
    "func arg": "(func, num_iters, execution_mode)",
    "comments": ""
}{
    "source file": "model_config.py",
    "line number": "100",
    "func name": "model_from_json",
    "func arg": "(json_string, custom_objects)",
    "comments": "Parses a JSON model configuration string and returns a model instance.\n\nUsage:\n\n>>> model = tf.keras.Sequential([ ...\n\n\n\n tf.keras.layers.Dense(5, input_shape=(3,)), ...\n\n\n\n tf.keras.layers.Softmax()]) >>> config = model.to_json() >>> loaded_model = tf.keras.models.model_from_json(config)\n\nArguments: json_string: JSON string encoding a model configuration. custom_objects: Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization.\n##### Returns\n"
}{}{
    "source file": "model_coverage_lib.py",
    "line number": "661",
    "func name": "test_keras_model_v2",
    "func arg": "(filename, input_shapes, input_data, input_data_range, **kwargs)",
    "comments": "Validates the tf.keras model converts to a TFLite model.\n\nConverts the tf.keras model to TFLite and checks the accuracy of the model on random data.\n##### Args\n* **filename**: Full filepath of HDF5 file containing the tf.keras model.\n\n* **input_shapes**: List of list of integers representing input shapes in the\n  order of the tf.keras model's .input attribute (e.g., [[1, 16, 16, 3]]).\n  (default None)\n\n* **input_data**: np.ndarray to pass into models during inference. (default None).\n\n* **input_data_range**: A map where the key is the input tensor name and\n  the value is a tuple (min_val, max_val) which specifies the value range of\n  the corresponding input tensor. For example, '{'input1'\n\n* ****kwargs**: Additional arguments to be passed into the converter.\n\n"
}{}{}{}{}{
    "source file": "model_subclassing_test_util.py",
    "line number": "109",
    "func name": "get_nested_model_3",
    "func arg": "(input_dim, num_classes)",
    "comments": ""
}{}{}{
    "source file": "models_test1.py",
    "line number": "307",
    "func name": "_has_placeholder",
    "func arg": "(graph)",
    "comments": ""
}{
    "source file": "models.py",
    "line number": "765",
    "func name": "create_tiny_embedding_conv_model",
    "func arg": "(fingerprint_input, model_settings, is_training)",
    "comments": "Builds a convolutional model aimed at microcontrollers.\n\nDevices like DSPs and microcontrollers can have very small amounts of memory and limited processing power. This model is designed to use less than 20KB of working RAM, and fit within 32KB of read-only (flash) memory.\n\nHere's the layout of the graph:\n\n(fingerprint_input) v [Conv2D]<-(weights) v [BiasAdd]<-(bias) v [Relu] v [Conv2D]<-(weights) v [BiasAdd]<-(bias) v [Relu] v [Conv2D]<-(weights) v [BiasAdd]<-(bias) v [Relu] v [MatMul]<-(weights) v [BiasAdd]<-(bias) v\n\nThis doesn't produce particularly accurate results, but it's designed to be used as the first stage of a pipeline, running on a low-energy piece of hardware that can always be on, and then wake higher-power chips when a possible utterance has been found, so that more accurate analysis can be done.\n\nDuring training, a dropout node is introduced after the relu, controlled by a placeholder.\n##### Args\n* **fingerprint_input**: TensorFlow node that will output audio feature vectors.\n\n* **model_settings**: Dictionary of information about the model.\n\n* **is_training**: Whether the model is going to be used for training.\n\n##### Returns\n"
}{
    "source file": "models1.py",
    "line number": "589",
    "func name": "clone_and_build_model",
    "func arg": "(model, input_tensors, target_tensors, custom_objects, compile_clone, in_place_reset, optimizer_iterations, optimizer_config)",
    "comments": "Clone a `Model` and build/compile it with the same settings used before.\n\nThis function can be be run in the same graph or in a separate graph from the model. When using a separate graph, `in_place_reset` must be `False`.\n\nNote that, currently, the clone produced from this function may not work with TPU DistributionStrategy. Try at your own risk.\n##### Args\n* **model**: `tf.keras.Model` object. Can be Functional, Sequential, or\n  sub-classed.\n\n* **input_tensors**: Optional list or dictionary of input tensors to build the\n  model upon. If not provided, placeholders will be created.\n\n* **target_tensors**: Optional list of target tensors for compiling the model. If\n  not provided, placeholders will be created.\n\n* **custom_objects**: Optional dictionary mapping string names to custom classes\n  or functions.\n\n* **compile_clone**: Boolean, whether to compile model clone (default `True`).\n\n* **in_place_reset**: Boolean, whether to reset the model in place. Only used if\n  the model is a subclassed model. In the case of a subclassed model,\n  this argument must be set to `True` (default `False`). To restore the\n  original model, use the function\n  `in_place_subclassed_model_state_restoration(model)`.\n\n* **optimizer_iterations**: An iterations variable that will be incremented by the\n  optimizer if the clone is compiled. This argument is used when a Keras\n  model is cloned into an Estimator model function, because Estimators\n  create their own global step variable.\n\n* **optimizer_config**: Optimizer config dictionary or list of dictionary\n  returned from `get_config()`. This argument should be defined if\n  `clone_and_build_model` is called in a different graph or session from\n  the original model, and the optimizer is an instance of `OptimizerV2`.\n\n##### Returns\n"
}{}{
    "source file": "modify_model_interface_lib_test.py",
    "line number": "31",
    "func name": "build_tflite_model_with_full_integer_quantization",
    "func arg": "()",
    "comments": ""
}{
    "source file": "modify_model_interface_lib.py",
    "line number": "52",
    "func name": "modify_model_interface",
    "func arg": "(input_file, output_file, input_type, output_type)",
    "comments": "Modify a quantized model's interface (input/output) from float to integer.\n\n\n##### Args\n* **input_file**: Full path name to the input tflite file.\n\n* **output_file**: Full path name to the output tflite file.\n\n* **input_type**: Final input interface type.\n\n* **output_type**: Final output interface type.\n\n"
}{
    "source file": "modify_model_interface.py",
    "line number": "38",
    "func name": "main",
    "func arg": "(_)",
    "comments": "Application run loop.\n\n\n"
}{}{}{
    "source file": "module_test1.py",
    "line number": "317",
    "func name": "get_name_scope",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "module_util.py",
    "line number": "33",
    "func name": "get_parent_dir_for_name",
    "func arg": "(module_name)",
    "comments": "Get parent directory for module with the given name.\n\n\n##### Args\n* **module_name**: Module name for e.g.\n  tensorflow_estimator.python.estimator.api._v1.estimator.\n\n##### Returns\n* **Given example above, it should return**: /pathtoestimator/tensorflow_estimator/python/estimator/api/_v1.\n\n"
}{}{
    "source file": "module_wrapper.py",
    "line number": "56",
    "func name": "has_deprecation_decorator",
    "func arg": "(symbol)",
    "comments": "Checks if given object has a deprecation decorator.\n\nWe check if deprecation decorator is in decorators as well as whether symbol is a class whose __init__ method has a deprecation decorator.\n##### Args\n* **symbol**: Python object.\n\n##### Returns\n"
}{
    "source file": "module.py",
    "line number": "323",
    "func name": "_flatten_module",
    "func arg": "(module, recursive, predicate, attribute_traversal_key, attributes_to_ignore, with_path, module_path, seen)",
    "comments": "Implementation of `flatten`.\n\n\n"
}{}{}{}{}{
    "source file": "monitored_session_test.py",
    "line number": "636",
    "func name": "busy_wait_for_coord_stop",
    "func arg": "(coord)",
    "comments": ""
}{
    "source file": "monitored_session.py",
    "line number": "434",
    "func name": "MonitoredTrainingSession",
    "func arg": "(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir, save_graph_def)",
    "comments": "Creates a `MonitoredSession` for training.\n\nFor a chief, this utility sets proper session initializer/restorer. It also creates hooks related to checkpoint and summary saving. For workers, this utility sets proper session creator which waits for the chief to initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for more information.\n##### Args\n* **master**: `String` the TensorFlow master to use.\n\n* **is_chief**: If `True`, it will take care of initialization and recovery the\n  underlying TensorFlow session. If `False`, it will wait on a chief to\n  initialize or recover the TensorFlow session.\n\n* **checkpoint_dir**: A string.  Optional path to a directory where to restore\n  variables.\n\n* **scaffold**: A `Scaffold` used for gathering or building supportive ops. If not\n  specified, a default one is created. It's used to finalize the graph.\n\n* **hooks**: Optional list of `SessionRunHook` objects.\n\n* **chief_only_hooks**: list of `SessionRunHook` objects. Activate these hooks if\n  `is_chief==True`, ignore otherwise.\n\n* **save_checkpoint_secs**: The frequency, in seconds, that a checkpoint is saved\n  using a default checkpoint saver. If both `save_checkpoint_steps` and\n  `save_checkpoint_secs` are set to `None`, then the default checkpoint\n  saver isn't used. If both are provided, then only `save_checkpoint_secs`\n  is used. Default 600.\n\n* **save_summaries_steps**: The frequency, in number of global steps, that the\n  summaries are written to disk using a default summary saver. If both\n  `save_summaries_steps` and `save_summaries_secs` are set to `None`, then\n  the default summary saver isn't used. Default 100.\n\n* **save_summaries_secs**: The frequency, in secs, that the summaries are written\n  to disk using a default summary saver.  If both `save_summaries_steps` and\n  `save_summaries_secs` are set to `None`, then the default summary saver\n  isn't used. Default not enabled.\n\n* **config**: an instance of `tf.compat.v1.ConfigProto` proto used to configure\n  the session. It's the `config` argument of constructor of\n  `tf.compat.v1.Session`.\n\n* **stop_grace_period_secs**: Number of seconds given to threads to stop after\n  `close()` has been called.\n\n* **log_step_count_steps**: The frequency, in number of global steps, that the\n  global step/sec is logged.\n\n* **max_wait_secs**: Maximum time workers should wait for the session to become\n  available. This should be kept relatively short to help detect incorrect\n  code, but sometimes may need to be increased if the chief takes a while to\n  start up.\n\n* **save_checkpoint_steps**: The frequency, in number of global steps, that a\n  checkpoint is saved using a default checkpoint saver. If both\n  `save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then\n  the default checkpoint saver isn't used. If both are provided, then only\n  `save_checkpoint_secs` is used. Default not enabled.\n\n* **summary_dir**: A string.  Optional path to a directory where to save\n  summaries. If None, checkpoint_dir is used instead.\n\n* **save_graph_def**: Whether to save the GraphDef and MetaGraphDef to\n  `checkpoint_dir`. The GraphDef is saved after the session is created as\n  `graph.pbtxt`. MetaGraphDefs are saved out for every checkpoint as\n  `model.ckpt-*.meta`.\n\n##### Returns\n"
}{}{
    "source file": "monitoring.py",
    "line number": "484",
    "func name": "monitored_timer",
    "func arg": "(cell)",
    "comments": "A function decorator for adding MonitoredTimer support.\n\nArguments: cell: the cell associated with the time metric that will be inremented.\n##### Returns\n"
}{}{}{
    "source file": "moving_averages_test1.py",
    "line number": "159",
    "func name": "_Repeat",
    "func arg": "(value, dim)",
    "comments": ""
}{
    "source file": "moving_averages.py",
    "line number": "195",
    "func name": "_zero_debias",
    "func arg": "(strategy, unbiased_var, value, decay)",
    "comments": "Compute the delta required for a debiased Variable.\n\nAll exponential moving averages initialized with Tensors are initialized to 0, and therefore are biased to 0. Variables initialized to 0 and used as EMAs are similarly biased. This function creates the debias updated amount according to a scale factor, as in (Kingma et al., 2015).\n\nTo demonstrate the bias the results from 0-initialization, take an EMA that was initialized to `0` with decay `b`. After `t` timesteps of seeing the constant `c`, the variable have the following value:\n\n``` EMA = 0*b^(t) + c*(1\n\n- b)*b^(t-1) + c*(1\n\n- b)*b^(t-2) + ... = c*(1\n\n- b^t) ```\n\nTo have the true value `c`, we would divide by the scale factor `1\n\n- b^t`.\n\nIn order to perform debiasing, we use two shadow variables. One keeps track of the biased estimate, and the other keeps track of the number of updates that have occurred.\n##### Args\n* **strategy**: `Strategy` used to create and update variables.\n\n* **unbiased_var**: A Variable representing the current value of the unbiased EMA.\n\n* **value**: A Tensor representing the most recent value.\n\n* **decay**: A Tensor representing `1-decay` for the EMA.\n\n##### Returns\n* **ferences**: \n\n* **Adam - A Method for Stochastic Optimization**: [Kingma et al., 2015](https\n\n"
}{
    "source file": "multi_arguments_results_v1.py",
    "line number": "52",
    "func name": "Test",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "multi_device_iterator_ops.py",
    "line number": "193",
    "func name": "_create_device_dataset",
    "func arg": "(prototype_ds, incarnation_id, prefetch_buffer_size, experimental_slack)",
    "comments": "Uses _prototype_device_datasets[i] to build a dataset for the device.\n\n\n"
}{
    "source file": "multi_device_iterator_test.py",
    "line number": "42",
    "func name": "skip_v2_test_combinations",
    "func arg": "()",
    "comments": ""
}{
    "source file": "multi_gpu_utils_test.py",
    "line number": "32",
    "func name": "check_if_compatible_devices",
    "func arg": "(gpus)",
    "comments": ""
}{
    "source file": "multi_gpu_utils.py",
    "line number": "42",
    "func name": "multi_gpu_model",
    "func arg": "(model, gpus, cpu_merge, cpu_relocation)",
    "comments": "Replicates a model on different GPUs.\n\nSpecifically, this function implements single-machine multi-GPU data parallelism. It works in the following way:\n\n- Divide the model's input(s) into multiple sub-batches.\n\n- Apply a model copy on each sub-batch. Every model copy is executed on a dedicated GPU.\n\n- Concatenate the results (on CPU) into one big batch.\n\nE.g. if your `batch_size` is 64 and you use `gpus=2`, then we will divide the input into 2 sub-batches of 32 samples, process each sub-batch on one GPU, then return the full batch of 64 processed samples.\n\nThis induces quasi-linear speedup on up to 8 GPUs.\n\nThis function is only available with the TensorFlow backend for the time being.\n\nArguments: model: A Keras model instance. To avoid OOM errors, this model could have been built on CPU, for instance (see usage example below). gpus: Integer >= 2, number of on GPUs on which to create model replicas. cpu_merge: A boolean value to identify whether to force merging model weights under the scope of the CPU or not. cpu_relocation: A boolean value to identify whether to create the model's weights under the scope of the CPU. If the model is not defined under any preceding device scope, you can still rescue it by activating this option.\n##### Returns\n* **ple 1**: Training models with weights merge on CPU\n\n* **with tf.device('/cpu**: 0')\n\n* **# Save model via the template model (which shares the same weights)**: \n\n* **ple 2**: Training models with weights merge on CPU using cpu_relocation\n\n* **ython\n ..\n # Not needed to change the device scope for model definition**: model = Xception(weights=None, ..)\n try\n\n* **ple 3**: Training models with weights merge on GPU (recommended for NV-link)\n\n"
}{
    "source file": "multi_process_lib.py",
    "line number": "50",
    "func name": "initialized",
    "func arg": "()",
    "comments": "Returns whether the module is initialized.\n\n\n"
}{}{
    "source file": "multi_process_runner_test.py",
    "line number": "67",
    "func name": "proc_func_that_sets_global",
    "func arg": "(val)",
    "comments": ""
}{
    "source file": "multi_process_runner.py",
    "line number": "994",
    "func name": "test_main",
    "func arg": "()",
    "comments": "Main function to be called within `__main__` of a test file.\n\n\n"
}{
    "source file": "multi_variables_v1.py",
    "line number": "39",
    "func name": "Test",
    "func arg": "()",
    "comments": ""
}{
    "source file": "multi_worker_callback_tf2_test.py",
    "line number": "75",
    "func name": "_get_task_config",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "multi_worker_test_base.py",
    "line number": "654",
    "func name": "is_chief",
    "func arg": "()",
    "comments": ""
}{
    "source file": "multi_worker_test.py",
    "line number": "67",
    "func name": "_clone_and_build_model",
    "func arg": "(model, strategy)",
    "comments": ""
}{
    "source file": "multi_worker_testing_utils.py",
    "line number": "52",
    "func name": "get_mnist_model",
    "func arg": "(input_shape)",
    "comments": "Define a deterministically-initialized CNN model for MNIST testing.\n\n\n"
}{}{}{
    "source file": "multi_worker_util.py",
    "line number": "264",
    "func name": "has_worker_context",
    "func arg": "()",
    "comments": "Returns whether a worker context has been entered.\n\n\n"
}{}{
    "source file": "multinomial_op_test.py",
    "line number": "216",
    "func name": "native_op_vs_composed_ops",
    "func arg": "(batch_size, num_classes, num_samples, num_iters)",
    "comments": ""
}{}{}{
    "source file": "nadam_test.py",
    "line number": "47",
    "func name": "nadam_update_numpy",
    "func arg": "(param, g_t, t, m, v, m_cache, alpha, beta1, beta2, epsilon)",
    "comments": ""
}{}{}{}{}{}{
    "source file": "nasnet.py",
    "line number": "792",
    "func name": "decode_predictions",
    "func arg": "(preds, top)",
    "comments": ""
}{
    "source file": "nccl_ops_test.py",
    "line number": "51",
    "func name": "_NcclBroadcast",
    "func arg": "(tensors, devices)",
    "comments": ""
}{
    "source file": "nccl_ops.py",
    "line number": "263",
    "func name": "_check_device",
    "func arg": "(tensor, expected)",
    "comments": ""
}{
    "source file": "nearest_upsample.py",
    "line number": "27",
    "func name": "make_nearest_upsample_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do nearest_upsample.\n\n\n"
}{
    "source file": "neg.py",
    "line number": "27",
    "func name": "make_neg_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do neg.\n\n\n"
}{}{
    "source file": "neon_depthwise_conv_op_test.py",
    "line number": "56",
    "func name": "CheckGradConfigsToTest",
    "func arg": "()",
    "comments": "Iterator for different convolution shapes, strides and paddings.\n\ncompute_gradient_error() is very expensive. So the configs should be relatively small.\n\nYields: Tuple (input_size, filter_size, out_size, stride, padding), the depthwise convolution parameters.\n"
}{}{}{
    "source file": "nest.py",
    "line number": "400",
    "func name": "map_structure_up_to",
    "func arg": "(shallow_tree, func)",
    "comments": "Applies a function or op to a number of partially flattened inputs.\n\nThe `inputs` are flattened up to `shallow_tree` before being mapped.\n\nUse Case:\n\nSometimes we wish to apply a function to a partially flattened sequence (for example when the function itself takes sequence inputs). We achieve this by specifying a shallow structure, `shallow_tree` we wish to flatten up to.\n\nThe `inputs`, can be thought of as having the same structure as `shallow_tree`, but with leaf nodes that are themselves tree structures.\n\nThis function, therefore, will return something with the same base structure as `shallow_tree`.\n\nExamples:\n\n```python ab_tuple = collections.namedtuple(\"ab_tuple\", \"a, b\") op_tuple = collections.namedtuple(\"op_tuple\", \"add, mul\") inp_val = ab_tuple(a=2, b=3) inp_ops = ab_tuple(a=op_tuple(add=1, mul=2), b=op_tuple(add=2, mul=3)) out = map_structure_up_to(inp_val, lambda val, ops: (val + ops.add) * ops.mul, inp_val, inp_ops)\n\n# Output is: ab_tuple(a=6, b=15) ```\n\n```python data_list = [[2, 4, 6, 8], [[1, 3, 5, 7, 9], [3, 5, 7]]] name_list = ['evens', ['odds', 'primes']] out = map_structure_up_to( name_list, lambda name, sec: \"first_{}_{}\".format(len(sec), name), name_list, data_list)\n\n# Output is: ['first_4_evens', ['first_5_odds', 'first_3_primes']] ```\n##### Args\n* **shallow_tree**: a shallow tree, common to all the inputs.\n\n* **func**: callable which will be applied to each input individually.\n\n* ***inputs**: arbitrarily nested combination of objects that are compatible with\n    shallow_tree. The function `func` is applied to corresponding\n    partially flattened elements of each input, so the function must support\n    arity of `len(inputs)`.\n\n##### Returns\n"
}{
    "source file": "nest1.py",
    "line number": "1405",
    "func name": "list_to_tuple",
    "func arg": "(structure)",
    "comments": "Replace all lists with tuples.\n\nThe fork of nest that tf.data uses treats lists as single elements, while tf.nest treats them as structures to recurse into. Keras has chosen to adopt the latter convention, and must therefore deeply replace all lists with tuples before passing structures to Dataset.from_generator.\n##### Args\n* **structure**: A nested structure to be remapped.\n\n##### Returns\n"
}{}{
    "source file": "nested_structure_coder.py",
    "line number": "157",
    "func name": "_is_named_tuple",
    "func arg": "(instance)",
    "comments": "Returns True iff `instance` is a `namedtuple`.\n\n\n##### Args\n* **instance**: An instance of a Python object.\n\n##### Returns\n"
}{}{}{}{}{
    "source file": "nn_grad.py",
    "line number": "1121",
    "func name": "_NthElementGrad",
    "func arg": "(op, grad)",
    "comments": "Return the gradients for NthElement.\n\n\n##### Args\n* **op**: The NthElementOp for which we need to generate gradients.\n\n* **grad**: Tensor. The gradients passed to the NthElementOp\n\n##### Returns\n"
}{
    "source file": "nn_impl.py",
    "line number": "2272",
    "func name": "sampled_softmax_loss",
    "func arg": "(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, remove_accidental_hits, partition_strategy, name, seed)",
    "comments": "Computes and returns the sampled softmax training loss.\n\nThis is a faster way to train a softmax classifier over a huge number of classes.\n\nThis operation is for training only.\n\nIt is generally an underestimate of the full softmax loss.\n\nA common use case is to use this method for training, and calculate the full softmax loss for evaluation or inference. In this case, you must set `partition_strategy=\"div\"` for the two losses to be consistent, as in the following example:\n\n```python if mode == \"train\": loss = tf.nn.sampled_softmax_loss( weights=weights, biases=biases, labels=labels, inputs=inputs, ..., partition_strategy=\"div\") elif mode == \"eval\": logits = tf.matmul(inputs, tf.transpose(weights)) logits = tf.nn.bias_add(logits, biases) labels_one_hot = tf.one_hot(labels, n_classes) loss = tf.nn.softmax_cross_entropy_with_logits( labels=labels_one_hot, logits=logits) ```\n\nSee our Candidate Sampling Algorithms Reference ([pdf](https://www.tensorflow.org/extras/candidate_sampling.pdf)). Also see Section 3 of (Jean et al., 2014) for the math.\n##### Args\n* **weights**: A `Tensor` of shape `[num_classes, dim]`, or a list of `Tensor`\n    objects whose concatenation along dimension 0 has shape\n    [num_classes, dim].  The (possibly-sharded) class embeddings.\n\n* **biases**: A `Tensor` of shape `[num_classes]`.  The class biases.\n\n* **labels**: A `Tensor` of type `int64` and shape `[batch_size,\n    num_true]`. The target classes.  Note that this format differs from\n    the `labels` argument of `nn.softmax_cross_entropy_with_logits`.\n\n* **inputs**: A `Tensor` of shape `[batch_size, dim]`.  The forward\n    activations of the input network.\n\n* **num_sampled**: An `int`.  The number of classes to randomly sample per batch.\n\n* **num_classes**: An `int`. The number of possible classes.\n\n* **num_true**: An `int`.  The number of target classes per training example.\n\n* **sampled_values**: a tuple of (`sampled_candidates`, `true_expected_count`,\n    `sampled_expected_count`) returned by a `*_candidate_sampler` function.\n    (if None, we default to `log_uniform_candidate_sampler`)\n\n* **remove_accidental_hits**: A `bool`.  whether to remove \"accidental hits\"\n    where a sampled class equals one of the target classes.  Default is\n    True.\n\n* **partition_strategy**: A string specifying the partitioning strategy, relevant\n    if `len(weights) > 1`. Currently `\"div\"` and `\"mod\"` are supported.\n    Default is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\n\n* **name**: A name for the operation (optional).\n\n* **seed**: random seed for candidate sampling. Default to None, which doesn't set\n    the op-level random seed for candidate sampling.\n\n##### Returns\n* **ferences**: \n\n* **On Using Very Large Target Vocabulary for Neural Machine Translation**: [Jean et al., 2014]\n  (https\n\n"
}{}{
    "source file": "nn_ops.py",
    "line number": "5615",
    "func name": "in_top_k_v2",
    "func arg": "(targets, predictions, k, name)",
    "comments": ""
}{}{}{}{}{
    "source file": "node.py",
    "line number": "277",
    "func name": "_serialize_keras_tensor",
    "func arg": "(t)",
    "comments": "Serializes a single Tensor passed to `call`.\n\n\n"
}{}{}{}{
    "source file": "noop_elimination_test.py",
    "line number": "34",
    "func name": "_test_combinations",
    "func arg": "()",
    "comments": ""
}{
    "source file": "norm_op_test.py",
    "line number": "67",
    "func name": "_GetNormOpTest",
    "func arg": "(dtype_, shape_, ord_, axis_, keep_dims_, use_static_shape_)",
    "comments": ""
}{
    "source file": "normal_test.py",
    "line number": "42",
    "func name": "try_import",
    "func arg": "(name)",
    "comments": ""
}{
    "source file": "normal.py",
    "line number": "275",
    "func name": "_kl_normal_normal",
    "func arg": "(n_a, n_b, name)",
    "comments": "Calculate the batched KL divergence KL(n_a || n_b) with n_a and n_b Normal.\n\n\n##### Args\n* **n_a**: instance of a Normal distribution object.\n\n* **n_b**: instance of a Normal distribution object.\n\n* **name**: (optional) Name to use for created operations.\n  default is \"kl_normal_normal\".\n\n##### Returns\n"
}{
    "source file": "normalization_adapt_benchmark.py",
    "line number": "41",
    "func name": "reduce_fn",
    "func arg": "(state, values)",
    "comments": "tf.data.Dataset-friendly implementation of mean and variance.\n\n\n"
}{
    "source file": "normalization_distribution_test.py",
    "line number": "42",
    "func name": "_get_layer_computation_test_cases",
    "func arg": "()",
    "comments": ""
}{
    "source file": "normalization_test.py",
    "line number": "474",
    "func name": "_run_layernorm_correctness_test",
    "func arg": "(layer, dtype)",
    "comments": ""
}{
    "source file": "normalization_test1.py",
    "line number": "44",
    "func name": "_get_layer_computation_test_cases",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "normalization_tpu_test.py",
    "line number": "34",
    "func name": "_get_layer_computation_test_cases",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "normalization.py",
    "line number": "925",
    "func name": "replace_in_base_docstring",
    "func arg": "(replacements)",
    "comments": ""
}{}{}{
    "source file": "normalization3.py",
    "line number": "181",
    "func name": "batch_normalization",
    "func arg": "(inputs, axis, momentum, epsilon, center, scale, beta_initializer, gamma_initializer, moving_mean_initializer, moving_variance_initializer, beta_regularizer, gamma_regularizer, beta_constraint, gamma_constraint, training, trainable, name, reuse, renorm, renorm_clipping, renorm_momentum, fused, virtual_batch_size, adjustment)",
    "comments": "Functional interface for the batch normalization layer from_config(Ioffe et al., 2015).\n\nNote: when training, the moving_mean and moving_variance need to be updated. By default the update ops are placed in `tf.GraphKeys.UPDATE_OPS`, so they need to be executed alongside the `train_op`. Also, be sure to add any batch_normalization ops before getting the update_ops collection. Otherwise, update_ops will be empty, and training/inference will not work properly. For example:\n\n```python x_norm = tf.compat.v1.layers.batch_normalization(x, training=training)\n\n# ...\n\nupdate_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS) train_op = optimizer.minimize(loss) train_op = tf.group([train_op, update_ops]) ```\n\nArguments: inputs: Tensor input. axis: An `int`, the axis that should be normalized (typically the features axis). For instance, after a `Convolution2D` layer with `data_format=\"channels_first\"`, set `axis=1` in `BatchNormalization`. momentum: Momentum for the moving average. epsilon: Small float added to variance to avoid dividing by zero. center: If True, add offset of `beta` to normalized tensor. If False, `beta` is ignored. scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the next layer is linear (also e.g. `nn.relu`), this can be disabled since the scaling can be done by the next layer. beta_initializer: Initializer for the beta weight. gamma_initializer: Initializer for the gamma weight. moving_mean_initializer: Initializer for the moving mean. moving_variance_initializer: Initializer for the moving variance. beta_regularizer: Optional regularizer for the beta weight. gamma_regularizer: Optional regularizer for the gamma weight. beta_constraint: An optional projection function to be applied to the `beta` weight after being updated by an `Optimizer` (e.g. used to implement norm constraints or value constraints for layer weights). The function must take as input the unprojected variable and must return the projected variable (which must have the same shape). Constraints are not safe to use when doing asynchronous distributed training. gamma_constraint: An optional projection function to be applied to the `gamma` weight after being updated by an `Optimizer`. training: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). Whether to return the output in training mode (normalized with statistics of the current batch) or in inference mode (normalized with moving statistics). **NOTE**: make sure to set this parameter correctly, or else your training/inference will not work properly. trainable: Boolean, if `True` also add variables to the graph collection `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable). name: String, the name of the layer. reuse: Boolean, whether to reuse the weights of a previous layer by the same name. renorm: Whether to use Batch Renormalization (Ioffe, 2017). This adds extra variables during training. The inference is the same for either value of this parameter. renorm_clipping: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to scalar `Tensors` used to clip the renorm correction. The correction `(r, d)` is used as `corrected_value = normalized_value * r + d`, with `r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin, dmax are set to inf, 0, inf, respectively. renorm_momentum: Momentum used to update the moving means and standard deviations with renorm. Unlike `momentum`, this affects training and should be neither too small (which would add noise) nor too large (which would give stale estimates). Note that `momentum` is still applied to get the means and variances for inference. fused: if `None` or `True`, use a faster, fused implementation if possible. If `False`, use the system recommended implementation. virtual_batch_size: An `int`. By default, `virtual_batch_size` is `None`, which means batch normalization is performed across the whole batch. When `virtual_batch_size` is not `None`, instead perform \"Ghost Batch Normalization\", which creates virtual sub-batches which are each normalized separately (with shared gamma, beta, and moving statistics). Must divide the actual batch size during execution. adjustment: A function taking the `Tensor` containing the (dynamic) shape of the input tensor and returning a pair (scale, bias) to apply to the normalized values (before gamma and beta), only during training. For example, if axis==-1, `adjustment = lambda shape: ( tf.random.uniform(shape[-1:], 0.93, 1.07), tf.random.uniform(shape[-1:], -0.1, 0.1))` will scale the normalized value by up to 7% up or down, then shift the result by up to 0.1 (with independent scaling and bias for each feature but shared across all examples), and finally apply gamma and/or beta. If `None`, no adjustment is applied. Cannot be specified if virtual_batch_size is specified.\n##### Returns\n"
}{
    "source file": "normalize_op_test.py",
    "line number": "61",
    "func name": "_GetNormalizeOpTest",
    "func arg": "(dtype_, shape_, ord_, axis_)",
    "comments": ""
}{
    "source file": "not_equal.py",
    "line number": "27",
    "func name": "make_not_equal_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do not equal.\n\n\n"
}{
    "source file": "notebook.py",
    "line number": "53",
    "func name": "main",
    "func arg": "(unused_argv)",
    "comments": ""
}{}{
    "source file": "np_array_ops.py",
    "line number": "1667",
    "func name": "_getitem",
    "func arg": "(slice_spec)",
    "comments": "Implementation of ndarray.__getitem__.\n\n\n"
}{}{
    "source file": "np_arrays.py",
    "line number": "330",
    "func name": "ndarray_to_tensor",
    "func arg": "(arr, dtype, name, as_ref)",
    "comments": ""
}{
    "source file": "np_dtypes.py",
    "line number": "104",
    "func name": "default_float_type",
    "func arg": "()",
    "comments": "Gets the default float type.\n\n\n##### Returns\n"
}{
    "source file": "np_export.py",
    "line number": "34",
    "func name": "np_export_constant",
    "func arg": "(module_name, name, value)",
    "comments": ""
}{}{
    "source file": "np_logic_test.py",
    "line number": "104",
    "func name": "make_numpy_compatible",
    "func arg": "(s)",
    "comments": ""
}{}{
    "source file": "np_math_ops.py",
    "line number": "1364",
    "func name": "einsum",
    "func arg": "(subscripts, **kwargs)",
    "comments": ""
}{}{
    "source file": "np_random.py",
    "line number": "95",
    "func name": "randint",
    "func arg": "(low, high, size, dtype)",
    "comments": ""
}{}{}{
    "source file": "np_utils.py",
    "line number": "85",
    "func name": "normalize",
    "func arg": "(x, axis, order)",
    "comments": "Normalizes a Numpy array.\n\nArguments: x: Numpy array to normalize. axis: axis along which to normalize. order: Normalization order (e.g. `order=2` for L2 norm).\n##### Returns\n"
}{
    "source file": "np_utils1.py",
    "line number": "600",
    "func name": "tf_rank",
    "func arg": "(t)",
    "comments": ""
}{}{}{}{
    "source file": "numerics.py",
    "line number": "76",
    "func name": "add_check_numerics_ops",
    "func arg": "()",
    "comments": "Connect a `tf.debugging.check_numerics` to every floating point tensor.\n\n`check_numerics` operations themselves are added for each `half`, `float`, or `double` tensor in the current default graph. For all ops in the graph, the `check_numerics` op for all of its (`half`, `float`, or `double`) inputs is guaranteed to run before the `check_numerics` op on any of its outputs.\n\nNote: This API is not compatible with the use of `tf.cond` or `tf.while_loop`, and will raise a `ValueError` if you attempt to call it in such a graph.\n##### Returns\n"
}{}{
    "source file": "numpy_dataset.py",
    "line number": "76",
    "func name": "one_host_numpy_dataset",
    "func arg": "(numpy_input, colocate_with, session)",
    "comments": "Create a dataset on `colocate_with` from `numpy_input`.\n\n\n"
}{}{}{}{
    "source file": "offline_analyzer.py",
    "line number": "30",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{}{}{}{
    "source file": "one_hot.py",
    "line number": "27",
    "func name": "make_one_hot_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do one_hot.\n\n\n"
}{}{}{}{
    "source file": "op_callbacks.py",
    "line number": "152",
    "func name": "invoke_op_callbacks",
    "func arg": "(op_type, inputs, attrs, outputs, op_name, graph)",
    "comments": "Invoke the callbacks that exist in the current scope (if any).\n\nIf no callbacks are present in the current scope, this method returns immediately.\n##### Args\n* **op_type**: Type of the operation (e.g., \"MatMul\").\n\n* **inputs**: Input tensors to the op. These are `EagerTensor`s in the case of\n  eager execution of ops or `FuncGraph`s, and are non-eager `Tensor`s in the\n  case of graph construction.\n\n* **attrs**: Attributes of the op, as `tuple` of alternating keys and values.\n\n* **outputs**: Output tensors from the op. These are `EagerTensor`s in the case of\n  eager execution and are non-eager `Tensor`s in the case of graph\n  construction.\n\n* **op_name**: Name of the op. Applicable if and only if this method is invoked\n  due to the graph construction of an op or the eager execution of of a\n  `FuncGraph`.\n\n* **graph**: The graph involved (if any).\n  - In the case if the eager execution of an op or FuncGraph, this is\n    `None`.\n  - In the case of the graph construction of an op, this is the `tf.Graph`\n    object being built.\n\n##### Returns\n"
}{}{
    "source file": "op_def_library.py",
    "line number": "299",
    "func name": "_apply_op_helper",
    "func arg": "(op_type_name, name, **keywords)",
    "comments": "Implementation of apply_op that returns output_structure, op.\n\n\n"
}{
    "source file": "op_def_registry.py",
    "line number": "59",
    "func name": "sync",
    "func arg": "()",
    "comments": "No-op. Used to synchronize the contents of the Python registry with C++.\n\n\n"
}{
    "source file": "op_hint.py",
    "line number": "1305",
    "func name": "convert_op_hints_to_stubs",
    "func arg": "(session, graph_def, write_callback)",
    "comments": "Converts a graphdef with LiteOp hints into stub operations.\n\nThis is used to prepare for toco conversion of complex intrinsic usages. Note: only one of session or graph_def should be used, not both.\n##### Args\n* **session**: A TensorFlow session that contains the graph to convert.\n\n* **graph_def**: A graph def that we should convert.\n\n* **write_callback**: A function pointer that can be used to write intermediate\n  steps of graph transformation (optional).\n\n##### Returns\n"
}{}{
    "source file": "op_selector.py",
    "line number": "368",
    "func name": "map_subgraph",
    "func arg": "(init_tensor, sources, disallowed_placeholders, visited_ops, op_outputs, add_sources)",
    "comments": "Walk a Graph and capture the subgraph between init_tensor and sources.\n\nNote: This function mutates visited_ops and op_outputs.\n\nArguments: init_tensor:\n\nA Tensor or Operation where the subgraph terminates. sources:\n\nA set of Tensors where subgraph extraction should stop. disallowed_placeholders: An optional set of ops which may not appear in the lifted graph. Defaults to all placeholders. visited_ops: A set of operations which were visited in a prior pass. op_outputs: A defaultdict containing the outputs of an op which are to be copied into the new subgraph. add_sources: A boolean indicating whether placeholders which are not in sources should be allowed.\n##### Returns\n"
}{}{}{
    "source file": "ops_test1.py",
    "line number": "3022",
    "func name": "_calc_a_forward_flops",
    "func arg": "(unused_graph, unused_node)",
    "comments": ""
}{}{
    "source file": "ops_util.py",
    "line number": "35",
    "func name": "get_potentially_supported_ops",
    "func arg": "()",
    "comments": "Returns operations potentially supported by TensorFlow Lite.\n\nThe potentially support list contains a list of ops that are partially or fully supported, which is derived by simply scanning op names to check whether they can be handled without real conversion and specific parameters.\n\nGiven that some ops may be partially supported, the optimal way to determine if a model's operations are supported is by converting using the TensorFlow Lite converter.\n##### Returns\n"
}{
    "source file": "ops.py",
    "line number": "6982",
    "func name": "set_int_list_attr",
    "func arg": "(op, attr_name, ints)",
    "comments": "TF internal method used to set a list(int) attribute in the node_def.\n\n\n"
}{}{
    "source file": "optimization.py",
    "line number": "39",
    "func name": "optimize",
    "func arg": "(optimizations)",
    "comments": "A transformation that applies optimizations.\n\n\n##### Args\n* **optimizations**: (Optional.) A `tf.string` vector `tf.Tensor` identifying\n  optimizations to use. If not specified, the default set of optimizations\n  is applied.\n\n##### Returns\n"
}{}{}{
    "source file": "optimize_dataset_test.py",
    "line number": "44",
    "func name": "_captured_refvar_test_combinations",
    "func arg": "()",
    "comments": ""
}{
    "source file": "optimize_for_inference_lib.py",
    "line number": "419",
    "func name": "fuse_resize_and_conv",
    "func arg": "(input_graph_def, output_node_names)",
    "comments": "Merges preceding resize and mirror pad ops into a specialized convolution.\n\nThere's a common pattern of enlarging the input to a convolution using a resize operation, and also using MirrorPad to extend the boundaries to that zero edge pixels don't bleed inwards when convolving. This routine looks for that pattern of operations, and fuses them together into a Conv2DWithResizeOp.\n##### Args\n* **input_graph_def**: A GraphDef containing a model.\n\n* **output_node_names**: A list of names of the nodes that produce the final\n  results.\n\n##### Returns\n"
}{}{
    "source file": "optimize_for_inference.py",
    "line number": "110",
    "func name": "parse_args",
    "func arg": "()",
    "comments": "Parses command line arguments.\n\n\n"
}{
    "source file": "optimizer_combinations.py",
    "line number": "103",
    "func name": "distributions_and_v1_and_v2_optimizers",
    "func arg": "()",
    "comments": "A common set of combination with DistributionStrategies and Optimizers.\n\n\n"
}{}{
    "source file": "optimizer_v2_test.py",
    "line number": "1046",
    "func name": "make_model",
    "func arg": "()",
    "comments": "Constructs a simple ensemble of weak learners model.\n\n---------\n\n\n\n---------\n\n\n\n\n\n\n\n\n\n\n\n ---------\n\n\n\n--------- | Input |\n\n\n\n| Input |\n\n\n\n ...\n\n\n\n | Input |\n\n\n\n| Input | ---------\n\n\n\n---------\n\n\n\n\n\n\n\n\n\n\n\n ---------\n\n\n\n--------- |\n\n\n\n\n\n\n\n\n\n\n\n|\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n |\n\n\n\n\n\n\n\n\n\n\n\n| V\n\n\n\n\n\n\n\n\n\n\n\nV\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n V\n\n\n\n\n\n\n\n\n\n\n\nV ---------\n\n\n\n---------\n\n\n\n\n\n\n\n\n\n\n\n ---------\n\n\n\n--------- | Embed |\n\n\n\n| Embed |\n\n\n\n ...\n\n\n\n | Embed |\n\n\n\n| Embed | ---------\n\n\n\n---------\n\n\n\n\n\n\n\n\n\n\n\n ---------\n\n\n\n--------- |\n\n\n\n\n\n\n\n\n\n\n\n|\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n |\n\n\n\n\n\n\n\n\n\n\n\n| V\n\n\n\n\n\n\n\n\n\n\n\nV\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n V\n\n\n\n\n\n\n\n\n\n\n\nV ---------\n\n\n\n---------\n\n\n\n\n\n\n\n\n\n\n\n ---------\n\n\n\n--------- | Dense |\n\n\n\n| Dense |\n\n\n\n ...\n\n\n\n | Dense |\n\n\n\n| Dense | ---------\n\n\n\n---------\n\n\n\n\n\n\n\n\n\n\n\n ---------\n\n\n\n--------- \\\n\n\n\n\n\n\n\n\n\n\n\n|\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n |\n\n\n\n\n\n\n\n\n\n\n\n/ \\\n\n\n\n\n\n\n\n\n\n |\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n |\n\n\n\n\n\n\n\n\n\n / --------------------------------------------- | --------- | Dense | ---------\n\nThis topology is chosen because it exercises both dense and sparse update paths.\n##### Returns\n"
}{
    "source file": "optimizer_v2.py",
    "line number": "1308",
    "func name": "_get_slot_key_from_var",
    "func arg": "(var, slot_name)",
    "comments": "Get the slot key for the variable: var_name/slot_name.\n\n\n"
}{
    "source file": "optimizer.py",
    "line number": "197",
    "func name": "_get_processor",
    "func arg": "(v)",
    "comments": "The processor of v.\n\n\n"
}{
    "source file": "optimizers_test.py",
    "line number": "37",
    "func name": "_get_model",
    "func arg": "(input_dim, num_hidden, output_dim)",
    "comments": ""
}{}{
    "source file": "optimizers1.py",
    "line number": "873",
    "func name": "get",
    "func arg": "(identifier)",
    "comments": "Retrieves a Keras Optimizer instance.\n\nArguments: identifier: Optimizer identifier, one of\n\n- String: name of an optimizer\n\n- Dictionary: configuration dictionary.\n\n- Keras Optimizer instance (it will be returned unchanged).\n\n- TensorFlow Optimizer instance (it will be wrapped as a Keras Optimizer).\n##### Returns\n"
}{}{
    "source file": "optional_grad.py",
    "line number": "32",
    "func name": "_OptionalGetValueGrad",
    "func arg": "(unused_op)",
    "comments": ""
}{}{
    "source file": "optional_test.py",
    "line number": "80",
    "func name": "_get_next_as_optional_test_combinations",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "options.py",
    "line number": "89",
    "func name": "merge_options",
    "func arg": "()",
    "comments": "Merges the given options, returning the result as a new options object.\n\nThe input arguments are expected to have a matching type that derives from `OptionsBase` (and thus each represent a set of options). The method outputs an object of the same type created by merging the sets of options represented by the input arguments.\n\nThe sets of options can be merged as long as there does not exist an option with different non-default values.\n\nIf an option is an instance of `OptionsBase` itself, then this method is applied recursively to the set of options represented by this option.\n##### Args\n* ***options_list**: options to merge\n\n##### Returns\n"
}{}{
    "source file": "origin_info.py",
    "line number": "271",
    "func name": "resolve_entity",
    "func arg": "(node, source, entity)",
    "comments": "Like resolve, but extracts the context information from an entity.\n\n\n"
}{
    "source file": "output_init_files_test.py",
    "line number": "102",
    "func name": "_module_to_paths",
    "func arg": "(module)",
    "comments": "Get all API __init__.py file paths for the given module.\n\n\n##### Args\n* **module**: Module to get file paths for.\n\n##### Returns\n"
}{}{
    "source file": "pack.py",
    "line number": "28",
    "func name": "make_pack_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do stack.\n\n\n"
}{}{
    "source file": "packed_distributed_variable.py",
    "line number": "341",
    "func name": "_tensor_conversion_packed_var_and_device",
    "func arg": "(var, dtype, name, as_ref)",
    "comments": ""
}{}{
    "source file": "pad.py",
    "line number": "28",
    "func name": "make_pad_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do pad.\n\n\n"
}{}{}{}{
    "source file": "padv2.py",
    "line number": "28",
    "func name": "make_padv2_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do padv2.\n\n\n"
}{}{
    "source file": "parallel_device_test.py",
    "line number": "68",
    "func name": "_collective_sum",
    "func arg": "(inputs, num_replicas)",
    "comments": ""
}{}{
    "source file": "parallel_interleave_benchmark.py",
    "line number": "36",
    "func name": "_make_fake_dataset_fn",
    "func arg": "(initial_delay_us, remainder_delay_us)",
    "comments": "Returns a dataset that emulates a remote storage data source.\n\n\n##### Args\n* **initial_delay_us**: How long to wait before producing the first element.\n\n* **remainder_delay_us**: How long to wait before producing subsequent elements.\n\n##### Returns\n"
}{}{}{}{
    "source file": "parameter_server_strategy_test.py",
    "line number": "73",
    "func name": "create_test_objects",
    "func arg": "(cluster_spec, task_type, task_id, num_gpus, sess_config)",
    "comments": ""
}{}{
    "source file": "parameterized_truncated_normal_op_test.py",
    "line number": "419",
    "func name": "randn_sampler_switchover",
    "func arg": "(shape, num_iters, use_gpu)",
    "comments": ""
}{}{}{}{
    "source file": "parse_single_example_op_test.py",
    "line number": "69",
    "func name": "_compare_output_to_expected",
    "func arg": "(tester, dict_tensors, expected_tensors, flat_output)",
    "comments": ""
}{}{
    "source file": "parser.py",
    "line number": "309",
    "func name": "main",
    "func arg": "()",
    "comments": ""
}{
    "source file": "parser1.py",
    "line number": "1711",
    "func name": "generate_global_index",
    "func arg": "(library_name, index, reference_resolver)",
    "comments": "Given a dict of full names to python objects, generate an index page.\n\nThe index page generated contains a list of links for all symbols in `index` that have their own documentation page.\n##### Args\n* **library_name**: The name for the documented library to use in the title.\n\n* **index**: A dict mapping full names to python objects.\n\n* **reference_resolver**: An instance of ReferenceResolver.\n\n##### Returns\n"
}{
    "source file": "parsing_config.py",
    "line number": "879",
    "func name": "_build_ragged_tensors",
    "func arg": "(serialized_shape, ragged_values, ragged_row_splits, ragged_inner_splits)",
    "comments": "Builds RaggedTensors from the outputs of a parse op.\n\n\n"
}{
    "source file": "parsing_ops_test.py",
    "line number": "66",
    "func name": "_compare_output_to_expected",
    "func arg": "(tester, actual, expected)",
    "comments": ""
}{
    "source file": "parsing_ops.py",
    "line number": "110",
    "func name": "parse_example_dataset",
    "func arg": "(features, num_parallel_calls, deterministic)",
    "comments": "A transformation that parses `Example` protos into a `dict` of tensors.\n\nParses a number of serialized `Example` protos given in `serialized`. We refer to `serialized` as a batch with `batch_size` many entries of individual `Example` protos.\n\nThis op parses serialized examples into a dictionary mapping keys to `Tensor`, `SparseTensor`, and `RaggedTensor` objects. `features` is a dict from keys to `VarLenFeature`, `RaggedFeature`, `SparseFeature`, and `FixedLenFeature` objects. Each `VarLenFeature` and `SparseFeature` is mapped to a `SparseTensor`; each `RaggedFeature` is mapped to a `RaggedTensor`; and each `FixedLenFeature` is mapped to a `Tensor`. See `tf.io.parse_example` for more details about feature dictionaries.\n##### Args\n* **features**: A `dict` mapping feature keys to `FixedLenFeature`,\n  `VarLenFeature`, `RaggedFeature`, and `SparseFeature` values.\n\n* **num_parallel_calls**: (Optional.) A `tf.int32` scalar `tf.Tensor`,\n   representing the number of parsing processes to call in parallel.\n\n* **deterministic**: (Optional.) A boolean controlling whether determinism\n   should be traded for performance by allowing elements to be produced out\n   of order if some parsing calls complete faster than others. If\n   `deterministic` is `None`, the\n   `tf.data.Options.experimental_deterministic` dataset option (`True` by\n   default) is used to decide whether to produce elements\n   deterministically.\n\n##### Returns\n"
}{
    "source file": "parsing_ops1.py",
    "line number": "1042",
    "func name": "_assert_scalar",
    "func arg": "(value, name)",
    "comments": "Asserts that `value` is scalar, and returns `value`.\n\n\n"
}{}{}{}{
    "source file": "partitioned_variables_test.py",
    "line number": "310",
    "func name": "_IotaInitializer",
    "func arg": "(shape, dtype, partition_info)",
    "comments": ""
}{
    "source file": "partitioned_variables.py",
    "line number": "244",
    "func name": "create_partitioned_variables",
    "func arg": "(shape, slicing, initializer, dtype, trainable, collections, name, reuse)",
    "comments": "Create a list of partitioned variables according to the given `slicing`.\n\nCurrently only one dimension of the full variable can be sliced, and the full variable can be reconstructed by the concatenation of the returned list along that dimension.\n##### Args\n* **shape**: List of integers.  The shape of the full variable.\n\n* **slicing**: List of integers.  How to partition the variable.\n  Must be of the same length as `shape`.  Each value\n  indicate how many slices to create in the corresponding\n  dimension.  Presently only one of the values can be more than 1;\n  that is, the variable can only be sliced along one dimension.\n  For convenience, The requested number of partitions does not have to\n  divide the corresponding dimension evenly.  If it does not, the\n  shapes of the partitions are incremented by 1 starting from partition\n  0 until all slack is absorbed.  The adjustment rules may change in the\n  future, but as you can save/restore these variables with different\n  slicing specifications this should not be a problem.\n\n* **initializer**: A `Tensor` of shape `shape` or a variable initializer\n  function.  If a function, it will be called once for each slice,\n  passing the shape and data type of the slice as parameters.  The\n  function must return a tensor with the same shape as the slice.\n\n* **dtype**: Type of the variables. Ignored if `initializer` is a `Tensor`.\n\n* **trainable**: If True also add all the variables to the graph collection\n  `GraphKeys.TRAINABLE_VARIABLES`.\n\n* **collections**: List of graph collections keys to add the variables to.\n  Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.\n\n* **name**: Optional name for the full variable.  Defaults to\n  `\"PartitionedVariable\"` and gets uniquified automatically.\n\n* **reuse**: Boolean or `None`; if `True` and name is set, it would reuse\n  previously created variables. if `False` it will create new variables.\n  if `None`, it would inherit the parent scope reuse.\n\n##### Returns\n"
}{
    "source file": "pfor.py",
    "line number": "4413",
    "func name": "_convert_rfft",
    "func arg": "(pfor_input, _, op_func, attr_name)",
    "comments": ""
}{
    "source file": "pip_smoke_test.py",
    "line number": "102",
    "func name": "main",
    "func arg": "()",
    "comments": "This script runs the pip smoke test.\n\n\n"
}{}{
    "source file": "placeholder_with_default.py",
    "line number": "28",
    "func name": "make_placeholder_with_default_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to test placeholder_with_default.\n\n\n"
}{}{
    "source file": "plugin_asset.py",
    "line number": "84",
    "func name": "get_all_plugin_assets",
    "func arg": "(graph)",
    "comments": "Retrieve all PluginAssets stored in the graph collection.\n\n\n##### Args\n* **graph**: Optionally, the graph to get assets from. If unspecified, the default\n  graph is used.\n\n##### Returns\n"
}{}{
    "source file": "policy.py",
    "line number": "624",
    "func name": "deserialize",
    "func arg": "(config, custom_objects)",
    "comments": ""
}{
    "source file": "pool_test.py",
    "line number": "99",
    "func name": "pool_direct",
    "func arg": "(input, window_shape, pooling_type, padding, dilation_rate, strides, data_format)",
    "comments": "Numpy implementation of pooling.\n\nThis is intended for testing only, and therefore isn't particularly efficient.\n\nSee tensorflow.nn.pool.\n##### Args\n* **input**: numpy array of rank N+2.\n\n* **window_shape**: Sequence of N ints >= 1.\n\n* **pooling_type**: either \"MAX\" or \"AVG\".\n\n* **padding**: either \"SAME\" or \"VALID\".\n\n* **dilation_rate**: Sequence of N ints >= 1.\n\n* **strides**: Sequence of N ints >= 1.\n\n* **data_format**: If specified and starts with \"NC\", indicates that second\n  dimension, rather than the last dimension, specifies the channel.\n\n##### Returns\n"
}{
    "source file": "pool.py",
    "line number": "143",
    "func name": "make_max_pool_tests",
    "func arg": "(options)",
    "comments": ""
}{
    "source file": "pooling_ops_3d_test.py",
    "line number": "35",
    "func name": "_AvgPoolGrad",
    "func arg": "(inputs, outputs, output_gradients, ksize, strides, padding)",
    "comments": ""
}{
    "source file": "pooling_ops_3d_test1.py",
    "line number": "32",
    "func name": "GetTestConfigs",
    "func arg": "()",
    "comments": "Get all the valid tests configs to run.\n\n\n##### Returns\n"
}{
    "source file": "pooling_ops_test.py",
    "line number": "63",
    "func name": "GetTestConfigs",
    "func arg": "()",
    "comments": "Get all the valid tests configs to run.\n\n\n##### Returns\n"
}{
    "source file": "pooling_ops_test1.py",
    "line number": "1983",
    "func name": "GetMaxPoolGradGradTest",
    "func arg": "(input_size, filter_size, output_size, strides, padding)",
    "comments": ""
}{}{}{}{
    "source file": "pooling1.py",
    "line number": "429",
    "func name": "max_pooling3d",
    "func arg": "(inputs, pool_size, strides, padding, data_format, name)",
    "comments": "Max pooling layer for 3D inputs (e.g.\n\nvolumes).\n\nArguments: inputs: The tensor over which to pool. Must have rank 5. pool_size: An integer or tuple/list of 3 integers: (pool_depth, pool_height, pool_width) specifying the size of the pooling window. Can be a single integer to specify the same value for all spatial dimensions. strides: An integer or tuple/list of 3 integers, specifying the strides of the pooling operation. Can be a single integer to specify the same value for all spatial dimensions. padding: A string. The padding method, either 'valid' or 'same'. Case-insensitive. data_format: A string. The ordering of the dimensions in the inputs. `channels_last` (default) and `channels_first` are supported. `channels_last` corresponds to inputs with shape `(batch, depth, height, width, channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, depth, height, width)`. name: A string, the name of the layer.\n##### Returns\n"
}{}{}{
    "source file": "pprof_profiler.py",
    "line number": "405",
    "func name": "profile",
    "func arg": "(graph, run_metadata, output_dir)",
    "comments": "Generate profiles in pprof format.\n\nSee https://github.com/google/pprof/blob/master/proto/profile.proto for pprof proto format.\n##### Args\n* **graph**: A `Graph` object.\n\n* **run_metadata**: A `RunMetadata` proto.\n\n* **output_dir**: (string) Directory to output pprof profile to.\n  Profile files for each device will be stored in compressed\n  serialized proto format. If output_dir is None, profile protos\n  will be printed to stdout instead.\n\n##### Returns\n* **(Note**: this list will be empty if output_dir is None)\n\n"
}{}{}{}{}{}{}{}{}{
    "source file": "prefetching_ops.py",
    "line number": "260",
    "func name": "map_on_gpu",
    "func arg": "(map_func)",
    "comments": "Maps `map_func` across the elements of this dataset.\n\nNOTE: This is a highly experimental version of `tf.data.Dataset.map` that runs `map_func` on GPU. It must be used after applying the `tf.data.experimental.copy_to_device` transformation with a GPU device argument.\n##### Args\n* **map_func**: A function mapping a nested structure of tensors (having shapes\n  and types defined by `self.output_shapes` and `self.output_types`) to\n  another nested structure of tensors.\n\n##### Returns\n"
}{
    "source file": "prelu.py",
    "line number": "28",
    "func name": "make_prelu_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do PReLU.\n\n\n"
}{}{}{}{
    "source file": "pretty_docs.py",
    "line number": "320",
    "func name": "_build_aliases",
    "func arg": "(aliases)",
    "comments": ""
}{}{
    "source file": "pretty_printer.py",
    "line number": "128",
    "func name": "fmt",
    "func arg": "(node, color, noanno)",
    "comments": ""
}{}{}{
    "source file": "print_selective_registration_header.py",
    "line number": "47",
    "func name": "main",
    "func arg": "(unused_argv)",
    "comments": ""
}{}{
    "source file": "profile_analyzer_cli_test.py",
    "line number": "66",
    "func name": "_assert_no_lines_match",
    "func arg": "(pattern, lines)",
    "comments": ""
}{
    "source file": "profile_analyzer_cli.py",
    "line number": "768",
    "func name": "create_profiler_ui",
    "func arg": "(graph, run_metadata, ui_type, on_ui_exit, config)",
    "comments": "Create an instance of CursesUI based on a `tf.Graph` and `RunMetadata`.\n\n\n##### Args\n* **graph**: Python `Graph` object.\n\n* **run_metadata**: A `RunMetadata` protobuf object.\n\n* **ui_type**: (str) requested UI type, e.g., \"curses\", \"readline\".\n\n* **on_ui_exit**: (`Callable`) the callback to be called when the UI exits.\n\n* **config**: An instance of `cli_config.CLIConfig`.\n\n##### Returns\n"
}{}{
    "source file": "profile_context.py",
    "line number": "45",
    "func name": "_profiled_run",
    "func arg": "(fetches, feed_dict, options, run_metadata)",
    "comments": "Overwrites the session.run().\n\n\n"
}{
    "source file": "profiler_analysis_pb2_grpc.py",
    "line number": "96",
    "func name": "add_ProfileAnalysisServicer_to_server",
    "func arg": "(servicer, server)",
    "comments": ""
}{}{}{
    "source file": "profiler_client.py",
    "line number": "54",
    "func name": "monitor",
    "func arg": "(service_addr, duration_ms, monitoring_level, display_timestamp)",
    "comments": "Sends grpc requests to profiler server to perform on-demand monitoring.\n\nThis method will block caller thread until receives monitoring result.\n##### Args\n* **service_addr**: Address of profiler service e.g. localhost\n\n* **duration_ms**: Duration of tracing or monitoring in ms.\n\n* **monitoring_level**: Choose a monitoring level between 1 and 2 to monitor your\n  job. Level 2 is more verbose than level 1 and shows more metrics.\n\n* **display_timestamp**: Set to true to display timestamp in monitoring result.\n\n##### Returns\n"
}{
    "source file": "profiler_client1.py",
    "line number": "128",
    "func name": "_strip_prefix",
    "func arg": "(s, prefix)",
    "comments": ""
}{}{}{}{
    "source file": "profiler_v2.py",
    "line number": "166",
    "func name": "start_server",
    "func arg": "(port)",
    "comments": "Start a profiler grpc server that listens to given port.\n\nThe profiler server will exit when the process finishes. The service is defined in tensorflow/core/profiler/profiler_service.proto.\n##### Args\n* **port**: port profiler server listens to.\n\n* **ample usage**: ```python tf.profiler.experimental.server.start('6009') # do\n\n"
}{
    "source file": "profiler.py",
    "line number": "157",
    "func name": "start_profiler_server",
    "func arg": "(port)",
    "comments": "Start a profiler grpc server that listens to given port.\n\nThe profiler server will keep the program running even the training finishes. Please shutdown the server with CTRL-C. It can be used in both eager mode and graph mode. The service defined in tensorflow/core/profiler/profiler_service.proto. Please use tensorflow/contrib/tpu/profiler/capture_tpu_profile to capture tracable file following https://cloud.google.com/tpu/docs/cloud-tpu-tools#capture_trace\n##### Args\n* **port**: port profiler server listens to.\n\n"
}{}{}{}{}{}{}{}{}{}{}{}{}{}{
    "source file": "ps_values.py",
    "line number": "307",
    "func name": "_tensor_conversion_aggregate",
    "func arg": "(var, dtype, name, as_ref)",
    "comments": ""
}{}{}{}{
    "source file": "py_builtins.py",
    "line number": "627",
    "func name": "_py_sorted",
    "func arg": "(iterable, key, reverse)",
    "comments": ""
}{
    "source file": "py_checkpoint_reader.py",
    "line number": "85",
    "func name": "NewCheckpointReader",
    "func arg": "(filepattern)",
    "comments": "A function that returns a CheckPointReader.\n\n\n##### Args\n* **filepattern**: The filename.\n\n##### Returns\n"
}{}{
    "source file": "py_func_test1.py",
    "line number": "52",
    "func name": "matmul",
    "func arg": "(x, y)",
    "comments": ""
}{
    "source file": "py_func.py",
    "line number": "38",
    "func name": "wrap_py_func",
    "func arg": "(f, return_dtypes, args, kwargs, use_dummy_return)",
    "comments": "Helper that wraps a callable to py_func.\n\nThe helper passes tensor arguments through the py_func interface. Non-tensor arguments are allowed, and will be passed to f directly. Note that non-tensor arguments are captured by f will not update every time the wrapper is called (this is consistent with its argument list, which only includes the tensor arguments). In general, it's safest not to reuse this wrapper.\n##### Args\n* **f**: Callable\n\n* **return_dtypes**: None, individual of tuple/list of DType or MatchDType, the\n    data type for each of f's return value(s). Set to None if f has no\n    return values or use_dummy_return is True. Use MatchDType to define a\n    dtype identical to that of `i`th argument (argument 0 is the first);\n    an argument must of Tensor type if it is to be used with MatchDType.\n\n* **args**: Positional arguments for f, as list or tuple.\n\n* **kwargs**: Keyword arguments for f, as dict with string keys. May be None.\n\n* **use_dummy_return**: If True, the function will return a dummy value of 1\n    and discard its actual return value.\n\n##### Returns\n"
}{
    "source file": "py_guide_parser.py",
    "line number": "29",
    "func name": "md_files_in_dir",
    "func arg": "(py_guide_src_dir)",
    "comments": "Returns a list of filename (full_path, base) pairs for guide files.\n\n\n"
}{}{
    "source file": "python_memory_checker.py",
    "line number": "53",
    "func name": "_snapshot_diff",
    "func arg": "(old_snapshot, new_snapshot, exclude_ids)",
    "comments": ""
}{
    "source file": "python_object_to_proto_visitor.py",
    "line number": "191",
    "func name": "_IsProtoClass",
    "func arg": "(obj)",
    "comments": "Returns whether the passed obj is a Protocol Buffer class.\n\n\n"
}{}{}{
    "source file": "pywrap_dlopen_global_flags.py",
    "line number": "48",
    "func name": "reset_dlopen_flags",
    "func arg": "()",
    "comments": ""
}{
    "source file": "pywrap_mlir.py",
    "line number": "48",
    "func name": "experimental_run_pass_pipeline",
    "func arg": "(mlir_txt, pass_pipeline, show_debug_info)",
    "comments": ""
}{}{
    "source file": "pywrap_tf_session.py",
    "line number": "65",
    "func name": "TF_Reset",
    "func arg": "(target, containers, config)",
    "comments": ""
}{}{}{}{}{
    "source file": "qr_op_test1.py",
    "line number": "176",
    "func name": "_GetQrGradOpTest",
    "func arg": "(dtype_, shape_, full_matrices_)",
    "comments": ""
}{}{
    "source file": "qual_names.py",
    "line number": "255",
    "func name": "from_str",
    "func arg": "(qn_str)",
    "comments": ""
}{}{}{
    "source file": "quantization_test.py",
    "line number": "53",
    "func name": "_GetParams",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "quantize_training.py",
    "line number": "31",
    "func name": "do_quantize_training_on_graphdef",
    "func arg": "(input_graph, num_bits)",
    "comments": "A general quantization scheme is being developed in `tf.contrib.quantize`.\n\nConsider using that instead, though since it is in the tf.contrib namespace, it is not subject to backward compatibility guarantees.\n##### Args\n* **input_graph**: A `GraphDef`.\n\n* **num_bits**: The number of bits for quantize training.\n\n##### Returns\n"
}{}{}{}{
    "source file": "queue_runner_impl.py",
    "line number": "417",
    "func name": "start_queue_runners",
    "func arg": "(sess, coord, daemon, start, collection)",
    "comments": "Starts all queue runners collected in the graph.\n\nThis is a companion method to `add_queue_runner()`.\n\nIt just starts threads for all queue runners collected in the graph.\n\nIt returns the list of all threads.\n##### Args\n* **sess**: `Session` used to run the queue ops.  Defaults to the\n  default session.\n\n* **coord**: Optional `Coordinator` for coordinating the started threads.\n\n* **daemon**: Whether the threads should be marked as `daemons`, meaning\n  they don't block program exit.\n\n* **start**: Set to `False` to only create the threads, not start them.\n\n* **collection**: A `GraphKey` specifying the graph collection to\n  get the queue runners from.  Defaults to `GraphKeys.QUEUE_RUNNERS`.\n\n##### Returns\n"
}{}{}{
    "source file": "ragged_array_ops.py",
    "line number": "771",
    "func name": "_cross_internal",
    "func arg": "(inputs, hashed_output, num_buckets, hash_key, name)",
    "comments": "Generates feature cross from a list of ragged and dense tensors.\n\n\n"
}{}{
    "source file": "ragged_batch_gather_ops.py",
    "line number": "27",
    "func name": "batch_gather",
    "func arg": "(params, indices, name)",
    "comments": "Gathers slices from `params` according to `indices` with batch dims.\n\nThis operation is similar to `gather`, but it assumes that the leading `N` dimensions of `indices` and `params` are batch dimensions, and performs a gather within each batch.\n\nIn particular, when using this operation with `N` batch dimensions `B1...BN`:\n\n* `indices` has shape `[B1...BN, I]` * `params` has shape `[B1...BN, P1...PM]`. * `result` has shape `[B1...BN, I, P2...PM]`. * `result[b1...bN, i, p2...pM] = params[b1...bN, indices[b1...bN, i], p2...pM]`\n##### Args\n* **params**: A potentially ragged tensor with shape `[B1...BN, P1...PM]` (`N>=0`,\n  `M>0`).\n\n* **indices**: A potentially ragged tensor with shape `[B1...BN, I]` (`N>=0`).\n\n* **name**: A name for the operation (optional).\n\n##### Returns\n* **## Example**: \n\n"
}{
    "source file": "ragged_batch_gather_with_default_op.py",
    "line number": "144",
    "func name": "_get_pad_shape",
    "func arg": "(params, indices, row_splits_dtype)",
    "comments": "Gets the RaggedTensorDynamicShape for the pad tensor.\n\n\n"
}{}{}{
    "source file": "ragged_concat_ops.py",
    "line number": "315",
    "func name": "_concat_ragged_splits",
    "func arg": "(splits_list)",
    "comments": "Concatenates a list of RaggedTensor splits to form a single splits.\n\n\n"
}{
    "source file": "ragged_config.py",
    "line number": "22",
    "func name": "auto_cast_partition_dtype",
    "func arg": "()",
    "comments": "Whether incompatible row-partitioning dtypes should be auto-converted.\n\nIf true, then operations that combine RaggedTensors but have different row-partitioning tensor dtypes will be automatically cast to a compatible dtype (`tf.int64`).\n\nIf false, then such operations will result in an error.\n##### Returns\n"
}{
    "source file": "ragged_const_op_test.py",
    "line number": "404",
    "func name": "_normalize_pylist",
    "func arg": "(item)",
    "comments": "Convert all (possibly nested) np.arrays contained in item to list.\n\n\n"
}{
    "source file": "ragged_constant_value_op_test.py",
    "line number": "319",
    "func name": "_normalize_pylist",
    "func arg": "(item)",
    "comments": "Convert all (possibly nested) np.arrays contained in item to list.\n\n\n"
}{
    "source file": "ragged_conversion_ops.py",
    "line number": "144",
    "func name": "from_sparse",
    "func arg": "(st_input, name)",
    "comments": ""
}{
    "source file": "ragged_cross_op_test.py",
    "line number": "40",
    "func name": "sparse_const",
    "func arg": "(matrix)",
    "comments": ""
}{}{
    "source file": "ragged_dispatch.py",
    "line number": "591",
    "func name": "ragged_op_list",
    "func arg": "(tf_version)",
    "comments": "Returns a string listing operators that have dispathers registered.\n\n\n"
}{}{}{}{
    "source file": "ragged_factory_ops.py",
    "line number": "318",
    "func name": "placeholder",
    "func arg": "(dtype, ragged_rank, value_shape, name)",
    "comments": "Creates a placeholder for a `tf.RaggedTensor` that will always be fed.\n\n**Important**: This ragged tensor will produce an error if evaluated. Its value must be fed using the `feed_dict` optional argument to `Session.run()`, `Tensor.eval()`, or `Operation.run()`.\n\n@compatibility{eager} Placeholders are not compatible with eager execution.\n##### Args\n* **dtype**: The data type for the `RaggedTensor`.\n\n* **ragged_rank**: The ragged rank for the `RaggedTensor`\n\n* **value_shape**: The shape for individual flat values in the `RaggedTensor`.\n\n* **name**: A name for the operation (optional).\n\n##### Returns\n"
}{}{}{
    "source file": "ragged_functional_ops.py",
    "line number": "97",
    "func name": "_replace_ragged_with_flat_values",
    "func arg": "(value, nested_splits_lists)",
    "comments": "Replace RaggedTensors with their flat_values, and record their splits.\n\n\n##### Args\n* **value**: The value that should be transformed by replacing `RaggedTensors`.\n\n* **nested_splits_lists**: An output parameter used to record the `nested_splits`\n  for any `RaggedTensors` that were replaced.\n\n##### Returns\n"
}{}{}{
    "source file": "ragged_gather_ops.py",
    "line number": "473",
    "func name": "_ragged_gather_grad",
    "func arg": "(op)",
    "comments": "Gradient for RaggedGather op.\n\n\n"
}{
    "source file": "ragged_getitem_test.py",
    "line number": "50",
    "func name": "_make_tensor_slice_spec",
    "func arg": "(slice_spec, use_constant)",
    "comments": "Wraps all integers in an extended slice spec w/ a tensor.\n\nThis function is used to help test slicing when the slice spec contains tensors, rather than integers.\n##### Args\n* **slice_spec**: The extended slice spec.\n\n* **use_constant**: If true, then wrap each integer with a tf.constant.  If false,\n  then wrap each integer with a tf.placeholder.\n\n##### Returns\n"
}{
    "source file": "ragged_getitem.py",
    "line number": "459",
    "func name": "_if_ge_zero",
    "func arg": "(value, true_fn, false_fn)",
    "comments": "Returns `true_fn() if value >= 0 else false_fn()`.\n\n\n"
}{}{}{
    "source file": "ragged_map_ops.py",
    "line number": "174",
    "func name": "_ragged_type_to_spec",
    "func arg": "(t)",
    "comments": ""
}{
    "source file": "ragged_math_ops.py",
    "line number": "634",
    "func name": "_set_ragged_reduce_docstring",
    "func arg": "(func, combination, combined, default, example)",
    "comments": ""
}{}{}{}{
    "source file": "ragged_operators.py",
    "line number": "72",
    "func name": "_dummy_bool",
    "func arg": "(_)",
    "comments": "Dummy method to prevent a RaggedTensor from being used as a Python bool.\n\n\n"
}{}{}{}{}{}{
    "source file": "ragged_reduce_op_test.py",
    "line number": "38",
    "func name": "mean",
    "func arg": "()",
    "comments": ""
}{}{}{}{}{
    "source file": "ragged_segment_op_test.py",
    "line number": "47",
    "func name": "sqrt_n",
    "func arg": "(values)",
    "comments": ""
}{}{}{
    "source file": "ragged_squeeze_op.py",
    "line number": "31",
    "func name": "squeeze",
    "func arg": "(input, axis, name)",
    "comments": "Ragged compatible squeeze.\n\nIf `input` is a `tf.Tensor`, then this calls `tf.squeeze`.\n\nIf `input` is a `tf.RaggedTensor`, then this operation takes `O(N)` time, where `N` is the number of elements in the squeezed dimensions.\n##### Args\n* **input**: A potentially ragged tensor. The input to squeeze.\n\n* **axis**: An optional list of ints. Defaults to `None`. If the `input` is\n  ragged, it only squeezes the dimensions listed. It fails if `input` is\n  ragged and axis is []. If `input` is not ragged it calls tf.squeeze. Note\n  that it is an error to squeeze a dimension that is not 1. It must be in\n  the range of [-rank(input), rank(input)).\n\n* **ame**: A name for the operation (optional).\n\n##### Returns\n"
}{}{
    "source file": "ragged_string_ops.py",
    "line number": "924",
    "func name": "_nrows",
    "func arg": "(tensor, out_type)",
    "comments": ""
}{}{}{
    "source file": "ragged_tensor_shape.py",
    "line number": "601",
    "func name": "_ragged_tile_axis",
    "func arg": "(rt_input, axis, repeats, row_splits_dtype)",
    "comments": "Tile a dimension of a RaggedTensor to match a ragged shape.\n\n\n"
}{
    "source file": "ragged_tensor_test.py",
    "line number": "47",
    "func name": "int32array",
    "func arg": "(values)",
    "comments": ""
}{}{
    "source file": "ragged_tensor.py",
    "line number": "2744",
    "func name": "_get_optional_partition_dtype",
    "func arg": "(values)",
    "comments": "Returns the partition dtype, or None if None exists.\n\n\n"
}{}{}{
    "source file": "ragged_to_tensor_op_test.py",
    "line number": "50",
    "func name": "rebuild_ragged_tensor_with_value_rowids",
    "func arg": "(rt, feed_dict, sess)",
    "comments": "Returns a copy of `rt`, built using `from_value_rowids`.\n\nThis ensures that RaggedTensor._cached_value_rowids is populated, which triggers a different code-path for converting ragged tensors to tensors.\n\nIf `feed_dict` and `sess` are specified, then build the new `RaggedTensor` using placeholder tensors, and populate a feed dictionary that can be used to feed the placeholders.\n##### Args\n* **rt**: The RaggedTensor to copy.\n\n* **feed_dict**: If specified, then build the new `RaggedTensor` using\n  placeholders, and populate this dict with entries to feed those\n  placeholders.\n\n* **sess**: A session used to evaluate tensors; required if feed_dict is\n  specified.\n\n##### Returns\n"
}{}{
    "source file": "ragged_util.py",
    "line number": "70",
    "func name": "repeat_ranges",
    "func arg": "(params, splits, repeats)",
    "comments": "Repeats each range of `params` (as specified by `splits`) `repeats` times.\n\nLet the `i`th range of `params` be defined as `params[splits[i]:splits[i + 1]]`.\n\nThen this function returns a tensor containing range 0 repeated `repeats[0]` times, followed by range 1 repeated `repeats[1]`, ..., followed by the last range repeated `repeats[-1]` times.\n##### Args\n* **params**: The `Tensor` whose values should be repeated.\n\n* **splits**: A splits tensor indicating the ranges of `params` that should be\n  repeated.\n\n* **repeats**: The number of times each range should be repeated.  Supports\n  broadcasting from a scalar value.\n\n##### Returns\n* **## Example**: \n\n"
}{}{
    "source file": "ragged_where_op.py",
    "line number": "160",
    "func name": "_nrows",
    "func arg": "(rt_input, out_type)",
    "comments": ""
}{}{}{}{}{}{
    "source file": "random_grad.py",
    "line number": "139",
    "func name": "_StatelessParameterizedTruncatedNormalGrad",
    "func arg": "(op, grad)",
    "comments": "Returns the gradient of a TruncatedNormal sample w.r.t. parameters.\n\nThe gradient is computed using implicit differentiation (Figurnov et al., 2018).\n##### Args\n* **op**: A `StatelessParameterizedTruncatedNormal` operation. We assume that the\n  inputs to the operation are `shape`, `seed`, `mean`, `stddev`, `minval`,\n  and `maxval` tensors, and the output is the `sample` tensor.\n\n* **grad**: The incoming gradient `dloss / dsample` of the same shape as\n  `op.outputs[0]`.\n\n##### Returns\n* **ferences**: \n\n* **Implicit Reparameterization Gradients**: [Figurnov et al., 2018]\n  (http\n\n"
}{}{}{}{
    "source file": "random_ops1.py",
    "line number": "614",
    "func name": "random_poisson_v2",
    "func arg": "(shape, lam, dtype, seed, name)",
    "comments": "Draws `shape` samples from each of the given Poisson distribution(s).\n\n`lam` is the rate parameter describing the distribution(s).\n\nExample:\n\n```python samples = tf.random.poisson([10], [0.5, 1.5]) # samples has shape [10, 2], where each slice [:, 0] and [:, 1] represents # the samples drawn from each distribution\n\nsamples = tf.random.poisson([7, 5], [12.2, 3.3]) # samples has shape [7, 5, 2], where each slice [:, :, 0] and [:, :, 1] # represents the 7x5 samples drawn from each of the two distributions ```\n##### Args\n* **shape**: A 1-D integer Tensor or Python array. The shape of the output samples\n  to be drawn per \"rate\"-parameterized distribution.\n\n* **lam**: A Tensor or Python value or N-D array of type `dtype`.\n  `lam` provides the rate parameter(s) describing the poisson\n  distribution(s) to sample.\n\n* **dtype**: The type of the output\n\n* **seed**: A Python integer. Used to create a random seed for the distributions.\n  See\n  `tf.random.set_seed`\n  for behavior.\n\n* **name**: Optional name for the operation.\n\n##### Returns\n* **samples**: a `Tensor` of shape `tf.concat([shape, tf.shape(lam)], axis=0)`\n  with values of type `dtype`.\n\n"
}{}{}{}{
    "source file": "random_seed.py",
    "line number": "29",
    "func name": "get_seed",
    "func arg": "(seed)",
    "comments": "Returns the local seeds an operation should use given an op-specific seed.\n\nSee `random_seed.get_seed` for more details. This wrapper adds support for the case where `seed` may be a tensor.\n##### Args\n* **seed**: An integer or a `tf.int64` scalar tensor.\n\n##### Returns\n"
}{
    "source file": "random_seed1.py",
    "line number": "199",
    "func name": "set_seed",
    "func arg": "(seed)",
    "comments": "Sets the global random seed.\n\nOperations that rely on a random seed actually derive it from two seeds: the global and operation-level seeds. This sets the global seed.\n\nIts interactions with operation-level seeds is as follows:\n\n1. If neither the global seed nor the operation seed is set: A randomly picked seed is used for this op. 2. If the graph-level seed is set, but the operation seed is not: The system deterministically picks an operation seed in conjunction with the graph-level seed so that it gets a unique random sequence. Within the same version of tensorflow and user code, this sequence is deterministic. However across different versions, this sequence might change. If the code depends on particular seeds to work, specify both graph-level and operation-level seeds explicitly. 3. If the operation seed is set, but the global seed is not set: A default global seed and the specified operation seed are used to determine the random sequence. 4. If both the global and the operation seed are set: Both seeds are used in conjunction to determine the random sequence.\n\nTo illustrate the user-visible effects, consider these examples:\n\nIf neither the global seed nor the operation seed is set, we get different results for every call to the random op and every re-run of the program:\n\n```python print(tf.random.uniform([1]))\n\n# generates 'A1' print(tf.random.uniform([1]))\n\n# generates 'A2' ```\n\n(now close the program and run it again)\n\n```python print(tf.random.uniform([1]))\n\n# generates 'A3' print(tf.random.uniform([1]))\n\n# generates 'A4' ```\n\nIf the global seed is set but the operation seed is not set, we get different results for every call to the random op, but the same sequence for every re-run of the program:\n\n```python tf.random.set_seed(1234) print(tf.random.uniform([1]))\n\n# generates 'A1' print(tf.random.uniform([1]))\n\n# generates 'A2' ```\n\n(now close the program and run it again)\n\n```python tf.random.set_seed(1234) print(tf.random.uniform([1]))\n\n# generates 'A1' print(tf.random.uniform([1]))\n\n# generates 'A2' ```\n\nThe reason we get 'A2' instead 'A1' on the second call of `tf.random.uniform` above is because the second call uses a different operation seed.\n\nNote that `tf.function` acts like a re-run of a program in this case. When the global seed is set but operation seeds are not set, the sequence of random numbers are the same for each `tf.function`. For example:\n\n```python tf.random.set_seed(1234)\n\n@tf.function def f(): a = tf.random.uniform([1]) b = tf.random.uniform([1]) return a, b\n\n@tf.function def g(): a = tf.random.uniform([1]) b = tf.random.uniform([1]) return a, b\n\nprint(f())\n\n# prints '(A1, A2)' print(g())\n\n# prints '(A1, A2)' ```\n\nIf the operation seed is set, we get different results for every call to the random op, but the same sequence for every re-run of the program:\n\n```python print(tf.random.uniform([1], seed=1))\n\n# generates 'A1' print(tf.random.uniform([1], seed=1))\n\n# generates 'A2' ```\n\n(now close the program and run it again)\n\n```python print(tf.random.uniform([1], seed=1))\n\n# generates 'A1' print(tf.random.uniform([1], seed=1))\n\n# generates 'A2' ```\n\nThe reason we get 'A2' instead 'A1' on the second call of `tf.random.uniform` above is because the same `tf.random.uniform` kernel (i.e. internal representation) is used by TensorFlow for all calls of it with the same arguments, and the kernel maintains an internal counter which is incremented every time it is executed, generating different results.\n\nCalling `tf.random.set_seed` will reset any such counters:\n\n```python tf.random.set_seed(1234) print(tf.random.uniform([1], seed=1))\n\n# generates 'A1' print(tf.random.uniform([1], seed=1))\n\n# generates 'A2' tf.random.set_seed(1234) print(tf.random.uniform([1], seed=1))\n\n# generates 'A1' print(tf.random.uniform([1], seed=1))\n\n# generates 'A2' ```\n\nWhen multiple identical random ops are wrapped in a `tf.function`, their behaviors change because the ops no long share the same counter. For example:\n\n```python @tf.function def foo(): a = tf.random.uniform([1], seed=1) b = tf.random.uniform([1], seed=1) return a, b print(foo())\n\n# prints '(A1, A1)' print(foo())\n\n# prints '(A2, A2)'\n\n@tf.function def bar(): a = tf.random.uniform([1]) b = tf.random.uniform([1]) return a, b print(bar())\n\n# prints '(A1, A2)' print(bar())\n\n# prints '(A3, A4)' ```\n\nThe second call of `foo` returns '(A2, A2)' instead of '(A1, A1)' because `tf.random.uniform` maintains an internal counter. If you want `foo` to return '(A1, A1)' every time, use the stateless random ops such as `tf.random.stateless_uniform`. Also see `tf.random.experimental.Generator` for a new set of stateful random ops that use external variables to manage their states.\n##### Args\n* **seed**: integer.\n\n"
}{}{
    "source file": "randomize_weights.py",
    "line number": "34",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{}{}{}{
    "source file": "range.py",
    "line number": "27",
    "func name": "make_range_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do range.\n\n\n"
}{}{
    "source file": "rank.py",
    "line number": "27",
    "func name": "make_rank_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do rank.\n\n\n"
}{}{}{
    "source file": "raw_to_bitmap.py",
    "line number": "159",
    "func name": "main",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "reaching_definitions.py",
    "line number": "279",
    "func name": "resolve",
    "func arg": "(node, source_info, graphs, definition_factory)",
    "comments": "Resolves reaching definitions for each symbol.\n\n\n##### Args\n* **node**: ast.AST\n\n* **source_info**: transformer.SourceInfo\n\n* **graphs**: Dict[ast.FunctionDef, cfg.Graph]\n\n* **definition_factory**: Callable[[], Definition]\n\n##### Returns\n"
}{}{
    "source file": "reaching_fndefs.py",
    "line number": "170",
    "func name": "resolve",
    "func arg": "(node, source_info, graphs)",
    "comments": "Resolves reaching definitions for each symbol.\n\n\n##### Args\n* **node**: ast.AST\n\n* **source_info**: transformer.SourceInfo\n\n* **graphs**: Dict[ast.FunctionDef, cfg.Graph]\n\n##### Returns\n"
}{}{}{
    "source file": "readers.py",
    "line number": "1082",
    "func name": "_get_file_names",
    "func arg": "(file_pattern, shuffle)",
    "comments": "Parse list of file names from pattern, optionally shuffled.\n\n\n##### Args\n* **file_pattern**: File glob pattern, or list of glob patterns.\n\n* **shuffle**: Whether to shuffle the order of file names.\n\n##### Returns\n"
}{
    "source file": "readers1.py",
    "line number": "66",
    "func name": "_create_dataset_reader",
    "func arg": "(dataset_creator, filenames, num_parallel_reads)",
    "comments": "Creates a dataset that reads the given files using the given reader.\n\n\n##### Args\n* **dataset_creator**: A function that takes in a single file name and returns a\n  dataset.\n\n* **filenames**: A `tf.data.Dataset` containing one or more filenames.\n\n* **num_parallel_reads**: The number of parallel reads we should do.\n\n##### Returns\n"
}{}{}{}{
    "source file": "rebatch_dataset_test.py",
    "line number": "35",
    "func name": "_flat_shapes",
    "func arg": "(dataset)",
    "comments": ""
}{}{}{
    "source file": "reconstruction_ops.py",
    "line number": "32",
    "func name": "overlap_and_add",
    "func arg": "(signal, frame_step, name)",
    "comments": "Reconstructs a signal from a framed representation.\n\nAdds potentially overlapping frames of a signal with shape `[..., frames, frame_length]`, offsetting subsequent frames by `frame_step`. The resulting tensor has shape `[..., output_size]` where\n\noutput_size = (frames\n\n- 1) * frame_step + frame_length\n##### Args\n* **signal**: A [..., frames, frame_length] `Tensor`. All dimensions may be\n  unknown, and rank must be at least 2.\n\n* **frame_step**: An integer or scalar `Tensor` denoting overlap offsets. Must be\n  less than or equal to `frame_length`.\n\n* **name**: An optional name for the operation.\n\n##### Returns\n"
}{}{}{}{
    "source file": "recurrent_v2.py",
    "line number": "1666",
    "func name": "_read_variable_value",
    "func arg": "(v)",
    "comments": "Read the value of a resource variable if it is variable.\n\n\n"
}{
    "source file": "recurrent.py",
    "line number": "3074",
    "func name": "_config_for_enable_caching_device",
    "func arg": "(rnn_cell)",
    "comments": "Return the dict config for RNN cell wrt to enable_caching_device field.\n\nSince enable_caching_device is a internal implementation detail for speed up the RNN variable read when running on the multi remote worker setting, we don't want this config to be serialized constantly in the JSON. We will only serialize this field when a none default value is used to create the cell.\n##### Args\n* **rnn_cell**: the RNN cell for serialize.\n\n##### Returns\n"
}{}{
    "source file": "reduce_join_op_test.py",
    "line number": "48",
    "func name": "_joined_array",
    "func arg": "(num_dims, reduce_dim)",
    "comments": "Creates an ndarray with the result from reduce_join on input_array.\n\n\n##### Args\n* **num_dims**: The number of dimensions of the original input array.\n\n* **reduce_dim**: The dimension to reduce.\n\n##### Returns\n"
}{}{}{}{}{
    "source file": "reduce.py",
    "line number": "266",
    "func name": "make_reduce_any_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do any.\n\n\n"
}{}{
    "source file": "reduction_ops_test.py",
    "line number": "42",
    "func name": "_powerset",
    "func arg": "(iterable)",
    "comments": "Helper for generating all possible reduction_axes arguments.\n\nExample: powerset([0,1,2]): () (0,) (1,) (2,) (0,1) (0,2) (1,2) (0,1,2)\n##### Args\n* **iterable**: An iterable of items to generate the powerset of.\n\n##### Returns\n"
}{}{
    "source file": "reduction.py",
    "line number": "28",
    "func name": "get_reduce_op",
    "func arg": "(reduction_str)",
    "comments": "Translate a reduction string name to a reduction op.\n\n\n"
}{}{
    "source file": "regex_replace_op_test.py",
    "line number": "98",
    "func name": "as_tensor",
    "func arg": "(s)",
    "comments": ""
}{
    "source file": "registrations_util.py",
    "line number": "81",
    "func name": "combined_non_singular_hint",
    "func arg": "(operator_a, operator_b)",
    "comments": "Get combined hint for when .\n\n\n"
}{
    "source file": "registry_test.py",
    "line number": "28",
    "func name": "bar",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "regularizers.py",
    "line number": "372",
    "func name": "get",
    "func arg": "(identifier)",
    "comments": "Retrieve a regularizer instance from a config or identifier.\n\n\n"
}{
    "source file": "rejection_resample_benchmark.py",
    "line number": "31",
    "func name": "_time_resampling",
    "func arg": "(data_np, target_dist, init_dist, num_to_sample)",
    "comments": ""
}{}{
    "source file": "relu_op_test.py",
    "line number": "41",
    "func name": "_elu_grad_grad",
    "func arg": "(activation)",
    "comments": ""
}{
    "source file": "relu.py",
    "line number": "28",
    "func name": "make_relu_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do relu.\n\n\n"
}{
    "source file": "relu1.py",
    "line number": "28",
    "func name": "make_relu1_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do relu1.\n\n\n"
}{
    "source file": "relu6.py",
    "line number": "28",
    "func name": "make_relu6_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do relu6.\n\n\n"
}{
    "source file": "remote_benchmarks_test.py",
    "line number": "45",
    "func name": "run_benchmark",
    "func arg": "(func, num_iters, execution_mode)",
    "comments": ""
}{}{
    "source file": "remote_cluster_test.py",
    "line number": "46",
    "func name": "get_server_def",
    "func arg": "(job_name, local_server_port, remote_server_addresses, task_index)",
    "comments": "Returns a server def with a single job + multiple tasks.\n\n\n"
}{
    "source file": "remote_execution_test.py",
    "line number": "46",
    "func name": "get_server_def",
    "func arg": "(job_name, local_server_port, remote_server_addresses, task_index)",
    "comments": "Returns a server def with a single job + multiple tasks.\n\n\n"
}{}{
    "source file": "remote_mirrored_strategy_eager_test.py",
    "line number": "29",
    "func name": "get_gpus",
    "func arg": "()",
    "comments": ""
}{
    "source file": "remote_test.py",
    "line number": "617",
    "func name": "_strip_prefix",
    "func arg": "(s, prefix)",
    "comments": ""
}{
    "source file": "remote_utils.py",
    "line number": "21",
    "func name": "get_default_communication_protocol",
    "func arg": "()",
    "comments": ""
}{
    "source file": "remote.py",
    "line number": "222",
    "func name": "_strip_prefix",
    "func arg": "(s, prefix)",
    "comments": ""
}{
    "source file": "remove_init_variable_v1.py",
    "line number": "50",
    "func name": "Test",
    "func arg": "()",
    "comments": ""
}{}{}{}{}{
    "source file": "replicate_test.py",
    "line number": "187",
    "func name": "_get_server_def",
    "func arg": "(job_name, local_server_port, remote_server_addresses, task_index)",
    "comments": "Returns a server def with a single job + multiple tasks.\n\n\n"
}{
    "source file": "resampling.py",
    "line number": "230",
    "func name": "_calculate_acceptance_probs_with_mixing",
    "func arg": "(initial_probs, target_probs)",
    "comments": "Calculates the acceptance probabilities and mixing ratio.\n\nIn this case, we assume that we can *either* sample from the original data distribution with probability `m`, or sample from a reshaped distribution that comes from rejection sampling on the original distribution. This rejection sampling is done on a per-class basis, with `a_i` representing the probability of accepting data from class `i`.\n\nThis method is based on solving the following analysis for the reshaped distribution:\n\nLet F be the probability of a rejection (on any example). Let p_i be the proportion of examples in the data in class i (init_probs) Let a_i is the rate the rejection sampler should *accept* class i Let t_i is the target proportion in the minibatches for class i (target_probs)\n\n``` F = sum_i(p_i * (1-a_i)) = 1\n\n- sum_i(p_i * a_i)\n\n\n\n using sum_i(p_i) = 1 ```\n\nAn example with class `i` will be accepted if `k` rejections occur, then an example with class `i` is seen by the rejector, and it is accepted. This can be written as follows:\n\n``` t_i = sum_k=0^inf(F^k * p_i * a_i) = p_i * a_j / (1\n\n- F)\n\n\n\nusing geometric series identity, since 0 <= F < 1 = p_i * a_i / sum_j(p_j * a_j)\n\n\n\n\n\n\n\nusing F from above ```\n\nNote that the following constraints hold: ``` 0 <= p_i <= 1, sum_i(p_i) = 1 0 <= a_i <= 1 0 <= t_i <= 1, sum_i(t_i) = 1 ```\n\nA solution for a_i in terms of the other variables is the following: ```a_i = (t_i / p_i) / max_i[t_i / p_i]```\n\nIf we try to minimize the amount of data rejected, we get the following:\n\nM_max = max_i [ t_i / p_i ] M_min = min_i [ t_i / p_i ]\n\nThe desired probability of accepting data if it comes from class `i`:\n\na_i = (t_i/p_i\n\n- m) / (M_max\n\n- m)\n\nThe desired probability of pulling a data element from the original dataset, rather than the filtered one:\n\nm = M_min\n##### Args\n* **initial_probs**: A Tensor of the initial probability distribution, given or\n  estimated.\n\n* **target_probs**: A Tensor of the corresponding classes.\n\n##### Returns\n"
}{}{}{}{
    "source file": "reshape.py",
    "line number": "28",
    "func name": "make_reshape_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do reshape.\n\n\n"
}{
    "source file": "resize_bilinear.py",
    "line number": "27",
    "func name": "make_resize_bilinear_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do resize_bilinear.\n\n\n"
}{
    "source file": "resize_nearest_neighbor.py",
    "line number": "27",
    "func name": "make_resize_nearest_neighbor_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do resize_nearest_neighbor.\n\n\n"
}{
    "source file": "resnet_v2.py",
    "line number": "131",
    "func name": "decode_predictions",
    "func arg": "(preds, top)",
    "comments": ""
}{
    "source file": "resnet.py",
    "line number": "529",
    "func name": "decode_predictions",
    "func arg": "(preds, top)",
    "comments": ""
}{
    "source file": "resnet50_graph_test.py",
    "line number": "39",
    "func name": "random_batch",
    "func arg": "(batch_size)",
    "comments": ""
}{
    "source file": "resnet50_test_util.py",
    "line number": "48",
    "func name": "report",
    "func arg": "(benchmark, label, start, num_iters, device, batch_size, data_format, num_replicas)",
    "comments": ""
}{
    "source file": "resnet50_test.py",
    "line number": "76",
    "func name": "events_from_logdir",
    "func arg": "(logdir)",
    "comments": "Returns all events in the single eventfile in logdir.\n\n\n##### Args\n* **logdir**: The directory in which the single event file is sought.\n\n##### Returns\n"
}{}{}{
    "source file": "resolve_constant_strided_slice.py",
    "line number": "29",
    "func name": "make_resolve_constant_strided_slice_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to show strided_slice yields incorrect results.\n\n\n"
}{}{
    "source file": "resource_loader.py",
    "line number": "134",
    "func name": "readahead_file_path",
    "func arg": "(path, readahead)",
    "comments": "Readahead files not implemented; simply returns given path.\n\n\n"
}{}{}{
    "source file": "resource_variable_ops.py",
    "line number": "2140",
    "func name": "copy_to_graph_uninitialized",
    "func arg": "(var)",
    "comments": "Copies an existing variable to a new graph, with no initializer.\n\n\n"
}{
    "source file": "resources.py",
    "line number": "108",
    "func name": "initialize_resources",
    "func arg": "(resource_list, name)",
    "comments": "Initializes the resources in the given list.\n\n\n##### Args\n* **resource_list**: list of resources to initialize.\n\n* **name**: name of the initialization op.\n\n##### Returns\n"
}{}{
    "source file": "return_statements.py",
    "line number": "392",
    "func name": "transform",
    "func arg": "(node, ctx, default_to_null_return)",
    "comments": "Ensure a function has only a single return, at the end.\n\n\n"
}{
    "source file": "reuters.py",
    "line number": "155",
    "func name": "get_word_index",
    "func arg": "(path)",
    "comments": "Retrieves a dict mapping words to their index in the Reuters dataset.\n\nArguments: path: where to cache the data (relative to `~/.keras/dataset`).\n##### Returns\n"
}{}{}{}{
    "source file": "reverse_sequence.py",
    "line number": "27",
    "func name": "make_reverse_sequence_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do reverse_sequence.\n\n\n"
}{
    "source file": "reverse_v2.py",
    "line number": "27",
    "func name": "make_reverse_v2_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do reverse_v2.\n\n\n"
}{}{}{
    "source file": "revived_types.py",
    "line number": "174",
    "func name": "get_setter",
    "func arg": "(proto)",
    "comments": ""
}{
    "source file": "rfft2d.py",
    "line number": "28",
    "func name": "make_rfft2d_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do rfft2d.\n\n\n"
}{}{}{}{}{}{
    "source file": "rnn_cell_impl.py",
    "line number": "1349",
    "func name": "_check_supported_dtypes",
    "func arg": "(dtype)",
    "comments": ""
}{}{}{
    "source file": "rnn_cell_wrapper_impl.py",
    "line number": "507",
    "func name": "_enumerated_map_structure_up_to",
    "func arg": "(shallow_structure, map_fn, **kwargs)",
    "comments": ""
}{}{}{}{}{}{
    "source file": "rnn_grad_test.py",
    "line number": "143",
    "func name": "icfo_to_ifco",
    "func arg": "(w, b)",
    "comments": "Convert gates' weights and biases from ICFO to IFCO layout.\n\n\n"
}{
    "source file": "rnn_grad.py",
    "line number": "24",
    "func name": "_block_lstm_grad",
    "func arg": "(op)",
    "comments": "Gradient for the BlockLSTM op.\n\n\n"
}{
    "source file": "rnn_test.py",
    "line number": "685",
    "func name": "rnn_long_sequence_benchmark",
    "func arg": "(batch_size, seqlen, num_units, dynamic, swap_memory, nn)",
    "comments": ""
}{
    "source file": "rnn.py",
    "line number": "279",
    "func name": "bidirectional_dynamic_rnn",
    "func arg": "(cell_fw, cell_bw, inputs, sequence_length, initial_state_fw, initial_state_bw, dtype, parallel_iterations, swap_memory, time_major, scope)",
    "comments": "Creates a dynamic version of bidirectional recurrent neural network.\n\nTakes input and builds independent forward and backward RNNs. The input_size of forward and backward cell must match. The initial state for both directions is zero by default (but can be set optionally) and no intermediate states are ever returned -- the network is fully unrolled for the given (passed in) length(s) of the sequence(s) or completely unrolled if length(s) is not given.\n##### Args\n* **cell_fw**: An instance of RNNCell, to be used for forward direction.\n\n* **cell_bw**: An instance of RNNCell, to be used for backward direction.\n\n* **inputs**: The RNN inputs.\n  If time_major == False (default), this must be a tensor of shape\n\n* **sequence_length**: (optional) An int32/int64 vector, size `[batch_size]`,\n  containing the actual lengths for each of the sequences in the batch. If\n  not provided, all batch entries are assumed to be full sequences; and time\n  reversal is applied from time `0` to `max_time` for each sequence.\n\n* **initial_state_fw**: (optional) An initial state for the forward RNN. This must\n  be a tensor of appropriate type and shape `[batch_size,\n  cell_fw.state_size]`. If `cell_fw.state_size` is a tuple, this should be a\n  tuple of tensors having shapes `[batch_size, s] for s in\n  cell_fw.state_size`.\n\n* **initial_state_bw**: (optional) Same as for `initial_state_fw`, but using the\n  corresponding properties of `cell_bw`.\n\n* **dtype**: (optional) The data type for the initial states and expected output.\n  Required if initial_states are not provided or RNN states have a\n  heterogeneous dtype.\n\n* **parallel_iterations**: (Default\n\n* **swap_memory**: Transparently swap the tensors produced in forward inference\n  but needed for back prop from GPU to CPU.  This allows training RNNs which\n  would typically not fit on a single GPU, with very minimal (or no)\n  performance penalty.\n\n* **time_major**: The shape format of the `inputs` and `outputs` Tensors. If true,\n  these `Tensors` must be shaped `[max_time, batch_size, depth]`. If false,\n  these `Tensors` must be shaped `[batch_size, max_time, depth]`. Using\n  `time_major = True` is a bit more efficient because it avoids transposes\n  at the beginning and end of the RNN calculation.  However, most TensorFlow\n  data is batch-major, so by default this function accepts input and emits\n  output in batch-major form.\n\n* **scope**: VariableScope for the created subgraph; defaults to\n  \"bidirectional_rnn\"\n\n##### Returns\n* **A tuple (outputs, output_states) where**: outputs\n\n"
}{
    "source file": "rnn1.py",
    "line number": "1520",
    "func name": "static_bidirectional_rnn",
    "func arg": "(cell_fw, cell_bw, inputs, initial_state_fw, initial_state_bw, dtype, sequence_length, scope)",
    "comments": "Creates a bidirectional recurrent neural network.\n\nSimilar to the unidirectional case above (rnn) but takes input and builds independent forward and backward RNNs with the final forward and backward outputs depth-concatenated, such that the output will have the format [time][batch][cell_fw.output_size + cell_bw.output_size]. The input_size of forward and backward cell must match. The initial state for both directions is zero by default (but can be set optionally) and no intermediate states are ever returned -- the network is fully unrolled for the given (passed in) length(s) of the sequence(s) or completely unrolled if length(s) is not given.\n##### Args\n* **cell_fw**: An instance of RNNCell, to be used for forward direction.\n\n* **cell_bw**: An instance of RNNCell, to be used for backward direction.\n\n* **inputs**: A length T list of inputs, each a tensor of shape [batch_size,\n  input_size], or a nested tuple of such elements.\n\n* **initial_state_fw**: (optional) An initial state for the forward RNN. This must\n  be a tensor of appropriate type and shape `[batch_size,\n  cell_fw.state_size]`. If `cell_fw.state_size` is a tuple, this should be a\n  tuple of tensors having shapes `[batch_size, s] for s in\n  cell_fw.state_size`.\n\n* **initial_state_bw**: (optional) Same as for `initial_state_fw`, but using the\n  corresponding properties of `cell_bw`.\n\n* **dtype**: (optional) The data type for the initial state.  Required if either\n  of the initial states are not provided.\n\n* **sequence_length**: (optional) An int32/int64 vector, size `[batch_size]`,\n  containing the actual lengths for each of the sequences.\n\n* **scope**: VariableScope for the created subgraph; defaults to\n  \"bidirectional_rnn\"\n\n##### Returns\n* **A tuple (outputs, output_state_fw, output_state_bw) where**: outputs is a length `T` list of outputs (one for each input), which\n    are depth-concatenated forward and backward outputs.\n  output_state_fw is the final state of the forward rnn.\n  output_state_bw is the final state of the backward rnn.\n\n"
}{
    "source file": "round.py",
    "line number": "27",
    "func name": "make_round_tests",
    "func arg": "(options)",
    "comments": "Build the round op testing graph.\n\n\n"
}{
    "source file": "row_partition_test.py",
    "line number": "864",
    "func name": "_assert_row_partition_equal",
    "func arg": "(test_class, actual, expected)",
    "comments": ""
}{
    "source file": "row_partition.py",
    "line number": "1192",
    "func name": "_merge_tensors",
    "func arg": "(t1, t2, name, validate)",
    "comments": "Merge two optional Tensors with equal values into a single Tensor.\n\n\n##### Args\n* **t1**: tf.Tensor or None\n\n* **t2**: tf.Tensor or None\n\n* **name**: A name for the tensors (for error messages)\n\n* **validate**: If true, then check that `t1` is compatible with `t2` (if both are\n  non-None).\n\n##### Returns\n* **A pair `(merged_value, validated)`**: * `merged_value` is `t1` if it is not None; or `t2` otherwise.\n  * `validated` is true if we validated that t1 and t2 are equal (either\n    by adding a check, or because t1 is t2).\n\n"
}{
    "source file": "run_and_gather_logs_lib.py",
    "line number": "101",
    "func name": "run_and_gather_logs",
    "func arg": "(name, test_name, test_args, benchmark_type)",
    "comments": "Run the bazel test given by test_name.  Gather and return the logs.\n\n\n##### Args\n* **name**: Benchmark target identifier.\n\n* **test_name**: A unique bazel target, e.g. \"//path/to\n\n* **test_args**: A string containing all arguments to run the target with.\n\n* **benchmark_type**: A string representing the BenchmarkType enum; the\n  benchmark type for this target.\n\n##### Returns\n* **test_results**: A test_log_pb2.TestResults proto\n\n* **test_adjusted_name**: Unique benchmark name that consists of\n  benchmark name optionally followed by GPU type.\n\n"
}{
    "source file": "run_and_gather_logs.py",
    "line number": "69",
    "func name": "main",
    "func arg": "(unused_args)",
    "comments": ""
}{}{
    "source file": "run_metadata_test.py",
    "line number": "92",
    "func name": "_run_loop_model",
    "func arg": "()",
    "comments": ""
}{}{}{}{
    "source file": "save_context.py",
    "line number": "53",
    "func name": "in_save_context",
    "func arg": "()",
    "comments": "Returns whether under a save context.\n\n\n"
}{
    "source file": "save_impl.py",
    "line number": "634",
    "func name": "_get_layer_call_method",
    "func arg": "(layer)",
    "comments": ""
}{
    "source file": "save_options.py",
    "line number": "175",
    "func name": "_validate_namespace_whitelist",
    "func arg": "(namespace_whitelist)",
    "comments": "Validates namespace whitelist argument.\n\n\n"
}{}{}{
    "source file": "save_test1.py",
    "line number": "77",
    "func name": "_import_and_infer",
    "func arg": "(save_dir, inputs, signature_key)",
    "comments": "Import a SavedModel into a TF 1.x-style graph and run `signature_key`.\n\n\n"
}{
    "source file": "save.py",
    "line number": "138",
    "func name": "load_model",
    "func arg": "(filepath, custom_objects, compile, options)",
    "comments": "Loads a model saved via `model.save()`.\n\nUsage:\n\n>>> model = tf.keras.Sequential([ ...\n\n\n\n tf.keras.layers.Dense(5, input_shape=(3,)), ...\n\n\n\n tf.keras.layers.Softmax()]) >>> model.save('/tmp/model') >>> loaded_model = tf.keras.models.load_model('/tmp/model') >>> x = tf.random.uniform((10, 3)) >>> assert np.allclose(model.predict(x), loaded_model.predict(x))\n\nNote that the model weights may have different scoped names after being loaded. Scoped names include the model/layer names, such as `\"dense_1/kernel:0\"`. It is recommended that you use the layer properties to access specific variables, e.g. `model.get_layer(\"dense_1\").kernel`.\n\nArguments: filepath: One of the following:\n\n- String or `pathlib.Path` object, path to the saved model\n\n- `h5py.File` object from which to load the model custom_objects: Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization. compile: Boolean, whether to compile the model after loading. options: Optional `tf.saved_model.LoadOptions` object that specifies options for loading from SavedModel.\n##### Returns\n"
}{
    "source file": "save1.py",
    "line number": "40",
    "func name": "save",
    "func arg": "(model, filepath, overwrite, include_optimizer, signatures, options)",
    "comments": "Saves a model as a SavedModel to the filepath.\n\n\n##### Args\n* **model**: Keras model instance to be saved.\n\n* **filepath**: String path to save the model.\n\n* **overwrite**: whether to overwrite the existing filepath.\n\n* **include_optimizer**: If True, save the model's optimizer state.\n\n* **signatures**: Signatures to save with the SavedModel. Applicable to the 'tf'\n  format only. Please see the `signatures` argument in `tf.saved_model.save`\n  for details.\n\n* **options**: Optional `tf.saved_model.SaveOptions` object that specifies\n  options for saving to SavedModel.\n\n"
}{
    "source file": "save2.py",
    "line number": "1139",
    "func name": "_build_meta_graph",
    "func arg": "(obj, export_dir, signatures, options, meta_graph_def)",
    "comments": "Creates a MetaGraph under a SaveContext.\n\n\n"
}{}{
    "source file": "saveable_object_util.py",
    "line number": "504",
    "func name": "is_factory_for_restored_saveable_object",
    "func arg": "(factory)",
    "comments": ""
}{}{
    "source file": "saved_model_aot_compile.py",
    "line number": "441",
    "func name": "_signature_to_tf2xla_config",
    "func arg": "(signature_def, variable_nodes_to_feed)",
    "comments": "Convert `signature_def` to tf2xla config.  Returns a `tf2xla.Config` proto.\n\n\n##### Args\n* **signature_def**: Instance of `SignatureDef`.\n\n* **variable_nodes_to_feed**: List of tuples of form `(node_def, modified)`\n  corresponding to VarHandleOp, and a boolean `modified` that describes\n  whether the variable was modified during execution.\n\n##### Returns\n"
}{
    "source file": "saved_model_benchmark_util.py",
    "line number": "30",
    "func name": "save_and_load_benchmark",
    "func arg": "(app)",
    "comments": "Util for saved model benchmarks.\n\n\n"
}{
    "source file": "saved_model_cli_test.py",
    "line number": "51",
    "func name": "captured_output",
    "func arg": "()",
    "comments": ""
}{
    "source file": "saved_model_cli.py",
    "line number": "1179",
    "func name": "main",
    "func arg": "()",
    "comments": ""
}{
    "source file": "saved_model_error.py",
    "line number": "73",
    "func name": "main",
    "func arg": "(argv)",
    "comments": "test driver method writes the error message to stdout.\n\n\n"
}{
    "source file": "saved_model_experimental_test.py",
    "line number": "266",
    "func name": "load_model",
    "func arg": "(sess, path, mode)",
    "comments": ""
}{
    "source file": "saved_model_experimental.py",
    "line number": "379",
    "func name": "load_from_saved_model",
    "func arg": "(saved_model_path, custom_objects)",
    "comments": "Loads a keras Model from a SavedModel created by `export_saved_model()`.\n\nThis function reinstantiates model state by: 1) loading model topology from json (this will eventually come from metagraph). 2) loading model weights from checkpoint.\n\nExample:\n\n```python import tensorflow as tf\n\n# Create a tf.keras model. model = tf.keras.Sequential() model.add(tf.keras.layers.Dense(1, input_shape=[10])) model.summary()\n\n# Save the tf.keras model in the SavedModel format. path = '/tmp/simple_keras_model' tf.keras.experimental.export_saved_model(model, path)\n\n# Load the saved keras model back. new_model = tf.keras.experimental.load_from_saved_model(path) new_model.summary() ```\n##### Args\n* **saved_model_path**: a string specifying the path to an existing SavedModel.\n\n* **custom_objects**: Optional dictionary mapping names\n    (strings) to custom classes or functions to be\n    considered during deserialization.\n\n##### Returns\n"
}{}{}{
    "source file": "saved_model_test_base.py",
    "line number": "108",
    "func name": "load_and_run_with_saved_model_api",
    "func arg": "(distribution, saved_dir, predict_dataset, output_name)",
    "comments": "Loads a saved_model using tf.saved_model API, and runs it.\n\n\n"
}{}{}{
    "source file": "saved_model_test2.py",
    "line number": "70",
    "func name": "_run_signature",
    "func arg": "(session, meta_graph_def, inputs, signature_key)",
    "comments": ""
}{}{}{
    "source file": "saved_model_test5.py",
    "line number": "55",
    "func name": "tearDownModule",
    "func arg": "()",
    "comments": ""
}{
    "source file": "saved_model_utils_test.py",
    "line number": "33",
    "func name": "tearDownModule",
    "func arg": "()",
    "comments": ""
}{
    "source file": "saved_model_utils.py",
    "line number": "96",
    "func name": "get_meta_graph_def",
    "func arg": "(saved_model_dir, tag_set)",
    "comments": "Gets MetaGraphDef from SavedModel.\n\n\n##### Args\n* **saved_model_dir**: Directory containing the SavedModel to inspect.\n\n* **tag_set**: Group of tag(s) of the MetaGraphDef to load, in string format,\n    separated by ','. The empty string tag is ignored so that passing ''\n    means the empty tag set. For tag-set contains multiple tags, all tags\n    must be passed in.\n\n##### Returns\n"
}{}{}{}{}{}{}{
    "source file": "saver.py",
    "line number": "1637",
    "func name": "saver_from_object_based_checkpoint",
    "func arg": "(checkpoint_path, var_list, builder, names_to_keys, cached_saver)",
    "comments": "Return a `Saver` which reads from an object-based checkpoint.\n\nThis function validates that all variables in the variables list are remapped in the object-based checkpoint (or `names_to_keys` dict if provided). A saver will be created with the list of remapped variables.\n\nThe `cached_saver` argument allows the user to pass in a previously created saver, so multiple `saver.restore()` calls don't pollute the graph when graph building. This assumes that keys are consistent, meaning that the 1) `checkpoint_path` checkpoint, and 2) checkpoint used to create the `cached_saver` are the same type of object-based checkpoint. If this argument is set, this function will simply validate that all variables have been remapped by the checkpoint at `checkpoint_path`.\n\nNote that in general, `tf.train.Checkpoint` should be used to restore/save an object-based checkpoint.\n##### Args\n* **checkpoint_path**: string, path to object-based checkpoint\n\n* **var_list**: list of `Variables` that appear in the checkpoint. If `None`,\n  `var_list` will be set to all saveable objects.\n\n* **builder**: a `BaseSaverBuilder` instance. If `None`, a new `BulkSaverBuilder`\n  will be created.\n\n* **names_to_keys**: dict mapping string tensor names to checkpoint keys. If\n  `None`, this dict will be generated from the checkpoint file.\n\n* **cached_saver**: Cached `Saver` object with remapped variables.\n\n##### Returns\n"
}{
    "source file": "saving_utils_test.py",
    "line number": "250",
    "func name": "_import_and_infer",
    "func arg": "(save_dir, inputs)",
    "comments": "Import a SavedModel into a TF 1.x-style graph and run `signature_key`.\n\n\n"
}{
    "source file": "saving_utils.py",
    "line number": "313",
    "func name": "try_build_compiled_arguments",
    "func arg": "(model)",
    "comments": ""
}{
    "source file": "saving.py",
    "line number": "117",
    "func name": "independent_buffers",
    "func arg": "(parallel_device)",
    "comments": "Context manager which saves parallel buffers independently.\n\nCreates a ParallelDevice-aware variable subclass which saves buffers for each device separately.\n##### Args\n* **parallel_device**: A ParallelDevice object on which variables are placed.\n\n* **elds**: \n\n"
}{}{}{}{
    "source file": "scan_ops_test.py",
    "line number": "43",
    "func name": "handle_options",
    "func arg": "(func, x, axis, exclusive, reverse)",
    "comments": "Adds tf options to numpy scan ops.\n\n\n"
}{
    "source file": "scan_ops_test1.py",
    "line number": "44",
    "func name": "handle_options",
    "func arg": "(func, x, axis, exclusive, reverse)",
    "comments": "Adds tf options to numpy scan ops.\n\n\n"
}{
    "source file": "scan_ops.py",
    "line number": "158",
    "func name": "scan",
    "func arg": "(initial_state, scan_func)",
    "comments": "A transformation that scans a function across an input dataset.\n\nThis transformation is a stateful relative of `tf.data.Dataset.map`. In addition to mapping `scan_func` across the elements of the input dataset, `scan()` accumulates one or more state tensors, whose initial values are `initial_state`.\n##### Args\n* **initial_state**: A nested structure of tensors, representing the initial state\n  of the accumulator.\n\n* **scan_func**: A function that maps `(old_state, input_element)` to\n  `(new_state, output_element)`. It must take two arguments and return a\n  pair of nested structures of tensors. The `new_state` must match the\n  structure of `initial_state`.\n\n##### Returns\n"
}{}{
    "source file": "scatter_nd_op_test.py",
    "line number": "66",
    "func name": "_NumpyUpdate",
    "func arg": "(indices, updates, shape)",
    "comments": ""
}{
    "source file": "scatter_nd_ops_test.py",
    "line number": "103",
    "func name": "_NumpyMax",
    "func arg": "(ref, indices, updates)",
    "comments": ""
}{
    "source file": "scatter_nd.py",
    "line number": "28",
    "func name": "make_scatter_nd_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do scatter_nd.\n\n\n"
}{
    "source file": "scatter_ops_test.py",
    "line number": "102",
    "func name": "_NumpyUpdateScalar",
    "func arg": "(ref, indices, update)",
    "comments": ""
}{
    "source file": "scikit_learn_test.py",
    "line number": "86",
    "func name": "assert_regression_works",
    "func arg": "(reg)",
    "comments": ""
}{}{}{
    "source file": "script_ops.py",
    "line number": "566",
    "func name": "numpy_function",
    "func arg": "(func, inp, Tout, name)",
    "comments": "Wraps a python function and uses it as a TensorFlow op.\n\nGiven a python function `func` wrap this function as an operation in a TensorFlow function. `func` must take numpy arrays as its arguments and return numpy arrays as its outputs.\n\nThe following example creates a TensorFlow graph with `np.sinh()` as an operation in the graph:\n\n>>> def my_numpy_func(x): ...\n\n # x will be a numpy array with the contents of the input to the ...\n\n # tf.function ...\n\n return np.sinh(x) >>> @tf.function(input_signature=[tf.TensorSpec(None, tf.float32)]) ... def tf_function(input): ...\n\n y = tf.numpy_function(my_numpy_func, [input], tf.float32) ...\n\n return y * y >>> tf_function(tf.constant(1.)) <tf.Tensor: shape=(), dtype=float32, numpy=1.3810978>\n\nComparison to `tf.py_function`: `tf.py_function` and `tf.numpy_function` are very similar, except that `tf.numpy_function` takes numpy arrays, and not `tf.Tensor`s. If you want the function to contain `tf.Tensors`, and have any TensorFlow operations executed in the function be differentiable, please use `tf.py_function`.\n\nNote: The `tf.numpy_function` operation has the following known limitations:\n\n* The body of the function (i.e. `func`) will not be serialized in a `tf.SavedModel`. Therefore, you should not use this function if you need to serialize your model and restore it in a different environment.\n\n* The operation must run in the same address space as the Python program that calls `tf.numpy_function()`. If you are using distributed TensorFlow, you must run a `tf.distribute.Server` in the same process as the program that calls `tf.numpy_function`\n\nyou must pin the created operation to a device in that server (e.g. using `with tf.device():`).\n\n* Since the function takes numpy arrays, you cannot take gradients through a numpy_function. If you require something that is differentiable, please consider using tf.py_function.\n\n* The resulting function is assumed stateful and will never be optimized.\n##### Args\n* **func**: A Python function, which accepts `numpy.ndarray` objects as arguments\n  and returns a list of `numpy.ndarray` objects (or a single\n  `numpy.ndarray`). This function must accept as many arguments as there are\n  tensors in `inp`, and these argument types will match the corresponding\n  `tf.Tensor` objects in `inp`. The returns `numpy.ndarray`s must match the\n  number and types defined `Tout`.\n  Important Note\n\n* **inp**: A list of `tf.Tensor` objects.\n\n* **Tout**: A list or tuple of tensorflow data types or a single tensorflow data\n  type if there is only one, indicating what `func` returns.\n\n* **name**: (Optional) A name for the operation.\n\n##### Returns\n"
}{}{}{
    "source file": "segment_id_ops.py",
    "line number": "80",
    "func name": "segment_ids_to_row_splits",
    "func arg": "(segment_ids, num_segments, out_type, name)",
    "comments": "Generates the RaggedTensor `row_splits` corresponding to a segmentation.\n\n\n##### Args\n* **segment_ids**: A 1-D integer Tensor.\n\n* **num_segments**: A scalar integer indicating the number of segments.  Defaults\n  to `max(segment_ids) + 1` (or zero if `segment_ids` is empty).\n\n* **out_type**: The dtype for the return value.  Defaults to `segment_ids.dtype`,\n  or `tf.int64` if `segment_ids` does not have a dtype.\n\n* **name**: A name prefix for the returned tensor (optional).\n\n##### Returns\n"
}{}{}{
    "source file": "selective_registration_header_lib.py",
    "line number": "194",
    "func name": "get_header",
    "func arg": "(graphs, proto_fileformat, default_ops)",
    "comments": "Computes a header for use with tensorflow SELECTIVE_REGISTRATION.\n\n\n##### Args\n* **graphs**: a list of paths to GraphDef files to include.\n\n* **proto_fileformat**: optional format of proto file, either 'textproto',\n  'rawproto' (default) or ops_list. The ops_list is the file contain the\n  list of ops in JSON format, Ex\n\n* **default_ops**: optional comma-separated string of operator\n\n##### Returns\n"
}{}{
    "source file": "self_adjoint_eig_op_test1.py",
    "line number": "184",
    "func name": "_GetSelfAdjointEigGradTest",
    "func arg": "(dtype_, shape_, compute_v_)",
    "comments": ""
}{
    "source file": "self_check.py",
    "line number": "34",
    "func name": "preload_check",
    "func arg": "()",
    "comments": "Raises an exception if the environment is not correctly configured.\n\n\n"
}{}{}{
    "source file": "sequence_feature_column_integration_test.py",
    "line number": "200",
    "func name": "_make_sequence_example",
    "func arg": "()",
    "comments": ""
}{
    "source file": "sequence_feature_column_integration_test1.py",
    "line number": "253",
    "func name": "_make_sequence_example",
    "func arg": "()",
    "comments": ""
}{
    "source file": "sequence_feature_column_test.py",
    "line number": "143",
    "func name": "_get_sparse_tensors",
    "func arg": "(column, features)",
    "comments": ""
}{
    "source file": "sequence_feature_column_test1.py",
    "line number": "42",
    "func name": "_initialized_session",
    "func arg": "(config)",
    "comments": ""
}{
    "source file": "sequence_feature_column.py",
    "line number": "375",
    "func name": "_assert_all_equal_and_return",
    "func arg": "(tensors, name)",
    "comments": "Asserts that all tensors are equal and returns the first one.\n\n\n"
}{
    "source file": "sequence_feature_column1.py",
    "line number": "165",
    "func name": "_assert_all_equal_and_return",
    "func arg": "(tensors, name)",
    "comments": "Asserts that all tensors are equal and returns the first one.\n\n\n"
}{}{
    "source file": "sequence.py",
    "line number": "93",
    "func name": "pad_sequences",
    "func arg": "(sequences, maxlen, dtype, padding, truncating, value)",
    "comments": "Pads sequences to the same length.\n\nThis function transforms a list (of length `num_samples`) of sequences (lists of integers) into a 2D Numpy array of shape `(num_samples, num_timesteps)`. `num_timesteps` is either the `maxlen` argument if provided, or the length of the longest sequence in the list.\n\nSequences that are shorter than `num_timesteps` are padded with `value` until they are `num_timesteps` long.\n\nSequences longer than `num_timesteps` are truncated so that they fit the desired length.\n\nThe position where padding or truncation happens is determined by the arguments `padding` and `truncating`, respectively. Pre-padding or removing values from the beginning of the sequence is the default.\n\n>>> sequence = [[1], [2, 3], [4, 5, 6]] >>> tf.keras.preprocessing.sequence.pad_sequences(sequence) array([[0, 0, 1], [0, 2, 3], [4, 5, 6]], dtype=int32)\n\n>>> tf.keras.preprocessing.sequence.pad_sequences(sequence, value=-1) array([[-1, -1,\n\n1], [-1,\n\n2,\n\n3], [ 4,\n\n5,\n\n6]], dtype=int32)\n\n>>> tf.keras.preprocessing.sequence.pad_sequences(sequence, padding='post') array([[1, 0, 0], [2, 3, 0], [4, 5, 6]], dtype=int32)\n\n>>> tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=2) array([[0, 1], [2, 3], [5, 6]], dtype=int32)\n\nArguments: sequences: List of sequences (each sequence is a list of integers). maxlen: Optional Int, maximum length of all sequences. If not provided, sequences will be padded to the length of the longest individual sequence. dtype: (Optional, defaults to int32). Type of the output sequences. To pad sequences with variable length strings, you can use `object`. padding: String, 'pre' or 'post' (optional, defaults to 'pre'): pad either before or after each sequence. truncating: String, 'pre' or 'post' (optional, defaults to 'pre'): remove values from sequences larger than `maxlen`, either at the beginning or at the end of the sequences. value: Float or String, padding value. (Optional, defaults to 0.)\n##### Returns\n"
}{}{
    "source file": "sequential.py",
    "line number": "547",
    "func name": "track_nodes_created_by_last_call",
    "func arg": "(layer, created_nodes)",
    "comments": "Adds to `created_nodes` the nodes created by the last call to `layer`.\n\n\n"
}{}{}{}{}{}{
    "source file": "serialization.py",
    "line number": "333",
    "func name": "_get_registered_object",
    "func arg": "(name, custom_objects, module_objects)",
    "comments": ""
}{
    "source file": "serialization1.py",
    "line number": "159",
    "func name": "deserialize",
    "func arg": "(config, custom_objects)",
    "comments": "Instantiates a layer from a config dictionary.\n\nArguments: config: dict of the form {'class_name': str, 'config': dict} custom_objects: dict mapping class names (or function names) of custom (non-Keras) objects to class/functions\n##### Returns\n"
}{
    "source file": "serialization2.py",
    "line number": "29",
    "func name": "get_json_type",
    "func arg": "(obj)",
    "comments": "Serializes any object to a JSON-serializable structure.\n\nArguments: obj: the object to serialize\n##### Returns\n"
}{}{}{}{}{}{}{}{}{}{
    "source file": "server_lib1.py",
    "line number": "31",
    "func name": "_make_server_def",
    "func arg": "(server_or_cluster_def, job_name, task_index, protocol, config)",
    "comments": "Creates a `tf.train.ServerDef` protocol buffer.\n\n\n##### Args\n* **server_or_cluster_def**: A `tf.train.ServerDef` or `tf.train.ClusterDef`\n  protocol buffer, or a `tf.train.ClusterSpec` object, describing the server\n  to be defined and/or the cluster of which it is a member.\n\n* **job_name**: (Optional.) Specifies the name of the job of which the server is a\n  member. Defaults to the value in `server_or_cluster_def`, if specified.\n\n* **task_index**: (Optional.) Specifies the task index of the server in its job.\n  Defaults to the value in `server_or_cluster_def`, if specified. Otherwise\n  defaults to 0 if the server's job has only one task.\n\n* **protocol**: (Optional.) Specifies the protocol to be used by the server.\n  Acceptable values include `\"grpc\", \"grpc+verbs\"`. Defaults to the value in\n  `server_or_cluster_def`, if specified. Otherwise defaults to `\"grpc\"`.\n\n* **config**: (Options.) A `tf.compat.v1.ConfigProto` that specifies default\n  configuration options for all sessions that run on this server.\n\n##### Returns\n"
}{}{}{}{}{}{
    "source file": "session_debug_testlib.py",
    "line number": "58",
    "func name": "no_rewrite_session_config",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "session_manager.py",
    "line number": "515",
    "func name": "_ready",
    "func arg": "(op, sess, msg)",
    "comments": "Checks if the model is ready or not, as determined by op.\n\n\n##### Args\n* **op**: An op, either _ready_op or _ready_for_local_init_op, which defines the\n  readiness of the model.\n\n* **sess**: A `Session`.\n\n* **msg**: A message to log to warning if not ready\n\n##### Returns\n"
}{}{
    "source file": "session_ops.py",
    "line number": "291",
    "func name": "_get_handle_deleter",
    "func arg": "(graph, deleter_key, handle)",
    "comments": "Return a deletion subgraph for this handle.\n\n\n"
}{}{}{
    "source file": "session_support.py",
    "line number": "301",
    "func name": "stop_worker_watchdog",
    "func arg": "()",
    "comments": "Stop global worker watchdog.\n\n\n"
}{}{
    "source file": "session.py",
    "line number": "573",
    "func name": "_name_list",
    "func arg": "(tensor_list)",
    "comments": "Utility function for transitioning to the new session API.\n\n\n##### Args\n* **tensor_list**: a list of `Tensor`s.\n\n##### Returns\n"
}{}{
    "source file": "sets_impl.py",
    "line number": "294",
    "func name": "set_union",
    "func arg": "(a, b, validate_indices)",
    "comments": "Compute set union of elements in last dimension of `a` and `b`.\n\nAll but the last dimension of `a` and `b` must match.\n\nExample:\n\n```python import tensorflow as tf import collections\n\n# [[{1, 2}, {3}], [{4}, {5, 6}]] a = collections.OrderedDict([ ((0, 0, 0), 1), ((0, 0, 1), 2), ((0, 1, 0), 3), ((1, 0, 0), 4), ((1, 1, 0), 5), ((1, 1, 1), 6), ]) a = tf.sparse.SparseTensor(list(a.keys()), list(a.values()), dense_shape=[2, 2, 2])\n\n# [[{1, 3}, {2}], [{4, 5}, {5, 6, 7, 8}]] b = collections.OrderedDict([ ((0, 0, 0), 1), ((0, 0, 1), 3), ((0, 1, 0), 2), ((1, 0, 0), 4), ((1, 0, 1), 5), ((1, 1, 0), 5), ((1, 1, 1), 6), ((1, 1, 2), 7), ((1, 1, 3), 8), ]) b = tf.sparse.SparseTensor(list(b.keys()), list(b.values()), dense_shape=[2, 2, 4])\n\n# `set_union` is applied to each aligned pair of sets. tf.sets.union(a, b)\n\n# The result will be a equivalent to either of: # # np.array([[{1, 2, 3}, {2, 3}], [{4, 5}, {5, 6, 7, 8}]]) # # collections.OrderedDict([ #\n\n\n\n ((0, 0, 0), 1), #\n\n\n\n ((0, 0, 1), 2), #\n\n\n\n ((0, 0, 2), 3), #\n\n\n\n ((0, 1, 0), 2), #\n\n\n\n ((0, 1, 1), 3), #\n\n\n\n ((1, 0, 0), 4), #\n\n\n\n ((1, 0, 1), 5), #\n\n\n\n ((1, 1, 0), 5), #\n\n\n\n ((1, 1, 1), 6), #\n\n\n\n ((1, 1, 2), 7), #\n\n\n\n ((1, 1, 3), 8), # ]) ```\n##### Args\n* **a**: `Tensor` or `SparseTensor` of the same type as `b`. If sparse, indices\n    must be sorted in row-major order.\n\n* **b**: `Tensor` or `SparseTensor` of the same type as `a`. If sparse, indices\n    must be sorted in row-major order.\n\n* **validate_indices**: Whether to validate the order and range of sparse indices\n   in `a` and `b`.\n\n##### Returns\n"
}{
    "source file": "sets_test.py",
    "line number": "50",
    "func name": "_dense_to_sparse",
    "func arg": "(dense, dtype)",
    "comments": ""
}{}{}{
    "source file": "setup.py",
    "line number": "137",
    "func name": "get_pybind_include",
    "func arg": "()",
    "comments": "pybind11 include directory is not correctly resolved.\n\nThis fixes include directory to /usr/local/pythonX.X\n##### Returns\n"
}{}{}{
    "source file": "setup3.py",
    "line number": "216",
    "func name": "find_files",
    "func arg": "(pattern, root)",
    "comments": "Return all the files matching pattern below root dir.\n\n\n"
}{
    "source file": "shape_ops_test.py",
    "line number": "39",
    "func name": "_sparsify",
    "func arg": "(x, thresh, index_dtype)",
    "comments": ""
}{}{
    "source file": "shape_ops.py",
    "line number": "60",
    "func name": "frame",
    "func arg": "(signal, frame_length, frame_step, pad_end, pad_value, axis, name)",
    "comments": "Expands `signal`'s `axis` dimension into frames of `frame_length`.\n\nSlides a window of size `frame_length` over `signal`'s `axis` dimension with a stride of `frame_step`, replacing the `axis` dimension with `[frames, frame_length]` frames.\n\nIf `pad_end` is True, window positions that are past the end of the `axis` dimension are padded with `pad_value` until the window moves fully past the end of the dimension. Otherwise, only window positions that fully overlap the `axis` dimension are produced.\n\nFor example:\n\n>>> # A batch size 3 tensor of 9152 audio samples. >>> audio = tf.random.normal([3, 9152]) >>> >>> # Compute overlapping frames of length 512 with a step of 180 (frames overlap >>> # by 332 samples). By default, only 49 frames are generated since a frame >>> # with start position j*180 for j > 48 would overhang the end. >>> frames = tf.signal.frame(audio, 512, 180) >>> frames.shape.assert_is_compatible_with([3, 49, 512]) >>> >>> # When pad_end is enabled, the final two frames are kept (padded with zeros). >>> frames = tf.signal.frame(audio, 512, 180, pad_end=True) >>> frames.shape.assert_is_compatible_with([3, 51, 512])\n\nIf the dimension along `axis` is N, and `pad_end=False`, the number of frames can be computed by: ```python num_frames = 1 + (N\n\n- frame_size) // frame_step ``` If `pad_end=True`, the number of frames can be computed by: ```python num_frames = -(-N // frame_step) # ceiling division ```\n##### Args\n* **signal**: A `[..., samples, ...]` `Tensor`. The rank and dimensions\n  may be unknown. Rank must be at least 1.\n\n* **frame_length**: The frame length in samples. An integer or scalar `Tensor`.\n\n* **frame_step**: The frame hop size in samples. An integer or scalar `Tensor`.\n\n* **pad_end**: Whether to pad the end of `signal` with `pad_value`.\n\n* **pad_value**: An optional scalar `Tensor` to use where the input signal\n  does not exist when `pad_end` is True.\n\n* **axis**: A scalar integer `Tensor` indicating the axis to frame. Defaults to\n  the last axis. Supports negative values for indexing from the end.\n\n* **name**: An optional name for the operation.\n\n##### Returns\n"
}{
    "source file": "shape.py",
    "line number": "28",
    "func name": "make_shape_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do shape.\n\n\n"
}{}{}{}{
    "source file": "sharded_variable_test.py",
    "line number": "42",
    "func name": "_load_and_run",
    "func arg": "(model_dir, inputs, signature_key)",
    "comments": "Load a SavedModel into a TF 1.x-style graph and run `signature_key`.\n\n\n"
}{}{}{
    "source file": "shared_variable_creator.py",
    "line number": "38",
    "func name": "make_fn",
    "func arg": "(shared_variable_store, device_id)",
    "comments": "Construct the variable creator function for device `device_id`.\n\nConstructs custom variable creator functions for the given device. On first device (device_id == 0), it creates the variable using the `next_creator`, and stores it in the provided `shared_variable_store`. On all other devices (device_id > 0), it tries to re-use the variable already created with the same name. If no such variable exists, it throws an error. Additionally, we de-uniquify variable names before checking for matches. This helps re-use variables which are intended to be the same but have different names due to variable uniquification happening upstream. Since this might mean we may have multiple variables with the same canonical name, we store them in a list per canonical name and return them in the same order as well.\n##### Args\n* **shared_variable_store**: A dictionary that we will use to store variables\n  created on the first device, and re-used by creators for other devices.\n\n* **device_id**: Integer index of the device whose creator should be\n  constructed.\n\n##### Returns\n"
}{
    "source file": "shared_variable_v1.py",
    "line number": "41",
    "func name": "Test",
    "func arg": "()",
    "comments": ""
}{}{}{}{}{
    "source file": "shuffle_ops.py",
    "line number": "60",
    "func name": "shuffle_and_repeat",
    "func arg": "(buffer_size, count, seed)",
    "comments": "Shuffles and repeats a Dataset, reshuffling with each repetition.\n\n>>> d = tf.data.Dataset.from_tensor_slices([1, 2, 3]) >>> d = d.apply(tf.data.experimental.shuffle_and_repeat(2, count=2)) >>> [elem.numpy() for elem in d] # doctest: +SKIP [2, 3, 1, 1, 3, 2]\n\n```python dataset.apply( tf.data.experimental.shuffle_and_repeat(buffer_size, count, seed)) ```\n\nproduces the same output as\n\n```python dataset.shuffle( buffer_size, seed=seed, reshuffle_each_iteration=True).repeat(count) ```\n\nIn each repetition, this dataset fills a buffer with `buffer_size` elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, set the buffer size equal to the full size of the dataset.\n\nFor instance, if your dataset contains 10,000 elements but `buffer_size` is set to 1,000, then `shuffle` will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.\n##### Args\n* **buffer_size**: A `tf.int64` scalar `tf.Tensor`, representing the maximum\n  number elements that will be buffered when prefetching.\n\n* **count**: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the number\n  of times the dataset should be repeated. The default behavior (if `count`\n  is `None` or `-1`) is for the dataset be repeated indefinitely.\n\n* **seed**: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n  seed that will be used to create the distribution. See\n  `tf.random.set_seed` for behavior.\n\n##### Returns\n"
}{}{
    "source file": "sigmoid.py",
    "line number": "27",
    "func name": "make_sigmoid_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do sigmoid.\n\n\n"
}{}{
    "source file": "signature_def_utils_impl.py",
    "line number": "368",
    "func name": "load_op_from_signature_def",
    "func arg": "(signature_def, key, import_scope)",
    "comments": "Load an Op from a SignatureDef created by op_signature_def().\n\n\n##### Args\n* **signature_def**: a SignatureDef proto\n\n* **key**: string key to op in the SignatureDef outputs.\n\n* **import_scope**: Scope used to import the op\n\n##### Returns\n"
}{}{
    "source file": "signature_def_utils_test1.py",
    "line number": "48",
    "func name": "_make_signature",
    "func arg": "(inputs, outputs, name)",
    "comments": ""
}{
    "source file": "signature_def_utils.py",
    "line number": "78",
    "func name": "clear_signature_defs",
    "func arg": "(tflite_model)",
    "comments": "Clears SignatureDefs from the Metadata of a TfLite flatbuffer buffer.\n\n\n##### Args\n* **tflite_model**: TFLite model buffer to remove signature_defs.\n\n##### Returns\n* **buffer**: A TFLite model binary identical to model buffer with\n  no SignatureDef metadata.\n\n"
}{}{
    "source file": "signature_serialization.py",
    "line number": "265",
    "func name": "validate_saveable_view",
    "func arg": "(saveable_view)",
    "comments": "Performs signature-related sanity checks on `saveable_view`.\n\n\n"
}{
    "source file": "simple_console_for_windows.py",
    "line number": "26",
    "func name": "main",
    "func arg": "(_)",
    "comments": "Run an interactive console.\n\n\n"
}{
    "source file": "simple_console.py",
    "line number": "26",
    "func name": "main",
    "func arg": "(_)",
    "comments": "Run an interactive console.\n\n\n"
}{
    "source file": "simple_models.py",
    "line number": "35",
    "func name": "_get_data_for_simple_models",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "simple_save.py",
    "line number": "35",
    "func name": "simple_save",
    "func arg": "(session, export_dir, inputs, outputs, legacy_init_op)",
    "comments": "Convenience function to build a SavedModel suitable for serving.\n\nIn many common cases, saving models for serving will be as simple as:\n\nsimple_save(session, export_dir, inputs={\"x\": x, \"y\": y}, outputs={\"z\": z})\n\nAlthough in many cases it's not necessary to understand all of the many ways to configure a SavedModel, this method has a few practical implications:\n\n- It will be treated as a graph for inference / serving (i.e. uses the tag `saved_model.SERVING`)\n\n- The SavedModel will load in TensorFlow Serving and supports the [Predict API](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/predict.proto). To use the Classify, Regress, or MultiInference APIs, please use either [tf.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) or the lower level [SavedModel APIs](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md).\n\n- Some TensorFlow ops depend on information on disk or other information called \"assets\". These are generally handled automatically by adding the assets to the `GraphKeys.ASSET_FILEPATHS` collection. Only assets in that collection are exported; if you need more custom behavior, you'll need to use the [SavedModelBuilder](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/builder.py).\n\nMore information about SavedModel and signatures can be found here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md.\n##### Args\n* **session**: The TensorFlow session from which to save the meta graph and\n    variables.\n\n* **export_dir**: The path to which the SavedModel will be stored.\n\n* **inputs**: dict mapping string input names to tensors. These are added\n    to the SignatureDef as the inputs.\n\n* **outputs**: dict mapping string output names to tensors. These are added\n    to the SignatureDef as the outputs.\n\n* **legacy_init_op**: Legacy support for op or group of ops to execute after the\n    restore op upon a load.\n\n"
}{}{
    "source file": "single_loss_example.py",
    "line number": "82",
    "func name": "batchnorm_example",
    "func arg": "(optimizer_fn, batch_per_epoch, momentum, renorm, update_ops_in_replica_mode)",
    "comments": "Example of non-distribution-aware legacy code with batch normalization.\n\n\n"
}{}{}{
    "source file": "sleep.py",
    "line number": "37",
    "func name": "sleep",
    "func arg": "(sleep_microseconds)",
    "comments": "Sleeps for `sleep_microseconds` before producing each input element.\n\n\n##### Args\n* **sleep_microseconds**: The number of microseconds to sleep before producing an\n  input element.\n\n##### Returns\n"
}{}{}{
    "source file": "slice.py",
    "line number": "29",
    "func name": "make_slice_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do slice.\n\n\n"
}{}{}{
    "source file": "slices.py",
    "line number": "84",
    "func name": "transform",
    "func arg": "(node, ctx)",
    "comments": ""
}{
    "source file": "slices1.py",
    "line number": "143",
    "func name": "_py_set_item",
    "func arg": "(target, i, x)",
    "comments": "Overload of set_item that executes a Python list modification.\n\n\n"
}{}{
    "source file": "slot_creator.py",
    "line number": "177",
    "func name": "create_zeros_slot",
    "func arg": "(primary, name, dtype, colocate_with_primary)",
    "comments": "Create a slot initialized to 0 with same shape as the primary object.\n\n\n##### Args\n* **primary**: The primary `Variable` or `Tensor`.\n\n* **name**: Name to use for the slot variable.\n\n* **dtype**: Type of the slot variable.  Defaults to the type of `primary`.\n\n* **colocate_with_primary**: Boolean.  If True the slot is located\n  on the same device as `primary`.\n\n##### Returns\n"
}{}{
    "source file": "slurm_cluster_resolver.py",
    "line number": "159",
    "func name": "get_num_gpus",
    "func arg": "()",
    "comments": "Returns the number of GPUs visible on the current node.\n\nCurrently only implemented for NVIDIA GPUs.\n"
}{
    "source file": "smart_cond_test.py",
    "line number": "32",
    "func name": "raise_exception",
    "func arg": "()",
    "comments": ""
}{
    "source file": "smart_cond.py",
    "line number": "93",
    "func name": "smart_case",
    "func arg": "(pred_fn_pairs, default, exclusive, name)",
    "comments": "Like tf.case, except attempts to statically evaluate predicates.\n\nIf any predicate in `pred_fn_pairs` is a bool or has a constant value, the associated callable will be called or omitted depending on its value. Otherwise this functions like tf.case.\n##### Args\n* **pred_fn_pairs**: Dict or list of pairs of a boolean scalar tensor and a\n               callable which returns a list of tensors.\n\n* **default**: Optional callable that returns a list of tensors.\n\n* **exclusive**: True iff at most one predicate is allowed to evaluate to `True`.\n\n* **name**: A name for this operation (optional).\n\n##### Returns\n"
}{}{}{}{
    "source file": "snapshot.py",
    "line number": "258",
    "func name": "snapshot",
    "func arg": "(path, compression, reader_func, shard_func)",
    "comments": "API to persist the output of the input dataset.\n\nThe snapshot API allows users to transparently persist the output of their preprocessing pipeline to disk, and materialize the pre-processed data on a different training run.\n\nThis API enables repeated preprocessing steps to be consolidated, and allows re-use of already processed data, trading off disk storage and network bandwidth for freeing up more valuable CPU resources and accelerator compute time.\n\nhttps://github.com/tensorflow/community/blob/master/rfcs/20200107-tf-data-snapshot.md has detailed design documentation of this feature.\n\nUsers can specify various options to control the behavior of snapshot, including how snapshots are read from and written to by passing in user-defined functions to the `reader_func` and `shard_func` parameters.\n\n`shard_func` is a user specified function that maps input elements to snapshot shards.\n\nUsers may want to specify this function to control how snapshot files should be written to disk. Below is an example of how a potential shard_func could be written.\n\n```python dataset = ... dataset = dataset.enumerate() dataset = dataset.apply(tf.data.experimental.snapshot(\"/path/to/snapshot/dir\", shard_func=lambda x, y: x % NUM_SHARDS, ...)) dataset = dataset.map(lambda x, y: y) ```\n\n`reader_func` is a user specified function that accepts a single argument: (1) a Dataset of Datasets, each representing a \"split\" of elements of the original dataset. The cardinality of the input dataset matches the number of the shards specified in the `shard_func` (see above). The function should return a Dataset of elements of the original dataset.\n\nUsers may want specify this function to control how snapshot files should be read from disk, including the amount of shuffling and parallelism.\n\nHere is an example of a standard reader function a user can define. This function enables both dataset shuffling and parallel reading of datasets:\n\n```python def user_reader_func(datasets): # shuffle the datasets splits datasets = datasets.shuffle(NUM_CORES) # read datasets in parallel and interleave their elements return datasets.interleave(lambda x: x, num_parallel_calls=AUTOTUNE)\n\ndataset = dataset.apply(tf.data.experimental.snapshot(\"/path/to/snapshot/dir\", reader_func=user_reader_func)) ```\n\nBy default, snapshot parallelizes reads by the number of cores available on the system, but will not attempt to shuffle the data.\n##### Args\n* **path**: Required. A directory to use for storing / loading the snapshot to /\n  from.\n\n* **compression**: Optional. The type of compression to apply to the snapshot\n  written to disk. Supported options are `GZIP`, `SNAPPY`, `AUTO` or None.\n  Defaults to AUTO, which attempts to pick an appropriate compression\n  algorithm for the dataset.\n\n* **reader_func**: Optional. A function to control how to read data from snapshot\n  shards.\n\n* **shard_func**: Optional. A function to control how to shard data when writing a\n  snapshot.\n\n##### Returns\n"
}{}{}{
    "source file": "softmax.py",
    "line number": "27",
    "func name": "make_softmax_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do softmax.\n\n\n"
}{}{}{
    "source file": "solve_registrations.py",
    "line number": "180",
    "func name": "_solve_linear_operator_circulant_circulant",
    "func arg": "(linop_a, linop_b)",
    "comments": ""
}{}{}{
    "source file": "sort_ops.py",
    "line number": "200",
    "func name": "_ascending_sort",
    "func arg": "(values, axis, return_argsort)",
    "comments": ""
}{
    "source file": "source_remote_test.py",
    "line number": "42",
    "func name": "line_number_above",
    "func arg": "()",
    "comments": ""
}{
    "source file": "source_remote.py",
    "line number": "200",
    "func name": "send_eager_tracebacks",
    "func arg": "(destinations, origin_stack, send_source)",
    "comments": "Send the tracebacks of an eager execution call to debug server(s).\n\n\n##### Args\n* **destinations**: gRPC destination addresses, a `str` or a `list` of `str`s,\n  e.g., \"localhost\n\n* **origin_stack**: The traceback of the eager operation invocation.\n\n* **send_source**: Whether the source files involved in the op tracebacks but\n  outside the TensorFlow library are to be sent.\n\n"
}{
    "source file": "source_utils_test.py",
    "line number": "74",
    "func name": "_find_preceding_ast_node",
    "func arg": "(node, lineno)",
    "comments": "Find the ast node immediately before and not including lineno.\n\n\n"
}{
    "source file": "source_utils.py",
    "line number": "324",
    "func name": "annotate_source_against_profile",
    "func arg": "(profile_data, source_file_path, node_name_filter, op_type_filter, min_line, max_line)",
    "comments": "Annotate a Python source file with profiling information at each line.\n\n(The annotation doesn't change the source file itself.)\n##### Args\n* **profile_data**: (`list` of `ProfileDatum`) A list of `ProfileDatum`.\n\n* **source_file_path**: (`str`) Path to the source file being annotated.\n\n* **node_name_filter**: Regular expression to filter by node name.\n\n* **op_type_filter**: Regular expression to filter by op type.\n\n* **min_line**: (`None` or `int`) The 1-based line to start annotate the source\n  file from (inclusive).\n\n* **max_line**: (`None` or `int`) The 1-based line number to end the annotation\n  at (exclusive).\n\n##### Returns\n"
}{
    "source file": "space_to_batch_nd.py",
    "line number": "28",
    "func name": "make_space_to_batch_nd_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do space_to_batch_nd.\n\n\n"
}{
    "source file": "space_to_depth.py",
    "line number": "27",
    "func name": "make_space_to_depth_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do space_to_depth.\n\n\n"
}{
    "source file": "spacetobatch_op_test.py",
    "line number": "30",
    "func name": "space_to_batch_direct",
    "func arg": "(input_array, block_shape, paddings)",
    "comments": "Direct Python implementation of space-to-batch conversion.\n\nThis is used for tests only.\n##### Args\n* **input_array**: N-D array\n\n* **block_shape**: 1-D array of shape [num_block_dims].\n\n* **paddings**: 2-D array of shape [num_block_dims, 2].\n\n##### Returns\n"
}{
    "source file": "spacetobatch_op_test1.py",
    "line number": "34",
    "func name": "space_to_batch_direct",
    "func arg": "(input_array, block_shape, paddings)",
    "comments": "Direct Python implementation of space-to-batch conversion.\n\nThis is used for tests only.\n##### Args\n* **input_array**: N-D array\n\n* **block_shape**: 1-D array of shape [num_block_dims].\n\n* **paddings**: 2-D array of shape [num_block_dims, 2].\n\n##### Returns\n"
}{}{
    "source file": "sparse_add_op_test.py",
    "line number": "217",
    "func name": "_s2d_add_vs_sparse_add",
    "func arg": "(sparsity, n, m, num_iters)",
    "comments": ""
}{}{
    "source file": "sparse_conditional_accumulator_test.py",
    "line number": "35",
    "func name": "_indexedslice",
    "func arg": "(x, noshape)",
    "comments": ""
}{}{
    "source file": "sparse_csr_matrix_grad.py",
    "line number": "226",
    "func name": "_SparseMatrixMulGrad",
    "func arg": "(op, grad)",
    "comments": "Gradient for sparse_matrix_mul op.\n\n\n"
}{
    "source file": "sparse_csr_matrix_ops.py",
    "line number": "147",
    "func name": "matmul",
    "func arg": "(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, name)",
    "comments": "Perform a sparse matrix matmul between `a` and `b`.\n\nPerforms a contraction between `a` and `b` along the two innermost dimensions. If both `a` and `b` are instances of `SparseMatrix`, returns a new instance of `SparseMatrix` (same type as `a`).\n\nIf one is not an instance of `SparseMatrix`, returns a dense `Tensor`:\n\n``` c = opA(a) . opB(b) ``` where `opA` (resp. `opB`) is the transpose or hermitian transpose depending on the values of `transpose_a` (resp. `transpose_b`) and `adjoint_a` (resp. `adjoint_b`).\n##### Args\n* **a**: `Tensor` or `SparseMatrix`, having rank `2` or `3`.\n\n* **b**: `Tensor` or `SparseMatrix`, having rank `2` or `3`.\n\n* **transpose_a**: Python `bool`.\n\n* **transpose_b**: Python `bool`.\n\n* **adjoint_a**: Python `bool`.\n\n* **adjoint_b**: Python `bool`.\n\n* **name**: Optional name to use when creating ops.\n\n##### Returns\n"
}{
    "source file": "sparse_grad.py",
    "line number": "316",
    "func name": "_SparseToDenseGrad",
    "func arg": "(op, grad)",
    "comments": ""
}{
    "source file": "sparse_matmul_op_test.py",
    "line number": "31",
    "func name": "RandMatrix",
    "func arg": "(rows, cols, tr, round_bfloat)",
    "comments": ""
}{
    "source file": "sparse_ops_test.py",
    "line number": "41",
    "func name": "_sparsify",
    "func arg": "(x, thresh, index_dtype)",
    "comments": ""
}{}{
    "source file": "sparse_ops.py",
    "line number": "2847",
    "func name": "_take_many_sparse_from_tensors_map",
    "func arg": "(sparse_map_op, sparse_handles, rank, name)",
    "comments": "Read `SparseTensors` from a `SparseTensorsMap` and concatenate them.\n\nThe input `sparse_handles` must be a string matrix of shape `[N, 1]` where `N` is the minibatch size and the rows correspond to packed outputs of `add_sparse_to_tensors_map`.\n\nThe ranks of the original `SparseTensor` objects must all match.\n\nWhen the final `SparseTensor` is created, it has rank one higher than the ranks of the incoming `SparseTensor` objects (they have been concatenated along a new row dimension).\n\nThe output `SparseTensor` object's shape values for all dimensions but the first are the max across the input `SparseTensor` objects' shape values for the corresponding dimensions.\n\nIts first shape value is `N`, the minibatch size.\n\nThe input `SparseTensor` objects' indices are assumed ordered in standard lexicographic order.\n\nIf this is not the case, after this step run `sparse.reorder` to restore index ordering.\n\nFor example, if the serialized input is a `[2, 3]` matrix representing two original `SparseTensor` objects:\n\nindex = [ 0] [10] [20] values = [1, 2, 3] shape = [50]\n\nand\n\nindex = [ 2] [10] values = [4, 5] shape = [30]\n\nthen the final deserialized `SparseTensor` will be:\n\nindex = [0\n\n0] [0 10] [0 20] [1\n\n2] [1 10] values = [1, 2, 3, 4, 5] shape = [2 50]\n##### Args\n* **sparse_map_op**: The `Operation` that created the original handles.\n  Usually this is, e.g., `add_sparse_to_tensors_map(...).op`.\n\n* **sparse_handles**: 2-D `Tensor` of type `string` of shape `[N, 1]`.\n  The serialized and packed `SparseTensor` objects.\n\n* **rank**: (optional) Python int, the rank of the `SparseTensor` objects.\n\n* **name**: A name prefix for the returned tensors (optional)\n\n##### Returns\n"
}{}{}{}{}{}{}{
    "source file": "sparse_tensor_dense_matmul_op_test.py",
    "line number": "389",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{}{
    "source file": "sparse_tensor.py",
    "line number": "443",
    "func name": "is_sparse",
    "func arg": "(x)",
    "comments": "Check whether `x` is sparse.\n\nCheck whether an object is a `tf.sparse.SparseTensor` or `tf.compat.v1.SparseTensorValue`.\n##### Args\n* **x**: A python object to check.\n\n##### Returns\n"
}{}{}{}{
    "source file": "sparse_to_dense_op_test.py",
    "line number": "31",
    "func name": "_SparseToDense",
    "func arg": "(sparse_indices, output_size, sparse_values, default_value, validate_indices)",
    "comments": ""
}{
    "source file": "sparse_to_dense.py",
    "line number": "29",
    "func name": "make_sparse_to_dense_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do sparse to dense.\n\n\n"
}{
    "source file": "sparse_xent_op_test.py",
    "line number": "356",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "sparse.py",
    "line number": "137",
    "func name": "serialize_sparse_tensors",
    "func arg": "(tensors)",
    "comments": "Serializes sparse tensors.\n\n\n##### Args\n* **tensors**: a tensor structure to serialize.\n\n##### Returns\n"
}{}{}{}{
    "source file": "special_functions.py",
    "line number": "92",
    "func name": "stack",
    "func arg": "(list_or_tensor, element_dtype, strict)",
    "comments": "Stacks the input, if it admits the notion of stacking.\n\nFor example, a list of tensors can be stacked into a larger tensor. This function is similar to tf.stack, but it accepts non-lists and lists of non-tensors as arguments. In the latter case, the function does nothing.\n##### Args\n* **list_or_tensor**: Any\n\n* **element_dtype**: tf.DType, optional dtypedtype for the elements in the list.\n    Required if the input is stackable, and the list is untyped.\n\n* **strict**: bool, if True an error is raised if the input is not stackable.\n    Otherwise the function is a no-op.\n\n##### Returns\n"
}{}{
    "source file": "special_math_ops.py",
    "line number": "1159",
    "func name": "_einsum_v2_parse_and_resolve_equation",
    "func arg": "(equation, input_shapes)",
    "comments": "Helper which validates einsum equation and resolves input shapes.\n\n\n"
}{
    "source file": "special_math_test.py",
    "line number": "65",
    "func name": "_log1p",
    "func arg": "(x)",
    "comments": ""
}{
    "source file": "special_math_test1.py",
    "line number": "70",
    "func name": "_value_and_gradient",
    "func arg": "(fn)",
    "comments": "Calls `fn` and computes the gradient of the result wrt `arg`.\n\n\n"
}{
    "source file": "special_math.py",
    "line number": "429",
    "func name": "log_cdf_laplace",
    "func arg": "(x, name)",
    "comments": "Log Laplace distribution function.\n\nThis function calculates `Log[L(x)]`, where `L(x)` is the cumulative distribution function of the Laplace distribution, i.e.\n\n```L(x) := 0.5 * int_{-infty}^x e^{-|t|} dt```\n\nFor numerical accuracy, `L(x)` is computed in different ways depending on `x`,\n\n``` x <= 0: Log[L(x)] = Log[0.5] + x, which is exact\n\n0 < x: Log[L(x)] = Log[1\n\n- 0.5 * e^{-x}], which is exact ```\n##### Args\n* **x**: `Tensor` of type `float32`, `float64`.\n\n* **name**: Python string. A name for the operation (default=\"log_ndtr\").\n\n##### Returns\n"
}{}{
    "source file": "spectral_ops.py",
    "line number": "375",
    "func name": "inverse_mdct",
    "func arg": "(mdcts, window_fn, norm, name)",
    "comments": "Computes the inverse modified DCT of `mdcts`.\n\nTo reconstruct an original waveform, the same window function should be used with `mdct` and `inverse_mdct`.\n\nExample usage:\n\n>>> @tf.function ... def compare_round_trip(): ...\n\n samples = 1000 ...\n\n frame_length = 400 ...\n\n halflen = frame_length // 2 ...\n\n waveform = tf.random.normal(dtype=tf.float32, shape=[samples]) ...\n\n waveform_pad = tf.pad(waveform, [[halflen, 0],]) ...\n\n mdct = tf.signal.mdct(waveform_pad, frame_length, pad_end=True, ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n window_fn=tf.signal.vorbis_window) ...\n\n inverse_mdct = tf.signal.inverse_mdct(mdct, ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n window_fn=tf.signal.vorbis_window) ...\n\n inverse_mdct = inverse_mdct[halflen: halflen + samples] ...\n\n return waveform, inverse_mdct >>> waveform, inverse_mdct = compare_round_trip() >>> np.allclose(waveform.numpy(), inverse_mdct.numpy(), rtol=1e-3, atol=1e-4) True\n\nImplemented with TPU/GPU-compatible ops and supports gradients.\n##### Args\n* **mdcts**: A `float32`/`float64` `[..., frames, frame_length // 2]`\n  `Tensor` of MDCT bins representing a batch of `frame_length // 2`-point\n  MDCTs.\n\n* **window_fn**: A callable that takes a frame_length and a `dtype` keyword\n  argument and returns a `[frame_length]` `Tensor` of samples in the\n  provided datatype. If set to `None`, a rectangular window with a scale of\n  1/sqrt(2) is used. For perfect reconstruction of a signal from `mdct`\n  followed by `inverse_mdct`, please use `tf.signal.vorbis_window`,\n  `tf.signal.kaiser_bessel_derived_window` or `None`. If using another\n  window function, make sure that w[n]^2 + w[n + frame_length // 2]^2 = 1\n  and w[n] = w[frame_length - n - 1] for n = 0,...,frame_length // 2 - 1 to\n  achieve perfect reconstruction.\n\n* **norm**: If \"ortho\", orthonormal inverse DCT4 is performed, if it is None,\n  a regular dct4 followed by scaling of `1/frame_length` is performed.\n\n* **name**: An optional name for the operation.\n\n##### Returns\n"
}{
    "source file": "split_benchmark.py",
    "line number": "34",
    "func name": "build_graph",
    "func arg": "(device, input_shape, output_sizes, axis)",
    "comments": "Build a graph containing a sequence of split operations.\n\n\n##### Args\n* **device**: string, the device to run on.\n\n* **input_shape**: shape of the input tensor.\n\n* **output_sizes**: size of each output along axis.\n\n* **axis**: axis to be split along.\n\n##### Returns\n"
}{}{
    "source file": "split.py",
    "line number": "28",
    "func name": "make_split_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do tf.split.\n\n\n"
}{
    "source file": "splitv.py",
    "line number": "28",
    "func name": "make_splitv_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do tf.split_v.\n\n\n"
}{}{}{}{
    "source file": "squeeze_transpose.py",
    "line number": "27",
    "func name": "make_squeeze_transpose_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do squeeze followed by transpose.\n\n\n"
}{
    "source file": "squeeze.py",
    "line number": "27",
    "func name": "make_squeeze_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do squeeze.\n\n\n"
}{
    "source file": "stack_op_test.py",
    "line number": "34",
    "func name": "np_split_squeeze",
    "func arg": "(array, axis)",
    "comments": ""
}{}{}{
    "source file": "stack_trace_example.py",
    "line number": "83",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{}{}{}{}{
    "source file": "state_ops.py",
    "line number": "821",
    "func name": "batch_scatter_update",
    "func arg": "(ref, indices, updates, use_locking, name)",
    "comments": "Generalization of `tf.compat.v1.scatter_update` to axis different than 0.\n\nAnalogous to `batch_gather`. This assumes that `ref`, `indices` and `updates` have a series of leading dimensions that are the same for all of them, and the updates are performed on the last dimension of indices. In other words, the dimensions should be the following:\n\n`num_prefix_dims = indices.ndims\n\n- 1` `batch_dim = num_prefix_dims + 1` `updates.shape = indices.shape + var.shape[batch_dim:]`\n\nwhere\n\n`updates.shape[:num_prefix_dims]` `== indices.shape[:num_prefix_dims]` `== var.shape[:num_prefix_dims]`\n\nAnd the operation performed can be expressed as:\n\n`var[i_1, ..., i_n, indices[i_1, ..., i_n, j]] = updates[i_1, ..., i_n, j]`\n\nWhen indices is a 1D tensor, this operation is equivalent to `tf.compat.v1.scatter_update`.\n\nTo avoid this operation there would be 2 alternatives: 1) Reshaping the variable by merging the first `ndims` dimensions. However, this is not possible because `tf.reshape` returns a Tensor, which we cannot use `tf.compat.v1.scatter_update` on. 2) Looping over the first `ndims` of the variable and using `tf.compat.v1.scatter_update` on the subtensors that result of slicing the first dimension. This is a valid option for `ndims = 1`, but less efficient than this implementation.\n\nSee also `tf.compat.v1.scatter_update` and `tf.compat.v1.scatter_nd_update`.\n##### Args\n* **ref**: `Variable` to scatter onto.\n\n* **indices**: Tensor containing indices as described above.\n\n* **updates**: Tensor of updates to apply to `ref`.\n\n* **use_locking**: Boolean indicating whether to lock the writing operation.\n\n* **name**: Optional scope name string.\n\n##### Returns\n"
}{
    "source file": "stateful_random_ops_test.py",
    "line number": "55",
    "func name": "xla_device_name",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "stateful_random_ops.py",
    "line number": "935",
    "func name": "set_global_generator",
    "func arg": "(generator)",
    "comments": "Replaces the global generator with another `Generator` object.\n\nThis function creates a new Generator object (and the Variable object within), which does not work well with tf.function because (1) tf.function puts restrictions on Variable creation thus reset_global_generator can't be freely used inside tf.function; (2) redirecting a global variable to a new object is problematic with tf.function because the old object may be captured by a 'tf.function'ed function and still be used by it. A 'tf.function'ed function only keeps weak references to variables, so deleting a variable and then calling that function again may raise an error, as demonstrated by random_test.py/RandomTest.testResetGlobalGeneratorBadWithDefun .\n##### Args\n* **generator**: the new `Generator` object.\n\n"
}{
    "source file": "stateless_random_ops_tes1.py",
    "line number": "38",
    "func name": "invert_philox",
    "func arg": "(key, value)",
    "comments": "Invert the Philox bijection.\n\n\n"
}{}{
    "source file": "stateless_random_ops.py",
    "line number": "625",
    "func name": "stateless_parameterized_truncated_normal",
    "func arg": "(shape, seed, means, stddevs, minvals, maxvals, name)",
    "comments": "Outputs random values from a truncated normal distribution.\n\nThe generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.\n\n Examples:\n\nSample from a Truncated normal, with deferring shape parameters that broadcast.\n\n>>> means = 0. >>> stddevs = tf.math.exp(tf.random.uniform(shape=[2, 3])) >>> minvals = [-1., -2., -1000.] >>> maxvals = [[10000.], [1.]] >>> y = tf.random.stateless_parameterized_truncated_normal( ...\n\n shape=[10, 2, 3], seed=[7, 17], ...\n\n means=means, stddevs=stddevs, minvals=minvals, maxvals=maxvals) >>> y.shape TensorShape([10, 2, 3])\n##### Args\n* **shape**: A 1-D integer `Tensor` or Python array. The shape of the output\n  tensor.\n\n* **seed**: A shape [2] Tensor, the seed to the random number generator. Must have\n  dtype `int32` or `int64`. (When using XLA, only `int32` is allowed.)\n\n* **means**: A `Tensor` or Python value of type `dtype`. The mean of the truncated\n  normal distribution. This must broadcast with `stddevs`, `minvals` and\n  `maxvals`, and the broadcasted shape must be dominated by `shape`.\n\n* **stddevs**: A `Tensor` or Python value of type `dtype`. The standard deviation\n  of the truncated normal distribution. This must broadcast with `means`,\n  `minvals` and `maxvals`, and the broadcasted shape must be dominated by\n  `shape`.\n\n* **minvals**: A `Tensor` or Python value of type `dtype`. The minimum value of\n  the truncated normal distribution. This must broadcast with `means`,\n  `stddevs` and `maxvals`, and the broadcasted shape must be dominated by\n  `shape`.\n\n* **maxvals**: A `Tensor` or Python value of type `dtype`. The maximum value of\n  the truncated normal distribution. This must broadcast with `means`,\n  `stddevs` and `minvals`, and the broadcasted shape must be dominated by\n  `shape`.\n\n* **name**: A name for the operation (optional).\n\n##### Returns\n"
}{}{}{}{
    "source file": "stats_dataset_test_base.py",
    "line number": "329",
    "func name": "_events_from_logdir",
    "func arg": "(logdir)",
    "comments": "Returns all events in the single eventfile in logdir.\n\n\n##### Args\n* **logdir**: The directory in which the single event file is sought.\n\n##### Returns\n"
}{}{
    "source file": "stats_ops.py",
    "line number": "75",
    "func name": "latency_stats",
    "func arg": "(tag)",
    "comments": "Records the latency of producing each element of the input dataset.\n\nTo consume the statistics, associate a `StatsAggregator` with the output dataset.\n##### Args\n* **tag**: String. All statistics recorded by the returned transformation will\n  be associated with the given `tag`.\n\n##### Returns\n"
}{}{
    "source file": "status_bar.py",
    "line number": "23",
    "func name": "SetupStatusBarInsideGoogle",
    "func arg": "(unused_link_text, unused_port)",
    "comments": ""
}{}{}{}{
    "source file": "strategy_combinations.py",
    "line number": "354",
    "func name": "all_strategy_combinations_minus_default",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "strategy_test_lib.py",
    "line number": "775",
    "func name": "_all_mean",
    "func arg": "(value)",
    "comments": ""
}{
    "source file": "strided_slice_np_style.py",
    "line number": "29",
    "func name": "make_strided_slice_np_style_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to test strided_slice in np style.\n\n\n"
}{
    "source file": "strided_slice.py",
    "line number": "208",
    "func name": "make_strided_slice_1d_exhaustive_tests",
    "func arg": "(options)",
    "comments": "Make a set of exhaustive tests for 1D strided_slice.\n\n\n"
}{}{}{}{}{
    "source file": "string_lookup_test.py",
    "line number": "51",
    "func name": "_get_end_to_end_test_cases",
    "func arg": "()",
    "comments": ""
}{}{}{}{}{
    "source file": "string_ops.py",
    "line number": "544",
    "func name": "string_join",
    "func arg": "(inputs, separator, name)",
    "comments": "Perform element-wise concatenation of a list of string tensors.\n\nGiven a list of string tensors of same shape, performs element-wise concatenation of the strings of the same index in all tensors.\n\n >>> tf.strings.join(['abc','def']).numpy() b'abcdef' >>> tf.strings.join([['abc','123'], ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n['def','456'], ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n['ghi','789']]).numpy() array([b'abcdefghi', b'123456789'], dtype=object) >>> tf.strings.join([['abc','123'], ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n['def','456']], ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nseparator=\" \").numpy() array([b'abc def', b'123 456'], dtype=object)\n##### Args\n* **inputs**: A list of `tf.Tensor` objects of same size and `tf.string` dtype.\n\n* **separator**: A string added between each string being joined.\n\n* **name**: A name for the operation (optional).\n\n##### Returns\n"
}{}{}{}{}{}{}{
    "source file": "strip_strings.py",
    "line number": "34",
    "func name": "main",
    "func arg": "(_)",
    "comments": "Application run loop.\n\n\n"
}{
    "source file": "strip_unused_lib.py",
    "line number": "92",
    "func name": "strip_unused_from_files",
    "func arg": "(input_graph, input_binary, output_graph, output_binary, input_node_names, output_node_names, placeholder_type_enum)",
    "comments": "Removes unused nodes from a graph file.\n\n\n"
}{}{
    "source file": "strip_unused.py",
    "line number": "54",
    "func name": "main",
    "func arg": "(unused_args)",
    "comments": ""
}{}{
    "source file": "structure.py",
    "line number": "407",
    "func name": "type_spec_from_value",
    "func arg": "(element, use_fallback)",
    "comments": "Creates a type specification for the given value.\n\n\n##### Args\n* **element**: The element to create the type specification for.\n\n* **use_fallback**: Whether to fall back to converting the element to a tensor\n  in order to compute its `TypeSpec`.\n\n##### Returns\n"
}{}{
    "source file": "structured_tensor_slice_test.py",
    "line number": "49",
    "func name": "_make_tensor_slice_spec",
    "func arg": "(slice_spec, use_constant)",
    "comments": "Wraps all integers in an extended slice spec w/ a tensor.\n\nThis function is used to help test slicing when the slice spec contains tensors, rather than integers.\n##### Args\n* **slice_spec**: The extended slice spec.\n\n* **use_constant**: If true, then wrap each integer with a tf.constant.  If false,\n  then wrap each integer with a tf.placeholder.\n\n##### Returns\n"
}{}{}{
    "source file": "structured_tensor.py",
    "line number": "1129",
    "func name": "_merge_dims",
    "func arg": "(value, outer_axis, inner_axis)",
    "comments": "Merges `outer_axis...inner_axis` of `value` into a single dimension.\n\n\n"
}{
    "source file": "student_t_test.py",
    "line number": "37",
    "func name": "try_import",
    "func arg": "(name)",
    "comments": ""
}{}{}{}{
    "source file": "subscribe.py",
    "line number": "313",
    "func name": "subscribe",
    "func arg": "(tensors, side_effects)",
    "comments": "Subscribe to a tensor.\n\nThis method will attach side effect graphs to a given set of tensors. Set of tensors follows from session.run and supports single `Tensor`, `list`, nested `list`, `tuple`, `namedtuple`, or `dict`. It returns the tensors in the same passed in structure, but as clones with side effects applied. The supplied side effect graphs are specified as a constructor function which takes the target tensor and constructs a side effect graph and returns a list of ops that should be control dependencies on fetching the tensor. It will append 'subscription' to the name scope of the tensor for every node in the side effect graph. These control dependencies are what trigger the side effects. Subscribe will construct the additions to your graph and return the created identity tensor downstream of the control dependencies. Use these tensors as you would normally in the rest of your tensorflow code. If a given tensor has already been subscribed or a tensor returned by a call to subscribe is passed, the previously created identity tensor will be reused and the side effect graphs will be added to the existing ones.\n##### Args\n* **tensors**: `Tensor` or set of tensors to subscribe to. Set of tensors format\n  follows from `Session.run` and supports single `Tensor`, `list`, nested\n  `list`, `tuple`, `namedtuple`, or `dict`.\n\n* **side_effects**: Function(s) that takes a `Tensor`, construct a subgraph, and\n  return a nonempty list of control dependencies. This can be a single\n  function or list of functions.\n\n##### Returns\n"
}{}{}{}{
    "source file": "summary_iterator.py",
    "line number": "44",
    "func name": "summary_iterator",
    "func arg": "(path)",
    "comments": "Returns a iterator for reading `Event` protocol buffers from an event file.\n\nYou can use this function to read events written to an event file. It returns a Python iterator that yields `Event` protocol buffers.\n\nExample: Print the contents of an events file.\n\n```python for e in tf.compat.v1.train.summary_iterator(path to events file): print(e) ```\n\nExample: Print selected summary values.\n\n```python # This example supposes that the events file contains summaries with a # summary value tag 'loss'.\n\nThese could have been added by calling # `add_summary()`, passing the output of a scalar summary op created with # with: `tf.compat.v1.summary.scalar('loss', loss_tensor)`. for e in tf.compat.v1.train.summary_iterator(path to events file): for v in e.summary.value: if v.tag == 'loss': print(v.simple_value) ``` Example: Continuously check for new summary values.\n\n```python summaries = tf.compat.v1.train.summary_iterator(path to events file) while True: for e in summaries: for v in e.summary.value: if v.tag == 'loss': print(v.simple_value) # Wait for a bit before checking the file for any new events time.sleep(wait time) ```\n\nSee the protocol buffer definitions of [Event](https://www.tensorflow.org/code/tensorflow/core/util/event.proto) and [Summary](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto) for more information about their attributes.\n##### Args\n* **path**: The path to an event file created by a `SummaryWriter`.\n\n##### Returns\n"
}{
    "source file": "summary_op_util.py",
    "line number": "27",
    "func name": "skip_summary",
    "func arg": "()",
    "comments": "Determines if summary should be skipped.\n\nIf using multiple replicas in distributed strategy, skip summaries on all replicas except the first one (replica_id=0).\n##### Returns\n"
}{
    "source file": "summary_op_util1.py",
    "line number": "72",
    "func name": "summary_scope",
    "func arg": "(name, family, default_name, values)",
    "comments": "Enters a scope used for the summary and yields both the name and tag.\n\nTo ensure that the summary tag name is always unique, we create a name scope based on `name` and use the full scope name in the tag.\n\nIf `family` is set, then the tag name will be '<family>/<scope_name>', where `scope_name` is `<outer_scope>/<family>/<name>`. This ensures that `family` is always the prefix of the tag (and unmodified), while ensuring the scope respects the outer scope from this summary was created.\n##### Args\n* **name**: A name for the generated summary node.\n\n* **family**: Optional; if provided, used as the prefix of the summary tag name.\n\n* **default_name**: Optional; if provided, used as default name of the summary.\n\n* **values**: Optional; passed as `values` parameter to name_scope.\n\n* **elds**: \n\n"
}{
    "source file": "summary_ops_test.py",
    "line number": "128",
    "func name": "events_from_logdir",
    "func arg": "(logdir)",
    "comments": "Returns all events in the single eventfile in logdir.\n\n\n##### Args\n* **logdir**: The directory in which the single event file is sought.\n\n##### Returns\n"
}{
    "source file": "summary_ops_test1.py",
    "line number": "1246",
    "func name": "to_numpy",
    "func arg": "(summary_value)",
    "comments": ""
}{
    "source file": "summary_ops_v2.py",
    "line number": "1359",
    "func name": "trace_off",
    "func arg": "()",
    "comments": "Stops the current trace and discards any collected information.\n\n\n"
}{}{}{}{}{}{
    "source file": "summary.py",
    "line number": "410",
    "func name": "get_summary_description",
    "func arg": "(node_def)",
    "comments": "Given a TensorSummary node_def, retrieve its SummaryDescription.\n\nWhen a Summary op is instantiated, a SummaryDescription of associated metadata is stored in its NodeDef. This method retrieves the description.\n##### Args\n* **node_def**: the node_def_pb2.NodeDef of a TensorSummary op\n\n##### Returns\n"
}{
    "source file": "supervisor_test.py",
    "line number": "56",
    "func name": "_summary_iterator",
    "func arg": "(test_dir)",
    "comments": "Reads events from test_dir/events.\n\n\n##### Args\n* **test_dir**: Name of the test directory.\n\n##### Returns\n"
}{}{}{
    "source file": "svd_op_test1.py",
    "line number": "271",
    "func name": "_GetSvdGradGradOpTest",
    "func arg": "(dtype_, shape_, compute_uv_, full_matrices_)",
    "comments": ""
}{
    "source file": "sync_replicas_optimizer_test.py",
    "line number": "35",
    "func name": "get_workers",
    "func arg": "(num_workers, replicas_to_aggregate, workers)",
    "comments": ""
}{}{}{
    "source file": "sysconfig.py",
    "line number": "91",
    "func name": "get_build_info",
    "func arg": "()",
    "comments": "Get a dictionary describing TensorFlow's build environment.\n\nValues are generated when TensorFlow is compiled, and are static for each TensorFlow package. The return value is a dictionary with string keys such as:\n\n- cuda_version\n\n- cudnn_version\n\n- is_cuda_build\n\n- is_rocm_build\n\n- msvcp_dll_names\n\n- nvcuda_dll_name\n\n- cudart_dll_name\n\n- cudnn_dll_name\n\nNote that the actual keys and values returned by this function is subject to change across different versions of TensorFlow or across platforms.\n##### Returns\n"
}{
    "source file": "system_info_lib.py",
    "line number": "146",
    "func name": "gather_platform_info",
    "func arg": "()",
    "comments": "Gather platform info.\n\n\n"
}{
    "source file": "system_info.py",
    "line number": "25",
    "func name": "main",
    "func arg": "(unused_args)",
    "comments": ""
}{
    "source file": "table_utils_test.py",
    "line number": "34",
    "func name": "get_table",
    "func arg": "(dtype, oov_tokens)",
    "comments": ""
}{
    "source file": "table_utils.py",
    "line number": "190",
    "func name": "convert_to_ndarray",
    "func arg": "(x, dtype)",
    "comments": "Convert 'x' to a numpy array.\n\n\n"
}{}{}{}{
    "source file": "take_while_ops.py",
    "line number": "56",
    "func name": "take_while",
    "func arg": "(predicate)",
    "comments": "A transformation that stops dataset iteration based on a `predicate`.\n\n\n##### Args\n* **predicate**: A function that maps a nested structure of tensors (having shapes\n  and types defined by `self.output_shapes` and `self.output_types`) to a\n  scalar `tf.bool` tensor.\n\n##### Returns\n"
}{}{
    "source file": "tanh.py",
    "line number": "28",
    "func name": "make_tanh_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do tanh.\n\n\n"
}{
    "source file": "tape_test.py",
    "line number": "55",
    "func name": "gradient_is_constant",
    "func arg": "(x)",
    "comments": ""
}{
    "source file": "tape.py",
    "line number": "224",
    "func name": "could_possibly_record",
    "func arg": "()",
    "comments": "Returns True if any tape is active.\n\n\n"
}{}{
    "source file": "template_test.py",
    "line number": "65",
    "func name": "variable_scoped_function_with_local_variable",
    "func arg": "()",
    "comments": ""
}{
    "source file": "template.py",
    "line number": "234",
    "func name": "_skip_common_stack_elements",
    "func arg": "(stacktrace, base_case)",
    "comments": "Skips items that the target stacktrace shares with the base stacktrace.\n\n\n"
}{
    "source file": "templates_test.py",
    "line number": "42",
    "func name": "_parse_with_unset_ctx",
    "func arg": "(expr_source)",
    "comments": ""
}{
    "source file": "templates.py",
    "line number": "279",
    "func name": "replace_as_expression",
    "func arg": "(template, **replacements)",
    "comments": "Variant of replace that generates expressions, instead of code blocks.\n\n\n"
}{
    "source file": "temporal_sample_weights_correctness_test.py",
    "line number": "70",
    "func name": "run_with_different_sample_weight_mode_inputs",
    "func arg": "(fn, partial_sw)",
    "comments": "Executes the given function with different sample weight mode inputs.\n\n\n##### Args\n* **fn**: Training or eval function to execute.\n\n* **partial_sw**: Boolean flag to indicate whether temporal sample weight mode\n  should be set partially just for one output.\n\n"
}{
    "source file": "tensor_array_grad.py",
    "line number": "243",
    "func name": "_TensorArraySplitGrad",
    "func arg": "(op, flow)",
    "comments": "Gradient for TensorArraySplit.\n\n\n##### Args\n* **op**: Forward TensorArraySplit op.\n\n* **flow**: Gradient `Tensor` flow to TensorArraySplit.\n\n##### Returns\n"
}{
    "source file": "tensor_array_ops_test.py",
    "line number": "42",
    "func name": "_make_converter",
    "func arg": "(dtype)",
    "comments": ""
}{
    "source file": "tensor_array_ops_test1.py",
    "line number": "65",
    "func name": "_make_ta",
    "func arg": "(size, name, dtype, infer_shape)",
    "comments": ""
}{}{
    "source file": "tensor_array_ops.py",
    "line number": "1308",
    "func name": "_check_dtypes",
    "func arg": "(value, dtype)",
    "comments": ""
}{
    "source file": "tensor_conversion_registry.py",
    "line number": "114",
    "func name": "get",
    "func arg": "(query)",
    "comments": "Get conversion function for objects of `cls`.\n\n\n##### Args\n* **query**: The type to query for.\n\n##### Returns\n"
}{
    "source file": "tensor_forest_ops.py",
    "line number": "76",
    "func name": "tree_variable",
    "func arg": "(tree_config, name, container)",
    "comments": ""
}{}{
    "source file": "tensor_format.py",
    "line number": "488",
    "func name": "numeric_summary",
    "func arg": "(tensor)",
    "comments": "Get a text summary of a numeric tensor.\n\nThis summary is only available for numeric (int*, float*, complex*) and Boolean tensors.\n##### Args\n* **tensor**: (`numpy.ndarray`) the tensor value object to be summarized.\n\n##### Returns\n"
}{}{}{
    "source file": "tensor_list.py",
    "line number": "26",
    "func name": "dynamic_list_append",
    "func arg": "(target, element)",
    "comments": "Converts a list append call inline.\n\n\n"
}{}{}{}{
    "source file": "tensor_shape.py",
    "line number": "1238",
    "func name": "unknown_shape",
    "func arg": "(rank, **kwargs)",
    "comments": "Returns an unknown TensorShape, optionally with a known rank.\n\n\n##### Args\n* **rank**: (Optional) If specified, the number of dimensions in the shape.\n\n* ****kwargs**: For backwards compatibility.\n\n##### Returns\n"
}{}{}{
    "source file": "tensor_test.py",
    "line number": "44",
    "func name": "_create_tensor",
    "func arg": "(value, device, dtype)",
    "comments": ""
}{}{
    "source file": "tensor_tracer_report.py",
    "line number": "223",
    "func name": "proto_fingerprint",
    "func arg": "(message_proto)",
    "comments": ""
}{
    "source file": "tensor_tracer.py",
    "line number": "368",
    "func name": "_trace_files_need_precreated",
    "func arg": "(output_dir)",
    "comments": "Return True if trace files must be pre-created by users.\n\n\n"
}{}{
    "source file": "tensor_util.py",
    "line number": "1042",
    "func name": "maybe_set_static_shape",
    "func arg": "(tensor, shape)",
    "comments": "Sets the shape of `tensor` to the `shape`'s constant value, if inferrable.\n\nThis is a temporary workaround to fix shape inference across functional op boundaries. E.g.\n\n```python shape = tf.constant([3]) @tf.function def f(): u = tf.random_uniform(shape) return u ```\n\nIf we were to rely solely on C++ shape inference, the shape of `u` inside `f` would be unknown because C++ shape inference is not aware of the outer graph and all it sees is a Placeholder node when backtracing the captured tensor for `shape`. `maybe_set_static_shape` computes the static shape value of `shape` by traversing the `FuncGraph` boundaries and sets the correct shape.\n\nA longer term solution would be to fix C++ shape inference.\n##### Args\n* **tensor**: A tensor.\n\n* **shape**: A shape tensor.\n\n"
}{}{
    "source file": "tensorboard_logging.py",
    "line number": "168",
    "func name": "fatal",
    "func arg": "(message)",
    "comments": ""
}{
    "source file": "tensordot_op_test.py",
    "line number": "143",
    "func name": "_get_tensordot_tests",
    "func arg": "(dtype_, rank_a_, rank_b_, num_dims_, dynamic_shape_)",
    "comments": ""
}{
    "source file": "tensorflow_op_layer_test.py",
    "line number": "220",
    "func name": "_reuse_ancillary_layer",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "tensors.py",
    "line number": "51",
    "func name": "is_range_tensor",
    "func arg": "(t)",
    "comments": "Returns True if a tensor is the result of a tf.range op. Best effort.\n\n\n"
}{}{
    "source file": "test_base.py",
    "line number": "54",
    "func name": "v2_only_combinations",
    "func arg": "()",
    "comments": "Returns the default test combinations for v1 only tf.data tests.\n\n\n"
}{}{
    "source file": "test_combinations.py",
    "line number": "410",
    "func name": "_get_name",
    "func arg": "(value, index)",
    "comments": ""
}{}{}{
    "source file": "test_streaming_accuracy.py",
    "line number": "111",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{}{}{
    "source file": "test_util.py",
    "line number": "50",
    "func name": "_gather",
    "func arg": "(strategy, value)",
    "comments": "Gathers a single value.\n\n\n"
}{
    "source file": "test_util1.py",
    "line number": "3270",
    "func name": "set_producer_version",
    "func arg": "(graph, producer_version)",
    "comments": "Sets graph.graph_def_versions.producer to `producer_version`.\n\n\n"
}{
    "source file": "test_util2.py",
    "line number": "76",
    "func name": "create_identity_with_nan_gradients_fn",
    "func arg": "(have_nan_gradients)",
    "comments": "Returns a function that optionally has NaN gradients.\n\nThis serves as a hook to introduce NaN gradients to a model. This returns an identity function. The identity's gradient function will check if the boolean tensor `have_nan_gradients` is True. If so, the gradient will be NaN. Otherwise, the gradient will also be the identity.\n##### Args\n* **have_nan_gradients**: A scalar boolean tensor. If True, gradients will be NaN.\n  Otherwise, the gradient function is the identity function.\n\n##### Returns\n"
}{
    "source file": "test_util3.py",
    "line number": "72",
    "func name": "evaluate_tflite_model",
    "func arg": "(tflite_model, input_ndarrays)",
    "comments": "Evaluates the provided tf.lite model with the given input ndarrays.\n\n\n##### Args\n* **tflite_model**: bytes. The serialized tf.lite model.\n\n* **input_ndarrays**: A list of NumPy arrays to feed as input to the model.\n\n##### Returns\n"
}{}{
    "source file": "test_utils1.py",
    "line number": "71",
    "func name": "RunWithWarmup",
    "func arg": "(sess, op_to_run, feed_dict, options, run_metadata)",
    "comments": "Runs a graph a few times to ensure that its clusters are compiled.\n\n\n"
}{
    "source file": "test_utils2.py",
    "line number": "218",
    "func name": "build_mock_model",
    "func arg": "()",
    "comments": "Creates an object containing an example model.\n\n\n"
}{
    "source file": "test.py",
    "line number": "27",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{
    "source file": "test1.py",
    "line number": "106",
    "func name": "is_built_with_xla",
    "func arg": "()",
    "comments": "Returns whether TensorFlow was built with XLA support.\n\n\n"
}{
    "source file": "testing_utils.py",
    "line number": "908",
    "func name": "_set_v2_dtype_behavior",
    "func arg": "(fn, enabled)",
    "comments": "Returns version of 'fn' that runs with v2 dtype behavior on or off.\n\n\n"
}{
    "source file": "testing.py",
    "line number": "28",
    "func name": "fake_tf",
    "func arg": "()",
    "comments": "Creates a fake module that looks like TensorFlow, for testing.\n\n\n"
}{
    "source file": "testing1.py",
    "line number": "62",
    "func name": "sleep",
    "func arg": "(sleep_microseconds)",
    "comments": "Sleeps for `sleep_microseconds` before producing each input element.\n\n\n##### Args\n* **sleep_microseconds**: The number of microseconds to sleep before producing an\n  input element.\n\n##### Returns\n"
}{}{
    "source file": "text_dataset.py",
    "line number": "186",
    "func name": "path_to_string_content",
    "func arg": "(path, max_length)",
    "comments": ""
}{}{}{
    "source file": "text_vectorization_distribution_test.py",
    "line number": "37",
    "func name": "get_layer_class",
    "func arg": "()",
    "comments": ""
}{
    "source file": "text_vectorization_test.py",
    "line number": "1432",
    "func name": "custom_split_fn",
    "func arg": "(x)",
    "comments": ""
}{}{}{
    "source file": "text.py",
    "line number": "61",
    "func name": "one_hot",
    "func arg": "(input_text, n, filters, lower, split)",
    "comments": "One-hot encodes a text into a list of word indexes of size `n`.\n\nThis function receives as input a string of text and returns a list of encoded integers each corresponding to a word (or token) in the given input string.\n\nArguments: input_text: Input text (string). n: int. Size of vocabulary. filters: list (or concatenation) of characters to filter out, such as punctuation. Default: ``!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n``, includes basic punctuation, tabs, and newlines. lower: boolean. Whether to set the text to lowercase. split: str. Separator for word splitting.\n##### Returns\n"
}{}{
    "source file": "tf_contextlib_test.py",
    "line number": "41",
    "func name": "test_params_and_defaults",
    "func arg": "(a, b, c, d)",
    "comments": ""
}{
    "source file": "tf_contextlib.py",
    "line number": "25",
    "func name": "contextmanager",
    "func arg": "(target)",
    "comments": "A tf_decorator-aware wrapper for `contextlib.contextmanager`.\n\nUsage is identical to `contextlib.contextmanager`.\n##### Args\n* **target**: A callable to be wrapped in a contextmanager.\n\n##### Returns\n"
}{
    "source file": "tf_decorator_test.py",
    "line number": "195",
    "func name": "test_wrapper",
    "func arg": "(**kwargs)",
    "comments": ""
}{
    "source file": "tf_decorator.py",
    "line number": "200",
    "func name": "unwrap",
    "func arg": "(maybe_tf_decorator)",
    "comments": "Unwraps an object into a list of TFDecorators and a final target.\n\n\n##### Args\n* **maybe_tf_decorator**: Any callable object.\n\n##### Returns\n"
}{}{}{
    "source file": "tf_doctest.py",
    "line number": "219",
    "func name": "setUpModule",
    "func arg": "()",
    "comments": ""
}{
    "source file": "tf_export_test.py",
    "line number": "33",
    "func name": "_test_function2",
    "func arg": "(unused_arg)",
    "comments": ""
}{
    "source file": "tf_export.py",
    "line number": "394",
    "func name": "kwarg_only",
    "func arg": "(f)",
    "comments": "A wrapper that throws away all non-kwarg arguments.\n\n\n"
}{}{
    "source file": "tf_inspect_test.py",
    "line number": "52",
    "func name": "test_decorated_function_with_defaults",
    "func arg": "(a, b, c)",
    "comments": "Test Decorated Function With Defaults Docstring.\n\n\n"
}{
    "source file": "tf_inspect.py",
    "line number": "405",
    "func name": "stack",
    "func arg": "(context)",
    "comments": "TFDecorator-aware replacement for inspect.stack.\n\n\n"
}{
    "source file": "tf_logging.py",
    "line number": "324",
    "func name": "_get_thread_id",
    "func arg": "()",
    "comments": "Get id of current thread, suitable for logging as an unsigned quantity.\n\n\n"
}{}{
    "source file": "tf_optimizer.py",
    "line number": "27",
    "func name": "OptimizeGraph",
    "func arg": "(config_proto, metagraph, verbose, graph_id, cluster, strip_default_attributes)",
    "comments": "Optimize the provided metagraph.\n\nFor best results, the signature_def field in `metagraph` should be populated with information about input (feed) and output (fetch) tensors.\n##### Args\n* **config_proto**: a ConfigProto protobuf.\n\n* **metagraph**: a MetagraphDef protobuf.\n\n* **verbose**: whether to log optimization results.\n\n* **graph_id**: a string identifying this graph.\n\n* **cluster**: a grappler cluster object representing hardware resources\n    available to run this graph.\n\n* **strip_default_attributes**: whether graph node attributes having default\n    values should be removed after all the optimization passes. This\n    option is useful if the resulting graph will be executed by an older\n    process that might not know some of the recently added attributes.\n\n"
}{}{}{}{}{
    "source file": "tf_record.py",
    "line number": "174",
    "func name": "tf_record_random_reader",
    "func arg": "(path)",
    "comments": "Creates a reader that allows random-access reads from a TFRecords file.\n\nThe created reader object has the following method:\n\n- `read(offset)`, which returns a tuple of `(record, ending_offset)`, where `record` is the TFRecord read at the offset, and `ending_offset` is the ending offset of the read record.\n\nThe method throws a `tf.errors.DataLossError` if data is corrupted at the given offset. The method throws `IndexError` if the offset is out of range for the TFRecords file.\n\n Usage example: ```py reader = tf_record_random_reader(file_path)\n\nrecord_1, offset_1 = reader.read(0)\n\n# 0 is the initial offset. # offset_1 is the ending offset of the 1st record and the starting offset of # the next.\n\nrecord_2, offset_2 = reader.read(offset_1) # offset_2 is the ending offset of the 2nd record and the starting offset of # the next. # We can jump back and read the first record again if so desired. reader.read(0) ```\n##### Args\n* **path**: The path to the TFRecords file.\n\n##### Returns\n"
}{
    "source file": "tf_should_use_test.py",
    "line number": "36",
    "func name": "reroute_error",
    "func arg": "()",
    "comments": "Temporarily reroute errors written to tf_logging.error into `captured`.\n\n\n"
}{
    "source file": "tf_should_use.py",
    "line number": "216",
    "func name": "should_use_result",
    "func arg": "(fn, warn_in_eager, error_in_function)",
    "comments": "Function wrapper that ensures the function's output is used.\n\nIf the output is not used, a `logging.error` is logged.\n\nIf `error_in_function` is set, then a `RuntimeError` will be raised at the end of function tracing if the output is not used by that point.\n\nAn output is marked as used if any of its attributes are read, modified, or updated.\n\nExamples when the output is a `Tensor` include:\n\n- Using it in any capacity (e.g. `y = t + 0`, `sess.run(t)`)\n\n- Accessing a property (e.g. getting `t.name` or `t.op`).\n\n- Calling `t.mark_used()`.\n\nNote, certain behaviors cannot be tracked\n\n- for these the object may not be marked as used.\n\nExamples include:\n\n- `t != 0`.\n\nIn this case, comparison is done on types / ids.\n\n- `isinstance(t, tf.Tensor)`.\n\nSimilar to above.\n##### Args\n* **fn**: The function to wrap.\n\n* **warn_in_eager**: Whether to create warnings in Eager as well.\n\n* **error_in_function**: Whether to raise an error when creating a tf.function.\n\n##### Returns\n"
}{
    "source file": "tf_stack_test.py",
    "line number": "61",
    "func name": "convert_stack_frame",
    "func arg": "(frame)",
    "comments": "Converts a TF stack frame into Python's.\n\n\n"
}{
    "source file": "tf_stack.py",
    "line number": "131",
    "func name": "extract_stack",
    "func arg": "(limit)",
    "comments": "A lightweight, extensible re-implementation of traceback.extract_stack.\n\nNOTE(mrry): traceback.extract_stack eagerly retrieves the line of code for each stack frame using linecache, which results in an abundance of stat() calls. This implementation does not retrieve the code, and any consumer should apply _convert_stack to the result to obtain a traceback that can be formatted etc. using traceback methods.\n##### Args\n* **limit**: A limit on the number of frames to return.\n\n##### Returns\n"
}{
    "source file": "tf_trt_integration_test_base.py",
    "line number": "967",
    "func name": "_AddTests",
    "func arg": "(test_class)",
    "comments": "Adds test methods to TfTrtIntegrationTestBase.\n\n\n"
}{}{
    "source file": "tf_upgrade_v2_main.py",
    "line number": "58",
    "func name": "main",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "tf_upgrade_v2_test.py",
    "line number": "60",
    "func name": "get_func_and_args_from_str",
    "func arg": "(call_str)",
    "comments": "Parse call string to get function and argument names.\n\n\n##### Args\n* **call_str**: Call string must be in the form\n\n##### Returns\n"
}{
    "source file": "tf_upgrade_v2.py",
    "line number": "2596",
    "func name": "_string_split_rtype_transformer",
    "func arg": "(parent, node, full_name, name, logs)",
    "comments": "Update tf.strings.split arguments: result_type, source.\n\n\n"
}{}{}{
    "source file": "tf_utils.py",
    "line number": "540",
    "func name": "_astuple",
    "func arg": "(attrs)",
    "comments": "Converts the given attrs to tuple non-recursively.\n\n\n"
}{
    "source file": "tf2_test.py",
    "line number": "34",
    "func name": "unset_environ",
    "func arg": "()",
    "comments": ""
}{
    "source file": "tf2.py",
    "line number": "42",
    "func name": "enabled",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "tfconfig_cluster_resolver.py",
    "line number": "46",
    "func name": "_get_value_in_tfconfig",
    "func arg": "(key, default)",
    "comments": ""
}{}{
    "source file": "tflite_convert.py",
    "line number": "639",
    "func name": "main",
    "func arg": "()",
    "comments": ""
}{}{
    "source file": "tfprof_logger.py",
    "line number": "193",
    "func name": "write_op_log",
    "func arg": "(graph, log_dir, op_log, run_meta, add_trace)",
    "comments": "Log provided 'op_log', and add additional model information below.\n\nThe API also assigns ops in tf.compat.v1.trainable_variables() an op type called '_trainable_variables'. The API also logs 'flops' statistics for ops with op.RegisterStatistics() defined. flops calculation depends on Tensor shapes defined in 'graph', which might not be complete. 'run_meta', if provided, completes the shape information with best effort.\n##### Args\n* **graph**: tf.Graph. If None and eager execution is not enabled, use\n    default graph.\n\n* **log_dir**: directory to write the log file.\n\n* **op_log**: (Optional) OpLogProto proto to be written. If not provided, an new\n    one is created.\n\n* **run_meta**: (Optional) RunMetadata proto that helps flops computation using\n    run time shape information.\n\n* **add_trace**: Whether to add python code trace information.\n    Used to support \"code\" view.\n\n"
}{}{
    "source file": "threadpool.py",
    "line number": "78",
    "func name": "override_threadpool",
    "func arg": "(dataset, thread_pool)",
    "comments": "Returns a new dataset that uses the given thread pool for its operations.\n\n\n##### Args\n* **dataset**: A `tf.data.Dataset` object.\n\n* **thread_pool**: A `PrivateThreadPool` object.\n\n##### Returns\n"
}{
    "source file": "tile.py",
    "line number": "27",
    "func name": "make_tile_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do tile.\n\n\n"
}{}{}{}{
    "source file": "timeseries.py",
    "line number": "202",
    "func name": "sequences_from_indices",
    "func arg": "(array, indices_ds, start_index, end_index)",
    "comments": ""
}{
    "source file": "toco_convert.py",
    "line number": "78",
    "func name": "toco_convert",
    "func arg": "(options, graph_def, input_tensors, output_tensors, **kwargs)",
    "comments": "Convert a model's graph def into a tflite model.\n\nNOTE: this currently shells out to the toco binary, but we would like convert to Python API tooling in the future.\n##### Args\n* **options**: An Options instance.\n\n* **graph_def**: A GraphDef object.\n\n* **input_tensors**: List of input tensor tuples `(name, shape, type)`.\n\n* **output_tensors**: List of output tensors (names).\n\n* ****kwargs**: Extra options to be passed.\n\n##### Returns\n"
}{
    "source file": "toco_from_protos_test.py",
    "line number": "30",
    "func name": "TensorName",
    "func arg": "(x)",
    "comments": "Get the canonical (non foo:0 name).\n\n\n"
}{
    "source file": "toco_from_protos.py",
    "line number": "61",
    "func name": "main",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "topk.py",
    "line number": "28",
    "func name": "make_topk_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do topk.\n\n\n"
}{}{
    "source file": "topology.py",
    "line number": "36",
    "func name": "_tpu_host_device_name",
    "func arg": "(job, task)",
    "comments": "Returns the device name for the CPU device on `task` of `job`.\n\n\n"
}{}{
    "source file": "tpu_cluster_resolver_test.py",
    "line number": "95",
    "func name": "mock_not_running_in_gce_urlopen",
    "func arg": "(cls, **kwargs)",
    "comments": ""
}{}{
    "source file": "tpu_cluster_resolver1.py",
    "line number": "40",
    "func name": "is_running_in_gce",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "tpu_embedding_gradient.py",
    "line number": "129",
    "func name": "get_gradients_through_dummy_table_variables",
    "func arg": "(tpu_embedding)",
    "comments": "Get gradients wrt the activations of each feature.\n\n\n##### Args\n* **tpu_embedding**: TPUEmbedding, create dummy table variable to be used with\n  tpu_embedding.\n\n##### Returns\n"
}{
    "source file": "tpu_embedding_v2_correctness_test.py",
    "line number": "624",
    "func name": "_get_variable",
    "func arg": "(variable)",
    "comments": ""
}{}{
    "source file": "tpu_embedding_v2_test.py",
    "line number": "1227",
    "func name": "_get_variable",
    "func arg": "(variable)",
    "comments": ""
}{}{
    "source file": "tpu_embedding_v2.py",
    "line number": "1355",
    "func name": "make_sharded_variable_creator",
    "func arg": "(hosts)",
    "comments": "Makes a sharded variable creator given a list of hosts.\n\n\n##### Args\n* **hosts**: a list of tensorflow devices on which to shard the tensors.\n\n##### Returns\n"
}{
    "source file": "tpu_embedding.py",
    "line number": "2227",
    "func name": "_create_partitioned_variables",
    "func arg": "(name, num_hosts, vocabulary_size, embedding_dimension, initializer, collections)",
    "comments": "Creates PartitionedVariables based on `num_hosts` for `table`.\n\n\n"
}{}{
    "source file": "tpu_feed.py",
    "line number": "110",
    "func name": "tag_sharding_attribute_for_dequeued_tensors",
    "func arg": "(dequeues, dims)",
    "comments": "Tags appropriate XLA sharding attribute to the dequeued tensors.\n\n\n##### Args\n* **dequeues**: A list of dequeued tensors on TPU.\n\n* **dims**: A list of integer describes how the tensor is partitioned.\n\n##### Returns\n"
}{
    "source file": "tpu_function.py",
    "line number": "64",
    "func name": "on_device_training_loop",
    "func arg": "(func)",
    "comments": ""
}{}{
    "source file": "tpu_ops.py",
    "line number": "450",
    "func name": "enqueue_tpu_embedding_ragged_tensor_batch",
    "func arg": "(sample_splits, embedding_indices, aggregation_weights, table_ids, device_ordinal, max_sequence_lengths, combiners, mode_override, name)",
    "comments": "A placeholder op for enqueueing embedding IDs to the TPU.\n\n\n##### Args\n* **sample_splits**: A list of rank 1 Tensors specifying the break points for\n  splitting embedding_indices and aggregation_weights into rows. It\n  corresponds to ids.row_splits in embedding_lookup(), when ids is a\n  RaggedTensor. Both int32 and int64 are allowed and will be converted to\n  int32 internally.\n\n* **embedding_indices**: A list of rank 1 Tensors, indices into the embedding\n  tables. It corresponds to ids.values in embedding_lookup(), when ids is a\n  RaggedTensor. Both int32 and int64 are allowed and will be converted to\n  int32 internally.\n\n* **aggregation_weights**: A list of rank 1 Tensors containing per training\n  example aggregation weights. It corresponds to the values field of a\n  RaggedTensor with the same row_splits as ids in embedding_lookup(), when\n  ids is a RaggedTensor. Both float32 and float64 are allowed and will be\n  converted to float32 internally.\n\n* **table_ids**: A list of integers specifying the identifier of the embedding\n  table (offset of TableDescriptor in the TPUEmbeddingConfiguration) to\n  lookup the corresponding input. The ith input is looked up using\n  table_ids[i]. The size of the table_ids list must be equal to that of\n  sample_indices, embedding_indices and aggregation_weights.\n\n* **device_ordinal**: The TPU device to use. Should be >= 0 and less than the\n  number of TPU cores in the task on which the node is placed.\n\n* **max_sequence_lengths**: A list of integers, the size of which is equal to\n  sample_indices. If equal to 0, the corresponding feature is considered to\n  be a non-sequence feature, If greater than 0, the corresponding feature is\n  a sequence feature with the given maximal length. If None, then we assume\n  a list of all zeroes.\n\n* **combiners**: A list of string scalars, one for each embedding table that\n  specify how to normalize the embedding activations after weighted\n  summation. Supported combiners are 'mean', 'sum', or 'sqrtn'. It is\n  invalid to have the sum of the weights be 0 for 'mean' or the sum of the\n  squared weights be 0 for 'sqrtn'. If combiners isn't passed, the default\n  is to use 'sum' for all tables (optional).\n\n* **mode_override**: A string input that overrides the mode specified in the\n  TPUEmbeddingConfiguration. Supported values are {'unspecified',\n  'inference', 'training', 'backward_pass_only'}. When set to 'unspecified',\n  the mode set in TPUEmbeddingConfiguration is used, otherwise mode_override\n  is used (optional).\n\n* **name**: A name for the operation (optional).\n\n##### Returns\n"
}{}{}{
    "source file": "tpu_outside_compilation_test.py",
    "line number": "57",
    "func name": "get_tpu_strategy",
    "func arg": "()",
    "comments": ""
}{}{}{
    "source file": "tpu_strategy_test_utils.py",
    "line number": "43",
    "func name": "get_tpu_strategy",
    "func arg": "()",
    "comments": ""
}{
    "source file": "tpu_strategy_test.py",
    "line number": "70",
    "func name": "get_tpu_strategy",
    "func arg": "(enable_packed_var)",
    "comments": ""
}{
    "source file": "tpu_strategy_test1.py",
    "line number": "41",
    "func name": "get_tpu_strategy",
    "func arg": "()",
    "comments": ""
}{
    "source file": "tpu_strategy_util.py",
    "line number": "139",
    "func name": "shutdown_tpu_system",
    "func arg": "(cluster_resolver)",
    "comments": "Shuts down the TPU devices.\n\nThis will clear all caches, even those that are maintained through sequential calls to tf.tpu.experimental.initialize_tpu_system, such as the compilation cache.\n##### Args\n* **cluster_resolver**: A tf.distribute.cluster_resolver.TPUClusterResolver,\n    which provides information about the TPU cluster.\n\n"
}{
    "source file": "tpu_strategy.py",
    "line number": "1228",
    "func name": "_set_last_step_outputs",
    "func arg": "(ctx, last_step_tensor_outputs)",
    "comments": "Sets the last step outputs on the given context.\n\n\n"
}{
    "source file": "tpu_system_metadata.py",
    "line number": "198",
    "func name": "master_job",
    "func arg": "(master, cluster_def)",
    "comments": "Returns the canonical job name to use to place TPU computations on.\n\n\n##### Args\n* **master**: A `string` representing the TensorFlow master to use.\n\n* **cluster_def**: A ClusterDef object describing the TPU cluster.\n\n##### Returns\n"
}{
    "source file": "tpu_test_wrapper_test.py",
    "line number": "202",
    "func name": "_write_and_load_module",
    "func arg": "(source)",
    "comments": ""
}{
    "source file": "tpu_test_wrapper.py",
    "line number": "146",
    "func name": "run_user_main",
    "func arg": "(wrapped_test_module)",
    "comments": "Runs the \"if __name__ == '__main__'\" at the bottom of a module.\n\nTensorFlow practice is to have a main if at the bottom of the module which might call an API compat function before calling test.main().\n\nSince this is a statement, not a function, we can't cleanly reference it, but we can inspect it from the user module and run it in the context of that module so all imports and variables are available to it.\n##### Args\n* **wrapped_test_module**: The user-provided test code to run.\n\n"
}{
    "source file": "tpu_test.py",
    "line number": "158",
    "func name": "find_xla_einsum",
    "func arg": "(g)",
    "comments": ""
}{
    "source file": "tpu_values.py",
    "line number": "183",
    "func name": "enclosing_tpu_context",
    "func arg": "()",
    "comments": "Returns the TPUReplicateContext, which exists inside a tpu.rewrite().\n\n\n"
}{
    "source file": "tpu.py",
    "line number": "2116",
    "func name": "prune_unconnected_ops_from_xla",
    "func arg": "(prune_graph)",
    "comments": "Prunes unconnected ops as listed in _UNCONNECTED_OPS_TO_PRUNE.\n\n\n##### Args\n* **prune_graph**: A tensorflow graph from which we wish to prune unconnected ops\n  as listed in _UNCONNECTED_OPS_TO_PRUNE.  In general, these ops should have\n  no inputs and no consumers. These can often be left behind due to graph\n  construction rewiring (for instance TF-Hub). While they never execute,\n  they will cause XLA compile to fail so we strip them from XLA compile by\n  removing the tpu_replicate attribute.\n\n"
}{}{}{}{}{
    "source file": "traceme.py",
    "line number": "24",
    "func name": "traceme_wrapper",
    "func arg": "(func)",
    "comments": ""
}{}{}{}{}{}{
    "source file": "tracking.py",
    "line number": "360",
    "func name": "cached_per_instance",
    "func arg": "(f)",
    "comments": "Lightweight decorator for caching lazily constructed properties.\n\nWhen to use: This decorator provides simple caching with minimal overhead. It is designed for properties which are expensive to compute and static over the life of a class instance, and provides no mechanism for cache invalidation. Thus it is best suited for lazily exposing derived properties of other static data.\n\nFor classes with custom getattr / setattr behavior (such as trackable objects), storing cache results as object attributes is not performant. Instead, a specialized cache can significantly reduce property lookup overhead. (While still allowing the decorated property to be lazily computed.) Consider the following class:\n\n``` class MyClass(object): def __setattr__(self, key, value): # Some expensive class specific code # ... # ...\n\nsuper(MyClass, self).__setattr__(key, value)\n\n@property def thing(self): # `thing` is expensive to compute (and may not even be requested), so we # want to lazily compute it and then cache it. output = getattr(self, '_thing', None) if output is None: self._thing = output = compute_thing(self) return output ```\n\nIt's also worth noting that ANY overriding of __setattr__, even something as simple as: ``` def __setattr__(self, key, value): super(MyClass, self).__setattr__(key, value) ```\n\nSlows down attribute assignment by nearly 10x.\n\nBy contrast, replacing the definition of `thing` with the following sidesteps the expensive __setattr__ altogether:\n\n''' @property @tracking.cached_per_instance def thing(self): # `thing` is expensive to compute (and may not even be requested), so we # want to lazily compute it and then cache it. return compute_thing(self) '''\n\nPerformance: The overhead for this decorator is ~0.4 us / call. A much lower overhead implementation (~0.085 us / call) can be achieved by using a custom dict type:\n\n``` def dict_based_cache(f): class Cache(dict): __slots__ = () def __missing__(self, key): self[key] = output = f(key) return output\n\nreturn property(Cache().__getitem__) ```\n\nHowever, that implementation holds class instances as keys, and as a result blocks garbage collection. (And modifying it to use weakref's as keys raises the lookup overhead to ~0.4 us) As a result, the WeakKeyDictionary implementation below turns out to be more prudent.\n##### Args\n* **f**: The function to cache.\n\n##### Returns\n"
}{
    "source file": "train_test1.py",
    "line number": "32",
    "func name": "requires_contrib",
    "func arg": "(test_method)",
    "comments": ""
}{}{
    "source file": "train1.py",
    "line number": "88",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "train2.py",
    "line number": "111",
    "func name": "train_net",
    "func arg": "(model, model_path, train_len, train_data, valid_len, valid_data, test_len, test_data, kind)",
    "comments": "Trains the model.\n\n\n"
}{}{
    "source file": "training_arrays.py",
    "line number": "560",
    "func name": "_update_sample_weight_mode",
    "func arg": "(model, mode, inputs)",
    "comments": "Updates the sample_weight_mode of a given model.\n\n\n"
}{}{
    "source file": "training_distributed.py",
    "line number": "763",
    "func name": "_train_with_multi_worker",
    "func arg": "(method)",
    "comments": "Decorator that handles multi worker training with distribution strategy.\n\n\n"
}{}{
    "source file": "training_eager.py",
    "line number": "326",
    "func name": "test_on_batch",
    "func arg": "(model, inputs, targets, sample_weights, output_loss_metrics)",
    "comments": "Calculates the loss for one input batch.\n\nArguments: model: Model whose loss has to be calculated. inputs: Input batch data. targets: Target batch data. sample_weights: Sample weight batch data. output_loss_metrics: List of metrics that are used to aggregated output loss values.\n##### Returns\n* **Dict with three items**: 'total_loss'\n\n"
}{
    "source file": "training_generator_test.py",
    "line number": "67",
    "func name": "custom_generator_changing_batch_size",
    "func arg": "(mode)",
    "comments": ""
}{
    "source file": "training_generator.py",
    "line number": "533",
    "func name": "_get_num_samples_or_steps",
    "func arg": "(data, steps_per_epoch)",
    "comments": "Returns number of samples or steps, and whether to use steps count mode.\n\n\n"
}{}{
    "source file": "training_integration_test.py",
    "line number": "104",
    "func name": "_gather_test_cases",
    "func arg": "()",
    "comments": ""
}{
    "source file": "training_loop.py",
    "line number": "181",
    "func name": "repeat",
    "func arg": "(n, body, inputs, infeed_queue, name)",
    "comments": "Builds a training loop that executes a fixed number of iterations.\n\nThe set of loop-carried tensors correspond to `inputs`. `body` must be a function that takes and returns the values of the loop-carried tensors.\n##### Args\n* **n**: the number of loop iterations\n\n* **body**: a Python function that builds the loop body.\n\n* **inputs**: a list of initial values passed into the training loop or\n  None (equivalent to an empty list).\n\n* **infeed_queue**: if not None, the infeed queue from which to append a tuple\n  of arguments as inputs to condition.\n\n* **name**: (Deprecated) Does nothing.\n\n##### Returns\n"
}{}{}{}{}{}{
    "source file": "training_util.py",
    "line number": "242",
    "func name": "_increment_global_step",
    "func arg": "(increment, graph)",
    "comments": ""
}{
    "source file": "training_utils_test.py",
    "line number": "309",
    "func name": "cause_error",
    "func arg": "(f)",
    "comments": ""
}{
    "source file": "training_utils.py",
    "line number": "2042",
    "func name": "unpack_validation_data",
    "func arg": "(validation_data, raise_if_ambiguous)",
    "comments": "Unpack validation data based input type.\n\nThe validation data is not touched if its dataset or dataset iterator. For other type of input (Numpy or tensor), it will be unpacked into tuple of 3 which is x, y and sample weights.\n##### Args\n* **validation_data**: dataset, dataset iterator, or numpy, tensor tuple.\n\n* **raise_if_ambiguous**: boolean on whether to fail if validation_data cannot be\n  parsed. Otherwise simply return validation_data, None, None and defer the\n  decision to the caller.\n\n##### Returns\n"
}{
    "source file": "training_v1.py",
    "line number": "3203",
    "func name": "_non_none_constant_value",
    "func arg": "(v)",
    "comments": ""
}{}{
    "source file": "training1.py",
    "line number": "2791",
    "func name": "_is_hdf5_filepath",
    "func arg": "(filepath)",
    "comments": ""
}{}{
    "source file": "transform_arduino_source.py",
    "line number": "108",
    "func name": "parse_args",
    "func arg": "()",
    "comments": "Converts the raw arguments into accessible flags.\n\n\n"
}{}{
    "source file": "transform_source.py",
    "line number": "171",
    "func name": "parse_args",
    "func arg": "()",
    "comments": "Converts the raw arguments into accessible flags.\n\n\n"
}{
    "source file": "transformed_distribution.py",
    "line number": "117",
    "func name": "_is_scalar_from_shape",
    "func arg": "(shape)",
    "comments": "Returns `True` `Tensor` if `Tensor` shape implies a scalar.\n\n\n"
}{}{}{}{
    "source file": "transpiler.py",
    "line number": "38",
    "func name": "_wrap_into_factory",
    "func arg": "(nodes, entity_name, inner_factory_name, outer_factory_name, closure_vars, factory_args, future_features)",
    "comments": "Wraps an AST into the body of a factory with consistent lexical context.\n\nThe AST is expected to define some symbol with a name given by `entity_name`.\n\nThis mechanism ensures that the resulting transformed entity has lexical scoping identical to that of the source entity, while allowing extra parametrization.\n\nTwo nested factories achieve the following:\n\n1. The inner factory dynamically creates the entity represented by `nodes`. 2. The inner factory is parametrized by a custom set of arguments. 3. The inner factory has a closure identical to that of the transformed entity. 4. The inner factory has local variables named like `args`, which `nodes` may use as additional parameters. 5. The inner factory returns the variables given by `entity_name`. 6. The outer factory is niladic. 7. The outer factory has no closure. 8. The outer factory creates the necessary lexical scope for the inner factory, so that the loaded code has the given configuration for closure/globals. 9. The outer factory returns the inner factory.\n\nRoughly speaking, the following code is generated:\n\nfrom __future__ import future_feature_1 from __future__ import future_feature_2 ...\n\ndef outer_factory(): closure_var_1 = None closure_var_2 = None ...\n\ndef inner_factory(arg_1, arg_2, ...): <<nodes>> return entity\n\nreturn inner_factory\n\nThe lexical scoping is created using dummy symbol declarations which create local fariables in the body of the outer factory, so that the Python parser correctly marks them as free non-global variables upon load (that is, it creates cell slots for each symbol. Thes symbols are initialized with None, but their values are not expected to be used; instead, the caller is expected to replace them with the cells of the source entity. For more details, see: https://docs.python.org/3/reference/executionmodel.html#binding-of-names\n##### Args\n* **nodes**: Tuple[ast.AST], the source code to wrap.\n\n* **entity_name**: Union[Text, ast.AST], the name of the principal entity that\n  `nodes` define.\n\n* **inner_factory_name**: Text, the name of the inner factory.\n\n* **outer_factory_name**: Text, the name of the outer factory.\n\n* **closure_vars**: Iterable[Text], names of the closure variables for the inner\n  factory.\n\n* **factory_args**: Iterable[Text], names of additional arguments for the\n  inner factory. Useful to configure variables that the converted code can\n  use. Typically, these are modules.\n\n* **future_features**: Iterable[Text], names of future statements to associate the\n  code with.\n\n##### Returns\n"
}{
    "source file": "transpose_benchmark.py",
    "line number": "34",
    "func name": "build_graph",
    "func arg": "(device, input_shape, perm, datatype, num_iters)",
    "comments": "builds a graph containing a sequence of conv2d operations.\n\n\n##### Args\n* **device**: String, the device to run on.\n\n* **input_shape**: Shape of the input tensor.\n\n* **perm**: A list of ints with the same length as input tensor's dimension.\n\n* **datatype**: numpy data type of the input tensor.\n\n* **num_iters**: number of iterations to run transpose.\n\n##### Returns\n"
}{
    "source file": "transpose_conv.py",
    "line number": "33",
    "func name": "make_transpose_conv_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do transpose_conv.\n\n\n"
}{}{
    "source file": "transpose.py",
    "line number": "28",
    "func name": "make_transpose_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do transpose.\n\n\n"
}{}{}{
    "source file": "traverse.py",
    "line number": "25",
    "func name": "obtain_all_variant_tensor_ops",
    "func arg": "(dataset)",
    "comments": "Given an input dataset, finds all dataset ops used for construction.\n\nA series of transformations would have created this dataset with each transformation including zero or more Dataset ops, each producing a dataset variant tensor. This method outputs all of them.\n##### Args\n* **dataset**: Dataset to find variant tensors for.\n\n##### Returns\n"
}{
    "source file": "traverse1.py",
    "line number": "77",
    "func name": "traverse",
    "func arg": "(root, visit)",
    "comments": "Recursively enumerate all members of `root`.\n\nSimilar to the Python library function `os.path.walk`.\n\nTraverses the tree of Python objects starting with `root`, depth first. Parent-child relationships in the tree are defined by membership in modules or classes. The function `visit` is called with arguments `(path, parent, children)` for each module or class `parent` found in the tree of python objects starting with `root`. `path` is a string containing the name with which `parent` is reachable from the current context. For example, if `root` is a local class called `X` which contains a class `Y`, `visit` will be called with `('Y', X.Y, children)`).\n\nIf `root` is not a module or class, `visit` is never called. `traverse` never descends into built-in modules.\n\n`children`, a list of `(name, object)` pairs are determined by `tf_inspect.getmembers`. To avoid visiting parts of the tree, `children` can be modified in place, using `del` or slice assignment.\n\nCycles (determined by reference equality, `is`) stop the traversal. A stack of objects is kept to find cycles. Objects forming cycles may appear in `children`, but `visit` will not be called with any object as `parent` which is already in the stack.\n\nTraversing system modules can take a long time, it is advisable to pass a `visit` callable which blacklists such modules.\n##### Args\n* **root**: A python object with which to start the traversal.\n\n* **visit**: A function taking arguments `(path, parent, children)`. Will be\n  called for each object found in the traversal.\n\n"
}{}{
    "source file": "tridiagonal_solve_op_test.py",
    "line number": "67",
    "func name": "_tf_ones",
    "func arg": "(shape)",
    "comments": ""
}{
    "source file": "tridiagonal_solve_ops_test.py",
    "line number": "43",
    "func name": "_tf_ones",
    "func arg": "(shape)",
    "comments": ""
}{}{}{
    "source file": "trt_convert.py",
    "line number": "1270",
    "func name": "create_inference_graph",
    "func arg": "(input_graph_def, outputs, max_batch_size, max_workspace_size_bytes, precision_mode, minimum_segment_size, is_dynamic_op, maximum_cached_engines, input_saved_model_dir, input_saved_model_tags, input_saved_model_signature_key, output_saved_model_dir, session_config)",
    "comments": "Python wrapper for the TRT transformation.\n\n\n##### Args\n* **input_graph_def**: a GraphDef object containing a model to be transformed. If\n  set to None, the graph will be read from the SavedModel loaded from\n  input_saved_model_dir.\n\n* **outputs**: list of tensors or node names for the model outputs. Only used when\n  input_graph_def is not None.\n\n* **max_batch_size**: max size for the input batch.\n\n* **max_workspace_size_bytes**: the maximum GPU temporary memory which the TRT\n  engine can use at execution time. This corresponds to the 'workspaceSize'\n  parameter of nvinfer1\n\n* **precision_mode**: one of TrtPrecisionMode.supported_precision_modes().\n\n* **minimum_segment_size**: the minimum number of nodes required for a subgraph to\n  be replaced by TRTEngineOp.\n\n* **is_dynamic_op**: whether to generate dynamic TRT ops which will build the TRT\n  network and engine at run time.\n\n* **maximum_cached_engines**: max number of cached TRT engines in dynamic TRT ops.\n  If the number of cached engines is already at max but none of them can\n  serve the input, the TRTEngineOp will fall back to run the TF function\n  based on which the TRTEngineOp is created.\n\n* **input_saved_model_dir**: the directory to load the SavedModel which contains\n  the input graph to transforms. Used only when input_graph_def is None.\n\n* **input_saved_model_tags**: list of tags to load the SavedModel.\n\n* **input_saved_model_signature_key**: the key of the signature to optimize the\n  graph for.\n\n* **output_saved_model_dir**: if not None, construct a SavedModel using the\n  returned GraphDef and save it to the specified directory. This option only\n  works when the input graph is loaded from a SavedModel, i.e. when\n  input_saved_model_dir is specified and input_graph_def is None.\n\n* **session_config**: the ConfigProto used to create a Session. It's also used as\n  a template to create a TRT-enabled ConfigProto for conversion. If not\n  specified, a default ConfigProto will be used.\n\n##### Returns\n"
}{}{}{
    "source file": "type_inference.py",
    "line number": "276",
    "func name": "resolve",
    "func arg": "(node, source_info, graphs, resolver)",
    "comments": "Performs type inference.\n\n\n##### Args\n* **node**: ast.AST\n\n* **source_info**: transformer.SourceInfo\n\n* **graphs**: Dict[ast.FunctionDef, cfg.Graph]\n\n* **resolver**: Resolver\n\n##### Returns\n"
}{}{
    "source file": "type_spec.py",
    "line number": "590",
    "func name": "register_type_spec_from_value_converter",
    "func arg": "(type_object, converter_fn, allow_subclass)",
    "comments": "Registers a function for converting values with a given type to TypeSpecs.\n\nIf multiple registered `type_object`s match a value, then the most recent registration takes precedence.\n\nCustom converters should not be defined for `CompositeTensor`s; use `CompositeTensor._type_spec` instead.\n##### Args\n* **type_object**: A Python `type` object representing the type of values\n  accepted by `converter_fn`.\n\n* **converter_fn**: A function that takes one argument (an instance of the\n  type represented by `type_object`) and returns a `TypeSpec`.\n\n* **allow_subclass**: If true, then use `isinstance(value, type_object)` to\n  check for matches.  If false, then use `type(value) is type_object`.\n\n"
}{}{}{
    "source file": "ui_factory.py",
    "line number": "26",
    "func name": "get_ui",
    "func arg": "(ui_type, on_ui_exit, available_ui_types, config)",
    "comments": "Create a `base_ui.BaseUI` subtype.\n\nThis factory method attempts to fallback to other available ui_types on ImportError. For example, if `ui_type` is `curses`, but `curses` cannot be imported properly, e.g., on Windows, will fallback to `readline`.\n##### Args\n* **ui_type**: (`str`) requested UI type. Currently supported\n\n* **on_ui_exit**: (`Callable`) the callback to be called when the UI exits.\n\n* **available_ui_types**: (`None` or `list` of `str`) Manually-set available\n  ui_types.\n\n* **config**: An instance of `cli_config.CLIConfig()` carrying user-facing\n  configurations.\n\n##### Returns\n"
}{
    "source file": "unary_ops_test.py",
    "line number": "37",
    "func name": "nhwc_to_format",
    "func arg": "(x, data_format)",
    "comments": "Converts a numpy array from NHWC format to `data_format`.\n\n\n"
}{}{}{}{}{}{
    "source file": "unfused_gru.py",
    "line number": "27",
    "func name": "make_unfused_gru_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests for unfused gru op.\n\n\n"
}{
    "source file": "unicode_decode_op_test.py",
    "line number": "86",
    "func name": "_make_sparse_tensor",
    "func arg": "(indices, values, dense_shape, dtype)",
    "comments": ""
}{}{}{}{}{
    "source file": "unidirectional_sequence_lstm.py",
    "line number": "29",
    "func name": "make_unidirectional_sequence_lstm_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do unidirectional_sequence_lstm.\n\n\n"
}{}{
    "source file": "unidirectional_sequence_rnn.py",
    "line number": "29",
    "func name": "make_unidirectional_sequence_rnn_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do unidirectional_sequence_rnn.\n\n\n"
}{
    "source file": "uniform_test.py",
    "line number": "37",
    "func name": "try_import",
    "func arg": "(name)",
    "comments": ""
}{}{}{}{}{
    "source file": "unique.py",
    "line number": "27",
    "func name": "make_unique_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests for Unique op.\n\n\n"
}{
    "source file": "unique1.py",
    "line number": "27",
    "func name": "unique",
    "func arg": "()",
    "comments": "Creates a `Dataset` from another `Dataset`, discarding duplicates.\n\nUse this transformation to produce a dataset that contains one instance of each unique element in the input. For example:\n\n```python dataset = tf.data.Dataset.from_tensor_slices([1, 37, 2, 37, 2, 1])\n\n# Using `unique()` will drop the duplicate elements. dataset = dataset.apply(tf.data.experimental.unique())\n\n# ==> { 1, 37, 2 } ```\n##### Returns\n"
}{
    "source file": "unpack.py",
    "line number": "27",
    "func name": "make_unpack_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do unpack.\n\n\n"
}{
    "source file": "unroll_batch_matmul.py",
    "line number": "27",
    "func name": "make_unroll_batch_matmul_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to test unroll_batch_matmul.\n\n\n"
}{}{
    "source file": "unstack_op_test.py",
    "line number": "31",
    "func name": "np_split_squeeze",
    "func arg": "(array, axis)",
    "comments": ""
}{}{
    "source file": "unsupported_features_checker.py",
    "line number": "60",
    "func name": "verify",
    "func arg": "(node)",
    "comments": ""
}{
    "source file": "update_version.py",
    "line number": "265",
    "func name": "main",
    "func arg": "()",
    "comments": "This script updates all instances of version in the tensorflow directory.\n\nRequirements: version: The version tag OR nightly: Create a nightly tag with current date\n"
}{
    "source file": "upgrade_schema_test.py",
    "line number": "242",
    "func name": "JsonDumpAndFlush",
    "func arg": "(data, fp)",
    "comments": "Write the dictionary `data` to a JSON file `fp` (and flush).\n\n\n##### Args\n* **data**: in a dictionary that is JSON serializable.\n\n* **fp**: File-like object\n\n"
}{
    "source file": "upgrade_schema.py",
    "line number": "344",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{
    "source file": "upload_test_benchmarks.py",
    "line number": "242",
    "func name": "main",
    "func arg": "()",
    "comments": ""
}{
    "source file": "use_mnist_cnn.py",
    "line number": "112",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{
    "source file": "use_model_in_sequential_keras.py",
    "line number": "67",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{
    "source file": "use_rnn_cell.py",
    "line number": "33",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{
    "source file": "use_text_embedding_in_dataset.py",
    "line number": "65",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{
    "source file": "use_text_rnn_model.py",
    "line number": "32",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{
    "source file": "user_ops.py",
    "line number": "30",
    "func name": "my_fact",
    "func arg": "()",
    "comments": "Example of overriding the generated code for an Op.\n\n\n"
}{
    "source file": "util_ops.py",
    "line number": "32",
    "func name": "gcd",
    "func arg": "(a, b, name)",
    "comments": "Returns the greatest common divisor via Euclid's algorithm.\n\n\n##### Args\n* **a**: The dividend. A scalar integer `Tensor`.\n\n* **b**: The divisor. A scalar integer `Tensor`.\n\n* **name**: An optional name for the operation.\n\n##### Returns\n"
}{}{
    "source file": "util_test1.py",
    "line number": "55",
    "func name": "_logit",
    "func arg": "(x)",
    "comments": ""
}{}{}{}{
    "source file": "util.py",
    "line number": "399",
    "func name": "convert_bytes_to_c_source",
    "func arg": "(data, array_name, max_line_width, include_guard, include_path, use_tensorflow_license)",
    "comments": "Returns strings representing a C constant array containing `data`.\n\n\n##### Args\n* **data**: Byte array that will be converted into a C constant.\n\n* **array_name**: String to use as the variable name for the constant array.\n\n* **max_line_width**: The longest line length, for formatting purposes.\n\n* **include_guard**: Name to use for the include guard macro definition.\n\n* **include_path**: Optional path to include in the source file.\n\n* **use_tensorflow_license**: Whether to include the standard TensorFlow Apache2\n  license in the generated files.\n\n##### Returns\n"
}{}{
    "source file": "util2.py",
    "line number": "103",
    "func name": "test_truncated_normal",
    "func arg": "(assert_equal, assert_all_close, n, y, mean_atol, median_atol, variance_rtol)",
    "comments": "Tests truncated normal distribution's statistics.\n\n\n"
}{
    "source file": "util3.py",
    "line number": "1361",
    "func name": "saver_with_op_caching",
    "func arg": "(obj)",
    "comments": "A TrackableSaver with a SaveableObject cache when graph building.\n\n\n"
}{
    "source file": "util4.py",
    "line number": "238",
    "func name": "get_total_loss",
    "func arg": "(add_regularization_losses, name, scope)",
    "comments": "Returns a tensor whose value represents the total loss.\n\nIn particular, this adds any losses you have added with `tf.add_loss()` to any regularization losses that have been added by regularization parameters on layers constructors e.g. `tf.layers`. Be very sure to use this if you are constructing a loss_op manually. Otherwise regularization arguments on `tf.layers` methods will not function.\n##### Args\n* **add_regularization_losses**: A boolean indicating whether or not to use the\n  regularization losses in the sum.\n\n* **name**: The name of the returned tensor.\n\n* **scope**: An optional scope name for filtering the losses to return. Note that\n  this filters the losses added with `tf.add_loss()` as well as the\n  regularization losses to that scope.\n\n##### Returns\n"
}{
    "source file": "util5.py",
    "line number": "1354",
    "func name": "parent_frame_arguments",
    "func arg": "()",
    "comments": "Returns parent frame arguments.\n\nWhen called inside a function, returns a dictionary with the caller's function arguments. These are positional arguments and keyword arguments (**kwargs), while variable arguments (*varargs) are excluded.\n\nWhen called at global scope, this will return an empty dictionary, since there are no arguments.\n\nWARNING: If caller function argument names are overloaded before invoking this method, then values will reflect the overloaded value. For this reason, we recommend calling `parent_frame_arguments` at the beginning of the function.\n"
}{}{
    "source file": "utils_impl.py",
    "line number": "259",
    "func name": "get_debug_dir",
    "func arg": "(export_dir)",
    "comments": "Returns path to the debug sub-directory in the SavedModel.\n\n\n"
}{}{}{
    "source file": "utils.py",
    "line number": "140",
    "func name": "_is_shape_and_default_value_compatible",
    "func arg": "(default_value, shape)",
    "comments": "Verifies compatibility of shape and default_value.\n\n\n"
}{
    "source file": "utils1.py",
    "line number": "242",
    "func name": "remove_training_arg",
    "func arg": "(index, args, kwargs)",
    "comments": ""
}{
    "source file": "utils2.py",
    "line number": "203",
    "func name": "constant_value",
    "func arg": "(pred)",
    "comments": "Return the bool value for `pred`, or None if `pred` had a dynamic value.\n\nArguments: pred: A scalar, either a Python bool or a TensorFlow boolean variable or tensor, or the Python integer 1 or 0.\n##### Returns\n"
}{}{
    "source file": "v2_compat.py",
    "line number": "82",
    "func name": "disable_v2_behavior",
    "func arg": "()",
    "comments": "Disables TensorFlow 2.x behaviors.\n\nThis function can be called at the beginning of the program (before `Tensors`, `Graphs` or other structures have been created, and before devices have been initialized. It switches all global behaviors that are different between TensorFlow 1.x and 2.x to behave as intended for 1.x.\n\nUser can call this function to disable 2.x behavior during complex migrations.\n"
}{
    "source file": "values_test.py",
    "line number": "2137",
    "func name": "_make_index_slices",
    "func arg": "(values, indices, dense_shape)",
    "comments": ""
}{
    "source file": "values_util.py",
    "line number": "218",
    "func name": "apply_aggregation",
    "func arg": "(strategy, value, aggregation, destinations)",
    "comments": ""
}{
    "source file": "values.py",
    "line number": "1460",
    "func name": "_is_sync_on_read",
    "func arg": "(val)",
    "comments": ""
}{}{}{
    "source file": "variable_scope_test.py",
    "line number": "1386",
    "func name": "axis0_into3_partitioner",
    "func arg": "(shape, **unused_kwargs)",
    "comments": ""
}{
    "source file": "variable_scope.py",
    "line number": "2744",
    "func name": "variable_creator_scope",
    "func arg": "(variable_creator)",
    "comments": "Scope which defines a variable creation function to be used by variable().\n\nvariable_creator is expected to be a function with the following signature:\n\n``` def variable_creator(next_creator, **kwargs) ```\n\nThe creator is supposed to eventually call the next_creator to create a variable if it does want to create a variable and not call Variable or ResourceVariable directly. This helps make creators composable. A creator may choose to create multiple variables, return already existing variables, or simply register that a variable was created and defer to the next creators in line. Creators can also modify the keyword arguments seen by the next creators.\n\nCustom getters in the variable scope will eventually resolve down to these custom creators when they do create variables.\n\nThe valid keyword arguments in kwds are:\n\n* initial_value: A `Tensor`, or Python object convertible to a `Tensor`, which is the initial value for the Variable. The initial value must have a shape specified unless `validate_shape` is set to False. Can also be a callable with no argument that returns the initial value when called. In that case, `dtype` must be specified. (Note that initializer functions from init_ops.py must first be bound to a shape before being used here.) * trainable: If `True`, the default, GradientTapes automatically watch uses of this Variable. * validate_shape: If `False`, allows the variable to be initialized with a value of unknown shape. If `True`, the default, the shape of `initial_value` must be known. * caching_device: Optional device string describing where the Variable should be cached for reading.\n\nDefaults to the Variable's device. If not `None`, caches on another device.\n\nTypical use is to cache on the device where the Ops using the Variable reside, to deduplicate copying through `Switch` and other conditional statements. * name: Optional name for the variable. Defaults to `'Variable'` and gets uniquified automatically. dtype: If set, initial_value will be converted to the given type. If `None`, either the datatype will be kept (if `initial_value` is a Tensor), or `convert_to_tensor` will decide. * constraint: A constraint function to be applied to the variable after updates by some algorithms. * synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. * aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`.\n\nThis set may grow over time, so it's important the signature of creators is as mentioned above.\n##### Args\n* **variable_creator**: the passed creator\n\n* **elds**: \n\n"
}{}{}{}{}{
    "source file": "variables.py",
    "line number": "100",
    "func name": "transform",
    "func arg": "(node, ctx)",
    "comments": ""
}{
    "source file": "variables1.py",
    "line number": "29",
    "func name": "ldu",
    "func arg": "(load_v, name)",
    "comments": "Load variable operator that returns Undefined when failing to evaluate.\n\nNote: the name (\"load or return undefined\") is abbreviated to minimize the amount of clutter in generated code.\n\nThis variant of `ld` is useful when loading symbols that may be undefined at runtime, such as composite symbols, and whether they are defined or not cannot be determined statically. For example `d['a']` is undefined when `d` is an empty dict.\n##### Args\n* **load_v**: Lambda that executes the actual read.\n\n* **name**: Human-readable name of the symbol being read.\n\n##### Returns\n"
}{
    "source file": "variables2.py",
    "line number": "3383",
    "func name": "report_uninitialized_variables",
    "func arg": "(var_list, name)",
    "comments": "Adds ops to list the names of uninitialized variables.\n\nWhen run, it returns a 1-D tensor containing the names of uninitialized variables if there are any, or an empty array if there are none.\n##### Args\n* **var_list**: List of `Variable` objects to check. Defaults to the value of\n  `global_variables() + local_variables()`\n\n* **name**: Optional name of the `Operation`.\n\n##### Returns\n"
}{}{}{}{
    "source file": "version_utils.py",
    "line number": "125",
    "func name": "is_v1_layer_or_model",
    "func arg": "(obj)",
    "comments": ""
}{}{}{}{}{}{}{}{
    "source file": "vgg16.py",
    "line number": "235",
    "func name": "decode_predictions",
    "func arg": "(preds, top)",
    "comments": ""
}{
    "source file": "vgg19.py",
    "line number": "240",
    "func name": "decode_predictions",
    "func arg": "(preds, top)",
    "comments": ""
}{}{
    "source file": "virtual_root_template_v1.__init__.py",
    "line number": "63",
    "func name": "_forward_module",
    "func arg": "(old_name)",
    "comments": ""
}{
    "source file": "virtual_root_template_v2.__init__.py",
    "line number": "63",
    "func name": "_forward_module",
    "func arg": "(old_name)",
    "comments": ""
}{}{
    "source file": "vis_utils.py",
    "line number": "281",
    "func name": "plot_model",
    "func arg": "(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi)",
    "comments": "Converts a Keras model to dot format and save to a file.\n\nExample:\n\n```python input = tf.keras.Input(shape=(100,), dtype='int32', name='input') x = tf.keras.layers.Embedding( output_dim=512, input_dim=10000, input_length=100)(input) x = tf.keras.layers.LSTM(32)(x) x = tf.keras.layers.Dense(64, activation='relu')(x) x = tf.keras.layers.Dense(64, activation='relu')(x) x = tf.keras.layers.Dense(64, activation='relu')(x) output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x) model = tf.keras.Model(inputs=[input], outputs=[output]) dot_img_file = '/tmp/model_1.png' tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True) ```\n\nArguments: model: A Keras model instance to_file: File name of the plot image. show_shapes: whether to display shape information. show_dtype: whether to display layer dtypes. show_layer_names: whether to display layer names. rankdir: `rankdir` argument passed to PyDot, a string specifying the format of the plot: 'TB' creates a vertical plot; 'LR' creates a horizontal plot. expand_nested: Whether to expand nested models into clusters. dpi: Dots per inch.\n##### Returns\n"
}{}{
    "source file": "visualize.py",
    "line number": "506",
    "func name": "main",
    "func arg": "(argv)",
    "comments": ""
}{}{}{
    "source file": "warm_starting_util.py",
    "line number": "413",
    "func name": "warm_start",
    "func arg": "(ckpt_to_initialize_from, vars_to_warm_start, var_name_to_vocab_info, var_name_to_prev_var_name)",
    "comments": "Warm-starts a model using the given settings.\n\nIf you are using a tf.estimator.Estimator, this will automatically be called during training.\n##### Args\n* **ckpt_to_initialize_from**: [Required] A string specifying the directory with\n  checkpoint file(s) or path to checkpoint from which to warm-start the\n  model parameters.\n\n* **vars_to_warm_start**: [Optional] One of the following\n\n* **var_name_to_vocab_info**: [Optional] Dict of variable names (strings) to\n  `tf.estimator.VocabInfo`. The variable names should be \"full\" variables,\n  not the names of the partitions.  If not explicitly provided, the variable\n  is assumed to have no (changes to) vocabulary.\n\n* **var_name_to_prev_var_name**: [Optional] Dict of variable names (strings) to\n  name of the previously-trained variable in `ckpt_to_initialize_from`. If\n  not explicitly provided, the name of the variable is assumed to be same\n  between previous checkpoint and current model.  Note that this has no\n  effect on the set of variables that is warm-started, and only controls\n  name mapping (use `vars_to_warm_start` for controlling what variables to\n  warm-start).\n\n"
}{}{
    "source file": "wav_to_features.py",
    "line number": "125",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "weights_broadcast_ops.py",
    "line number": "136",
    "func name": "broadcast_weights",
    "func arg": "(weights, values)",
    "comments": "Broadcast `weights` to the same shape as `values`.\n\nThis returns a version of `weights` following the same broadcast rules as `mul(weights, values)`, but limited to the weights shapes allowed by `assert_broadcastable`. When computing a weighted average, use this function to broadcast `weights` before summing them; e.g., `reduce_sum(w * v) / reduce_sum(_broadcast_weights(w, v))`.\n##### Args\n* **weights**: `Tensor` whose shape is broadcastable to `values` according to the\n  rules of `assert_broadcastable`.\n\n* **values**: `Tensor` of any shape.\n\n##### Returns\n"
}{
    "source file": "weights_broadcast_test.py",
    "line number": "32",
    "func name": "_test_values",
    "func arg": "(shape)",
    "comments": ""
}{}{
    "source file": "where.py",
    "line number": "27",
    "func name": "make_where_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do where.\n\n\n"
}{
    "source file": "while_test.py",
    "line number": "260",
    "func name": "is_compile_on_demand",
    "func arg": "()",
    "comments": ""
}{
    "source file": "while_v2_indexed_slices_rewriter.py",
    "line number": "290",
    "func name": "_flatten",
    "func arg": "(arg)",
    "comments": ""
}{
    "source file": "while_v2_test.py",
    "line number": "1818",
    "func name": "GetOptimizedGraph",
    "func arg": "()",
    "comments": ""
}{
    "source file": "while_v2.py",
    "line number": "1302",
    "func name": "_set_read_only_resource_inputs_attr",
    "func arg": "(op, branch_graphs)",
    "comments": "Sets the list of resource inputs which are read-only.\n\nThis is used by AutomaticControlDependencies.\n##### Args\n* **op**: While Operation.\n\n* **branch_graphs**: List of branch FuncGraphs.\n\n"
}{}{}{
    "source file": "window_ops_test.py",
    "line number": "44",
    "func name": "_scipy_raised_cosine",
    "func arg": "(length, symmetric, a, b)",
    "comments": "A simple implementation of a raised cosine window that matches SciPy.\n\nhttps://en.wikipedia.org/wiki/Window_function#Hann_window https://github.com/scipy/scipy/blob/v0.14.0/scipy/signal/windows.py#L615\n##### Args\n* **length**: The window length.\n\n* **symmetric**: Whether to create a symmetric window.\n\n* **a**: The alpha parameter of the raised cosine window.\n\n* **b**: The beta parameter of the raised cosine window.\n\n##### Returns\n"
}{
    "source file": "window_ops.py",
    "line number": "203",
    "func name": "_raised_cosine_window",
    "func arg": "(name, default_name, window_length, periodic, dtype, a, b)",
    "comments": "Helper function for computing a raised cosine window.\n\n\n##### Args\n* **name**: Name to use for the scope.\n\n* **default_name**: Default name to use for the scope.\n\n* **window_length**: A scalar `Tensor` or integer indicating the window length.\n\n* **periodic**: A bool `Tensor` indicating whether to generate a periodic or\n  symmetric window.\n\n* **dtype**: A floating point `DType`.\n\n* **a**: The alpha parameter to the raised cosine window.\n\n* **b**: The beta parameter to the raised cosine window.\n\n##### Returns\n"
}{}{
    "source file": "word2vec_basic.py",
    "line number": "360",
    "func name": "main",
    "func arg": "(unused_argv)",
    "comments": ""
}{}{}{}{
    "source file": "wrap_function.py",
    "line number": "633",
    "func name": "function_from_graph_def",
    "func arg": "(graph_def, inputs, outputs)",
    "comments": "Creates a ConcreteFunction from a GraphDef.\n\n\n##### Args\n* **graph_def**: A GraphDef to make a function out of.\n\n* **inputs**: A Tensor name or nested structure of names in `graph_def` which\n  should be inputs to the function.\n\n* **outputs**: A Tensor name or nested structure of names in `graph_def` which\n  should be outputs of the function.\n\n##### Returns\n"
}{
    "source file": "wrap_toco.py",
    "line number": "55",
    "func name": "wrapped_experimental_mlir_sparsify",
    "func arg": "(input_data_str)",
    "comments": "Wraps experimental mlir sparsify model.\n\n\n"
}{}{
    "source file": "wrappers_test.py",
    "line number": "1273",
    "func name": "_to_list",
    "func arg": "(ls)",
    "comments": ""
}{}{}{}{}{}{}{
    "source file": "xception.py",
    "line number": "323",
    "func name": "decode_predictions",
    "func arg": "(preds, top)",
    "comments": ""
}{}{
    "source file": "xla_client_test.py",
    "line number": "2103",
    "func name": "InstantiateTests",
    "func arg": "(globals_dict, backend_fn, test_prefix, **kw)",
    "comments": ""
}{
    "source file": "xla_client.py",
    "line number": "687",
    "func name": "heap_profile",
    "func arg": "(client)",
    "comments": "Returns a gzipped pprof protocol buffer containing a heap profile.\n\n\n"
}{
    "source file": "xla_control_flow_ops_test.py",
    "line number": "125",
    "func name": "_make_unstacked",
    "func arg": "(cond, body, pfor_config)",
    "comments": ""
}{}{}{
    "source file": "xla_literal.py",
    "line number": "85",
    "func name": "ConvertNumpyArrayToLiteral",
    "func arg": "(value)",
    "comments": "Converts a Numpy array or a nested tuple thereof to an XLA literal.\n\n\n"
}{
    "source file": "xla_ops_grad.py",
    "line number": "25",
    "func name": "_XlaClusterOutputGrad",
    "func arg": "(_, grad)",
    "comments": ""
}{}{
    "source file": "xla_shape.py",
    "line number": "147",
    "func name": "CreateShapeFromDtypeAndTuple",
    "func arg": "(dtype, shape_tuple)",
    "comments": "Create a shape from a Numpy dtype and a sequence of nonnegative integers.\n\n\n##### Args\n* **dtype**: a numpy dtype, e.g. np.dtype('int32').\n\n* **shape_tuple**: a sequence of nonnegative integers.\n\n##### Returns\n"
}{
    "source file": "xla_sharding.py",
    "line number": "279",
    "func name": "manual_to_auto_spmd_partition",
    "func arg": "(tensor, manual_sharding, full_shape)",
    "comments": "Switches from manual partitioning to automatic SPMD partitioning.\n\nConverts a shard-shaped tensor (manually partitioned in SPMD-style) to a full-shaped tensor to be partitioned automatically by the SPMD partitioner.\n##### Args\n* **tensor**: A tf.Tensor in shard shape.\n\n* **manual_sharding**: a serialized string of OpSharding to be used in manual\n  partitioning.\n\n* **full_shape**: the shape of tensor before partitioning.\n\n##### Returns\n"
}{}{
    "source file": "xla_test.py",
    "line number": "250",
    "func name": "Benchmark",
    "func arg": "(tf_bench, builder_fn, use_xla_jit, device, separate_compiled_gradients)",
    "comments": "Build a graph and run benchmarks against it, with or without XLA.\n\n\n##### Args\n* **tf_bench**: An instance of tf.test.Benchmark, used to run the benchmark.\n\n* **builder_fn**: A function that builds a graph when invoked, and returns\n    (name, fetches), where name is the name of the test, and fetches\n    is a list of tensors to fetch as output.\n\n* **use_xla_jit**: If true compile with the XLA JIT, otherwise use regular TF.\n\n* **device**: The tensorflow device to run on, e.g. \"cpu\", \"gpu\".\n\n* **separate_compiled_gradients**: If true put each gradient subgraph into a\n  separate compilation scope. This gives fine-grained control over which\n  portions of the graph will be compiled as a single unit. Compiling\n  gradients separately may yield better performance for some graphs.\n  The scope is named based on the scope of the forward computation as well\n  as the name of the gradients. As a result, the gradients will be compiled\n  in a scope that is separate from both the forward computation, and from\n  other gradients.\n\n"
}{}{
    "source file": "xla.py",
    "line number": "459",
    "func name": "scatter",
    "func arg": "(operand, scatter_indices, updates, update_computation, dimension_numbers, indices_are_sorted, name)",
    "comments": ""
}{
    "source file": "xla1.py",
    "line number": "591",
    "func name": "check_function_argument_count",
    "func arg": "(func, input_arity, infeed_queue)",
    "comments": "Validate the number of input arguments to an XLA function.\n\n\n##### Args\n* **func**: the Python function that will be called to generate the body of an XLA\n  computation graph.\n\n* **input_arity**: the number of explicit arguments supplied by the caller.\n\n* **infeed_queue**: if not None, the infeed queue that will supply\n  additional arguments to the function.\n\n##### Returns\n"
}{}{}{}{}{}{
    "source file": "zero_out_grad_2.py",
    "line number": "28",
    "func name": "_zero_out_grad",
    "func arg": "(op, grad)",
    "comments": "The gradients for `zero_out`.\n\n\n##### Args\n* **op**: The `zero_out` `Operation` that we are differentiating, which we can use\n  to find the inputs and outputs of the original op.\n\n* **grad**: Gradient with respect to the output of the `zero_out` op.\n\n##### Returns\n"
}{}{}{}{
    "source file": "zeros_like.py",
    "line number": "27",
    "func name": "make_zeros_like_tests",
    "func arg": "(options)",
    "comments": "Make a set of tests to do zeros_like.\n\n\n"
}{}{
    "source file": "zip_files.py",
    "line number": "32",
    "func name": "main",
    "func arg": "(_)",
    "comments": ""
}{
    "source file": "zip_test_utils.py",
    "line number": "262",
    "func name": "make_zip_of_tests",
    "func arg": "(options, test_parameters, make_graph, make_test_inputs, extra_toco_options, use_frozen_graph, expected_tf_failures)",
    "comments": "Helper to make a zip file of a bunch of TensorFlow models.\n\nThis does a cartesian product of the dictionary of test_parameters and calls make_graph() for each item in the cartesian product set. If the graph is built successfully, then make_test_inputs() is called to build expected input/output value pairs. The model is then converted to tflite with toco, and the examples are serialized with the tflite model into a zip file (2 files per item in the cartesian product set).\n##### Args\n* **options**: An Options instance.\n\n* **test_parameters**: Dictionary mapping to lists for each parameter.\n  e.g. `{\"strides\"\n\n* **make_graph**: function that takes current parameters and returns tuple\n  `[input1, input2, ...], [output1, output2, ...]`\n\n* **make_test_inputs**: function taking `curr_params`, `session`, `input_tensors`,\n  `output_tensors` and returns tuple `(input_values, output_values)`.\n\n* **extra_toco_options**: Additional toco options.\n\n* **use_frozen_graph**: Whether or not freeze graph before toco converter.\n\n* **expected_tf_failures**: Number of times tensorflow is expected to fail in\n  executing the input graphs. In some cases it is OK for TensorFlow to fail\n  because the one or more combination of parameters is invalid.\n\n"
}{
    "source file": "zip_test.py",
    "line number": "31",
    "func name": "_dataset_factory",
    "func arg": "(components)",
    "comments": ""
}