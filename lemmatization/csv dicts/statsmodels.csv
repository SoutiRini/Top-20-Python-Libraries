0,"{'func name': 'normal_ad', 'comments': 'Anderson-Darling test for normal distribution unknown mean and variance.\n\nParameters ---------- x : array_like The data array. axis : int The axis to perform the test along.\n##### Returns\n* **ad2 **: float\n    Anderson Darling test statistic.\n\n* **pval **: float\n    The pvalue for hypothesis that the data comes from a normal\n    distribution with unknown mean and variance.\n\n', 'stemmed comments': ['normal', 'x', 'hypothesi', 'array_lik', 'array', 'paramet', 'axi', 'darl', 'return', 'data', 'int', 'pval', 'varianc', 'distribut', 'statist', 'ad2', 'float', 'pvalu', 'test', 'mean', 'the', 'along', 'unknown', 'anderson', 'come', 'perform', 'andersondarl']}"
1,"{'func name': 'promax', 'comments': 'Performs promax rotation of the matrix :math:`A`.\n\nThis method was not very clear to me from the literature, this implementation is as I understand it should work.\n\nPromax rotation is performed in the following steps:\n\n* Determine varimax rotated patterns :math:`V`.\n\n* Construct a rotation target matrix :math:`|V_{ij}|^k/V_{ij}`\n\n* Perform procrustes rotation towards the target to obtain T\n\n* Determine the patterns\n\nFirst, varimax rotation a target matrix :math:`H` is determined with orthogonal varimax rotation. Then, oblique target rotation is performed towards the target.\n\nParameters ---------- A : numpy matrix non rotated factors k : float parameter, should be positive\n\nReferences ---------- [1] Browne (2001)\n\n- An overview of analytic rotation in exploratory factor analysis\n\n[2] Navarra, Simoncini (2010)\n\n- A guide to empirical orthogonal functions for climate data analysis\n', 'stemmed comments': ['A', 'k', 'analyt', '2', 'H', 'first', ']', 'V', 'understand', 'determin', 'T', 'math', 'overview', 'paramet', 'factor', 'clear', 'simoncini', 'method', 'brown', 'matrix', 'exploratori', 'construct', '2010', 'thi', 'An', 'data', 'step', 'function', 'analysi', 'ij', 'toward', 'guid', 'rotat', 'orthogon', '1', 'refer', '|^k/v_', 'procrust', 'then', 'numpi', '{', 'float', 'follow', 'work', 'I', 'implement', '}', '[', 'climat', 'pattern', '|v_', 'obtain', 'promax', 'navarra', 'literatur', 'varimax', 'non', 'posit', 'empir', 'obliqu', '2001', 'perform', 'target']}"
2,"{'func name': 'bds', 'comments': 'BDS Test Statistic for Independence of a Time Series\n\nParameters ---------- x : ndarray Observations of time series for which bds statistics is calculated. max_dim : int The maximum embedding dimension. epsilon : {float, None}, optional The threshold distance to use in calculating the correlation sum. distance : float, optional Specifies the distance multiplier to use when computing the test statistic if epsilon is omitted.\n##### Returns\n* **bds_stat **: float\n    The BDS statistic.\n\n* **pvalue **: float\n    The p-values associated with the BDS statistic.\n\n* **required to calculate the m-histories**: \n\n', 'stemmed comments': ['comput', 'x', 'bds_stat', 'use', 'paramet', 'seri', 'independ', 'return', 'requir', 'bd', 'embed', 'omit', 'int', 'threshold', 'calcul', 'statist', 'epsilon', 'correl', 'time', 'float', 'pvalu', 'option', 'test', '{', 'sum', 'associ', 'max_dim', 'mhistori', 'distanc', 'the', '}', 'observ', 'none', 'ndarray', 'dimens', 'multipli', 'specifi', 'maximum']}"
3,"{'func name': 'fit_constrained_wrap', 'comments': 'fit_constraint that returns a results instance\n\nThis is a development version for fit_constrained methods or fit_constrained as standalone function.\n\nIt will not work correctly for all models because creating a new results instance is not standardized for use outside the `fit` methods, and might need adjustements for this.\n\nThis is the prototype for the fit_constrained method that has been added to Poisson and GLM.\n', 'stemmed comments': ['adjust', 'use', 'return', 'method', 'fit_constraint', 'thi', 'model', 'develop', 'standard', 'function', 'version', 'result', 'new', 'standalon', 'instanc', 'outsid', 'ad', 'poisson', 'fit_constrain', 'need', 'work', 'creat', 'fit', 'glm', 'might', 'It', 'prototyp', 'correctli']}"
4,"{'func name': 'conditional_moment_test_regression', 'comments': 'generic conditional moment test based artificial regression\n\nthis is very experimental, no options implemented yet\n\nso far OPG regression, or artificial regression with Robust Wald test\n\nThe latter is (as far as I can see) the same as an overidentifying test in GMM where the test statistic is the value of the GMM objective function and it is assumed that parameters were estimated with optimial GMM, i.e. the weight matrix equal to the expectation of the score variance.\n', 'stemmed comments': ['experiment', 'see', 'regress', 'artifici', 'object', 'moment', 'paramet', 'yet', 'overidentifi', 'score', 'matrix', 'weight', 'equal', 'valu', 'estim', 'assum', 'latter', 'ie', 'function', 'varianc', 'optimi', 'base', 'condit', 'statist', 'expect', 'option', 'test', 'far', 'gmm', 'implement', 'I', 'the', 'robust', 'opg', 'wald', 'gener']}"
5,"{'func name': 'test_poisson_zeroinflation_brock', 'comments': 'score test for zero modification in Poisson, special case\n\nThis assumes that the Poisson model has a constant and that the zero modification probability is constant.\n\nThis is a special case of test_poisson_zeroinflation derived by van den Brock 1995.\n\nThe test reports two sided and one sided alternatives based on the normal distribution of the test statistic.\n', 'stemmed comments': ['normal', 'test_poisson_zeroinfl', 'deriv', '1995', 'probabl', 'side', 'report', 'score', 'thi', 'assum', 'van', 'two', 'model', 'brock', 'base', 'distribut', 'case', 'statist', 'constant', 'altern', 'poisson', 'test', 'the', 'modif', 'zero', 'one', 'special', 'den']}"
6,"{'func name': 'check_cont_fit', 'comments': '', 'stemmed comments': []}"
7,"{'func name': 'ff_partial_target', 'comments': 'Subroutine for the value of vgQ using orthogonal rotation towards a partial target matrix, i.e., we minimize\n\n.. math:: \\phi(L) =\\frac{1},{2}\\|W\\circ(L-H)\\|^2,\n\nwhere :math:`\\circ` is the element-wise product or Hadamard product and :math:`W` is a matrix whose entries can only be one or zero. Either :math:`L` should be provided or :math:`A` and :math:`T` should be provided.\n\nFor orthogonal rotations :math:`L` satisfies\n\n.. math:: L =\n\nAT,\n\nwhere :math:`T` is an orthogonal matrix.\n\nParameters ---------- H : numpy matrix target matrix W : numpy matrix (default matrix with equal weight one for all entries) matrix with weights, entries can either be one or zero L : numpy matrix (default None) rotated factors, i.e., :math:`L=A(T^*)^{-1}=AT` A : numpy matrix (default None) non rotated factors T : numpy matrix (default None) rotation matrix\n', 'stemmed comments': ['W', 'A', '2', 'elementwis', 'H', 'LH', 'use', 'T', 'math', 'paramet', 'factor', 'matrix', 'satisfi', 'weight', 'equal', 'valu', 'minim', 'ie', '=', 'for', 'l=a', 'provid', '\\circ', 'toward', 'L', 'rotat', 'orthogon', 'AT', 'T^', 'subroutin', '1', 'hadamard', 'whose', '\\phi', 'numpi', '{', '\\|w\\circ', '=\\frac', '}', 'zero', 'vgq', 'one', 'default', 'none', '=at', 'either', 'product', 'non', 'entri', '\\|^2', 'partial', '^', 'target']}"
8,"{'func name': 'gpke', 'comments': 'Returns the non-normalized Generalized Product Kernel Estimator\n\nParameters ---------- bw : 1-D ndarray The user-specified bandwidth parameters. data : 1D or 2-D ndarray The training data. data_predict : 1-D ndarray The evaluation points at which the kernel estimation is performed. var_type : str, optional The variable type (continuous, ordered, unordered). ckertype : str, optional The kernel used for the continuous variables. okertype : str, optional The kernel used for the ordered discrete variables. ukertype : str, optional The kernel used for the unordered discrete variables. tosum : bool, optional Whether or not to sum the calculated array of densities.\n\nDefault is True.\n##### Returns\n* **dens **: array_like\n    The generalized product kernel density estimator.\n\n* **The formula for the multivariate kernel estimator for the pdf is**: \n\n* **.. math**: \n\n', 'stemmed comments': ['array_lik', 'array', 'use', 'var_typ', 'bool', 'densiti', 'ukertyp', 'pdf', 'math', 'continu', 'paramet', 'formula', 'train', 'discret', 'return', 'data_predict', 'order', 'estim', 'evalu', 'data', 'point', 'multivari', '1D', 'calcul', 'tosum', 'type', 'true', 'str', 'variabl', 'option', 'nonnorm', 'sum', 'whether', 'kernel', 'bandwidth', 'the', 'okertyp', 'userspecifi', 'unord', 'default', '2D', 'bw', 'ndarray', 'product', 'ckertyp', 'perform', 'gener', 'den']}"
9,"{'func name': '_kernel_survfunc', 'comments': 'Estimate the marginal survival function under dependent censoring.\n\nParameters ---------- time : array_like The observed times for each subject status : array_like The status for each subject (1 indicates event, 0 indicates censoring) exog : array_like Covariates such that censoring is independent conditional on exog kfunc : function Kernel function freq_weights : array_like Optional frequency weights\n##### Returns\n* **probs **: array_like\n    The estimated survival probabilities\n\n* **times **: array_like\n    The times at which the survival probabilities are estimated\n\n* **Annals of Statistics 32 (4)**: 1533 55.\n\n* **doi**: 10.1214/009053604000000508.\n\n* **https**: //arxiv.org/pdf/math/0409180.pdf\n\n', 'stemmed comments': ['array_lik', 'margin', 'frequenc', 'probabl', '1533', '4', 'paramet', 'independ', 'return', 'subject', '55', 'weight', 'prob', 'estim', 'function', 'statu', '101214/009053604000000508', 'kfunc', 'condit', 'statist', 'time', '1', 'option', 'kernel', 'the', 'depend', 'doi', 'covari', '32', '0', 'observ', 'freq_weight', 'exog', 'http', '//arxivorg/pdf/math/0409180pdf', 'annal', 'indic', 'event', 'censor', 'surviv']}"
10,"{'func name': '_get_knmat', 'comments': '', 'stemmed comments': []}"
11,"{'func name': 'kstest_fit', 'comments': ""Test assumed normal or exponential distribution using Lilliefors' test.\n\nLilliefors' test is a Kolmogorov-Smirnov test with estimated parameters.\n\nParameters ---------- x : array_like, 1d Data to test. dist : {'norm', 'exp'}, optional The assumed distribution. pvalmethod : {'approx', 'table'}, optional The method used to compute the p-value of the test statistic. In general, 'table' is preferred and makes use of a very large simulation. 'approx' is only valid for normality. if `dist = 'exp'` `table` is always used. 'approx' uses the approximation formula of Dalal and Wilkinson, valid for pvalues < 0.1. If the pvalue is larger than 0.1, then the result of `table` is returned.\n##### Returns\n* **ksstat **: float\n    Kolmogorov-Smirnov test statistic with estimated mean and variance.\n\n* **pvalue **: float\n    If the pvalue is lower than some threshold, e.g. 0.05, then we can\n    reject the Null hypothesis that the sample comes from a normal\n    distribution.\n\n"", 'stemmed comments': ['comput', 'normal', 'null', 'x', 'hypothesi', 'array_lik', 'exponenti', 'ksstat', 'use', 'kolmogorovsmirnov', 'sampl', 'formula', 'paramet', 'make', 'return', 'approxim', 'method', '<', 'dist', 'simul', 'estim', 'valid', 'assum', 'eg', 'data', '=', 'exp', 'lower', 'pvalmethod', 'In', 'varianc', 'threshold', 'distribut', '01', 'result', 'lilliefor', 'wilkinson', 'prefer', 'statist', 'larg', 'alway', '005', 'approx', 'float', '1d', 'option', 'test', '{', 'pvalu', 'mean', 'dalal', 'the', '}', 'If', 'reject', 'tabl', 'larger', 'come', 'norm', 'gener']}"
12,"{'func name': 'score_test', 'comments': 'score test for restrictions or for omitted variables\n\nNull Hypothesis : constraints are satisfied\n\nAlternative Hypothesis : at least one of the constraints does not hold\n\nThis allows to specify restricted and unrestricted model properties in three different ways\n\n- fit_constrained result: model contains score and hessian function for the full, unrestricted model, but the parameter estimate in the results instance is for the restricted model. This is the case if the model was estimated with fit_constrained.\n\n- restricted model with variable addition: If exog_extra is not None, then it is assumed that the current model is a model with zero restrictions and the unrestricted model is given by adding exog_extra as additional explanatory variables.\n\n- unrestricted model with restricted parameters explicitly provided. If params_constrained is not None, then the model is assumed to be for the unrestricted model, but the provided parameters are for the restricted model. TODO: This case will currently only work for `nonrobust` cov_type, otherwise we will also need the restriction matrix provided by the user.\n\n Parameters ---------- exog_extra : None or array_like Explanatory variables that are jointly tested for inclusion in the model, i.e. omitted variables. params_constrained : array_like estimated parameter of the restricted model. This can be the parameter estimate for the current when testing for omitted variables. hypothesis : str, \'joint\' (default) or \'separate\' If hypothesis is \'joint\', then the chisquare test results for the joint hypothesis that all constraints hold is returned. If hypothesis is \'joint\', then z-test results for each constraint is returned. This is currently only implemented for cov_type=""nonrobust"". cov_type : str Warning: only partially implemented so far, currently only ""nonrobust"" and ""HC0"" are supported. If cov_type is None, then the cov_type specified in fit for the Wald tests is used. If the cov_type argument is not None, then it will be used instead of the Wald cov_type given in fit. k_constraints : int or None Number of constraints that were used in the estimation of params restricted relative to the number of exog in the model. This must be provided if no exog_extra are given. If exog_extra is not None, then k_constraints is assumed to be zero if it is None. observed : bool If True, then the observed Hessian is used in calculating the covariance matrix of the score. If false then the expected information matrix is used. This currently only applies to GLM where EIM is available. Warning: This option might still change.\n##### Returns\n* **chi2_stat **: float\n    chisquare statistic for the score test\n\n* **p-value **: float\n    P-value of the score test based on the chisquare distribution.\n\n* **df **: int\n    Degrees of freedom used in the p-value calculation. This is equal\n    to the number of constraints.\n\n* **Status**: experimental, several options are not implemented yet or are not\n\n* **cov_type is \'nonrobust\'**: \n\n', 'stemmed comments': ['warn', 'k_constraint', 'exog_extra', 'user', 'todo', 'params_constrain', 'addit', 'function', 'provid', 'statu', 'base', 'distribut', 'hessian', 'calcul', 'instanc', 'far', 'test', 'rel', 'option', 'must', 'jointli', 'chisquar', 'implement', 'zero', 'least', 'still', 'given', 'param', 'way', 'partial', 'specifi', 'explicitli', 'use', 'paramet', 'return', 'score', 'argument', 'equal', 'estim', 'ie', 'result', 'statist', 'ad', 'true', 'str', 'sever', 'variabl', 'float', 'pvalu', 'instead', 'joint', 'fit_constrain', 'ztest', 'allow', 'differ', 'cov_type=', 'chang', 'separ', 'exog', 'wald', 'number', 'fals', 'null', 'hypothesi', 'cov_typ', 'current', 'unrestrict', 'omit', 'assum', 'int', 'model', 'chi2_stat', 'case', 'properti', 'altern', 'inform', 'df', 'nonrobust', 'three', 'appli', 'work', 'support', 'covari', 'full', 'none', 'If', 'default', 'eim', 'experiment', 'array_lik', 'restrict', 'bool', 'yet', 'avail', 'matrix', 'satisfi', 'explanatori', 'thi', 'otherwis', 'inclus', 'hold', 'degre', 'contain', 'expect', 'also', 'freedom', 'constraint', 'need', 'hc0', 'fit', 'glm', 'one', 'might', 'observ']}"
13,"{'func name': 'params_transform_univariate', 'comments': 'results for univariate, nonlinear, monotonicaly transformed parameters\n\nThis provides transformed values, standard errors and confidence interval for transformations of parameters, for example in calculating rates with `exp(params)` in the case of Poisson or other models with exponential mean function.\n', 'stemmed comments': ['transform', 'exponenti', 'error', 'paramet', 'rate', 'thi', 'valu', 'model', 'monotonicali', 'exp', 'standard', 'function', 'provid', 'result', 'case', 'calcul', 'confid', 'poisson', 'mean', 'interv', 'univari', 'param', 'nonlinear', 'exampl']}"
14,"{'func name': 'get_prediction', 'comments': ""Compute prediction results.\n\nParameters ---------- exog : array_like, optional The values for which you want to predict. transform : bool, optional If the model was fit via a formula, do you want to pass exog through the formula. Default is True. E.g., if you fit a model y ~ log(x1) + log(x2), and transform is True, then you can pass a data structure that contains x1 and x2 in their original form. Otherwise, you'd need to log the data first. weights : array_like, optional Weights interpreted as in WLS, used for the variance of the predicted residual. row_labels : list A list of row labels to use.\n\nIf not provided, read `exog` is available. **kwargs Some models can take additional keyword arguments, see the predict method of the model for the details.\n##### Returns\n"", 'stemmed comments': ['comput', 'x1', 'A', 'structur', 'see', 'keyword', 'array_lik', 'transform', 'first', 'use', 'bool', 'formula', 'paramet', 'avail', 'argument', 'return', 'method', 'some', 'list', 'addit', 'weight', 'valu', 'via', 'data', 'model', 'otherwis', 'wl', 'read', 'provid', 'varianc', 'detail', 'result', 'residu', 'label', 'row_label', 'kwarg', '~', 'take', 'origin', 'true', 'x2', 'contain', 'interpret', 'option', 'need', 'the', 'fit', 'predict', 'default', 'If', 'form', 'pass', 'exog', 'log', 'row', 'want', 'd', 'Eg']}"
15,"{'func name': 'check_predict_types', 'comments': 'Check that the `predict` method of the given results object produces the correct output type.\n\nParameters ---------- results : Results\n', 'stemmed comments': ['predict', 'type', 'produc', 'correct', 'check', 'object', 'paramet', 'given', 'output', 'result', 'method']}"
16,"{'func name': 'series_density', 'comments': '', 'stemmed comments': []}"
17,"{'func name': 'pandas_wrapper_freq', 'comments': ""Return a new function that catches the incoming X, checks if it's pandas, calls the functions as is. Then wraps the results in the incoming index.\n\nDeals with frequencies. Expects that the function returns a tuple, a Bunch object, or a pandas-object.\n"", 'stemmed comments': ['deal', 'object', 'frequenc', 's', 'return', 'X', 'incom', 'catch', 'function', 'result', 'new', 'panda', 'expect', 'check', 'then', 'bunch', 'index', 'wrap', 'call', 'pandasobject', 'tupl']}"
18,"{'func name': 'get_versions', 'comments': 'Get version information or return default if unable to do so.\n\n\n', 'stemmed comments': ['default', 'inform', 'unabl', 'get', 'version', 'return']}"
19,"{'func name': 'rotate_factors', 'comments': ""Subroutine for orthogonal and oblique rotation of the matrix :math:`A`. For orthogonal rotations :math:`A` is rotated to :math:`L` according to\n\n.. math::\n\nL =\n\nAT,\n\nwhere :math:`T` is an orthogonal matrix. And, for oblique rotations :math:`A` is rotated to :math:`L` according to\n\n.. math::\n\nL =\n\nA(T^*)^{-1},\n\nwhere :math:`T` is a normal matrix.\n\nParameters ---------- A : numpy matrix (default None) non rotated factors method : str should be one of the methods listed below method_args : list additional arguments that should be provided with each method algorithm_kwargs : dictionary algorithm : str (default gpa) should be one of:\n\n* 'gpa': a numerical method * 'gpa_der_free': a derivative free numerical method * 'analytic' : an analytic method\n\nDepending on the algorithm, there are algorithm specific keyword arguments. For the gpa and gpa_der_free, the following keyword arguments are available:\n\nmax_tries : int (default 501) maximum number of iterations\n\ntol : float stop criterion, algorithm stops if Frobenius norm of gradient is smaller then tol\n\nFor analytic, the supported arguments depend on the method, see above.\n\nSee the lower level functions for more details.\n##### Returns\n* **The tuple **: math\n\n* **Below,\n    * **: math\n\n* **oblimin **: orthogonal or oblique rotation that minimizes\n    .. math\n\n* **orthomax **: orthogonal rotation that minimizes\n    .. math\n\n* **CF **: Crawford-Ferguson family for orthogonal and oblique rotation which\n\n* **minimizes**: .. math\n\n* **quartimax **: orthogonal rotation method\n    minimizes the orthomax objective with\n\n* **biquartimax **: orthogonal rotation method\n    minimizes the orthomax objective with\n\n* **varimax **: orthogonal rotation method\n    minimizes the orthomax objective with\n\n* **equamax **: orthogonal rotation method\n    minimizes the orthomax objective with\n\n* **parsimax **: orthogonal rotation method\n    minimizes the Crawford-Ferguson family objective with\n\n* **parsimony **: orthogonal rotation method\n    minimizes the Crawford-Ferguson family objective with\n\n* **quartimin **: oblique rotation method that minimizes\n    minimizes the oblimin objective with\n\n* **target **: orthogonal or oblique rotation that rotates towards a target\n\n* **matrix **: math\n\n* **partial_target **: orthogonal (default) or oblique rotation that partially\n\n* **rotates towards a target matrix **: math\n\n"", 'stemmed comments': ['dictionari', 'algorithm', 'object', 'T', 'gpa_der_fre', 'method_arg', 'addit', 'crawfordferguson', 'quartimax', '501', 'free', 'function', 'provid', 'max_tri', 'rotat', 'and', 'AT', 'T^', 'specif', 'numer', 'partial_target', 'obliqu', 'partial', 'smaller', 'see', 'deriv', 'parsimax', 'paramet', 'argument', 'return', 'method', 'iter', 'minim', 'for', 'lower', 'CF', 'toward', 'orthogon', 'str', 'float', '{', 'follow', 'non', 'number', 'maximum', 'keyword', 'math', 'gpa', 'factor', 'orthomax', 'level', 'oblimin', 'int', 'equamax', 'frobeniu', 'famili', 'support', 'default', 'none', 'gradient', 'varimax', 'norm', 'tupl', '^', 'target', 'algorithm_kwarg', 'A', 'normal', 'analyt', 'avail', 'biquartimax', 'parsimoni', 'matrix', 'tol', 'below', '=', 'detail', 'L', 'quartimin', 'subroutin', '1', 'numpi', 'accord', 'depend', '}', 'the', 'one', 'stop', 'criterion', 'list']}"
20,"{'func name': 'mackinnoncrit', 'comments': 'Returns the critical values for cointegrating and the ADF test.\n\nIn 2010 MacKinnon updated the values of his 1994 paper with critical values for the augmented Dickey-Fuller tests.\n\nThese new values are to be preferred and are used here.\n\nParameters ---------- N : int The number of series of I(1) series for which the null of non-cointegration is being tested.\n\nFor N > 12, the critical values are linearly interpolated (not yet implemented).\n\nFor the ADF test, N = 1. reg : str {\'c\', \'tc\', \'ctt\', \'nc\'} Following MacKinnon (1996), these stand for the type of regression run. \'c\' for constant and no trend, \'tc\' for constant with a linear trend, \'ctt\' for constant with a linear and quadratic trend, and \'nc\' for no constant.\n\nThe values for the no constant case are taken from the 1996 paper, as they were not updated for 2010 due to the unrealistic assumptions that would underlie such a case. nobs : int or np.inf This is the sample size.\n\nIf the sample size is numpy.inf, then the asymptotic critical values are returned.\n\nReferences ---------- .. [*] MacKinnon, J.G. 1994\n\n""Approximate Asymptotic Distribution Functions for Unit-Root and Cointegration Tests."" Journal of Business & Economics Statistics, 12.2, 167-76. .. [*] MacKinnon, J.G. 2010.\n\n""Critical Values for Cointegration Tests."" Queen\'s University, Dept of Economics Working Papers 1227. http://ideas.repec.org/p/qed/wpaper/1227.html\n', 'stemmed comments': ['N', 'quadrat', 'run', 'sampl', 'unrealist', 'stand', 's', '2010', 'underli', 'econom', 'function', 'c', '1996', 'distribut', 'cointegr', 'constant', 'refer', 'test', 'univers', 'implement', 'dickeyful', '[', 'noncointegr', 'asymptot', 'busi', 'adf', '1227', 'due', 'use', 'trend', '&', 'paramet', 'return', '1994', 'paper', 'nob', 'would', 'for', 'interpol', 'statist', 'str', '{', 'follow', 'I', 'journal', 'number', 'tc', 'null', 'npinf', '16776', '12', 'valu', 'int', 'In', '//ideasrepecorg/p/qed/wpaper/1227html', '122', 'these', 'case', 'type', 'work', 'If', 'reg', 'nc', 'linear', 'JG', 'http', 'numpyinf', 'taken', 'size', 'dept', 'regress', 'unitroot', 'queen', ']', 'seri', 'yet', 'approxim', 'critic', 'ctt', 'thi', '=', 'new', 'linearli', 'prefer', '1', 'updat', 'the', '>', '}', 'mackinnon', 'augment', 'assumpt']}"
21,"{'func name': 'mean_diff_plot', 'comments': ""Construct a Tukey/Bland-Altman Mean Difference Plot.\n\nTukey's Mean Difference Plot (also known as a Bland-Altman plot) is a graphical method to analyze the differences between two methods of measurement. The mean of the measures is plotted against their difference.\n\nFor more information see https://en.wikipedia.org/wiki/Bland-Altman_plot\n\nParameters ---------- m1 : array_like A 1-d array. m2 : array_like A 1-d array. sd_limit : float The limit of agreements expressed in terms of the standard deviation of the differences. If `md` is the mean of the differences, and `sd` is the standard deviation of those differences, then the limits of agreement that will be plotted are md +/- sd_limit * sd. The default of 1.96 will produce 95% confidence intervals for the means of the differences. If sd_limit = 0, no limits will be plotted, and the ylimit of the plot defaults to 3 standard deviations on either side of the mean. ax : AxesSubplot If `ax` is None, then a figure is created. If an axis instance is given, the mean difference plot is drawn on the axis. scatter_kwds : dict Options to to style the scatter plot. Accepts any keywords for the matplotlib Axes.scatter plotting method mean_line_kwds : dict Options to to style the scatter plot. Accepts any keywords for the matplotlib Axes.axhline plotting method limit_lines_kwds : dict Options to to style the scatter plot. Accepts any keywords for the matplotlib Axes.axhline plotting method\n##### Returns\n* **.. plot**: \n\n"", 'stemmed comments': ['A', 'see', 'plot', 'keyword', 'array_lik', 'tukey', 'deviat', '3', 'tukey/blandaltman', 'array', 'mean_line_kwd', 'limit', '//enwikipediaorg/wiki/blandaltman_plot', 'side', 'style', 'paramet', 'matplotlib', 'axi', 's', 'return', 'method', 'measur', 'construct', 'two', '=', 'for', 'standard', 'axessubplot', '196', 'sd_limit', 'accept', 'scatter', 'ax', 'figur', 'instanc', 'sd', 'scatter_kwd', 'axesaxhlin', '/', '95', 'inform', 'confid', 'm1', '1d', 'float', 'mean', 'also', 'graphic', 'term', '%', 'interv', 'drawn', 'express', 'the', 'creat', 'limit_lines_kwd', 'axesscatt', 'blandaltman', '0', 'default', 'If', 'none', 'produc', 'ylimit', 'differ', 'dict', 'either', 'm2', 'option', 'analyz', 'http', 'agreement', 'known', 'md', 'given']}"
22,"{'func name': 'anova_ols', 'comments': '', 'stemmed comments': []}"
23,"{'func name': '_ssr_reduced_model', 'comments': 'Residual sum of squares of OLS model excluding factors in `keys` Assumes x matrix is orthogonal\n\nParameters ---------- y : array_like dependent variable x : array_like independent variables term_slices : a dict of slices term_slices[key] is a boolean array specifies the parameters associated with the factor `key` params : ndarray OLS solution of y = x * params keys : keys for term_slices factors to be excluded\n##### Returns\n* **rss **: float\n    residual sum of squares\n\n* **df **: int\n    degrees of freedom\n\n', 'stemmed comments': ['exclud', 'x', 'array_lik', ']', 'array', 'boolean', 'paramet', 'independ', 'factor', 'rss', 'return', 'matrix', 'assum', 'int', 'model', '=', 'term_slic', 'degre', 'residu', 'orthogon', 'slice', 'variabl', 'key', 'float', 'df', 'sum', 'ol', 'associ', 'freedom', 'solut', 'depend', '[', 'ndarray', 'dict', 'squar', 'param', 'specifi']}"
24,"{'func name': 'ar_select_order', 'comments': ""Autoregressive AR-X(p) model order selection.\n\nParameters ---------- endog : array_like A 1-d endogenous response variable. The independent variable. maxlag : int The maximum lag to consider. ic : {'aic', 'hqic', 'bic'} The information criterion to use in the selection. glob : bool Flag indicating where to use a global search\n\nacross all combinations of lags.\n\nIn practice, this option is not computational feasible when maxlag is larger than 15 (or perhaps 20) since the global search requires fitting 2**maxlag models. %(auto_reg_params)s\n##### Returns\n"", 'stemmed comments': ['comput', 'A', '15', '2', 'array_lik', 'global', 'bic', 'use', 'bool', 'combin', 'p', 'aic', 'paramet', 'independ', 'return', 'requir', 'order', 'perhap', 'maxlag', 'endogen', 'int', 'model', 'auto_reg_param', 'across', 'In', '20', 'select', 'lag', 'endog', 'search', 'arx', 'feasibl', 'inform', 'variabl', '%', 'autoregress', '1d', '{', 'practic', 'option', 'glob', 'flag', 'the', '}', 'fit', 'indic', 'consid', 'respons', 'ic', 'criterion', 'sinc', 'larger', 'hqic', 'maximum']}"
25,"{'func name': 'tiny2zero', 'comments': 'replace abs values smaller than eps by zero, makes copy\n\n\n', 'stemmed comments': ['smaller', 'valu', 'ep', 'make', 'ab', 'zero', 'replac', 'copi']}"
26,"{'func name': '_check_estimable', 'comments': '', 'stemmed comments': []}"
27,"{'func name': 'deconvolve', 'comments': 'Deconvolves divisor out of signal, division of polynomials for n terms\n\ncalculates den^{-1} * num\n\nParameters ---------- num : array_like signal or lag polynomial denom : array_like coefficients of lag polynomial (linear filter) n : None or int number of terms of quotient\n##### Returns\n* **quot **: ndarray\n    quotient or filtered series\n\n* **rem **: ndarray\n    remainder\n\n', 'stemmed comments': ['coeffici', 'array_lik', 'remaind', 'divis', 'n', 'filter', 'paramet', 'seri', 'return', 'rem', 'polynomi', 'int', 'divisor', 'quotient', 'deconvolv', 'denom', 'lag', 'signal', 'calcul', 'quot', '1', 'term', '{', 'num', '}', 'none', 'ndarray', 'den^', 'linear', 'number']}"
28,"{'func name': 'arma_scoreobs', 'comments': 'Compute the score (gradient) per observation.\n\nParameters ---------- endog : ndarray The observed time-series process. ar_params : ndarray, optional Autoregressive coefficients, not including the zero lag. ma_params : ndarray, optional Moving average coefficients, not including the zero lag, where the sign convention assumes the coefficients are part of the lag polynomial on the right-hand-side of the ARMA definition (i.e. they have the same sign from the usual econometrics convention in which the coefficients are on the right-hand-side of the ARMA definition). sigma2 : ndarray, optional The ARMA innovation variance. Default is 1. prefix : str, optional The BLAS prefix associated with the datatype. Default is to find the best datatype based on given input. This argument is typically only used internally.\n##### Returns\n', 'stemmed comments': ['comput', 'coeffici', 'datatyp', 'use', 'paramet', 'ma_param', 'argument', 'score', 'return', 'per', 'polynomi', 'thi', 'assum', 'innov', 'part', 'ie', 'bla', 'timeseri', 'varianc', 'base', 'lag', 'endog', 'definit', 'usual', 'includ', 'ar_param', 'intern', 'str', '1', 'sigma2', 'autoregress', 'option', 'move', 'averag', 'associ', 'find', 'the', 'zero', 'econometr', 'best', 'arma', 'observ', 'default', 'ndarray', 'process', 'gradient', 'sign', 'prefix', 'given', 'typic', 'input', 'convent', 'righthandsid']}"
29,"{'func name': 'mc_summary', 'comments': '', 'stemmed comments': []}"
30,"{'func name': 'select_bandwidth', 'comments': 'Selects bandwidth for a selection rule bw\n\nthis is a wrapper around existing bandwidth selection rules\n\nParameters ---------- x : array_like Array for which to get the bandwidth bw : str name of bandwidth selection rule, currently supported are: %s kernel : not used yet\n##### Returns\n* **bw **: float\n    The estimate of the bandwidth\n\n', 'stemmed comments': ['x', 'array_lik', 'array', 'exist', 'use', 'paramet', 'yet', 'return', 'current', 'estim', 'around', 'select', 'name', 'str', '%', 'float', 'bandwidth', 'get', 'kernel', 'the', 'support', 'bw', 'wrapper', 'rule']}"
31,"{'func name': 'bkfilter', 'comments': 'Filter a time series using the Baxter-King bandpass filter.\n\nParameters ---------- x : array_like A 1 or 2d ndarray. If 2d, variables are assumed to be in columns. low : float Minimum period for oscillations, ie., Baxter and King suggest that the Burns-Mitchell U.S. business cycle has 6 for quarterly data and 1.5 for annual data. high : float Maximum period for oscillations BK suggest that the U.S. business cycle has 32 for quarterly data and 8 for annual data. K : int Lead-lag length of the filter. Baxter and King propose a truncation length of 12 for quarterly data and 3 for annual data.\n##### Returns\n* **the weights a[j] are computed **: \n\n* **and theta is a normalizing constant **: \n\n* **Baxter, M. and R. G. King. ""Measuring Business Cycles**: Approximate\n    Band-Pass Filters for Economic Time Series."" *Review of Economics and\n    Statistics*, 1999, 81(4), 575-593.\n\n* **.. plot**: \n\n', 'stemmed comments': ['comput', 'A', 'bandpass', 'baxterk', 'x', '15', 'length', 'array_lik', 'normal', '8', 'minimum', 'high', '3', ']', 'use', 'US', '6', 'K', 'filter', 'oscil', 'suggest', 'paramet', 'seri', 'baxter', '12', 'return', 'propos', 'truncat', 'approxim', '4', 'measur', 'cycl', 'weight', 'assum', 'data', 'int', 'ie', 'annual', 'econom', 'king', 'plot', '575593', 'theta', 'burnsmitchel', '1999', 'R', 'statist', 'constant', 'time', '1', 'variabl', 'float', 'j', 'quarterli', 'leadlag', '32', '[', 'low', 'If', '2d', 'ndarray', 'BK', '81', 'G', 'period', 'review', 'busi', 'M', 'column', 'maximum']}"
32,"{'func name': '_show_legend', 'comments': 'Utility function to show legend.\n\n\n', 'stemmed comments': ['show', 'function', 'util', 'legend']}"
33,"{'func name': '_zero_triband', 'comments': 'Explicitly zero out unused elements of a real symmetric banded matrix.\n\nINPUTS: a\n\n -- a real symmetric banded matrix (either upper or lower hald) lower\n\n -- if True, a is assumed to be the lower half\n', 'stemmed comments': ['hald', 'unus', 'matrix', 'symmetr', 'input', 'element', 'band', 'explicitli', 'true', 'either', 'assum', 'half', 'real', 'lower', 'upper', 'zero']}"
34,"{'func name': 'burg', 'comments': 'Estimate AR parameters using Burg technique.\n\nParameters ---------- endog : array_like or SARIMAXSpecification Input time series array, assumed to be stationary. ar_order : int, optional Autoregressive order. Default is 0. demean : bool, optional Whether to estimate and remove the mean from the process prior to fitting the autoregressive coefficients.\n##### Returns\n* **parameters **: SARIMAXParams object\n    Contains the parameter estimates from the final iteration.\n\n* **other_results **: Bunch\n    Includes one component, `spec`, which is the `SARIMAXSpecification`\n    instance corresponding to the input arguments.\n\n', 'stemmed comments': ['coeffici', 'array_lik', 'object', 'array', 'use', 'bool', 'compon', 'seri', 'paramet', 'AR', 'correspond', 'return', 'argument', 'iter', 'order', 'estim', 'assum', 'int', 'burg', 'prior', 'ar_ord', 'stationari', 'endog', 'final', 'instanc', 'includ', 'techniqu', 'contain', 'time', 'demean', 'sarimaxspecif', 'option', 'autoregress', 'mean', 'whether', 'spec', 'remov', 'bunch', 'fit', 'sarimaxparam', 'one', '0', 'default', 'process', 'other_result', 'input']}"
35,"{'func name': 'add_indep', 'comments': 'construct array with independent columns\n\nx is either iterable (list, tuple) or instance of ndarray or a subclass of it.\n\nIf x is an ndarray, then each column is assumed to represent a variable with observations in rows.\n', 'stemmed comments': ['instanc', 'If', 'construct', 'x', 'ndarray', 'observ', 'assum', 'either', 'variabl', 'array', 'tupl', 'repres', 'row', 'independ', 'subclass', 'column', 'list', 'iter']}"
36,"{'func name': 'cffilter', 'comments': 'Christiano Fitzgerald asymmetric, random walk filter.\n\nParameters ---------- x : array_like The 1 or 2d array to filter. If 2d, variables are assumed to be in columns. low : float Minimum period of oscillations. Features below low periodicity are filtered out. Default is 6 for quarterly data, giving a 1.5 year periodicity. high : float Maximum period of oscillations. Features above high periodicity are filtered out. Default is 32 for quarterly data, giving an 8 year periodicity. drift : bool Whether or not to remove a trend from the data. The trend is estimated as np.arange(nobs)*(x[-1]\n\n- x[0])/(len(x)-1).\n##### Returns\n* **cycle **: array_like\n    The features of x between the periodicities low and high.\n\n* **trend **: array_like\n    The trend in the data with the cycles removed.\n\n* **.. plot**: \n\n', 'stemmed comments': ['plot', 'x', '15', 'year', 'array_lik', '8', 'minimum', 'high', 'christiano', ']', 'array', 'bool', '6', 'trend', 'filter', 'oscil', 'len', 'paramet', 'give', 'return', 'cycl', 'estim', 'walk', 'assum', 'data', 'nob', 'nparang', 'featur', '/', '1', 'variabl', 'float', 'whether', 'remov', 'quarterli', 'the', 'drift', '32', '[', '0', 'low', 'default', 'If', '2d', 'asymmetr', 'random', 'fitzgerald', 'period', 'column', 'maximum']}"
37,"{'func name': 'nct_kurt_bug', 'comments': 'test for incorrect kurtosis of nct\n\nD. Hogben, R. S. Pinkham, M. B. Wilk: The Moments of the Non-Central t-DistributionAuthor(s): Biometrika, Vol. 48, No. 3/4 (Dec., 1961), pp. 465-468\n', 'stemmed comments': ['B', '48', 'wilk', '3/4', 'moment', 'tdistributionauthor', 'incorrect', 'S', '465468', '1961', 'vol', 'pp', 'R', 'No', 'test', 'noncentr', 'the', 'hogben', 'kurtosi', 'nct', 'D', 'biometrika', 'dec', 'M', 'pinkham']}"
38,"{'func name': 'chi2', 'comments': 'Cost function.\n\n\n', 'stemmed comments': ['function', 'cost']}"
39,"{'func name': 'c_sjt', 'comments': '', 'stemmed comments': []}"
40,"{'func name': 'setup', 'comments': '', 'stemmed comments': []}"
41,"{'func name': 'reset_randomstate', 'comments': 'Fixture that set the global RandomState to the fixed seed 1\n\nNotes ----- Used by passing as an argument to the function that uses the global RandomState\n\ndef test_some_plot(reset_randomstate): <test code>\n', 'stemmed comments': ['note', 'global', 'use', 'set', 'argument', '<', 'def', 'function', '1', 'test', 'code', 'reset_randomst', 'randomst', '>', 'test_some_plot', 'fix', 'pass', 'seed', 'fixtur']}"
42,"{'func name': 'cochrans_q', 'comments': ""Cochran's Q test for identical binomial proportions.\n\nParameters ---------- x : array_like, 2d (N, k) data with N cases and k variables return_object : bool Return values as bunch instead of as individual values.\n##### Returns\n* **statistic **: float\n   test statistic\n\n* **pvalue **: float\n   pvalue from the chisquare distribution\n\n* **https**: //en.wikipedia.org/wiki/Cochran_test\n\n"", 'stemmed comments': ['N', 'k', 'x', 'array_lik', 'bool', 'paramet', 'cochran', 'return', 's', 'Q', 'valu', '//enwikipediaorg/wiki/cochran_test', 'data', 'binomi', 'proport', 'distribut', 'return_object', 'case', 'statist', 'variabl', 'float', 'pvalu', 'test', 'instead', 'bunch', 'chisquar', 'ident', '2d', 'individu', 'http']}"
43,"{'func name': 'groupmean_d', 'comments': 'groupmeans using dummy variables\n\nParameters ---------- x : array_like, ndim data array, tested for 1,2 and 3 dimensions d : ndarray, 1d dummy variable, needs to have the same length as x in axis 0.\n##### Returns\n* **groupmeans **: ndarray, ndim-1\n    means for each group along axis 0, the levels\n    of the groups are the last axis\n\n', 'stemmed comments': ['length', 'x', 'array_lik', 'group', '3', 'array', 'use', 'paramet', 'axi', '12', 'return', 'dummi', 'ndim', 'level', 'data', 'last', 'variabl', '1d', 'test', 'mean', 'need', '0', 'along', 'ndarray', 'ndim1', 'dimens', 'groupmean']}"
44,"{'func name': 'wald_test_noncent_generic', 'comments': 'noncentrality parameter for a wald test\n\nThe null hypothesis is ``diff = r_matrix @ params\n\n- value = 0``\n\nParameters ---------- params : ndarray parameters of the model at which to evaluate noncentrality. This can be estimated parameters or parameters under an alternative. r_matrix : ndarray Restriction matrix or contrasts for the Null hypothesis\n\nvalue : None or ndarray Value of the linear combination of parameters under the null hypothesis. If value is None, then it will be replace by zero. cov_params : ndarray covariance matrix of the parameter estimates diff : None or ndarray If diff is not None, then it will be used instead of ``diff = r_matrix @ params\n\n- value`` joint : bool If joint is True, then the noncentrality parameter for the joint hypothesis will be returned. If joint is True, then an array of noncentrality parameters will be returned, where elements correspond to rows of the restriction matrix. This correspond to the `t_test` in models and is not a quadratic form.\n##### Returns\n* **nc **: float or ndarray\n    Noncentrality parameter for Wald tests, correspondig to `wald_test`\n    or `t_test` depending on whether `joint` is true or not.\n    It needs to be divided by nobs to obtain effect size.\n\n* **Status **: experimental, API will likely change\n\n', 'stemmed comments': ['experiment', 'quadrat', 'null', 'hypothesi', 'contrast', 't_test', 'restrict', 'array', 'use', 'r_matrix', 'combin', 'bool', 'paramet', 'api', 'correspond', 'return', 'matrix', 'thi', 'valu', 'estim', 'evalu', 'wald_test', 'model', '=', 'correspondig', 'nob', 'statu', 'diff', 'cov_param', 'like', 'true', 'altern', 'element', 'divid', 'float', 'effect', 'test', 'noncentr', 'instead', 'joint', 'whether', 'need', 'the', '@', 'depend', 'zero', 'covari', '0', 'none', 'If', 'ndarray', 'nc', 'form', 'It', 'obtain', 'linear', 'chang', 'size', 'wald', 'param', 'row', 'replac']}"
45,"{'func name': '_name_levels', 'comments': '', 'stemmed comments': []}"
46,"{'func name': 'transform_tev', 'comments': 't-EV model of Demarta and McNeil 2005\n\nrestrictions:\n\n- rho in (-1,1)\n\n- x > 0\n', 'stemmed comments': ['x', 'mcneil', 'restrict', '2005', 'model', 'rho', 'tev', 'demarta', '>', '11', '0']}"
47,"{'func name': 'yule_walker_acov', 'comments': 'Estimate AR(p) parameters from acovf using Yule-Walker equation.\n\nParameters ---------- acov : array_like, 1d auto-covariance order : int, optional The order of the autoregressive process.\n\nDefault is 1. inv : bool If inv is True the inverse of R is also returned.\n\nDefault is False.\n##### Returns\n* **rho **: ndarray\n    The estimated autoregressive coefficients\n\n* **Rinv **: ndarray\n    inverse of the Toepliz matrix\n\n', 'stemmed comments': ['fals', 'coeffici', 'array_lik', 'invers', 'use', 'bool', 'p', 'yulewalk', 'paramet', 'AR', 'acov', 'equat', 'autocovari', 'inv', 'order', 'return', 'matrix', 'estim', 'int', 'acovf', 'R', 'true', '1', 'rho', '1d', 'option', 'autoregress', 'also', 'the', 'default', 'If', 'ndarray', 'process', 'toepliz', 'rinv']}"
48,"{'func name': 'kernel_covariance', 'comments': 'Use kernel averaging to estimate a multivariate covariance function.\n\nThe goal is to estimate a covariance function C(x, y) = cov(Z(x), Z(y)) where x, y are vectors in R^p (e.g. representing locations in time or space), and Z(.) represents a multivariate process on R^p.\n\nThe data used for estimation can be observed at arbitrary values of the position vector, and there can be multiple independent observations from the process.\n\nParameters ---------- exog : array_like The rows of exog are realizations of the process obtained at specified points. loc : array_like The rows of loc are the locations (e.g. in space or time) at which the rows of exog are observed. groups : array_like The values of groups are labels for distinct independent copies of the process. kernel : MultivariateKernel instance, optional An instance of MultivariateKernel, defaults to GaussianMultivariateKernel. bw : array_like or scalar A bandwidth vector, or bandwidth multiplier.\n\nIf a 1d array, it contains kernel bandwidths for each component of the process, and must have length equal to the number of columns of exog.\n\nIf a scalar, bw is a bandwidth multiplier used to adjust the default bandwidth; if None, a default bandwidth is used.\n##### Returns\n* **.. [1] Genton M, W Kleiber (2015).  Cross covariance functions for\n    multivariate geostatics.  Statistical Science 30(2).\n    https**: //arxiv.org/pdf/1507.08017.pdf\n\n', 'stemmed comments': ['adjust', 'group', 'array', 'multipl', 'independ', ';', 'loc', 'C', 'eg', 'genton', 'function', 'space', 'instanc', '1d', 'option', 'averag', 'must', '[', 'multipli', 'multivariatekernel', 'specifi', 'M', 'copi', 'scalar', 'cross', 'use', 'compon', 'cov', 'vector', 'paramet', 'return', 'equal', 'estim', 'geostat', 'gaussianmultivariatekernel', 'multivari', 'label', 'statist', 'time', '30', 'bandwidth', 'process', 'obtain', 'exog', 'distinct', 'number', 'W', 'length', 'realiz', 'goal', 'valu', 'An', 'data', 'point', '//arxivorg/pdf/150708017pdf', 'kernel', 'repres', 'covari', 'default', 'If', 'none', 'http', 'column', 'r^p', 'A', '2015', 'x', '2', 'array_lik', ']', '=', 'locat', 'kleiber', 'contain', '1', 'the', 'Z', 'arbitrari', 'observ', 'bw', 'posit', 'row', 'scienc']}"
49,"{'func name': 'plot_corr_grid', 'comments': 'Create a grid of correlation plots.\n\nThe individual correlation plots are assumed to all have the same variables, axis labels can be specified only once.\n\nParameters ---------- dcorrs : list or iterable of ndarrays List of correlation matrices. titles : list[str], optional List of titles for the subplots.\n\nBy default no title are shown. ncols : int, optional Number of columns in the subplot grid.\n\nIf not given, the number of columns is determined automatically. normcolor : bool or tuple, optional If False (default), then the color coding range corresponds to the range of `dcorr`.\n\nIf True, then the color range is normalized to (-1, 1).\n\nIf this is a tuple of two numbers, then they define the range for the color bar. xnames : list[str], optional Labels for the horizontal axis.\n\nIf not given (None), then the matplotlib defaults (integers) are used.\n\nIf it is an empty list, [], then no ticks and labels are added. ynames : list[str], optional Labels for the vertical axis.\n\nWorks the same way as `xnames`. If not given, the same names as for `xnames` are re-used. fig : Figure, optional If given, this figure is simply returned.\n\nOtherwise a new figure is created. cmap : str or Matplotlib Colormap instance, optional The colormap for the plot.\n\nCan be any valid Matplotlib Colormap instance or name.\n##### Returns\n* **..plot **: \n\n', 'stemmed comments': ['fals', 'grid', 'plot', 'normal', 'reus', 'ncol', 'tick', 'integ', ']', 'use', 'dcorr', 'bool', 'determin', 'paramet', 'defin', 'axi', 'By', 'correspond', 'matplotlib', 'iter', 'return', 'matric', 'vertic', 'bar', 'valid', 'assum', 'two', 'int', 'normcolor', 'otherwis', 'can', 'simpli', 'name', 'new', 'label', 'figur', 'instanc', 'ad', 'correl', 'str', 'color', 'true', '1', 'variabl', 'option', 'code', 'rang', 'colormap', 'work', 'the', 'creat', 'shown', '[', 'number', 'default', 'If', 'titl', 'ndarray', 'xname', 'none', 'empti', 'yname', 'automat', 'fig', 'subplot', 'cmap', 'individu', 'horizont', 'given', 'way', 'tupl', 'specifi', 'column', 'list']}"
50,"{'func name': 'maxabsrel', 'comments': '', 'stemmed comments': []}"
51,"{'func name': 'get_robustcov_results', 'comments': ""create new results instance with robust covariance as default\n\nParameters ---------- cov_type : str the type of robust sandwich estimator to use. see Notes below use_t : bool If true, then the t distribution is used for inference. If false, then the normal distribution is used. kwds : depends on cov_type Required or optional arguments for robust covariance calculation. see Notes below\n##### Returns\n* **results **: results instance\n    This method creates a new results instance with the requested\n    robust covariance as the default covariance of the parameters.\n    Inferential statistics like p-values and hypothesis tests will be\n    based on this covariance matrix.\n\n* **Warning**: Some of the options and defaults in cov_kwds may be changed in a\n\n* **currently available**: \n\n* **- 'HC0', 'HC1', 'HC2', 'HC3' and no keyword arguments**: heteroscedasticity robust covariance\n\n* **- 'HAC' and keywords\n    - `maxlag` integer (required) **: number of lags to use\n    - `kernel` callable or str (optional)\n\n* **- 'cluster' and required keyword `groups`, integer group indicator\n    - `groups` array_like, integer (required) **: index of clusters or groups\n    - `use_correction` bool (optional)\n\n* **- 'hac-groupsum' Driscoll and Kraay, heteroscedasticity and\n    autocorrelation robust standard errors in panel data\n    keywords\n    - `time` array_like (required) **: index of time periods\n    - `maxlag` integer (required)\n\n* **- 'hac-panel' heteroscedasticity and autocorrelation robust standard\n    errors in panel data.\n    The data needs to be sorted in this case, the time series for\n    each panel unit or cluster need to be stacked. The membership to\n    a timeseries of an individual or group can be either specified by\n    group indicators or by increasing time periods.\n    keywords\n    - either `groups` or `time` **: array_like (required)\n      `groups`\n\n* **Reminder**: \n\n* **TODO**: Currently there is no check for extra or misspelled keywords,\n\n"", 'stemmed comments': ['warn', 'group', 'cluster', 'todo', 'callabl', 'may', 'membership', 'base', 'distribut', 'calcul', 'instanc', 'kraay', 'like', 'use_correct', 'check', 'option', 'test', 'index', 'indic', 'specifi', 'kwd', 'see', 'note', 'heteroscedast', 'use', 'paramet', 'argument', 'return', 'method', 'hac', 'estim', 'maxlag', 'lag', 'result', 'inferenti', 'statist', 'true', 'str', 'time', 'pvalu', 'infer', 'chang', 'misspel', 'number', 'stack', 'sandwich', 'fals', 'request', 'hypothesi', 'cov_kwd', 'keyword', 'integ', 'cov_typ', 'some', 'current', 'hacgroupsum', 'data', 'autocorrel', 'case', 'type', 'panel', 'hc3', 'kernel', 'use_t', 'creat', 'covari', 'robust', 'sort', 'increas', 'default', 'If', 'hc2', 'period', 'hacpanel', 'normal', 'array_lik', 'extra', 'bool', 'error', 'seri', 'avail', 'driscol', 'requir', 'matrix', 'hc1', 'thi', 'standard', 'timeseri', 'remind', 'new', 'unit', 'need', 'depend', 'hc0', 'the', 'either', 'individu']}"
52,"{'func name': 'split', 'comments': 'For each arg return a train and test subsets defined by indexes provided in train_indexes and test_indexes\n\n\n', 'stemmed comments': ['for', 'test', 'subset', 'index', 'provid', 'train_index', 'test_index', 'defin', 'train', 'return', 'arg']}"
53,"{'func name': 'handle_data', 'comments': '', 'stemmed comments': []}"
54,"{'func name': '_is_recarray', 'comments': 'Returns true if data is a recarray\n\n\n', 'stemmed comments': ['recarray', 'return', 'data', 'true']}"
55,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
56,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
57,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
58,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
59,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
60,"{'func name': 'load', 'comments': 'Load the China smoking/lung cancer data and return a Dataset class.\n\nParameters ---------- as_pandas : bool Flag indicating whether to return pandas DataFrames and Series or numpy recarrays and arrays.\n\nIf True, returns pandas.\n##### Returns\n* **Dataset instance**: See DATASET_PROPOSAL.txt for more information.\n\n', 'stemmed comments': ['see', 'load', 'array', 'bool', 'paramet', 'seri', 'dataset', 'return', 'class', 'data', 'instanc', 'smoking/lung', 'panda', 'true', 'inform', 'china', 'numpi', 'whether', 'dataset_proposaltxt', 'flag', 'recarray', 'as_panda', 'If', 'cancer', 'datafram', 'indic']}"
61,"{'func name': 'load', 'comments': 'Load the copper data and returns a Dataset class.\n\nParameters ---------- as_pandas : bool Flag indicating whether to return pandas DataFrames and Series or numpy recarrays and arrays.\n\nIf True, returns pandas.\n##### Returns\n* **Dataset instance**: See DATASET_PROPOSAL.txt for more information.\n\n', 'stemmed comments': ['see', 'load', 'array', 'bool', 'paramet', 'seri', 'dataset', 'return', 'class', 'data', 'instanc', 'panda', 'true', 'inform', 'numpi', 'whether', 'dataset_proposaltxt', 'flag', 'recarray', 'as_panda', 'If', 'copper', 'datafram', 'indic']}"
62,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
63,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
64,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
65,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
66,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
67,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
68,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
69,"{'func name': '__str__', 'comments': '', 'stemmed comments': []}"
70,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
71,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
72,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
73,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
74,"{'func name': '__str__', 'comments': '', 'stemmed comments': []}"
75,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
76,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
77,"{'func name': '__str__', 'comments': '', 'stemmed comments': []}"
78,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
79,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
80,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
81,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
82,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
83,"{'func name': 'dates_from_range', 'comments': ""Turns a sequence of date strings and returns a list of datetime.\n\nParameters ---------- start : str The first abbreviated date, for instance, '1965q1' or '1965m1' end : str, optional The last abbreviated date if length is None. length : int, optional The length of the returned array of end is None.\n\nExamples -------- >>> import statsmodels.api as sm >>> import pandas as pd >>> dates = pd.date_range('1960m1', length=nobs)\n##### Returns\n* **date_list **: ndarray\n    A list of datetime types.\n\n"", 'stemmed comments': ['1965m1', 'A', 'length', 'sm', 'first', 'array', 'start', 'string', 'paramet', 'return', '1960m1', 'pd', 'int', 'last', 'import', '=', 'exampl', 'instanc', 'type', 'date_list', 'panda', 'str', 'statsmodelsapi', 'end', 'sequenc', 'option', 'datetim', 'the', '>', 'length=nob', 'turn', 'none', 'ndarray', 'abbrevi', 'pddate_rang', 'date', 'list', '1965q1']}"
84,"{'func name': 'array_like', 'comments': '', 'stemmed comments': []}"
85,"{'func name': 'nottest', 'comments': '', 'stemmed comments': []}"
86,"{'func name': 'get_exog', 'comments': ""Returns an exog array with correlations determined by cor_length. The covariance matrix of exog will have (asymptotically, as :math:'N\\to\\inf') .. math:: Cov[i,j] = \\exp(-|i-j| / cor_length)\n\nHigher cor_length makes the problem more ill-posed, and easier to screw up with noise. BEWARE:\n\nWith very long correlation lengths, you often get a singular KKT matrix (during the l1_cvxopt_cp fit)\n"", 'stemmed comments': ['length', '\\exp', 'screw', ']', 'array', 'determin', 'cov', 'math', 'make', 'problem', 'return', 'matrix', 'with', 'illpos', 'higher', 'often', '=', '|ij|', 'nois', 'correl', '/', 'kkt', 'cor_length', 'j', 'singular', 'get', 'the', 'fit', 'covari', '[', 'n\\to\\inf', 'exog', 'asymptot', 'long', 'easier', 'l1_cvxopt_cp', 'bewar']}"
87,"{'func name': 'density_orthopoly', 'comments': '', 'stemmed comments': []}"
88,"{'func name': 'dentonm', 'comments': 'Modified Denton\'s method to convert low-frequency to high-frequency data.\n\nUses proportionate first-differences as the penalty function.\n\nSee notes.\n\nParameters ---------- indicator : array_like A low-frequency indicator series.\n\nIt is assumed that there are no pre-sample indicators.\n\nIe., the first indicators line up with the first benchmark. benchmark : array_like The higher frequency benchmark.\n\nA 1d or 2d data series in columns. If 2d, then M series are assumed. freq : str {""aq"",""qm"", ""other""} The frequency to use in the conversion.\n\n* ""aq""\n\n- Benchmarking an annual series to quarterly. * ""mq""\n\n- Benchmarking a quarterly series to monthly. * ""other""\n\n- Custom stride.\n\nA kwarg, k, must be supplied. **kwargs Additional keyword argument. For example:\n\n* k, an int, the number of high-frequency observations that sum to make an aggregate low-frequency observation. `k` is used with `freq` == ""other"".\n##### Returns\n* **transformed **: ndarray\n    The transformed series.\n\n* **Bloem, A.M, Dippelsman, R.J. and Maehle, N.O.  2001 Quarterly National\n    Accounts Manual--Concepts, Data Sources, and Compilation. IMF.\n    http**: //www.imf.org/external/pubs/ft/qna/2000/Textbook/index.htm\n\n* **Denton, F.T. 1971. ""Adjustment of monthly or quarterly series to annual\n    totals**: an approach based on quadratic minimization."" Journal of the\n    American Statistical Association. 99-102.\n\n', 'stemmed comments': ['adjust', 'quadrat', 'denton', 'frequenc', 'sourc', '==', 's', 'addit', 'imf', 'RJ', 'higher', 'function', 'base', 'firstdiffer', 'convert', 'highfrequ', 'modifi', '1d', 'approach', 'concept', 'must', 'associ', 'maehl', 'NO', 'indic', 'exampl', 'M', 'see', 'note', 'use', 'paramet', 'argument', 'return', 'method', 'manual', 'minim', 'for', 'aq', 'kwarg', 'statist', 'str', 'lowfrequ', 'presampl', '{', 'aggreg', 'proportion', 'journal', '2001', '1971', 'nation', 'number', 'total', 'suppli', 'k', 'monthli', 'keyword', 'transform', 'first', 'mq', 'assum', 'custom', 'data', 'int', 'annual', 'line', 'convers', 'benchmark', 'compil', 'account', 'sum', 'penalti', '99102', 'dippelsman', 'If', 'bloem', 'http', 'column', 'FT', 'A', 'array_lik', 'AM', 'seri', 'make', 'Ie', 'quarterli', 'the', '}', '//wwwimforg/external/pubs/ft/qna/2000/textbook/indexhtm', 'american', 'observ', '2d', 'ndarray', 'freq', 'It', 'stride', 'qm']}"
89,"{'func name': 'DescStat', 'comments': 'Returns an instance to conduct inference on descriptive statistics via empirical likelihood.  See DescStatUV and DescStatMV for more information.\n\nParameters ---------- endog : ndarray Array of data\n##### Returns\n', 'stemmed comments': ['instanc', 'see', 'statist', 'descript', 'descstatuv', 'ndarray', 'likelihood', 'inform', 'via', 'data', 'conduct', 'endog', 'array', 'infer', 'empir', 'paramet', 'descstatmv', 'return']}"
90,"{'func name': 'sign_test', 'comments': 'Signs test.\n\nParameters ---------- samp : array_like 1d array. The sample for which you want to perform the signs test. mu0 : float See Notes for the definition of the sign test. mu0 is 0 by default, but it is common to set it to the median.\n##### Returns\n', 'stemmed comments': ['see', 'note', 'array_lik', 'array', 'set', 'sampl', 'paramet', 'mu0', 'return', 'want', 'definit', 'float', '1d', 'test', 'the', 'median', 'samp', '0', 'default', 'sign', 'common', 'perform']}"
91,"{'func name': 'descstats', 'comments': ""Prints descriptive statistics for one or multiple variables.\n\nParameters ---------- data: numpy array `x` is the data\n\nv: list, optional A list of the column number or field names (for a recarray) of variables. Default is all columns.\n\naxis: 1 or 0 axis order of data.\n\nDefault is 0 for column-ordered data.\n\nExamples -------- >>> descstats(data.exog,v=['x_1','x_2','x_3'])\n"", 'stemmed comments': ['A', 'x', 'x_3', 'array', ']', 'multipl', 'v', 'paramet', 'axi', 'order', 'data', 'exampl', 'name', 'field', 'columnord', 'statist', 'v=', 'x_2', '1', 'variabl', 'numpi', 'option', '>', 'recarray', 'one', 'descstat', '0', '[', 'default', 'descript', 'x_1', 'dataexog', 'print', 'number', 'column', 'list']}"
92,"{'func name': 'func1', 'comments': 'made up example with sin, square\n\n\n', 'stemmed comments': ['sin', 'exampl', 'made', 'squar']}"
93,"{'func name': 'breaks_cusumolsresid', 'comments': 'Cusum test for parameter stability based on ols residuals.\n\nParameters ---------- resid : ndarray An array of residuals from an OLS estimation. ddof : int The number of parameters in the OLS estimation, used as degrees of freedom correction for error variance.\n##### Returns\n* **sup_b **: float\n    The test statistic, maximum of absolute value of scaled cumulative OLS\n    residuals.\n\n* **pval **: float\n    Probability of observing the data under the null hypothesis of no\n    structural change, based on asymptotic distribution which is a Brownian\n    Bridge\n\n* **crit**: list\n    The tabulated critical values, for alpha = 1%, 5% and 10%.\n\n* **Tested against R**: structchange.\n\n* **Not clear**: Assumption 2 in Ploberger, Kramer assumes that exog x have\n\n* **Econometrica 60, no. 2 (March 1992)**: 271-285.\n\n', 'stemmed comments': ['cumul', 'null', 'hypothesi', 'structur', '5', 'stabil', '2', 'x', 'cusum', 'resid', 'array', 'use', 'probabl', 'error', '60', 'paramet', 'clear', 'return', 'kramer', 'critic', 'ploberg', 'valu', 'estim', 'An', 'scale', 'absolut', 'int', 'sup_b', 'data', '=', 'pval', 'assum', '1992', '271285', 'bridg', 'varianc', 'degre', 'base', 'distribut', 'residu', 'crit', 'R', 'statist', 'not', 'alpha', 'correct', '1', 'structchang', 'tabul', 'float', '%', 'ddof', 'test', 'ol', 'freedom', '10', 'the', 'econometrica', 'observ', 'ndarray', 'chang', 'exog', 'asymptot', 'assumpt', 'number', 'brownian', 'list', 'maximum', 'march']}"
94,"{'func name': 'schout2contank', 'comments': '', 'stemmed comments': []}"
95,"{'func name': '_grass_opt', 'comments': 'Minimize a function on a Grassmann manifold.\n\nParameters ---------- params : array_like Starting value for the optimization. fun : function The function to be minimized. grad : function The gradient of fun. maxiter : int The maximum number of iterations. gtol : float Convergence occurs when the gradient norm falls below this value.\n##### Returns\n* **params **: array_like\n    The minimizing value for the objective function.\n\n* **fval **: float\n    The smallest achieved value of the objective function.\n\n* **cnvrg **: bool\n    True if the algorithm converged to a limit point.\n\n* **http**: //math.mit.edu/~edelman/publications/geometry_of_algorithms.pdf\n\n', 'stemmed comments': ['//mathmitedu/~edelman/publications/geometry_of_algorithmspdf', 'array_lik', 'grad', 'algorithm', 'object', 'fval', 'start', 'bool', 'limit', 'paramet', 'fun', 'achiev', 'return', 'iter', 'manifold', 'valu', 'minim', 'int', 'function', 'point', 'maxit', 'true', 'gtol', 'float', 'cnvrg', 'smallest', 'the', 'optim', 'gradient', 'occur', 'grassmann', 'converg', 'http', 'param', 'fall', 'norm', 'number', 'maximum']}"
96,"{'func name': '_check_at_is_all', 'comments': '', 'stemmed comments': []}"
97,"{'func name': '_validate_l1_method', 'comments': 'As of 0.10.0, the supported values for `method` in `fit_regularized` are ""l1"" and ""l1_cvxopt_cp"".  If an invalid value is passed, raise with a helpful error message\n\nParameters ---------- method : str\n', 'stemmed comments': ['If', 'invalid', 'valu', 'As', 'fit_regular', 'pass', 'rais', 'help', 'str', 'messag', '0100', 'error', 'paramet', 'support', 'l1', 'l1_cvxopt_cp', 'method']}"
98,"{'func name': 'distance_correlation', 'comments': ""Distance correlation.\n\nCalculate the empirical distance correlation as described in [1]_. This statistic is analogous to product-moment correlation and describes the dependence between `x` and `y`, which are random vectors of arbitrary length. The statistics' values range between 0 (implies independence) and 1 (implies complete dependence).\n\nParameters ---------- x : array_like, 1-D or 2-D If `x` is 1-D than it is assumed to be a vector of observations of a single random variable. If `x` is 2-D than the rows should be observations and the columns are treated as the components of a random vector, i.e., each column represents a different component of the random vector `x`. y : array_like, 1-D or 2-D Same as `x`, but only the number of observation has to match that of `x`. If `y` is 2-D note that the number of columns of `y` (i.e., the number of components in the random vector) does not need to match the number of columns in `x`.\n##### Returns\n"", 'stemmed comments': ['note', 'length', 'x', 'array_lik', 'singl', ']', 'compon', 'match', 'vector', 'paramet', 'independ', 'return', '_', 'thi', 'valu', 'assum', 'ie', '1D', 'calcul', 'productmo', 'statist', 'analog', 'correl', '1', 'variabl', 'rang', 'repres', 'distanc', 'complet', 'depend', 'the', 'need', '[', 'arbitrari', '0', 'observ', '2D', 'If', 'differ', 'same', 'random', 'describ', 'empir', 'impli', 'row', 'number', 'column', 'treat']}"
99,"{'func name': '_helper_fit_partition', 'comments': 'handles the model fitting for each machine. NOTE: this is primarily handled outside of DistributedModel because joblib cannot handle class methods.\n\nParameters ---------- self : DistributedModel class instance An instance of DistributedModel. pnum : scalar index of current partition. endog : array_like endogenous data for current partition. exog : array_like exogenous data for current partition. fit_kwds : dict-like Keywords needed for the model fitting. init_kwds_e : dict-like Additional init_kwds to add for each partition.\n##### Returns\n', 'stemmed comments': ['note', 'keyword', 'array_lik', 'partit', 'paramet', 'return', 'method', 'class', 'current', 'addit', 'An', 'endogen', 'data', 'model', 'self', 'init_kwds_', 'handl', 'endog', 'exogen', 'instanc', 'outsid', 'pnum', 'add', 'need', 'joblib', 'fit_kwd', 'index', 'fit', 'dictlik', 'init_kwd', 'machin', 'exog', 'distributedmodel', 'primarili', 'scalar']}"
100,"{'func name': '_endog_gen', 'comments': 'partitions endog data\n\n\n', 'stemmed comments': ['partit', 'endog', 'data']}"
101,"{'func name': 'remove_parameters', 'comments': 'Parameters ---------- docstring : str The docstring to modify. parameters : str, list[str] The names of the parameters to remove.\n\n\n##### Returns\n', 'stemmed comments': ['name', 'str', ']', 'modifi', 'return', 'docstr', 'remov', 'the', 'paramet', '[', 'list']}"
102,"{'func name': 'dot_plot', 'comments': 'Dot plotting (also known as forest and blobbogram).\n\nProduce a dotplot similar in style to those in Cleveland\'s ""Visualizing Data"" book ([1]_).\n\nThese are also known as ""forest plots"".\n\nParameters ---------- points : array_like The quantitative values to be plotted as markers. intervals : array_like The intervals to be plotted around the points.\n\nThe elements of `intervals` are either scalars or sequences of length 2.\n\nA scalar indicates the half width of a symmetric interval.\n\nA sequence of length 2 contains the left and right half-widths (respectively) of a nonsymmetric interval.\n\nIf None, no intervals are drawn. lines : array_like A grouping variable indicating which points/intervals are drawn on a common line.\n\nIf None, each point/interval appears on its own line. sections : array_like A grouping variable indicating which lines are grouped into sections.\n\nIf None, everything is drawn in a single section. styles : array_like A grouping label defining the plotting style of the markers and intervals. marker_props : dict A dictionary mapping style codes (the values in `styles`) to dictionaries defining key/value pairs to be passed as keyword arguments to `plot` when plotting markers.\n\nUseful keyword arguments are ""color"", ""marker"", and ""ms"" (marker size). line_props : dict A dictionary mapping style codes (the values in `styles`) to dictionaries defining key/value pairs to be passed as keyword arguments to `plot` when plotting interval lines.\n\nUseful keyword arguments are ""color"", ""linestyle"", ""solid_capstyle"", and ""linewidth"". split_names : str If not None, this is used to split the values of `lines` into substrings that are drawn in the left and right margins, respectively.\n\nIf None, the values of `lines` are drawn in the left margin. section_order : array_like The section labels in the order in which they appear in the dotplot. line_order : array_like The line labels in the order in which they appear in the dotplot. stacked : bool If True, when multiple points or intervals are drawn on the same line, they are offset from each other. styles_order : array_like If stacked=True, this is the order in which the point styles on a given line are drawn from top to bottom (if horizontal is True) or from left to right (if horizontal is False).\n\nIf None (default), the order is lexical. striped : bool If True, every other line is enclosed in a shaded box. horizontal : bool If True (default), the lines are drawn horizontally, otherwise they are drawn vertically. show_names : str Determines whether labels (names) are shown in the left and/or right margins (top/bottom margins if `horizontal` is True). If `both`, labels are drawn in both margins, if \'left\', labels are drawn in the left or top margin.\n\nIf `right`, labels are drawn in the right or bottom margin. fmt_left_name : callable The left/top margin names are passed through this function before drawing on the plot. fmt_right_name : callable The right/bottom marginnames are passed through this function before drawing on the plot. show_section_titles : bool or None If None, section titles are drawn only if there is more than one section.\n\nIf False/True, section titles are never/always drawn, respectively. ax : matplotlib.axes The axes on which the dotplot is drawn.\n\nIf None, a new axes is created.\n##### Returns\n* **fig **: Figure\n    The figure given by `ax.figure` or a new instance.\n\n* **.. [2] Jacoby, William G. (2006) ""The Dot Plot**: A Graphical Display\n   for Labeled Quantitative Values."" The Political Methodologist\n   14(1)\n\n* **This is a simple dotplot with one point per line**: \n\n* **the same line)**: \n\n', 'stemmed comments': ['show_nam', 'dictionari', 'group', 'multipl', 'determin', 'top/bottom', 'dot', 's', 'point/interv', '_', 'per', 'box', 'callabl', 'similar', 'function', 'respect', 'never/alway', 'ax', 'figur', 'instanc', 'line_prop', 'axe', 'jacobi', 'code', 'shade', '[', 'linewidth', 'fmt_right_nam', 'pass', 'stacked=tru', 'half', 'dotplot', 'common', 'given', 'indic', 'everi', 'scalar', 'plot', '14', 'marker_prop', 'margin', 'use', 'pair', 'paramet', 'argument', 'return', 'bottom', 'draw', 'william', 'line_ord', 'name', 'label', 'false/tru', 'element', 'str', 'ms', 'true', 'variabl', 'drawn', 'marginnam', 'whether', 'graphic', 'interv', 'right/bottom', 'map', 'width', 'G', 'split', 'stack', 'fals', 'length', 'section_ord', 'keyword', 'singl', 'stripe', 'points/interv', 'offset', 'defin', 'simpl', 'vertic', 'valu', 'top', 'data', 'point', 'line', 'these', 'around', 'linestyl', 'enclos', 'styles_ord', 'shown', 'creat', 'display', 'appear', 'none', 'If', 'substr', 'default', 'titl', 'visual', 'solid_capstyl', 'left/top', 'known', 'marker', 'size', 'cleveland', 'A', '2', 'blobbogram', 'array_lik', 'methodologist', 'and/or', ']', 'axfigur', 'bool', 'style', 'lexic', 'split_nam', '2006', 'order', 'symmetr', 'matplotlibax', 'thi', 'otherwis', 'fmt_left_nam', 'left', 'section', 'new', 'polit', 'key/valu', 'contain', 'color', '1', 'sequenc', 'also', 'nonsymmetr', 'forest', 'the', 'halfwidth', 'one', 'produc', 'book', 'fig', 'dict', 'either', 'everyth', 'right', 'horizont', 'quantit', 'show_section_titl']}"
103,"{'func name': 'durbin_levinson', 'comments': 'Estimate AR parameters at multiple orders using Durbin-Levinson recursions.\n\nParameters ---------- endog : array_like or SARIMAXSpecification Input time series array, assumed to be stationary. ar_order : int, optional Autoregressive order. Default is 0. demean : bool, optional Whether to estimate and remove the mean from the process prior to fitting the autoregressive coefficients. Default is True. adjusted : bool, optional Whether to use the ""adjusted"" autocovariance estimator, which uses n\n\n- h degrees of freedom rather than n. This option can result in a non-positive definite autocovariance matrix. Default is False.\n##### Returns\n* **parameters **: list of SARIMAXParams objects\n    List elements correspond to estimates at different `ar_order`. For\n    example, parameters[0] is an `SARIMAXParams` instance corresponding to\n    `ar_order=0`.\n\n* **other_results **: Bunch\n    Includes one component, `spec`, containing the `SARIMAXSpecification`\n    instance corresponding to the input arguments.\n\n', 'stemmed comments': ['adjust', 'fals', 'coeffici', 'array_lik', 'durbinlevinson', 'object', 'array', 'n', 'multipl', 'use', 'bool', ']', 'compon', 'seri', 'paramet', 'AR', 'correspond', 'return', 'argument', 'autocovari', 'rather', 'order', 'matrix', 'thi', 'estim', 'assum', 'int', 'for', 'prior', 'ar_ord', 'exampl', 'degre', 'stationari', 'endog', 'result', 'nonposit', 'definit', 'instanc', 'includ', 'true', 'time', 'element', 'contain', 'demean', 'sarimaxspecif', 'option', 'autoregress', 'mean', 'whether', 'h', 'remov', 'freedom', 'spec', 'bunch', 'fit', 'sarimaxparam', '[', 'ar_order=0', '0', 'one', 'default', 'differ', 'process', 'recurs', 'other_result', 'input', 'list']}"
104,"{'func name': '_norm_sf', 'comments': '', 'stemmed comments': []}"
105,"{'func name': '_opt_1d', 'comments': 'One-dimensional helper for elastic net.\n\nParameters ---------- func : function A smooth function of a single variable to be optimized with L1 penaty. grad : function The gradient of `func`. hess : function The Hessian of `func`. model : statsmodels model The model being fit. start : real A starting value for the function argument L1_wt : non-negative real The weight for the L1 penalty function. tol : non-negative real A convergence threshold. check_step : bool If True, check that the first step is an improvement and use bisection if it is not.\n\nIf False, return after the first step regardless.\n\nNotes ----- ``func``, ``grad``, and ``hess`` have argument signature (x, model), where ``x`` is a point in the parameter space and ``model`` is the model being fit.\n\nIf the log-likelihood for the model is exactly quadratic, the global minimum is returned in one step.\n\nOtherwise numerical bisection is used.\n##### Returns\n', 'stemmed comments': ['A', 'fals', 'note', 'x', 'improv', 'quadrat', 'grad', 'global', 'singl', 'minimum', 'first', 'start', 'use', 'bool', 'paramet', 'argument', 'return', 'onedimension', 'statsmodel', 'l1_wt', 'weight', 'valu', 'tol', 'hess', 'model', 'otherwis', 'step', 'function', 'point', 'space', 'threshold', 'elast', 'exactli', 'hessian', 'bisect', 'net', 'L1', 'true', 'loglikelihood', 'penati', 'variabl', 'check', 'penalti', 'the', 'fit', 'one', 'optim', 'helper', 'If', 'numer', 'regardless', 'smooth', 'gradient', 'check_step', 'signatur', 'converg', 'real', 'nonneg', 'func']}"
106,"{'func name': 'monotone_fn_inverter', 'comments': 'Given a monotone function fn (no checking is done to verify monotonicity) and a set of x values, return an linearly interpolated approximation to its inverse from its values on x.\n\n\n', 'stemmed comments': ['x', 'valu', 'invers', 'check', 'approxim', 'function', 'given', 'fn', 'monoton', 'set', 'done', 'interpol', 'return', 'linearli', 'verifi']}"
107,"{'func name': 'fit_mps', 'comments': 'Estimate distribution parameters with Maximum Product-of-Spacings\n\nParameters ---------- params : array_like, tuple ? parameters of the distribution funciton xsorted : array_like data that is already sorted dist : instance of a distribution class only cdf method is used\n##### Returns\n* **x **: ndarray\n    estimates for the parameters of the distribution given the data,\n    including loc and scale\n\n', 'stemmed comments': ['productofspac', 'x', 'array_lik', 'use', 'paramet', 'return', 'method', 'class', 'loc', 'dist', 'estim', 'scale', 'data', 'distribut', 'alreadi', 'instanc', 'includ', 'cdf', 'xsort', 'sort', 'funciton', 'ndarray', 'given', 'param', 'tupl', '?', 'maximum']}"
108,"{'func name': 'hqic_sigma', 'comments': 'Hannan-Quinn information criterion (HQC)\n\nParameters ---------- sigma2 : float estimate of the residual variance or determinant of Sigma_hat in the multivariate case. If islog is true, then it is assumed that sigma is already log-ed, for example logdetSigma. nobs : int number of observations df_modelwc : int number of parameters including constant\n##### Returns\n* **hqic **: float\n    information criterion\n\n', 'stemmed comments': ['hannanquinn', 'df_modelwc', 'determin', 'paramet', 'return', 'estim', 'assum', 'logdetsigma', 'nob', 'int', 'sigma', 'loge', 'varianc', 'alreadi', 'multivari', 'case', 'residu', 'includ', 'constant', 'true', 'inform', 'sigma2', 'float', 'sigma_hat', 'islog', 'observ', 'If', 'hqic', 'criterion', 'hqc', 'number', 'exampl']}"
109,"{'func name': 'examples_transf', 'comments': '', 'stemmed comments': []}"
110,"{'func name': 'maxabsrel', 'comments': '', 'stemmed comments': []}"
111,"{'func name': 'maxabsrel', 'comments': '', 'stemmed comments': []}"
112,"{'func name': 'probitloglike', 'comments': 'Log likelihood for the probit\n\n\n', 'stemmed comments': ['likelihood', 'probit', 'log']}"
113,"{'func name': 'true_pdf', 'comments': '', 'stemmed comments': []}"
114,"{'func name': 'print_results2', 'comments': '', 'stemmed comments': []}"
115,"{'func name': 'plot_acf_multiple', 'comments': '\n\n\n', 'stemmed comments': []}"
116,"{'func name': 'cov2corr', 'comments': '', 'stemmed comments': []}"
117,"{'func name': 'func2', 'comments': '', 'stemmed comments': []}"
118,"{'func name': 'test_squared_normal_chi2', 'comments': '', 'stemmed comments': []}"
119,"{'func name': 'pltxcorr', 'comments': ""call signature\n\ndef xcorr(self, x, y, normed=True, detrend=detrend_none, usevlines=True, maxlags=10, **kwargs):\n\nPlot the cross correlation between *x* and *y*.\n\nIf *normed* = *True*, normalize the data by the cross correlation at 0-th lag.\n\n*x* and y are detrended by the *detrend* callable (default no normalization).\n\n*x* and *y* must be equal length.\n\nData are plotted as ``plot(lags, c, **kwargs)``\n\nReturn value is a tuple (*lags*, *c*, *line*) where:\n\n- *lags* are a length ``2*maxlags+1`` lag vector\n\n- *c* is the ``2*maxlags+1`` auto correlation vector\n\n- *line* is a :class:`~matplotlib.lines.Line2D` instance returned by :func:`~matplotlib.pyplot.plot`.\n\nThe default *linestyle* is *None* and the default *marker* is 'o', though these can be overridden with keyword args.\n\nThe cross correlation is performed with :func:`numpy.correlate` with *mode* = 2.\n\nIf *usevlines* is *True*:\n\n:func:`~matplotlib.pyplot.vlines` rather than :func:`~matplotlib.pyplot.plot` is used to draw vertical lines from the origin to the xcorr.\n\nOtherwise the plotstyle is determined by the kwargs, which are :class:`~matplotlib.lines.Line2D` properties.\n\nThe return value is a tuple (*lags*, *c*, *linecol*, *b*) where *linecol* is the :class:`matplotlib.collections.LineCollection` instance and *b* is the *x*-axis.\n\n*maxlags* is a positive integer detailing the number of lags to show. The default value of *None* will return all ``(2*len(x)-1)`` lags.\n\n**Example:**\n\n:func:`~matplotlib.pyplot.xcorr` above, and :func:`~matplotlib.pyplot.acorr` below.\n\n**Example:**\n\n.. plot:: mpl_examples/pylab_examples/xcorr_demo.py\n"", 'stemmed comments': ['determin', 'linecol', 'callabl', 'def', 'self', 'c', 'mpl_examples/pylab_examples/xcorr_demopi', 'show', 'instanc', 'correl', 'overridden', 'must', '~matplotliblinesline2d', 'exampl', 'arg', 'plot', 'plotstyl', 'cross', 'use', 'vector', 'len', 'axi', 'return', 'class', 'equal', 'draw', 'maxlag', 'maxlags=10', 'though', 'lag', 'kwarg', 'origin', 'true', 'mode', 'call', 'detrend', 'auto', 'usevlin', 'number', 'length', '~matplotlibpyplotplot', 'keyword', 'integ', '~matplotlibpyplotacorr', 'rather', 'vertic', 'valu', 'data', 'line', 'properti', 'b', 'linestyl', 'detrend=detrend_non', 'default', 'If', 'none', 'signatur', 'xcorr', 'marker', 'norm', 'tupl', 'func', 'normal', 'x', '2', 'numpycorrel', 'otherwis', '=', 'maxlags1', '~matplotlibpyplotvlin', 'detail', 'usevlines=tru', '1', '~matplotlibpyplotxcorr', 'the', 'matplotlibcollectionslinecollect', 'normed=tru', 'posit', '0th', 'perform']}"
120,"{'func name': 'meanp', 'comments': '', 'stemmed comments': []}"
121,"{'func name': 'test_nb2', 'comments': '', 'stemmed comments': []}"
122,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
123,"{'func name': 'mvnormcdf', 'comments': 'multivariate normal cumulative distribution function\n\nThis is a wrapper for scipy.stats.kde.mvn.mvndst which calculates a rectangular integral over a multivariate normal distribution.\n\nParameters ---------- lower, upper : array_like, 1d lower and upper integration limits with length equal to the number of dimensions of the multivariate normal distribution. It can contain -np.inf or np.inf for open integration intervals mu : array_lik, 1d list or array of means cov : array_like, 2d specifies covariance matrix optional keyword parameters to influence integration * maxpts : int, maximum number of function values allowed. This parameter can be used to limit the time. A sensible strategy is to start with `maxpts` = 1000*N, and then increase `maxpts` if ERROR is too large. * abseps : float absolute error tolerance. * releps : float relative error tolerance.\n##### Returns\n* **cdfvalue **: float\n    value of the integral\n\n* **mvstdnormcdf **: location and scale standardized multivariate normal cdf\n\n', 'stemmed comments': ['cumul', 'normal', 'length', 'A', 'N', 'keyword', 'array_lik', 'rectangular', 'scipystatskdemvnmvndst', 'array', 'use', 'start', 'limit', 'npinf', 'strategi', 'cov', 'error', 'cdfvalu', 'paramet', 'sensibl', 'return', 'mu', 'matrix', 'thi', 'equal', 'open', 'valu', 'absep', 'absolut', 'int', '1000', '=', 'locat', 'scale', 'standard', 'lower', 'function', 'toler', 'mvstdnormcdf', 'upper', 'distribut', 'multivari', 'calcul', 'larg', 'contain', 'time', 'float', '1d', 'option', 'mean', 'rel', 'cdf', 'interv', 'integr', 'specifi', 'covari', 'increas', 'allow', '2d', 'influenc', 'dimens', 'It', 'maxpt', 'wrapper', 'relep', 'number', 'list', 'maximum']}"
124,"{'func name': '_check_args_2', 'comments': '', 'stemmed comments': []}"
125,"{'func name': '_recode', 'comments': 'Recode categorial data to int factor.\n\nParameters ---------- x : array_like array like object supporting with numpy array methods of categorially coded data. levels : dict mapping of labels to integer-codings\n##### Returns\n* **out **: instance numpy.ndarray\n\n', 'stemmed comments': ['x', 'array_lik', 'object', 'array', 'categori', 'paramet', 'factor', 'return', 'method', 'level', 'data', 'int', 'recod', 'label', 'instanc', 'like', 'integercod', 'numpi', 'code', 'support', 'numpyndarray', 'map', 'dict']}"
126,"{'func name': 'atleast_2dcols', 'comments': '', 'stemmed comments': []}"
127,"{'func name': 'spdar1', 'comments': '', 'stemmed comments': []}"
128,"{'func name': 'miso_lfilter', 'comments': 'Filter multiple time series into a single time series.\n\nUses a convolution to merge inputs, and then lfilter to produce output.\n\nParameters ---------- ar : array_like The coefficients of autoregressive lag polynomial including lag zero, ar(L) in the expression ar(L)y_t. ma : array_like, same ndim as x, currently 2d The coefficient of the moving average lag polynomial, ma(L) in ma(L)x_t. x : array_like The 2-d input data series, time in rows, variables in columns. useic : bool Flag indicating whether to use initial conditions.\n##### Returns\n* **y **: ndarray\n    The filtered output series.\n\n* **inp **: ndarray, 1d\n    The combined input series.\n\n* **miso_lfilter find array y such that**: ar(L)y_t = ma(L)x_t\n\n', 'stemmed comments': ['x', 'coeffici', 'initi', 'array_lik', 'merg', 'singl', 'array', 'multipl', 'use', 'y_t', 'bool', 'lfilter', 'filter', 'combin', 'paramet', 'seri', 'return', 'inp', 'current', 'ndim', 'polynomi', 'data', '=', 'output', 'lag', 'useic', 'L', 'condit', 'includ', 'ar', 'miso_lfilt', 'time', 'variabl', 'autoregress', '1d', 'move', 'averag', 'whether', 'flag', 'find', 'express', 'the', 'zero', 'x_t', 'produc', '2d', 'ndarray', 'convolut', 'row', 'indic', 'input', 'column']}"
129,"{'func name': 'getquotes', 'comments': '', 'stemmed comments': []}"
130,"{'func name': 'savetxt', 'comments': ""Save an array to a text file.\n\nThis is just a copy of numpy.savetxt patched to support structured arrays or a header of names.\n\nDoes not include py3 support now in savetxt.\n\nParameters ---------- fname : filename or file handle If the filename ends in ``.gz``, the file is automatically saved in compressed gzip format.\n\n`loadtxt` understands gzipped files transparently. X : array_like Data to be saved to a text file. names : list, optional If given names will be the column header in the text file.\n\nIf None and X is a structured or recarray then the names are taken from X.dtype.names. fmt : str or sequence of strs A single format (%10.5f), a sequence of formats, or a multi-format string, e.g. 'Iteration %d -- %10.5f', in which case `delimiter` is ignored. delimiter : str Character separating columns.\n\nSee Also -------- save : Save an array to a binary file in NumPy ``.npy`` format savez : Save several arrays into a ``.npz`` compressed archive\n\nNotes ----- Further explanation of the `fmt` parameter (``%[flag]width[.precision]specifier``):\n\nflags: ``-`` : left justify\n\n``+`` : Forces to preceed result with + or -.\n\n``0`` : Left pad the number with zeros instead of space (see width).\n\nwidth: Minimum number of characters to be printed. The value is not truncated if it has more characters.\n\nprecision:\n\n- For integer specifiers (eg. ``d,i,o,x``), the minimum number of digits.\n\n- For ``e, E`` and ``f`` specifiers, the number of digits to print after the decimal point.\n\n- For ``g`` and ``G``, the maximum number of significant digits.\n\n- For ``s``, the maximum number of characters.\n\nspecifiers: ``c`` : character\n\n``d`` or ``i`` : signed decimal integer\n\n``e`` or ``E`` : scientific notation with ``e`` or ``E``.\n\n``f`` : decimal floating point\n\n``g,G`` : use the shorter of ``e,E`` or ``f``\n\n``o`` : signed octal\n\n``s`` : str of characters\n\n``u`` : unsigned decimal integer\n\n``x,X`` : unsigned hexadecimal integer\n\nThis explanation of ``fmt`` is not complete, for an exhaustive specification see [1]_.\n\nReferences ---------- .. [1] `Format Specification Mini-Language <http://docs.python.org/library/string.html# format-specification-mini-language>`_, Python Documentation.\n\nExamples -------- >>> savetxt('test.out', x, delimiter=',')\n\n # x is an array >>> savetxt('test.out', (x,y,z))\n\n # x,y,z equal sized 1D arrays >>> savetxt('test.out', x, fmt='%1.4e')\n\n # use exponential notation\n"", 'stemmed comments': ['notat', 'minimum', 'array', 'multiformat', 'further', '_', 'exhaust', 'document', 'eg', 'digit', 'space', 'c', 'octal', 'py3', 'specif', 'refer', 'option', 'complet', 'zero', '[', 'xdtypenam', 'f', 'unsign', 'given', 'pad', 'print', 'specifi', 'exampl', 'copi', 'gzip', 'see', 'note', 'justifi', 'save', 'u', 'use', 'paramet', 'ignor', 'iter', 'shorter', 'equal', 'transpar', 'fname', 'for', 'handl', 'name', 'result', 'gz', '//docspythonorg/library/stringhtml', 'str', 'loadtxt', 'sever', 'savetxt', 'float', 'python', 'instead', 'scientif', 'width', 'signific', 'separ', 'G', 'decim', '105f', 'doe', 'number', 'maximum', 'structur', 'singl', 'integ', 'exponenti', 'understand', 'format', 'patch', 'valu', 'data', 'fmt=', 'fmt', 'point', 'E', 'case', 'filenam', 'includ', 'npi', 'formatspecificationminilanguag', 'minilanguag', 'support', 'recarray', 'savez', '0', 'none', 'If', 'automat', 'sign', 'binari', 'e', 'http', 'taken', 'forc', 'column', 'text', 'size', 'A', 'x', 'prece', 'header', 'array_lik', ']', 'charact', 'string', 'numpysavetxt', '14e', 'explan', 'truncat', 'X', '<', 'thi', 'left', 'hexadecim', '1D', 'z', 'archiv', 'delimiter=', '1', 'end', '%', 'sequenc', 'also', 'numpi', 'flag', 'the', '>', 'compress', 'precis', 'g', 'testout', 'npz', 'file', 'list', 'delimit']}"
131,"{'func name': 'log_plus_1', 'comments': '', 'stemmed comments': []}"
132,"{'func name': 'make_hypotheses_matrices', 'comments': '\n\n\n', 'stemmed comments': []}"
133,"{'func name': 'banddepth', 'comments': 'Calculate the band depth for a set of functional curves.\n\nBand depth is an order statistic for functional data (see `fboxplot`), with a higher band depth indicating larger ""centrality"".\n\nIn analog to scalar data, the functional curve with highest band depth is called the median curve, and the band made up from the first N/2 of N curves is the 50% central region.\n\nParameters ---------- data : ndarray The vectors of functions to create a functional boxplot from. The first axis is the function index, the second axis the one along which the function is defined.\n\nSo ``data[0, :]`` is the first functional curve. method : {\'MBD\', \'BD2\'}, optional Whether to use the original band depth (with J=2) of [1]_ or the modified band depth.\n\nSee Notes for details.\n##### Returns\n* **.. [3] Y. Sun, M. G. Gentonb and D. W. Nychkac, ""Exact fast computation\n       of band depth for large functional datasets**: How quickly can one\n       million curves be ranked?"", Journal for the Rapid Dissemination\n       of Statistics Research, vol. 1, pp. 68-74, 2012.\n\n', 'stemmed comments': ['N', 'fast', 'dataset', 'depth', '_', 'higher', 'vol', 'function', 'quickli', '2012', '6874', 'boxplot', 'million', 'calcul', 'larg', 'research', '50', 'modifi', 'option', 'index', 'sun', '[', 'n/2', 'D', 'indic', 'fboxplot', '?', 'M', 'scalar', 'comput', 'see', 'note', 'use', 'set', 'highest', 'vector', 'paramet', 'rapid', 'axi', 'nychkac', 'return', 'method', 'Y', 'curv', 'how', 'statist', 'origin', 'central', '{', 'whether', 'mbd', 'call', 'dissemin', 'G', 'journal', 'W', '3', 'first', 'defin', 'So', 'data', 'bd2', 'second', 'In', 'j=2', 'creat', 'median', '0', 'made', ']', 'order', 'pp', 'detail', 'analog', 'band', '1', '%', 'gentonb', 'exact', 'the', '}', 'one', 'along', 'ndarray', 'rank', 'region', 'larger']}"
134,"{'func name': '_split_train_test_smoothers', 'comments': 'split smoothers in test and train sets and create GenericSmoothers\n\nNote: this does not take exog_linear into account\n', 'stemmed comments': ['take', 'note', 'exog_linear', 'account', 'smoother', 'test', 'set', 'split', 'creat', 'train', 'genericsmooth']}"
135,"{'func name': 'default_smoother', 'comments': '\n\n\n', 'stemmed comments': []}"
136,"{'func name': 'gendat_nominal', 'comments': '', 'stemmed comments': []}"
137,"{'func name': 'gendat_nested1', 'comments': '', 'stemmed comments': []}"
138,"{'func name': 'generate_poisson', 'comments': '', 'stemmed comments': []}"
139,"{'func name': 'gendat_overdispersed', 'comments': '', 'stemmed comments': []}"
140,"{'func name': 'gendat_nested1', 'comments': '', 'stemmed comments': []}"
141,"{'func name': 'make_augmented_matrix', 'comments': 'augment endog, exog and weights with stochastic restriction matrix\n\nParameters ---------- endog : ndarray response or endogenous variable exog : ndarray design matrix, matrix of exogenous or explanatory variables penalty_matrix : ndarray, 2-Dim square penality matrix for quadratic penalization weights : ndarray weights for WLS\n##### Returns\n* **endog_aug **: ndarray\n    augmented response variable\n\n* **exog_aug **: ndarray\n    augmented design matrix\n\n* **weights_aug **: ndarray\n    augmented weights for WLS\n\n', 'stemmed comments': ['quadrat', 'design', 'restrict', 'exog_aug', 'paramet', 'return', 'matrix', 'weight', 'explanatori', 'weights_aug', 'endogen', 'wl', '2dim', 'stochast', 'penalty_matrix', 'endog', 'exogen', 'endog_aug', 'variabl', 'respons', 'ndarray', 'exog', 'squar', 'augment', 'penal']}"
142,"{'func name': '_score_test_submodel', 'comments': 'Return transformation matrices for design matrices.\n\nParameters ---------- par : instance The parent model sub : instance The sub-model\n', 'stemmed comments': ['matric', 'instanc', 'transform', 'design', 'model', 'submodel', 'sub', 'parent', 'par', 'paramet', 'return', 'the']}"
143,"{'func name': '_check_convergence', 'comments': '', 'stemmed comments': []}"
144,"{'func name': 'write_formula_api', 'comments': '', 'stemmed comments': []}"
145,"{'func name': '_ll_nb2', 'comments': '', 'stemmed comments': []}"
146,"{'func name': 'meanexcess_dist', 'comments': '', 'stemmed comments': []}"
147,"{'func name': 'double_it', 'comments': '', 'stemmed comments': []}"
148,"{'func name': 'merge_tuple', 'comments': '', 'stemmed comments': []}"
149,"{'func name': 'gls', 'comments': ""Estimate ARMAX parameters by GLS.\n\nParameters ---------- endog : array_like Input time series array. exog : array_like, optional Array of exogenous regressors. If not included, then `include_constant` must be True, and then `exog` will only include the constant column. order : tuple, optional The (p,d,q) order of the ARIMA model. Default is (0, 0, 0). seasonal_order : tuple, optional The (P,D,Q,s) order of the seasonal ARIMA model. Default is (0, 0, 0, 0). include_constant : bool, optional Whether to add a constant term in `exog` if it's not already there. The estimate of the constant will then appear as one of the `exog` parameters. If `exog` is None, then the constant will represent the mean of the process. Default is True if the specified model does not include integration and False otherwise. n_iter : int, optional Optionally iterate feasible GSL a specific number of times. Default is to iterate to convergence. If set, this argument overrides the `max_iter` and `tolerance` arguments. max_iter : int, optional Maximum number of feasible GLS iterations. Default is 50. If `n_iter` is set, it overrides this argument. tolerance : float, optional Tolerance for determining convergence of feasible GSL iterations. If `iter` is set, this argument has no effect. Default is 1e-8. arma_estimator : str, optional The estimator used for estimating the ARMA model. This option should not generally be used, unless the default method is failing or is otherwise unsuitable. Not all values will be valid, depending on the specified model orders (`order` and `seasonal_order`). Possible values are: * 'innovations_mle'\n\n- can be used with any specification * 'statespace'\n\n- can be used with any specification * 'hannan_rissanen'\n\n- can be used with any ARMA non-seasonal model * 'yule_walker'\n\n- only non-seasonal consecutive autoregressive (AR) models * 'burg'\n\n- only non-seasonal, consecutive autoregressive (AR) models * 'innovations'\n\n- only non-seasonal, consecutive moving average (MA) models. The default is 'innovations_mle'. arma_estimator_kwargs : dict, optional Arguments to pass to the ARMA estimator.\n##### Returns\n* **parameters **: SARIMAXParams object\n    Contains the parameter estimates from the final iteration.\n\n* **other_results **: Bunch\n    Includes eight components\n\n"", 'stemmed comments': ['n_iter', 'object', 'array', 'regressor', 'p', 'determin', 's', 'arma_estimator_kwarg', 'P', 'burg', 'arma_estim', 'endog', 'overrid', 'exogen', 'constant', 'feasibl', 'seasonal_ord', 'not', 'specif', '50', 'autoregress', 'option', 'add', 'averag', 'must', 'integr', 'pass', 'D', 'specifi', 'use', 'compon', 'set', 'paramet', 'argument', 'Q', 'iter', 'method', 'return', 'estim', 'alreadi', 'final', 'true', 'time', 'season', 'str', 'unsuit', 'hannan_rissanen', 'float', 'gsl', 'whether', 'include_const', 'sarimaxparam', 'arma', 'unless', 'process', 'exog', 'converg', 'number', 'gener', 'maximum', 'fals', 'eight', 'armax', 'AR', 'consecut', 'valu', 'int', 'model', 'statespac', 'nonseason', 'includ', 'yule_walk', 'effect', 'mean', 'move', 'repres', 'gl', '0', 'appear', 'default', 'If', 'none', 'tupl', 'input', '1e8', 'column', 'innovations_ml', 'arima', 'array_lik', 'max_it', 'bool', 'seri', 'order', 'thi', 'valid', 'otherwis', 'innov', 'toler', 'contain', 'possibl', 'term', 'MA', 'bunch', 'the', 'depend', 'one', 'fail', 'q', 'dict', 'other_result']}"
150,"{'func name': 'spec_hausman', 'comments': 'Hausmans specification test\n\nParameters ---------- params_e : ndarray efficient and consistent under Null hypothesis, inconsistent under alternative hypothesis params_i: ndarray consistent under Null hypothesis, consistent under alternative hypothesis cov_params_e : ndarray, 2d covariance matrix of parameter estimates for params_e cov_params_i : ndarray, 2d covariance matrix of parameter estimates for params_i\n\nexample instrumental variables OLS estimator is `e`, IV estimator is `i`\n\n Notes -----\n\nTodos,Issues\n\n- check dof calculations and verify for linear case\n\n- check one-sided hypothesis\n\n References ---------- Greene section 5.5 p.82/83\n', 'stemmed comments': ['note', 'null', 'hypothesi', 'consist', 'dof', 'onesid', 'inconsist', 'paramet', 'todo', 'cov_params_', 'p82/83', 'matrix', '55', 'estim', 'effici', 'hausman', 'section', 'issu', 'green', 'case', 'verifi', 'calcul', 'params_', 'altern', 'specif', 'refer', 'variabl', 'check', 'test', 'ol', 'params_i', 'covari', 'cov_params_i', 'ndarray', '2d', 'instrument', 'linear', 'e', 'IV', 'exampl']}"
151,"{'func name': 'bootstrap2', 'comments': 'Monte Carlo (or parametric bootstrap) p-values for gof\n\ncurrently hardcoded for A^2 only\n\nnon vectorized, loops over all parametric bootstrap replications and calculates and returns specific p-value,\n\nrename function to less generic\n', 'stemmed comments': ['parametr', 'renam', 'gof', 'vector', 'return', 'current', 'bootstrap', 'function', 'hardcod', 'less', 'calcul', 'carlo', 'specif', 'pvalu', 'a^2', 'mont', 'non', 'replic', 'loop', 'gener']}"
152,"{'func name': 'chisquare_effectsize', 'comments': 'effect size for a chisquare goodness-of-fit test\n\nParameters ---------- probs0 : array_like probabilities or cell frequencies under the Null hypothesis probs1 : array_like probabilities or cell frequencies under the Alternative hypothesis probs0 and probs1 need to have the same length in the ``axis`` dimension. and broadcast in the other dimensions Both probs0 and probs1 are normalized to add to one (in the ``axis`` dimension). correction : None or tuple If None, then the effect size is the chisquare statistic divide by the number of observations. If the correction is a tuple (nobs, df), then the effectsize is corrected to have less bias and a smaller variance. However, the correction can make the effectsize negative. In that case, the effectsize is set to zero. Pederson and Johnson (1990) as referenced in McLaren et all. (1994) cohen : bool If True, then the square root is returned as in the definition of the effect size by Cohen (1977), If False, then the original effect size is returned. axis : int If the probability arrays broadcast to more than 1 dimension, then this is the axis over which the sums are taken.\n##### Returns\n* **effectsize **: float\n    effect size of chisquare test\n\n', 'stemmed comments': ['smaller', 'normal', 'null', 'hypothesi', 'length', 'fals', 'array_lik', 'array', 'frequenc', 'probabl', 'probs0', 'broadcast', 'set', 'bool', 'both', 'paramet', '1977', 'axi', 'make', 'return', '1994', 'cohen', 'root', 'nob', 'effects', 'int', 'cell', 'bia', 'varianc', 'In', 'johnson', 'less', 'case', 'definit', 'statist', 'origin', 'divid', 'altern', 'correct', 'true', '1', 'float', 'effect', 'test', 'add', 'df', 'sum', 'probs1', 'need', 'goodnessoffit', 'chisquar', 'zero', 'one', 'et', 'none', 'If', 'observ', 'dimens', 'referenc', 'howev', 'squar', 'pederson', 'taken', 'mclaren', 'neg', '1990', 'tupl', 'number', 'size']}"
153,"{'func name': '_check_for_ppf', 'comments': '', 'stemmed comments': []}"
154,"{'func name': 'labelizer', 'comments': '', 'stemmed comments': []}"
155,"{'func name': '_make_generic_names', 'comments': '', 'stemmed comments': []}"
156,"{'func name': 'hannan_rissanen', 'comments': 'Estimate ARMA parameters using Hannan-Rissanen procedure.\n\nParameters ---------- endog : array_like Input time series array, assumed to be stationary. ar_order : int Autoregressive order ma_order : int Moving average order demean : bool, optional Whether to estimate and remove the mean from the process prior to fitting the ARMA coefficients. Default is True. initial_ar_order : int, optional Order of long autoregressive process used for initial computation of residuals. unbiased: bool, optional Whether or not to apply the bias correction step. Default is True if the estimated coefficients from the previous step imply a stationary and invertible process and False otherwise.\n##### Returns\n* **parameters **: SARIMAXParams object\n\n* **other_results **: Bunch\n    Includes three components\n\n* **possible. See test_hannan_rissanen**: \n\n', 'stemmed comments': ['comput', 'fals', 'see', 'coeffici', 'initi', 'array_lik', 'ma_ord', 'invert', 'object', 'array', 'use', 'bool', 'compon', 'seri', 'paramet', 'return', 'order', 'estim', 'assum', 'int', 'otherwis', 'step', 'prior', 'bia', 'test_hannan_rissanen', 'ar_ord', 'stationari', 'endog', 'residu', 'includ', 'true', 'time', 'correct', 'possibl', 'demean', 'autoregress', 'option', 'mean', 'appli', 'move', 'averag', 'whether', 'remov', 'three', 'bunch', 'hannanrissanen', 'fit', 'sarimaxparam', 'arma', 'default', 'process', 'unbias', 'other_result', 'long', 'impli', 'initial_ar_ord', 'procedur', 'input', 'previou']}"
157,"{'func name': '_holt_win_mul_add_dam', 'comments': 'Multiplicative and Multiplicative Damped with Additive Seasonal Minimization Function (M,A) & (M,Ad)\n\n\n', 'stemmed comments': ['addit', 'A', 'season', 'minim', 'multipl', 'damp', 'function', '&', 'Ad', 'M']}"
158,"{'func name': 'hpfilter', 'comments': 'Hodrick-Prescott filter.\n\nParameters ---------- x : array_like The time series to filter, 1-d. lamb : float The Hodrick-Prescott smoothing parameter. A value of 1600 is suggested for quarterly data. Ravn and Uhlig suggest using a value of 6.25 (1600/4**4) for annual data and 129600 (1600*3**4) for monthly data.\n##### Returns\n* **cycle **: ndarray\n    The estimated cycle in the data given lamb.\n\n* **trend **: ndarray\n    The estimated trend in the data given lamb.\n\n* **Hodrick, R.J, and E. C. Prescott. 1980. ""Postwar U.S. Business Cycles**: An\n    Empirical Investigation."" `Carnegie Mellon University discussion\n    paper no. 451`.\n\n* **>>> gdp_decomp[[""realgdp"", ""trend""]][""2000-03-31""**: ].plot(ax=ax,\n\n* **.. plot**: \n\n', 'stemmed comments': ['1600', 'A', 'gdp_decomp', 'plot', 'x', 'monthli', 'array_lik', '3', ']', 'use', 'US', 'trend', 'hodrickprescott', 'filter', 'suggest', '4', 'seri', 'paramet', '625', 'postwar', 'return', 'cycl', 'C', 'RJ', 'valu', 'estim', 'hodrick', 'An', 'paper', 'data', 'annual', '1600/4', 'carnegi', 'E', 'mellon', '451', 'investig', 'time', 'float', '1d', 'univers', 'quarterli', 'the', '>', 'ax=ax', '[', 'uhlig', 'ndarray', 'ravn', 'lamb', 'realgdp', 'smooth', 'prescott', 'empir', 'given', '20000331', '1980', 'busi', 'discuss', '129600']}"
159,"{'func name': 'gencrossentropy', 'comments': ""Generalized cross-entropy measures.\n\nParameters ---------- px : array_like Discrete probability distribution of random variable X py : array_like Discrete probability distribution of random variable Y pxpy : 2d array_like, optional Joint probability distribution of X and Y.\n\nIf pxpy is None, X and Y are assumed to be independent. logbase : int or np.e, optional Default is 2 (bits) measure : str, optional The measure is the type of generalized cross-entropy desired. 'T' is the cross-entropy version of the Tsallis measure.\n\n'CR' is Cressie-Read measure.\n"", 'stemmed comments': ['pxpi', '2', 'array_lik', 'crossentropi', 'probabl', 'T', 'paramet', 'independ', 'discret', 'tsalli', 'X', 'measur', 'px', 'cressieread', 'assum', 'desir', 'int', 'Y', 'version', 'distribut', 'bit', 'type', 'str', 'variabl', 'option', 'joint', 'CR', 'the', 'none', 'If', 'logbas', '2d', 'default', 'random', 'npe', 'gener', 'py']}"
160,"{'func name': '_initialization_heuristic', 'comments': '', 'stemmed comments': []}"
161,"{'func name': 'innovations_mle', 'comments': ""Estimate SARIMA parameters by MLE using innovations algorithm.\n\nParameters ---------- endog : array_like Input time series array. order : tuple, optional The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters. Default is (0, 0, 0). seasonal_order : tuple, optional The (P,D,Q,s) order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity. Default is (0, 0, 0, 0). demean : bool, optional Whether to estimate and remove the mean from the process prior to fitting the SARIMA coefficients. Default is True. enforce_invertibility : bool, optional Whether or not to transform the MA parameters to enforce invertibility in the moving average component of the model. Default is True. start_params : array_like, optional Initial guess of the solution for the loglikelihood maximization. The AR polynomial must be stationary. If `enforce_invertibility=True` the MA poylnomial must be invertible. If not provided, default starting parameters are computed using the Hannan-Rissanen method. minimize_kwargs : dict, optional Arguments to pass to scipy.optimize.minimize.\n##### Returns\n* **parameters **: SARIMAXParams object\n\n* **other_results **: Bunch\n    Includes four components\n\n* **Note**: we do not include `enforce_stationarity` as an argument, because this\n\n* **TODO**: add support for secondary optimization that does not enforce\n      stationarity / invertibility, starting from first step's parameters\n\n"", 'stemmed comments': ['four', 'sarima', 'algorithm', 'array', 'object', 'p', 'todo', 's', 'start_param', 'P', 'provid', 'endog', 'poylnomi', 'seasonal_ord', '/', 'option', 'add', 'averag', 'remov', 'must', 'optim', 'pass', 'D', 'comput', 'note', 'coeffici', 'invert', 'use', 'compon', 'paramet', 'argument', 'Q', 'method', 'return', 'estim', 'mle', 'step', 'season', 'time', 'true', 'whether', 'secondari', 'solut', 'sarimaxparam', 'differ', 'enforce_stationar', 'process', 'enforc', 'number', 'initi', 'transform', 'first', 'start', 'AR', 'maxim', 'model', 'includ', 'loglikelihood', 'mean', 'move', 'support', '0', 'default', 'If', 'enforce_invertibility=tru', 'period', 'tupl', 'input', 'array_lik', 'bool', 'scipyoptimizeminim', 'stationar', 'minimize_kwarg', 'seri', 'order', 'polynomi', 'innov', 'prior', 'stationari', 'demean', 'MA', 'bunch', 'the', 'fit', 'hannanrissanen', 'enforce_invert', 'dict', 'q', 'other_result', 'guess']}"
162,"{'func name': 'cohens_kappa', 'comments': 'Compute Cohen\'s kappa with variance and equal-zero test\n\nParameters ---------- table : array_like, 2-Dim square array with results of two raters, one rater in rows, second rater in columns weights : array_like The interpretation of weights depends on the wt argument. If both are None, then the simple kappa is computed. see wt for the case when wt is not None If weights is two dimensional, then it is directly used as a weight matrix. For computing the variance of kappa, the maximum of the weights is assumed to be smaller or equal to one. TODO: fix conflicting definitions in the 2-Dim case for wt : {None, str} If wt and weights are None, then the simple kappa is computed. If wt is given, but weights is None, then the weights are set to be [0, 1, 2, ..., k]. If weights is a one-dimensional array, then it is used to construct the weight matrix given the following options.\n\nwt in [\'linear\', \'ca\' or None] : use linear weights, Cicchetti-Allison actual weights are linear in the score ""weights"" difference wt in [\'quadratic\', \'fc\'] : use linear weights, Fleiss-Cohen actual weights are squared in the score ""weights"" difference wt = \'toeplitz\' : weight matrix is constructed as a toeplitz matrix from the one dimensional weights.\n\nreturn_results : bool If True (default), then an instance of KappaResults is returned. If False, then only kappa is computed and returned.\n##### Returns\n* **example**: \n\n', 'stemmed comments': ['quadrat', 'wt', 'array', 'todo', 's', 'construct', 'cicchettiallison', 'return_result', 'ca', 'instanc', 'fleisscohen', 'option', 'test', '[', 'kappa', 'tabl', 'given', 'equalzero', 'exampl', 'comput', 'smaller', 'see', 'use', 'set', 'paramet', 'return', 'argument', 'score', 'onedimension', 'cohen', 'weight', 'equal', 'two', 'actual', '2dim', 'for', 'result', 'definit', 'true', 'str', '{', 'follow', 'differ', 'fix', 'squar', 'maximum', 'k', 'fals', 'simpl', 'assum', 'second', 'case', 'interpret', '0', 'none', 'If', 'default', 'rater', 'linear', 'column', '2', 'kapparesult', 'array_lik', ']', 'bool', 'matrix', '=', 'varianc', '1', 'the', 'depend', '}', 'fc', 'one', 'directli', 'toeplitz', 'row', 'conflict', 'dimension']}"
163,"{'func name': 'kdensityfft', 'comments': 'Rosenblatt-Parzen univariate kernel density estimator\n\nParameters ---------- X : array_like The variable for which the density estimate is desired. kernel : str ONLY GAUSSIAN IS CURRENTLY IMPLEMENTED. ""bi"" for biweight ""cos"" for cosine ""epa"" for Epanechnikov, default ""epa2"" for alternative Epanechnikov ""gau"" for Gaussian. ""par"" for Parzen ""rect"" for rectangular ""tri"" for triangular bw : str, float ""scott""\n\n- 1.059 * A * nobs ** (-1/5.), where A is min(std(X),IQR/1.34) ""silverman""\n\n- .9 * A * nobs ** (-1/5.), where A is min(std(X),IQR/1.34) If a float is given, it is the bandwidth. weights : array or None WEIGHTS ARE NOT CURRENTLY IMPLEMENTED. Optional\n\nweights. If the X value is clipped, then this weight is also dropped. gridsize : int If gridsize is None, min(len(X), 512) is used. Note that the provided number is rounded up to the next highest power of 2. adjust : float An adjustment factor for the bw. Bandwidth becomes bw * adjust. clip : tuple Observations in X that are outside of the range given by clip are dropped. The number of observations in X is then shortened. cut : float Defines the length of the grid past the lowest and highest values of X so that the kernel goes to zero. The end points are -/+ cut*bw*{X.min() or X.max()} retgrid : bool Whether or not to return the grid over which the density is estimated.\n##### Returns\n* **density **: ndarray\n    The densities estimated at the grid points.\n\n* **grid **: ndarray, optional\n    The grid points at which the density is estimated.\n\n* **Jones, M.C. and H.W. Lotwick. (1984) `Remark AS R50**: A Remark on Algorithm\n    AS 176. Kernal Density Estimation Using the Fast Fourier Transform`.\n    Journal of the Royal Statistical Society. Series C. 33.1, 120-2.\n\n', 'stemmed comments': ['adjust', 'MC', 'epanechnikov', 'algorithm', 'array', 'fast', 'triangular', 'bi', 'retgrid', 'C', 'kernal', 'provid', 'outsid', 'not', '1202', '/', 'clip', 'option', '331', 'par', 'implement', 'zero', 'HW', 'biweight', 'given', 'scott', '1/5', 'note', 'grid', '176', 'use', 'len', 'highest', 'paramet', 'return', 'weight', 'min', 'estim', 'xmin', 'nob', 'gaussian', 'statist', 'str', 'r50', 'power', 'variabl', 'float', 'gridsiz', '{', 'whether', 'bandwidth', 'onli', 'std', '512', 'journal', '9', 'number', 'past', 'epa', 'length', 'jone', 'transform', 'rectangular', 'gau', 'rect', 'defin', 'factor', 'current', 'valu', 'An', 'remark', 'int', 'point', 'societi', 'altern', 'becom', '1984', 'kernel', 'univari', 'default', 'If', 'none', 'silverman', 'tupl', 'lowest', 'A', '2', 'array_lik', '1059', 'densiti', 'bool', 'seri', 'rosenblattparzen', 'fourier', 'X', 'are', 'lotwick', 'xmax', 'desir', 'cosin', 'epa2', 'iqr/134', 'round', 'co', 'goe', 'drop', 'end', 'next', 'AS', 'also', 'rang', 'IS', 'the', 'parzen', 'shorten', '}', 'royal', 'observ', 'bw', 'ndarray', 'cut', 'tri']}"
164,"{'func name': 'test_kde_1d', 'comments': '', 'stemmed comments': []}"
165,"{'func name': 'kdesum', 'comments': '', 'stemmed comments': []}"
166,"{'func name': 'wang_ryzin_reg', 'comments': 'A version for the Wang-Ryzin kernel for nonparametric regression.\n\nSuggested by Li and Racine in [1] ch.4\n', 'stemmed comments': ['A', 'ch4', 'nonparametr', 'regress', '1', ']', 'wangryzin', 'kernel', 'Li', 'suggest', 'version', 'racin', '[']}"
167,"{'func name': 'example2', 'comments': '', 'stemmed comments': []}"
168,"{'func name': 'parse', 'comments': '', 'stemmed comments': []}"
169,"{'func name': '_hessian_wrapper', 'comments': 'Wraps the hessian up in the form for cvxopt.\n\ncvxopt wants the hessian of the objective function and the constraints. Since our constraints are linear, this part is all zeros.\n', 'stemmed comments': ['form', 'zero', 'linear', 'object', 'part', 'function', 'constraint', 'sinc', 'cvxopt', 'want', 'wrap', 'hessian']}"
170,"{'func name': '_fprime_ieqcons', 'comments': 'Derivative of the inequality constraints\n\n\n', 'stemmed comments': ['deriv', 'constraint', 'inequ']}"
171,"{'func name': 'do_trim_params', 'comments': 'Trims (set to zero) params that are zero at the theoretical minimum. Uses heuristics to account for the solver not actually finding the minimum.\n\nIn all cases, if alpha[i] == 0, then do not trim the ith param. In all cases, do nothing with the added variables.\n\nParameters ---------- params : ndarray model parameters.\n\nNot including added variables. k_params : Int Number of parameters alpha : ndarray regularization coefficients score : Function. score(params) should return a 1-d vector of derivatives of the unpenalized objective function. passed : bool True if the QC check passed trim_mode : \'auto, \'size\', or \'off\' If not \'off\', trim (set to zero) parameters that would have been zero if the solver reached the theoretical minimum. If \'auto\', trim params using the Theory above. If \'size\', trim params if they have very small absolute value size_trim_tol : float or \'auto\' (default = \'auto\') For use when trim_mode === \'size\' auto_trim_tol : float For sue when trim_mode == \'auto\'.\n\nUse qc_tol : float Print warning and do not allow auto trim when (ii) in ""Theory"" (above) is violated by this much.\n##### Returns\n* **params **: ndarray\n    Trimmed model parameters\n\n* **trimmed **: ndarray of booleans\n    trimmed[i] == True if the ith parameter was trimmed.\n\n', 'stemmed comments': ['unpen', 'coeffici', '===', 'warn', 'deriv', 'ii', 'minimum', ']', 'object', 'off', 'use', 'bool', 'trim_mod', 'qc_tol', 'set', 'solver', 'vector', 'theori', 'paramet', 'reach', 'boolean', '==', 'return', 'score', 'k_param', 'valu', 'absolut', 'int', 'model', 'actual', 'would', '=', 'regular', 'for', 'function', 'violat', 'In', 'auto_trim_tol', 'small', 'case', 'heurist', 'includ', 'not', 'alpha', 'account', 'ad', 'true', 'variabl', 'check', '1d', 'QC', 'float', 'find', 'print', 'ith', 'zero', '[', '0', 'much', 'default', 'If', 'trim', 'ndarray', 'size_trim_tol', 'allow', 'pass', 'sue', 'auto', 'theoret', 'param', 'noth', 'number', 'size']}"
172,"{'func name': 'simulations', 'comments': '', 'stemmed comments': []}"
173,"{'func name': 'mvn_nloglike_obs', 'comments': 'loglike multivariate normal\n\nassumes x is 1d, (nobs,) and sigma is 2d (nobs, nobs)\n\nbrute force from formula no checking of correct inputs use of inv and log-det should be replace with something more efficient\n', 'stemmed comments': ['normal', 'x', 'loglik', 'use', 'formula', 'inv', 'assum', 'nob', 'logdet', 'effici', 'sigma', 'brute', 'multivari', 'correct', 'check', '1d', '2d', 'someth', 'forc', 'input', 'replac']}"
174,"{'func name': 'maxabs', 'comments': '', 'stemmed comments': []}"
175,"{'func name': 'matrix_sqrt', 'comments': ""matrix square root for symmetric matrices\n\nUsage is for decomposing a covariance function S into a square root R such that\n\nR' R = S if inverse is False, or R' R = pinv(S) if inverse is True\n\nParameters ---------- mat : array_like, 2-d square symmetric square matrix for which square root or inverse square root is computed. There is no checking for whether the matrix is symmetric. A warning is issued if some singular values are negative, i.e. below the negative of the threshold. inverse : bool If False (default), then the matrix square root is returned. If inverse is True, then the matrix square root of the inverse matrix is returned. full : bool If full is False (default, then the square root has reduce number of rows if the matrix is singular, i.e. has singular values below the threshold. nullspace: bool If nullspace is true, then the matrix square root of the null space of the matrix is returned. threshold : float Singular values below the threshold are dropped.\n##### Returns\n* **msqrt **: ndarray\n    matrix square root or square root of inverse matrix.\n\n"", 'stemmed comments': ['comput', 'A', 'fals', 'null', 'usag', 'warn', 'invers', 'array_lik', 'bool', 'paramet', 'return', 'matrix', 'matric', 'root', 'symmetr', 'decompos', 'S', 'valu', 'msqrt', '=', 'ie', 'function', 'space', 'threshold', 'issu', 'there', 'mat', 'R', 'true', 'check', 'reduc', 'float', 'drop', 'pinv', 'whether', 'singular', 'covari', 'full', 'default', 'If', '2d', 'nullspac', 'ndarray', 'squar', 'row', 'neg', 'number']}"
176,"{'func name': 'burg', 'comments': ""Compute Burg's AP(p) parameter estimator.\n\nParameters ---------- endog : array_like The endogenous variable. order : int, optional Order of the AR.\n\nDefault is 1. demean : bool, optional Flag indicating to subtract the mean from endog before estimation.\n##### Returns\n* **rho **: ndarray\n    The AR(p) coefficients computed using Burg's algorithm.\n\n* **sigma2 **: float\n    The estimate of the residual variance.\n\n* **yule_walker **: Estimate AR parameters using the Yule-Walker method.\n\n"", 'stemmed comments': ['comput', 'subtract', 'coeffici', 'array_lik', 'algorithm', 'use', 'bool', 'p', 'yulewalk', 'paramet', 'AR', 's', 'return', 'method', 'order', 'AP', 'estim', 'endogen', 'int', 'burg', 'varianc', 'endog', 'residu', '1', 'demean', 'variabl', 'rho', 'sigma2', 'option', 'mean', 'float', 'yule_walk', 'flag', 'the', 'default', 'ndarray', 'indic']}"
177,"{'func name': 'qhat', 'comments': '', 'stemmed comments': []}"
178,"{'func name': 'exampletest', 'comments': '', 'stemmed comments': []}"
179,"{'func name': 'cy_kim_smoother_log', 'comments': 'Kim smoother in log space using Cython inner loop.\n\nParameters ---------- regime_transition : ndarray Matrix of regime transition probabilities, shaped either (k_regimes, k_regimes, 1) or if there are time-varying transition probabilities (k_regimes, k_regimes, nobs). predicted_joint_probabilities : ndarray Array containing Pr[S_t=s_t, ..., S_{t-order}=s_{t-order} | Y_{t-1}]\n\n- the joint probability of the current and previous `order` periods being in each combination of regimes conditional on time t-1 information. Shaped (k_regimes,) * (order + 1) + (nobs,). filtered_joint_probabilities : ndarray Array containing Pr[S_t=s_t, ..., S_{t-order}=s_{t-order} | Y_{t}]\n\n- the joint probability of the current and previous `order` periods being in each combination of regimes conditional on time t information. Shaped (k_regimes,) * (order + 1) + (nobs,).\n##### Returns\n* **smoothed_joint_probabilities **: ndarray\n    Array containing Pr[S_t=s_t, ..., S_{t-order}=s_{t-order} | Y_T] -\n    the joint probability of the current and previous `order` periods\n    being in each combination of regimes conditional on all information.\n    Shaped (k_regimes,) * (order + 1) + (nobs,).\n\n* **smoothed_marginal_probabilities **: ndarray\n    Array containing Pr[S_t=s_t | Y_T] - the probability of being in each\n    regime conditional on all information. Shaped (k_regimes, nobs).\n\n', 'stemmed comments': ['transit', 'smoothed_joint_prob', 'kim', 'smoother', 'regime_transit', 'S_', 't1', 'array', ']', 'use', 'probabl', 'combin', '=s_', 'y_t', 'paramet', '|', 'return', 'order', 'matrix', 'current', 'inner', 'Pr', 'nob', 'k_regim', 'shape', 'timevari', 'space', 'smoothed_marginal_prob', 'regim', 'condit', 'predicted_joint_prob', 'contain', 'time', '1', 'inform', '{', 'torder', 'joint', '}', '[', 's_t=s_t', 'ndarray', 'either', 'filtered_joint_prob', 'Y_', 'period', 'cython', 'log', 'loop', 'previou']}"
180,"{'func name': 'plothist', 'comments': '', 'stemmed comments': []}"
181,"{'func name': '_pvalue', 'comments': '', 'stemmed comments': []}"
182,"{'func name': '_fit_tau_iter_mm', 'comments': 'iterated method of moment estimate of between random effect variance\n\nThis repeatedly estimates tau, updating weights in each iteration see two-step estimators in DerSimonian and Kacker 2007\n\nParameters ---------- eff : ndarray effect sizes var_eff : ndarray variance of effect sizes tau2_start : float starting value for iteration atol : float, default: 1e-5 convergence tolerance for change in tau2 estimate between iterations maxiter : int maximum number of iterations\n##### Returns\n* **tau2 **: float\n    estimate of random effects variance tau squared\n\n* **converged **: bool\n    True if iteration has converged.\n\n', 'stemmed comments': ['see', 'start', 'bool', 'moment', 'atol', 'paramet', 'return', 'iter', 'method', 'twostep', 'weight', 'thi', 'valu', 'estim', 'dersimonian', 'eff', 'int', 'toler', 'varianc', '1e5', 'maxit', 'tau2', 'kacker', 'true', 'updat', 'float', 'effect', '2007', 'default', 'tau2_start', 'ndarray', 'repeatedli', 'tau', 'random', 'chang', 'var_eff', 'squar', 'converg', 'number', 'size', 'maximum']}"
183,"{'func name': '_handle_missing', 'comments': '', 'stemmed comments': []}"
184,"{'func name': 'mv_mixture_rvs', 'comments': 'Sample from a mixture of multivariate distributions.\n\nParameters ---------- prob : array_like Probability of sampling from each distribution in dist size : int The length of the returned sample. dist : array_like An iterable of distributions instances with callable method rvs. nvargs : int dimension of the multivariate distribution, could be inferred instead kwargs : tuple of dicts, optional ignored\n\nExamples -------- Say we want 2000 random variables from mixture of normals with two multivariate normal distributions, and we want to sample from the first with probability .4 and the second with probability .6.\n\nimport statsmodels.sandbox.distributions.mv_normal as mvd\n\ncov3 = np.array([[ 1.\n\n,\n\n0.5 ,\n\n0.75], [ 0.5 ,\n\n1.5 ,\n\n0.6 ], [ 0.75,\n\n0.6 ,\n\n2.\n\n]])\n\nmu = np.array([-1, 0.0, 2.0]) mu2 = np.array([4, 2.0, 2.0]) mvn3 = mvd.MVNormal(mu, cov3) mvn32 = mvd.MVNormal(mu2, cov3/2., 4) rvs = mix.mv_mixture_rvs([0.4, 0.6], 2000, [mvn3, mvn32], 3)\n', 'stemmed comments': ['06', 'normal', 'length', '15', '2', 'mixtur', 'array_lik', 'mvn32', '3', 'first', ']', 'mvn3', 'probabl', '6', 'sampl', '4', 'paramet', 'ignor', 'could', '075', 'return', 'mu', 'iter', 'method', 'mvdmvnormal', 'dist', 'prob', '05', 'callabl', 'An', 'two', 'int', 'import', '=', 'nparray', 'second', 'exampl', '2000', 'distribut', '20', 'multivari', 'say', 'kwarg', 'instanc', 'nvarg', '04', '1', 'mvd', 'variabl', 'option', 'instead', 'statsmodelssandboxdistributionsmv_norm', 'infer', 'the', '[', '00', 'dimens', 'dict', 'random', 'mu2', 'mixmv_mixture_rv', 'rv', 'tupl', 'want', 'cov3', 'size', 'cov3/2']}"
185,"{'func name': 'Rpp', 'comments': 'Hessian\n\n\n', 'stemmed comments': ['hessian']}"
186,"{'func name': '_check_index', 'comments': '', 'stemmed comments': []}"
187,"{'func name': 'se_cov', 'comments': 'get standard deviation from covariance matrix\n\njust a shorthand function np.sqrt(np.diag(cov))\n\nParameters ---------- cov : array_like, square covariance matrix\n##### Returns\n* **std **: ndarray\n    standard deviation from diagonal of cov\n\n', 'stemmed comments': ['matrix', 'npdiag', 'std', 'ndarray', 'shorthand', 'array_lik', 'deviat', 'squar', 'standard', 'function', 'diagon', 'get', 'cov', 'paramet', 'npsqrt', 'covari', 'return']}"
188,"{'func name': 'mosaic', 'comments': ""Create a mosaic plot from a contingency table.\n\nIt allows to visualize multivariate categorical data in a rigorous and informative way.\n\nParameters ---------- data : {dict, Series, ndarray, DataFrame} The contingency table that contains the data. Each category should contain a non-negative number with a tuple as index.\n\nIt expects that all the combination of keys to be represents; if that is not true, will automatically consider the missing values as 0.\n\nThe order of the keys will be the same as the one of insertion. If a dict of a Series (or any other dict like object) is used, it will take the keys as labels.\n\nIf a np.ndarray is provided, it will generate a simple numerical labels. index : list, optional Gives the preferred order for the category ordering. If not specified will default to the given order.\n\nIt does not support named indexes for hierarchical Series.\n\nIf a DataFrame is provided, it expects a list with the name of the columns. ax : Axes, optional The graph where display the mosaic. If not given, will create a new figure horizontal : bool, optional The starting direction of the split (by default along the horizontal axis) gap : {float, sequence[float]} The list of gaps to be applied on each subdivision. If the length of the given array is less of the number of subcategories (or if it's a single number) it will extend it with exponentially decreasing gaps properties : dict[str, callable], optional A function that for each tile in the mosaic take the key of the tile and returns the dictionary of properties of the generated Rectangle, like color, hatch or similar. A default properties set will be provided fot the keys whose color has not been defined, and will use color variation to help visually separates the various categories. It should return None to indicate that it should use the default property for the tile. A dictionary of the properties for each key can be passed, and it will be internally converted to the correct function labelizer : dict[str, callable], optional A function that generate the text to display at the center of each tile base on the key of that tile title : str, optional The title of the axis statistic : bool, optional If true will use a crude statistical model to give colors to the plot. If the tile has a constraint that is more than 2 standard deviation from the expected value under independence hypothesis, it will go from green to red (for positive deviations, blue otherwise) and will acquire an hatching when crosses the 3 sigma. axes_label : bool, optional Show the name of each value of each category on the axis (default) or hide them. label_rotation : {float, list[float]} The rotation of the axis label (if present). If a list is given each axis can have a different rotation\n##### Returns\n* **fig **: Figure\n    The figure containing the plot.\n\n* **rects **: dict\n    A dictionary that has the same keys of the original\n    dataset, that holds a reference to the coordinates of the\n    tile and the Rectangle that represent it.\n\n* **>>> data = {'a'**: 10, 'b'\n\n* **>>> data = {('a', 'b')**: 1, ('a', 'c')\n\n* **>>> props = lambda key**: {'color'\n\n* **>>> labelizer = lambda k**: {('a',)\n\n* **...                        ('c',)**: 'third'}[k]\n\n* **>>> data = pd.DataFrame({'gender'**: gender, 'pet'\n\n* **.. plot **: \n\n"", 'stemmed comments': ['dictionari', 'object', 'array', 'dataset', 'independ', 's', ';', 'callabl', 'similar', 'sigma', 'extend', 'provid', 'function', 'c', 'base', 'show', 'ax', 'figur', 'rotat', 'convert', 'rectangl', 'like', 'axe', 'intern', 'correct', 'refer', 'subdivis', 'whose', 'option', 'pet', 'insert', 'subcategori', 'index', 'hierarch', '[', 'numer', 'pass', 'tabl', 'each', 'given', 'datafram', 'way', 'indic', 'specifi', 'plot', 'gender', 'categor', 'deviat', 'cross', 'use', 'categori', 'set', 'paramet', 'axi', 'return', 'acquir', 'axes_label', 'multivari', 'mosaic', 'name', 'label', 'variat', 'take', 'statist', 'rigor', 'origin', 'true', 'str', 'hide', 'float', '10', '{', 'decreas', 'consid', 'allow', 'coordin', 'differ', 'separ', 'prop', 'split', 'number', 'gener', 'nonneg', 'k', 'length', 'hypothesi', 'singl', 'exponenti', '3', 'start', 'rect', 'defin', 'simpl', 'valu', 'help', 'data', 'model', 'green', 'less', 'present', 'properti', 'b', 'variou', 'label_rot', 'inform', 'appli', 'graph', 'repres', 'support', 'creat', 'display', '0', 'default', 'If', 'none', 'titl', 'lambda', 'visual', 'automat', 'go', 'crude', 'center', 'third', 'tupl', 'column', 'text', 'A', 'blue', '2', 'gap', ']', 'bool', 'combin', 'pddatafram', 'fot', 'seri', 'give', 'order', 'otherwis', '=', 'standard', 'hatch', 'hold', 'new', 'prefer', 'contain', 'color', 'expect', '1', 'key', 'sequenc', 'constraint', 'the', '}', 'tile', '>', 'one', 'npndarray', 'along', 'ndarray', 'fig', 'It', 'dict', 'miss', 'conting', 'posit', 'horizont', 'direct', 'red', 'list']}"
189,"{'func name': 'movmoment', 'comments': ""non-central moment\n\nParameters ---------- x : ndarray time series data windsize : int window size lag : 'lagged', 'centered', or 'leading' location of window relative to current position\n##### Returns\n* **mk **: ndarray\n    k-th moving non-central moment, with same shape as x\n\n"", 'stemmed comments': ['x', 'window', 'mk', 'moment', 'seri', 'paramet', 'return', 'current', 'data', 'int', 'locat', 'shape', 'kth', 'lag', 'time', 'rel', 'noncentr', 'move', 'lead', 'ndarray', 'windsiz', 'center', 'posit', 'size']}"
190,"{'func name': 'set_remove_subs', 'comments': 'remove sets that are subsets of another set from a list of tuples\n\nParameters ---------- ssli : list of tuples each tuple is considered as a set\n##### Returns\n* **part **: list of tuples\n    new list with subset tuples removed, it is sorted by set-length of tuples. The\n    list contains original tuples, duplicate elements are not removed.\n\n', 'stemmed comments': ['set', 'paramet', 'return', 'ssli', 'part', 'anoth', 'new', 'origin', 'contain', 'duplic', 'element', 'subset', 'remov', 'the', 'sort', 'consid', 'setlength', 'tupl', 'list']}"
191,"{'func name': 'pairwise_tukeyhsd', 'comments': 'Calculate all pairwise comparisons with TukeyHSD confidence intervals\n\nParameters ---------- endog : ndarray, float, 1d response variable groups : ndarray, 1d array with groups, can be string or integers alpha : float significance level for the test\n##### Returns\n* **results **: TukeyHSDResults instance\n    A results class containing relevant data and some post-hoc\n    calculations, including adjusted p-value\n\n', 'stemmed comments': ['A', 'adjust', 'tukeyhsd', 'group', 'integ', 'array', 'string', 'paramet', 'return', 'comparison', 'class', 'level', 'data', 'relev', 'endog', 'result', 'pairwis', 'calcul', 'instanc', 'includ', 'alpha', 'contain', 'confid', 'float', '1d', 'variabl', 'test', 'pvalu', 'posthoc', 'interv', 'respons', 'ndarray', 'signific', 'tukeyhsdresult']}"
192,"{'func name': 'multigroup', 'comments': 'Test if the given groups are different from the total partition.\n\nGiven a boolean array test if each group has a proportion of positives different than the complexive proportion. The test can be done as an exact Fisher test or approximated as a Chi squared test for more speed.\n\nParameters ---------- pvals: pandas series of boolean the significativity of the variables under analysis groups: dict of list the name of each category of variables under exam. each one is a list of the variables included exact: bool, optional If True (default) use the fisher exact test, otherwise use the chi squared test for contingencies tables. For high number of elements in the array the fisher test can be significantly slower than the chi squared. keep_all: bool, optional if False it will drop those groups where the fraction of positive is below the expected result. If True (default) it will keep all the significant results. alpha: float, optional the significativity level for the pvalue correction on the whole set of groups (not inside the groups themselves).\n##### Returns\n* **result_df**: pandas dataframe\n    for each group returns\n\n* **>>> url = ""https**: //raw.githubusercontent.com/vincentarelbundock/""\n\n', 'stemmed comments': ['fals', 'complex', 'group', 'partit', 'high', 'slower', 'array', 'keep', 'use', 'fisher', 'boolean', 'categori', 'exam', 'speed', 'bool', 'keep_al', 'paramet', 'seri', 'set', 'return', 'approxim', 'level', 'otherwis', '=', 'pval', 'for', 'proport', 'analysi', '//rawgithubusercontentcom/vincentarelbundock/', 'signif', 'name', 'result', 'url', 'includ', 'alpha', 'panda', 'true', 'element', 'fraction', 'expect', 'variabl', 'drop', 'float', 'option', 'test', 'pvalu', 'correct', 'result_df', 'exact', 'significantli', 'the', '>', 'done', 'whole', 'one', 'default', 'If', 'differ', 'dict', 'insid', 'signific', 'conting', 'squar', 'tabl', 'posit', 'chi', 'http', 'given', 'datafram', 'number', 'list', 'total']}"
193,"{'func name': 'local_fdr', 'comments': 'Calculate local FDR values for a list of Z-scores.\n\nParameters ---------- zscores : array_like A vector of Z-scores null_proportion : float The assumed proportion of true null hypotheses null_pdf : function mapping reals to positive reals The density of null Z-scores; if None, use standard normal deg : int The maximum exponent in the polynomial expansion of the density of non-null Z-scores nbins : int The number of bins for estimating the marginal density of Z-scores. alpha : float Use Poisson ridge regression with parameter alpha to estimate the density of non-null Z-scores.\n##### Returns\n* **fdr **: array_like\n    A vector of FDR values\n\n* **Model.  Statistical Science 23**: 1, 1-22.\n\n* **Basic use (the null Z-scores are taken to be standard normal)**: \n\n* **Use a Gaussian null distribution estimated from the data**: \n\n', 'stemmed comments': ['A', 'normal', 'null', 'regress', 'array_lik', 'hypothes', 'margin', 'densiti', 'use', 'nonnul', 'scienc', 'vector', 'expans', 'paramet', 'null_proport', 'return', ';', 'null_pdf', 'basic', 'deg', 'valu', 'polynomi', 'estim', 'assum', 'int', 'model', 'data', 'standard', 'function', 'zscore', 'proport', '122', 'distribut', 'calcul', 'gaussian', 'statist', 'alpha', 'true', 'expon', '1', 'float', '23', 'poisson', 'fdr', 'the', 'ridg', 'none', 'map', 'posit', 'real', 'nbin', 'taken', 'local', 'bin', 'number', 'list', 'maximum']}"
194,"{'func name': '_multivariate_test', 'comments': ""Multivariate linear model hypotheses testing\n\nFor y = x * params, where y are the dependent variables and x are the independent variables, testing L * params * M = 0 where L is the contrast matrix for hypotheses testing and M is the transformation matrix for transforming the dependent variables in y.\n\nAlgorithm: T = L*inv(X'X)*L' H = M'B'L'*inv(T)*LBM E =\n\nM'(Y'Y\n\n- B'X'XB)M And then finding the eigenvalues of inv(H + E)*H\n\n.. [*] https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_introreg_sect012.htm\n\nParameters ---------- %(hypotheses_doc)s k_xvar : int The number of independent variables k_yvar : int The number of dependent variables fn : function a function fn(contrast_L, transform_M) that returns E, H, q, df_resid where q is the rank of T matrix\n##### Returns\n* **results **: MANOVAResults\n\n"", 'stemmed comments': ['x', 'contrast', 'B', 'transform', 'H', 'hypothes', 'algorithm', ']', 'transform_m', 'T', 'k_yvar', 'fn', 'paramet', 'independ', 'return', 'inv', 'X', 'statug_introreg_sect012htm', 'matrix', 'xxb', 'model', '=', 'int', 'for', 'df_resid', 'function', 'y', 'Y', 'E', 'contrast_l', 'multivari', 'result', 'L', '//supportsascom/documentation/cdl/en/statug/63033/html/default/viewerhtm', 'and', 'variabl', 'eigenvalu', '%', 'test', 'find', 'manovaresult', 'depend', 'the', 'hypotheses_doc', 'k_xvar', '[', 'lbm', '0', 'rank', 'q', 'linear', 'http', 'param', 'number', 'M']}"
195,"{'func name': 'cc_stats', 'comments': ""MANOVA statistics based on canonical correlation coefficient\n\nCalculates Pillai's Trace, Wilk's Lambda, Hotelling's Trace and Roy's Largest Root.\n\nParameters ---------- x1, x2 : ndarrays, 2_D two 2-dimensional data arrays, observations in rows, variables in columns demean : bool If demean is true, then the mean is subtracted from each variable.\n##### Returns\n* **res **: dict\n    Dictionary containing the test statistics.\n\n* **missing**: F-statistics and p-values\n\n* **TODO**: should return a results class instead\n\n"", 'stemmed comments': ['subtract', 'x1', 'largest', 'coeffici', 'dictionari', 'wilk', 'array', 're', 'bool', 'fstatist', 'paramet', 'todo', 'pillai', 's', 'return', 'class', 'root', 'two', 'data', 'base', 'result', 'roy', 'calcul', 'statist', 'correl', 'x2', 'true', 'manova', 'demean', 'variabl', 'contain', 'pvalu', 'mean', 'test', 'instead', '2_d', '2dimension', 'observ', 'If', 'lambda', 'ndarray', 'dict', 'miss', 'canon', 'trace', 'row', 'column', 'hotel']}"
196,"{'func name': 'multivariate_t_rvs', 'comments': 'generate random variables of multivariate t distribution\n\nParameters ---------- m : array_like mean of random variable, length determines dimension of random variable S : array_like square array of covariance\n\nmatrix df : int or float degrees of freedom n : int number of observations, return random array will be (n, len(m))\n##### Returns\n* **rvs **: ndarray, (n, len(m))\n    each row is an independent draw of a multivariate t distributed\n    random variable\n\n', 'stemmed comments': ['length', 'array_lik', 'array', 'n', 'determin', 'len', 'paramet', 'independ', 'return', 'matrix', 'S', 'draw', 'int', 'degre', 'distribut', 'multivari', 'variabl', 'float', 'df', 'mean', 'freedom', 'covari', 'observ', 'ndarray', 'dimens', 'random', 'squar', 'row', 'rv', 'number', 'gener']}"
197,"{'func name': 'test_cov_oneway', 'comments': 'Multiple sample hypothesis test that covariance matrices are equal.\n\nThis is commonly known as Box-M test.\n\nThe Null and alternative hypotheses are\n\n.. math::\n\nH0 &: \\Sigma_i = \\Sigma_j\n\n\\text{ for all i and j} \\\\ H1 &: \\Sigma_i \\neq \\Sigma_j \\text{ for at least one i and j}\n\nwhere :math:`\\Sigma_i` is the covariance of sample `i`.\n\nParameters ---------- cov_list : list of array_like Covariance matrices of the sample, estimated with denominator ``(N\n\n- 1)``, i.e. `ddof=1`. nobs_list : list List of the number of observations used in the estimation of the covariance for each sample.\n##### Attributes\n* **Multivariate Analysis**: Rencher/Methods. Wiley Series in Probability and\n\n* **Statistics. Hoboken, NJ, USA**: John Wiley & Sons, Inc.\n\n* **https**: //doi.org/10.1002/9781118391686.\n\n* **StataCorp, L. P. Stata Multivariate Statistics**: Reference Manual.\n\n##### Returns\n* **res **: instance of HolderTuple\n    Results contains test statistic and pvalues for both chisquare and F\n    distribution based tests, identified by the name ending ""_chi2"" and\n    ""_f"".\n\n', 'stemmed comments': ['N', 'ddof=1', 'multipl', '\\\\', 'holdertupl', 'sampl', '\\text', 'P', 'base', 'distribut', 'instanc', 'refer', 'j', 'test', '//doiorg/101002/9781118391686', 'stata', 'chisquar', 'least', 'cov_list', 'use', '\\neq', '&', 'paramet', 'return', 'boxm', 'statacorp', 'equal', 'estim', 'manual', 'ie', 'H0', 'usa', 'F', 'john', 'multivari', 'result', 'name', '_f', 'statist', 'attribut', 'pvalu', '{', 'denomin', 'number', 'null', 'hypothesi', 'probabl', 'math', 'rencher/method', '_chi2', 'altern', '\\sigma_i', 'covari', '\\sigma_j', 'http', 'known', 'NJ', 'array_lik', 'hypothes', 're', 'seri', 'inc', 'matric', 'thi', '=', 'identifi', 'analysi', 'nobs_list', 'L', 'commonli', 'contain', '1', 'end', 'son', 'the', '}', 'one', 'observ', 'wiley', 'H1', 'hoboken', 'list']}"
198,"{'func name': 'mutualinfo_binned', 'comments': ""mutual information of two random variables estimated with kde\n\nNotes ----- bins='auto' selects the number of bins so that approximately 5 observations are expected to be in each bin under the assumption of independence. This follows roughly the description in Kahn et al. 2007\n"", 'stemmed comments': ['note', '5', 'kde', 'kahn', 'independ', 'approxim', 'bins=auto', 'thi', 'estim', 'two', 'select', 'roughli', 'inform', 'expect', 'variabl', 'follow', '2007', 'et', 'observ', 'descript', 'random', 'al', 'mutual', 'bin', 'assumpt', 'number']}"
199,"{'func name': 'quad2d', 'comments': '', 'stemmed comments': []}"
200,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
201,"{'func name': 'estimate_location', 'comments': 'M-estimator of location using self.norm and a current estimator of scale.\n\nThis iteratively finds a solution to\n\nnorm.psi((a-mu)/scale).sum() == 0\n\nParameters ---------- a : ndarray Array over which the location parameter is to be estimated scale : ndarray Scale parameter to be used in M-estimator norm : RobustNorm, optional Robust norm used in the M-estimator.\n\nThe default is HuberT(). axis : int, optional Axis along which to estimate the location parameter.\n\nThe default is 0. initial : ndarray, optional Initial condition for the location parameter.\n\nDefault is None, which uses the median of a. niter : int, optional Maximum number of iterations.\n\nThe default is 30. tol : float, optional Toleration for convergence.\n\nThe default is 1e-06.\n##### Returns\n* **mu **: ndarray\n    Estimate of location\n\n', 'stemmed comments': ['/scale', 'initi', 'robustnorm', 'array', 'use', 'hubert', 'paramet', 'axi', '==', 'return', 'mu', 'iter', 'mestim', 'current', 'thi', 'estim', 'scale', 'tol', 'niter', 'int', 'locat', 'toler', 'condit', '1e06', 'float', 'option', 'normpsi', 'sum', '30', 'amu', 'solut', 'selfnorm', 'find', 'the', 'median', 'robust', '0', 'default', 'along', 'none', 'ndarray', 'converg', 'norm', 'number', 'maximum', 'a']}"
202,"{'func name': 'approx_hess3', 'comments': '', 'stemmed comments': []}"
203,"{'func name': 'lstsq', 'comments': 'Shim that allows modern rcond setting with backward compat for NumPY earlier than 1.14\n\n\n', 'stemmed comments': ['allow', 'rcond', 'earlier', '114', 'numpi', 'compat', 'set', 'shim', 'backward', 'modern']}"
204,"{'func name': 'dropname', 'comments': 'drop names from a list of strings, names to drop are in space delimited list does not change original list\n\n\n', 'stemmed comments': ['origin', 'chang', 'drop', 'space', 'string', 'name', 'list', 'delimit']}"
205,"{'func name': 'equivalence_scale_oneway', 'comments': 'Oneway Anova test for equivalence of scale, variance or dispersion\n\nThis hypothesis test performs a oneway equivalence anova test on transformed data.\n\nNote, the interpretation of the equivalence margin `equiv_margin` will depend on the transformation of the data. Transformations like absolute deviation are not scaled to correspond to the variance under normal distribution.\n', 'stemmed comments': ['note', 'normal', 'hypothesi', 'transform', 'deviat', 'dispers', 'margin', 'equiv_margin', 'correspond', 'thi', 'scale', 'absolut', 'data', 'oneway', 'varianc', 'distribut', 'like', 'interpret', 'test', 'depend', 'anova', 'perform', 'equival']}"
206,"{'func name': 'get_file_obj', 'comments': ""Light wrapper to handle strings, path objects and let files (anything else) pass through.\n\nIt also handle '.gz' files.\n\nParameters ---------- fname : str, path object or file-like object File to open / forward mode : str Argument passed to the 'open' or 'gzip.open' function encoding : str For Python 3 only, specify the encoding of the file\n##### Returns\n"", 'stemmed comments': ['anyth', '3', 'object', 'els', 'let', 'path', 'light', 'string', 'paramet', 'argument', 'return', 'open', 'fname', 'filelik', 'for', 'function', 'handl', 'forward', 'gzipopen', 'gz', '/', 'str', 'encod', 'python', 'also', 'mode', 'It', 'pass', 'wrapper', 'file', 'specifi']}"
207,"{'func name': '_fit_basinhopping', 'comments': '', 'stemmed comments': []}"
208,"{'func name': 'summary_table', 'comments': 'Generate summary table of outlier and influence similar to SAS\n\nParameters ---------- alpha : float significance level for confidence interval\n##### Returns\n* **st **: SimpleTable\n   table with results that can be printed\n\n* **data **: ndarray\n   calculated measures and statistics for the table\n\n* **ss2 **: list[str]\n   column_names for table (Note\n\n', 'stemmed comments': ['note', 'column_nam', ']', 'sa', 'ss2', 'paramet', 'return', 'measur', 'level', 'similar', 'data', 'simplet', 'result', 'calcul', 'statist', 'alpha', 'str', 'confid', 'float', 'interv', 'outlier', '[', 'ndarray', 'influenc', 'signific', 'summari', 'tabl', 'print', 'gener', 'list', 'st']}"
209,"{'func name': '_pfixed', 'comments': '', 'stemmed comments': []}"
210,"{'func name': 'to_numpy', 'comments': 'Workaround legacy pandas lacking to_numpy\n\nParameters ---------- po : Pandas obkect\n##### Returns\n', 'stemmed comments': ['obkect', 'to_numpi', 'panda', 'paramet', 'po', 'legaci', 'workaround', 'return', 'lack']}"
211,"{'func name': 'whiten_individuals_loop', 'comments': 'apply linear transform for each individual\n\nloop version\n', 'stemmed comments': ['transform', 'linear', 'individu', 'appli', 'loop', 'version']}"
212,"{'func name': 'repanel_cov', 'comments': 'calculate error covariance matrix for random effects model\n\nParameters ---------- groups : ndarray, (nobs, nre) or (nobs,) array of group/category observations sigma : ndarray, (nre+1,) array of standard deviations of random effects, last element is the standard deviation of the idiosyncratic error\n##### Returns\n* **omega **: ndarray, (nobs, nobs)\n    covariance matrix of error\n\n* **omegainv **: ndarray, (nobs, nobs)\n    inverse covariance matrix of error\n\n* **omegainvsqrt **: ndarray, (nobs, nobs)\n    squareroot inverse covariance matrix of error\n    such that omega = omegainvsqrt * omegainvsqrt.T\n\n', 'stemmed comments': ['group/categori', 'omegainvsqrt', 'idiosyncrat', 'invers', 'group', 'deviat', 'array', 'error', 'paramet', 'nre', 'return', 'matrix', 'squareroot', 'nre1', 'nob', 'model', 'last', '=', 'sigma', 'standard', 'calcul', 'omegainv', 'element', 'effect', 'covari', 'observ', 'ndarray', 'omega', 'random', 'omegainvsqrtt']}"
213,"{'func name': 'parallel_func', 'comments': 'Return parallel instance with delayed function\n\nUtil function to use joblib only if available\n\nParameters ---------- func: callable A function n_jobs: int Number of jobs to run in parallel verbose: int Verbosity level\n##### Returns\n* **parallel**: instance of joblib.Parallel or list\n    The parallel object\n\n* **my_func**: callable\n    func if not parallel or delayed(func)\n\n* **n_jobs**: int\n    Number of jobs >= 0\n\n', 'stemmed comments': ['A', 'parallel', 'util', 'verbos', 'run', 'object', 'use', 'paramet', 'delay', 'avail', 'return', 'level', 'callabl', 'int', '=', 'function', 'instanc', 'joblibparallel', 'my_func', 'joblib', 'the', '>', '0', 'job', 'n_job', 'number', 'list', 'func']}"
214,"{'func name': 'load_results_jmulti', 'comments': ""Parameters ---------- dataset : module A data module in the statsmodels/datasets directory that defines a __str__() method returning the dataset's name. dt_s_list : list A list of strings where each string represents a combination of deterministic terms.\n\n\n##### Returns\n* **result **: dict\n    A dict (keys\n\n"", 'stemmed comments': ['A', 'modul', 'combin', 'string', 'paramet', 'defin', 'dataset', 's', 'return', 'method', 'data', 'dt_s_list', 'name', 'result', 'determinist', 'statsmodels/dataset', 'key', 'term', 'repres', 'directori', 'dict', '__str__', 'list']}"
215,"{'func name': 'load_results_jmulti', 'comments': ""Parameters ---------- dataset : module A data module in the statsmodels/datasets directory that defines a __str__() method returning the dataset's name. dt_s_list : list A list of strings where each string represents a combination of deterministic terms.\n\n\n##### Returns\n* **result **: dict\n    A dict (keys\n\n"", 'stemmed comments': ['A', 'modul', 'combin', 'string', 'paramet', 'defin', 'dataset', 's', 'return', 'method', 'data', 'dt_s_list', 'name', 'result', 'determinist', 'statsmodels/dataset', 'key', 'term', 'repres', 'directori', 'dict', '__str__', 'list']}"
216,"{'func name': 'make_plot', 'comments': '', 'stemmed comments': []}"
217,"{'func name': 'pca', 'comments': ""Perform Principal Component Analysis (PCA).\n\nParameters ---------- data : ndarray Variables in columns, observations in rows. ncomp : int, optional Number of components to return.\n\nIf None, returns the as many as the smaller to the number of rows or columns of data. standardize : bool, optional Flag indicating to use standardized data with mean 0 and unit variance.\n\nstandardized being True implies demean. demean : bool, optional Flag indicating whether to demean data before computing principal components.\n\ndemean is ignored if standardize is True. normalize : bool , optional Indicates whether th normalize the factors to have unit inner product.\n\nIf False, the loadings will have unit inner product. gls : bool, optional Flag indicating to implement a two-step GLS estimator where in the first step principal components are used to estimate residuals, and then the inverse residual variance is used as a set of weights to estimate the final principal components weights : ndarray, optional Series weights to use after transforming data according to standardize or demean when computing the principal components. method : str, optional Determines the linear algebra routine uses.\n\n'eig', the default, uses an eigenvalue decomposition. 'svd' uses a singular value decomposition.\n##### Returns\n* **factors **: {ndarray, DataFrame}\n    Array (nobs, ncomp)of of principal components (also known as scores).\n\n* **loadings **: {ndarray, DataFrame}\n    Array (ncomp, nvar) of  principal component loadings for constructing\n    the factors.\n\n* **projection **: {ndarray, DataFrame}\n    Array (nobs, nvar) containing the projection of the data onto the ncomp\n    estimated factors.\n\n* **rsquare **: {ndarray, Series}\n    Array (ncomp,) where the element in the ith position is the R-square\n    of including the fist i principal components.  The values are\n    calculated on the transformed data, not the original data.\n\n* **ic **: {ndarray, DataFrame}\n    Array (ncomp, 3) containing the Bai and Ng (2003) Information\n    criteria.  Each column is a different criteria, and each row\n    represents the number of included factors.\n\n* **eigenvals **: {ndarray, Series}\n    Array of eigenvalues (nvar,).\n\n* **eigenvecs **: {ndarray, DataFrame}\n    Array of eigenvectors. (nvar, nvar).\n\n"", 'stemmed comments': ['invers', 'array', 'determin', 'twostep', 'algebra', 'construct', 'calcul', 'option', 'eig', 'singular', 'implement', 'Ng', 'eigenvec', 'each', 'eigenvector', 'datafram', 'indic', 'comput', 'smaller', 'load', 'use', 'compon', 'set', 'paramet', 'ignor', 'return', 'score', 'method', 'weight', 'estim', 'nob', 'step', 'final', 'eigenv', 'origin', 'true', 'str', 'element', 'variabl', 'eigenvalu', '{', 'whether', '2003', 'onto', 'differ', 'product', 'nvar', 'number', 'fals', 'transform', 'ncomp', '3', 'first', 'bai', 'fist', 'factor', 'th', 'decomposit', 'valu', 'data', 'int', 'residu', 'includ', 'inform', 'mean', 'repres', 'gl', '0', 'routin', 'none', 'If', 'default', 'linear', 'known', 'column', 'normal', 'bool', 'project', 'seri', 'inner', 'standard', 'varianc', 'analysi', 'princip', 'svd', 'rsquar', 'contain', 'unit', 'demean', 'criteria', 'also', 'flag', 'accord', 'ith', '}', 'the', 'observ', 'ndarray', 'pca', 'ic', 'posit', 'row', 'impli', 'perform', 'mani']}"
218,"{'func name': 'coef_restriction_diffseq', 'comments': '', 'stemmed comments': []}"
219,"{'func name': 'scatter_ellipse', 'comments': 'Create a grid of scatter plots with confidence ellipses.\n\nell_kwds, plot_kdes not used yet\n\nlooks ok with 5 or 6 variables, too crowded with 8, too empty with 1\n\nParameters ---------- data : array_like Input data. level : scalar, optional Default is 0.9. varnames : list[str], optional Variable names.\n\nUsed for y-axis labels, and if `add_titles` is True also for titles.\n\nIf not given, integers 1..data.shape[1] are used. ell_kwds : dict, optional UNUSED plot_kwds : dict, optional UNUSED add_titles : bool, optional Whether or not to add titles to each subplot.\n\nDefault is False. Titles are constructed from `varnames`. keep_ticks : bool, optional If False (default), remove all axis ticks. fig : Figure, optional If given, this figure is simply returned.\n\nOtherwise a new figure is created.\n##### Returns\n* **..plot **: \n\n', 'stemmed comments': ['plot_kd', 'fals', 'grid', 'plot', '5', 'keep_tick', 'array_lik', 'tick', '8', 'integ', ']', 'use', 'bool', '6', 'look', 'paramet', 'yet', 'axi', 'return', 'unus', 'construct', 'level', 'data', 'otherwis', 'simpli', 'scatter', 'name', 'new', 'label', 'figur', 'plot_kwd', 'true', 'str', '1', 'confid', 'variabl', 'option', 'yaxi', 'also', 'whether', 'add', 'ell_kwd', 'remov', 'creat', '[', 'ellips', 'varnam', 'default', 'If', 'titl', 'crowd', 'empti', 'dict', 'fig', 'datashap', 'subplot', 'given', '09', 'ok', 'input', 'add_titl', 'list', 'scalar']}"
220,"{'func name': 'beanplot', 'comments': 'helper function to try out different plot options\n\n\n', 'stemmed comments': ['helper', 'plot', 'differ', 'option', 'function', 'tri']}"
221,"{'func name': 'plot_loadings', 'comments': 'Plot factor loadings in 2-d plots\n\nParameters ---------- loadings : array like Each column is a component (or factor) col_names : a list of strings column names of `loadings` row_names : a list of strings row names of `loadings` loading_pairs : None or a list of tuples Specify plots. Each tuple (i, j) represent one figure, i and j is the loading number for x-axis and y-axis, respectively. If `None`, all combinations of the loadings will be plotted. percent_variance : array_like The percent variance explained by each factor.\n##### Returns\n* **figs **: a list of figure handles\n\n', 'stemmed comments': ['row_nam', 'plot', 'load', 'array_lik', 'col_nam', 'array', 'compon', 'combin', 'string', 'paramet', 'factor', 'return', 'loading_pair', 'respect', 'varianc', 'percent', 'name', 'handl', 'figur', 'percent_vari', 'like', 'j', 'yaxi', 'repres', 'the', 'one', 'number', 'explain', 'none', 'If', '2d', 'fig', 'each', 'xaxi', 'row', 'tupl', 'specifi', 'column', 'list']}"
222,"{'func name': '_get_irf_plot_config', 'comments': '', 'stemmed comments': []}"
223,"{'func name': 'rainbow', 'comments': 'Returns a list of colors sampled at equal intervals over the spectrum.\n\nParameters ---------- n : int The number of colors to return\n##### Returns\n* **R **: (n,3) array\n    An of rows of RGB color values\n\n', 'stemmed comments': ['array', 'n', 'sampl', 'paramet', 'return', 'n3', 'rgb', 'equal', 'valu', 'An', 'int', 'R', 'color', 'interv', 'the', 'row', 'spectrum', 'number', 'list']}"
224,"{'func name': 'ftest_power', 'comments': 'Calculate the power of a F-test.\n\nParameters ---------- effect_size : float standardized effect size, mean divided by the standard deviation. effect size has to be positive. df_num : int or float numerator degrees of freedom. df_denom : int or float denominator degrees of freedom. alpha : float in interval (0,1) significance level, e.g. 0.05, is the probability of a type I error, that is wrong rejections if the Null Hypothesis is true. ncc : int degrees of freedom correction for non-centrality parameter. see Notes\n##### Returns\n* **power **: float\n    Power of the test, e.g. 0.8, is one minus the probability of a\n    type II error. Power is the probability that the test correctly\n    rejects the Null Hypothesis if the Alternative Hypothesis is true.\n\n* **ncc=1 matches the non-centrality parameter in R**: \n\n', 'stemmed comments': ['wrong', 'see', 'null', 'hypothesi', 'note', 'deviat', 'ncc', 'probabl', 'match', 'error', 'paramet', 'return', 'effect_s', 'level', '08', 'eg', 'int', 'standard', 'degre', 'minu', 'ncc=1', '01', 'calcul', 'df_num', 'R', 'type', 'alpha', 'divid', '005', 'true', 'correct', 'power', 'altern', 'float', 'effect', 'mean', 'freedom', 'noncentr', 'test', 'interv', 'I', 'one', 'denomin', 'numer', 'reject', 'signific', 'II', 'posit', 'ftest', 'df_denom', 'correctli', 'size']}"
225,"{'func name': '_glm_basic_scr', 'comments': 'The basic SCR from (Sun et al. Annals of Statistics 2000).\n\nComputes simultaneous confidence regions (SCR).\n\nParameters ---------- result : results instance The fitted GLM results instance exog : array_like The exog values spanning the interval alpha : float `1\n\n- alpha` is the coverage probability.\n##### Returns\n', 'stemmed comments': ['comput', 'array_lik', 'coverag', 'probabl', 'paramet', 'return', 'span', 'basic', 'valu', '2000', 'result', 'instanc', 'statist', 'alpha', '1', 'confid', 'float', 'interv', 'simultan', 'the', 'glm', 'et', 'fit', 'sun', 'region', 'scr', 'al', 'exog', 'annal']}"
226,"{'func name': 'wls_prediction_std', 'comments': 'calculate standard deviation and confidence interval for prediction\n\napplies to WLS and OLS, not to general GLS, that is independently but not identically distributed observations\n\nParameters ---------- res : regression result instance results of WLS or OLS regression required attributes see notes exog : array_like (optional) exogenous variables for points to predict weights : scalar or array_like (optional) weights as defined for WLS (inverse of variance of observation) alpha : float (default: alpha = 0.05) confidence level for two-sided hypothesis\n##### Returns\n* **predstd **: array_like, 1d\n    standard error of prediction\n    same length as rows of exog\n\n* **interval_l, interval_u **: array_like\n    lower und upper confidence bounds\n\n* **res.model.predict() **: predicted values or\n\n* **res.fittedvalues **: values used in estimation\n\n* **res.cov_params() **: covariance matrix of parameter estimates\n\n* **testing status**: not compared with other packages\n\n', 'stemmed comments': ['see', 'note', 'twosid', 'hypothesi', 'regress', 'array_lik', 'invers', 'length', 'deviat', 'interval_l', 're', 'use', 'error', 'paramet', 'independ', 'defin', 'return', 'requir', 'matrix', 'weight', 'level', 'valu', 'estim', 'wl', '=', 'standard', 'lower', 'point', 'varianc', 'statu', 'distribut', 'upper', 'result', 'exogen', 'calcul', 'instanc', 'und', 'rescov_param', 'alpha', '005', 'attribut', 'confid', 'variabl', 'float', 'option', 'appli', 'ol', '1d', 'test', 'interv', 'resmodelpredict', 'gl', 'covari', 'interval_u', 'predict', 'ident', 'observ', 'default', 'predstd', 'bound', 'packag', 'resfittedvalu', 'compar', 'exog', 'row', 'gener', 'scalar']}"
227,"{'func name': 'show_versions', 'comments': 'List the versions of statsmodels and any installed dependencies\n\nParameters ---------- show_dirs : bool Flag indicating to show module locations\n', 'stemmed comments': ['statsmodel', 'modul', 'instal', 'depend', 'paramet', 'locat', 'bool', 'show_dir', 'flag', 'version', 'show', 'indic', 'list']}"
228,"{'func name': '_check_args', 'comments': '', 'stemmed comments': []}"
229,"{'func name': '_confint_riskratio_paired_nam', 'comments': 'confidence interval for marginal risk ratio for matched pairs\n\nneed full table\n\nsuccess fail\n\nmarginal success\n\n\n\nx11\n\n\n\nx10\n\nx1. fail\n\n\n\n\n\n x01\n\n\n\nx00\n\nx0. marginal\n\n x.1\n\n\n\nx.0\n\n n\n\nThe confidence interval is for the ratio p1 / p0 where p1 = x1. / n and p0\n\n- x.1 / n Todo: rename p1 to pa and p2 to pb, so we have a, b for treatment and 0, 1 for success/failure\n\ncurrent namings follow Nam 2009\n\nstatus testing: compared to example in Nam 2009 internal polynomial coefficients in calculation correspond at around 4 decimals confidence interval agrees only at 2 decimals\n', 'stemmed comments': ['x1', 'p1', 'x11', 'coeffici', '2', 'renam', 'agre', 'margin', 'n', 'match', 'pair', '4', 'todo', 'correspond', 'x01', 'current', 'polynomi', 'x10', '=', 'ratio', 'pa', 'statu', 'around', 'x0', 'p0', 'success', 'name', 'calcul', 'b', 'x00', '/', 'intern', '1', 'confid', 'nam', 'test', 'follow', 'need', 'interv', 'the', 'success/failur', 'full', '0', 'pb', 'fail', 'p2', 'compar', 'tabl', 'decim', 'treatment', 'risk', 'exampl', '2009']}"
230,"{'func name': 'with_metaclass', 'comments': 'Create a base class with a metaclass.\n\n\n', 'stemmed comments': ['base', 'creat', 'class', 'metaclass']}"
231,"{'func name': 'psturng', 'comments': 'Evaluates the probability from 0 to q for a studentized range having v degrees of freedom and r samples.\n\nParameters ---------- q : (scalar, array_like) quantile value of Studentized Range q >= 0. r : (scalar, array_like) The number of samples r >= 2 and r <= 200 (values over 200 are permitted but not recommended) v : (scalar, array_like) The sample degrees of freedom if p >= .9: v >=1 and v >= inf else: v >=2 and v >= inf\n##### Returns\n* **p **: (scalar, array_like)\n    1. - area from zero to q under the Studentized Range\n    distribution. When v == 1, p is bound between .001\n    and .1, when v > 1, p is bound between .001 and .9.\n    Values between .5 and .9 are 1st order appoximations.\n\n', 'stemmed comments': ['5', '2', 'array_lik', 'inf', 'els', 'probabl', 'p', 'sampl', 'v', 'paramet', '200', 'return', '==', 'order', '<', 'valu', 'evalu', '1st', '=', 'permit', '=1', 'degre', 'distribut', '001', 'recommend', '1', 'freedom', 'rang', 'when', '>', 'the', 'quantil', 'student', 'zero', '0', 'bound', 'q', 'r', '=2', 'appoxim', '9', 'number', 'area', 'scalar']}"
232,"{'func name': 'chamberlain', 'comments': '', 'stemmed comments': []}"
233,"{'func name': 'fit_model', 'comments': '', 'stemmed comments': []}"
234,"{'func name': 'prob_quantize_cdf_old', 'comments': 'quantize a continuous distribution given by a cdf\n\nold version without precomputing cdf values\n\nParameters ---------- binsx : array_like, 1d binedges\n', 'stemmed comments': ['precomput', 'valu', 'array_lik', 'paramet', '1d', 'quantiz', 'cdf', 'given', 'continu', 'version', 'distribut', 'binedg', 'binsx', 'old', 'without']}"
235,"{'func name': 'tost_poisson_2indep', 'comments': ""Equivalence test based on two one-sided `test_proportions_2indep`\n\nThis assumes that we have two independent binomial samples.\n\nThe Null and alternative hypothesis for equivalence testing are\n\nH0: g1 / g2 <= low or upp <= g1 / g2 H1: low < g1 / g2 < upp\n\nwhere g1 and g2 are the Poisson rates.\n\nParameters ---------- count1: int Number of events in first sample exposure1: float Total exposure (time * subjects) in first sample count2: int Number of events in first sample exposure2: float Total exposure (time * subjects) in first sample low, upp : equivalence margin for the ratio of Poisson rates method: string Method for the test statistic and the p-value. Defaults to `'score'`. Current Methods are based on Gu et. al 2008 Implemented are 'wald', 'score' and 'sqrt' based asymptotic normal distribution, and the exact conditional test 'exact-cond', and its mid-point version 'cond-midp', see Notes\n##### Returns\n* **pvalue **: float\n    p-value is the max of the pvalues of the two one-sided tests\n\n* **t1 **: test results\n    results instance for one-sided hypothesis at the upper margin\n\n* **'wald'**: method W1A, wald test, variance based on separate estimates\n\n* **'score'**: method W2A, score test, variance based on estimate under Null\n\n* **'wald-log'**: W3A  not implemented\n\n* **'sqrt'**: W5A, based on variance stabilizing square root transformation\n\n* **'exact-cond'**: exact conditional test based on binomial distribution\n\n* **'cond-midp'**: midpoint-pvalue of exact conditional test\n\n* **Gu, Ng, Tang, Schucany 2008**: Testing the Ratio of Two Poisson Rates,\n\n"", 'stemmed comments': ['midpoint', 'sampl', 'independ', 'subject', 'root', 'exposure2', 'w1a', 'base', 'distribut', 'instanc', '/', 'test', 'implement', 'Ng', 'waldlog', 'schucani', 'test_proportions_2indep', 'al', 'max', 'asymptot', 'see', 'note', 't1', 'margin', 'paramet', 'return', 'score', 'method', 'estim', 'two', 'H0', 'g1', 'binomi', 'ratio', 'upper', 'result', 'statist', 'sqrt', 'time', 'float', 'pvalu', 'tang', 'separ', 'squar', 'upp', 'wald', 'exposur', 'number', 'total', 'null', 'hypothesi', 'stabil', 'transform', 'onesid', 'first', 'count1', 'exposure1', 'rate', 'current', 'assum', 'int', 'g2', 'version', 'condmidp', 'condit', 'altern', 'low', 'default', 'exactcond', 'event', 'equival', 'normal', 'midpointpvalu', 'string', '<', 'Gu', '2008', 'thi', '=', 'varianc', 'w2a', 'poisson', 'exact', 'the', 'et', 'w3a', 'H1', 'count2', 'w5a']}"
236,"{'func name': 'ewma', 'comments': '', 'stemmed comments': []}"
237,"{'func name': 'added_variable_resids', 'comments': ""Residualize the endog variable and a 'focus' exog variable in a regression model with respect to the other exog variables.\n\nParameters ---------- results : regression results instance A fitted model including the focus exog and all other predictors of interest. focus_exog : {int, str} The column of results.model.exog or a variable name that is to be residualized against the other predictors. resid_type : str The type of residuals to use for the dependent variable.\n\nIf None, uses `resid_deviance` for GLM/GEE and `resid` otherwise. use_glm_weights : bool Only used if the model is a GLM or GEE.\n\nIf True, the residuals for the focus predictor are computed using WLS, with the weights obtained from the IRLS calculations for fitting the GLM.\n\nIf False, unweighted regression is used. fit_kwargs : dict, optional Keyword arguments to be passed to fit when refitting the model.\n##### Returns\n* **endog_resid **: array_like\n    The residuals for the original exog\n\n* **focus_exog_resid **: array_like\n    The residuals for the focus predictor\n\n"", 'stemmed comments': ['comput', 'A', 'glm/gee', 'fals', 'regress', 'keyword', 'array_lik', 'irl', 'resid', 'use', 'bool', 'paramet', 'focu', 'argument', 'return', 'resid_devi', 'weight', 'predictor', 'int', 'model', 'otherwis', 'gee', 'wl', 'fit_kwarg', 'refit', 'respect', 'result', 'endog', 'residu', 'name', 'calcul', 'instanc', 'resid_typ', 'includ', 'type', 'origin', 'true', 'str', 'interest', 'variabl', 'use_glm_weight', 'option', '{', 'focus_exog', 'the', '}', 'fit', 'depend', 'onli', 'resultsmodelexog', 'glm', 'focus_exog_resid', 'none', 'If', 'dict', 'pass', 'obtain', 'exog', 'endog_resid', 'unweight', 'column']}"
238,"{'func name': '_calc_approx_inv_cov', 'comments': 'calculates the approximate inverse covariance matrix\n\nParameters ---------- nodewise_row_l : list A list of array-like object where each object corresponds to the nodewise_row values for the corresponding variable, should be length p. nodewise_weight_l : list A list of scalars where each scalar corresponds to the nodewise_weight value for the corresponding variable, should be length p.\n##### Returns\n', 'stemmed comments': ['A', 'length', 'invers', 'object', 'p', 'paramet', 'correspond', 'return', 'approxim', 'matrix', 'valu', 'arraylik', 'calcul', 'variabl', 'nodewise_weight_l', 'nodewise_row_l', 'nodewise_row', 'covari', 'nodewise_weight', 'list', 'scalar']}"
239,"{'func name': '_shift_intercept', 'comments': 'A convenience function to make the SAS covariance matrix compatible with statsmodels.rlm covariance\n\n\n', 'stemmed comments': ['A', 'matrix', 'statsmodelsrlm', 'compat', 'sa', 'function', 'conveni', 'make', 'covari']}"
240,"{'func name': 'scale_transform', 'comments': ""Transform data for variance comparison for Levene type tests\n\nParameters ---------- data : array_like observations for the data center : str in ['median', 'mean', 'trimmed'] the statistic that is used as center for the data transformation transform : 'abs', 'square', 'identity' or a callable the transform for the centered data trim_frac : float in [0, 0.5) Fraction of observations that are trimmed on each side of the sorted observations. This is only used if center is `trimmed`. axis : int axis along which the data are transformed when centering.\n##### Returns\n* **res **: ndarray\n    transformed data in the same shape as the original data.\n\n"", 'stemmed comments': ['transform', 'array_lik', ']', 're', 'use', 'side', 'paramet', 'axi', 'return', 'comparison', '05', 'thi', 'callabl', 'data', 'int', 'shape', 'varianc', 'ab', 'type', 'statist', 'origin', 'str', 'fraction', 'float', 'test', 'mean', 'median', 'sort', '[', '0', 'ident', 'observ', 'along', 'trim', 'ndarray', 'leven', 'trim_frac', 'squar', 'center']}"
241,"{'func name': '_check_convergence', 'comments': '', 'stemmed comments': []}"
242,"{'func name': 'plot_weights', 'comments': '', 'stemmed comments': []}"
243,"{'func name': 'strip4', 'comments': '', 'stemmed comments': []}"
244,"{'func name': 'brentq_expanding', 'comments': 'find the root of a function in one variable by expanding and brentq\n\nAssumes function ``func`` is monotonic.\n\nParameters ---------- func : callable function for which we find the root ``x`` such that ``func(x) = 0`` low : float or None lower bound for brentq upp : float or None upper bound for brentq args : tuple optional additional arguments for ``func`` xtol : float parameter x tolerance given to brentq start_low : float (positive) or None starting bound for expansion with increasing ``x``. It needs to be positive. If None, then it is set to 1. start_upp : float (negative) or None starting bound for expansion with decreasing ``x``. It needs to be negative. If None, then it is set to -1. increasing : bool or None If None, then the function is evaluated at the initial bounds to determine wether the function is increasing or not. If increasing is True (False), then it is assumed that the function is monotonically increasing (decreasing). max_it : int maximum number of expansion steps. maxiter_bq : int maximum number of iterations of brentq. factor : float expansion factor for step of shifting the bounds interval, default is 10. full_output : bool, optional If full_output is False, the root is returned. If full_output is True, the return value is (x, r), where x is the root, and r is a RootResults object.\n##### Returns\n* **x **: float\n    root of the function, value at which ``func(x) = 0``.\n\n* **info **: RootResult (optional)\n    returned if ``full_output`` is True.\n    attributes\n\n', 'stemmed comments': ['fals', 'x', 'initi', 'max_it', 'object', 'start', 'bool', 'set', 'determin', 'expans', 'paramet', 'factor', 'argument', 'return', 'iter', 'addit', 'root', 'start_upp', 'callabl', 'valu', 'assum', 'evalu', 'expand', 'int', '=', 'step', 'lower', 'function', 'toler', 'upper', 'arg', 'xtol', 'full_output', 'maxiter_bq', 'wether', 'true', '1', 'attribut', 'variabl', 'float', '10', 'option', 'rootresult', 'decreas', 'need', 'find', 'start_low', 'interv', 'shift', 'one', '0', 'increas', 'low', 'none', 'If', 'bound', 'default', 'info', 'It', 'r', 'upp', 'posit', 'given', 'monoton', 'neg', 'tupl', 'number', 'brentq', 'maximum', 'func']}"
245,"{'func name': 'impute_ros', 'comments': 'Impute censored dataset using Regression on Order Statistics (ROS).\n\nMethod described in *Nondetects and Data Analysis* by Dennis R. Helsel (John Wiley, 2005) to estimate the left-censored (non-detect) values of a dataset. When there is insufficient non-censorded data, simple substitution is used.\n\nParameters ---------- observations : str or array-like Label of the column or the float array of censored observations\n\ncensorship : str Label of the column or the bool array of the censorship status of the observations.\n\n* True if censored, * False if uncensored\n\ndf : pandas.DataFrame, optional If `observations` and `censorship` are labels, this is the DataFrame that contains those columns.\n\nmin_uncensored : int (default is 2) The minimum number of uncensored values required before ROS can be used to impute the censored observations. When this criterion is not met, simple substituion is used instead.\n\nmax_fraction_censored : float (default is 0.8) The maximum fraction of censored data below which ROS can be used to impute the censored observations. When this fraction is exceeded, simple substituion is used instead.\n\nsubstitution_fraction : float (default is 0.5) The fraction of the detection limit to be used during simple substitution of the censored values.\n\ntransform_in : callable (default is numpy.log) Transformation to be applied to the values prior to fitting a line to the plotting positions vs. uncensored values.\n\ntransform_out : callable (default is numpy.exp) Transformation to be applied to the imputed censored values estimated from the previously computed best-fit line.\n\nas_array : bool (default is True) When True, a numpy array of the imputed observations is returned. Otherwise, a modified copy of the original dataframe with all of the intermediate calculations is returned.\n##### Returns\n* **imputed **: numpy.array (default) or pandas.DataFrame\n    The final observations where the censored values have either been\n    imputed through ROS or substituted as a fraction of the\n    detection limit.\n\n', 'stemmed comments': ['previous', 'vs', 'minimum', 'array', 'dataset', 'callabl', 'statu', 'calcul', 'fraction', 'detect', 'modifi', 'option', 'imput', 'describ', 'datafram', 'numpyarray', 'censor', 'copi', 'comput', 'plot', 'use', 'transform_out', 'paramet', 'leftcensor', 'return', 'method', 'numpyexp', 'estim', 'max_fraction_censor', 'john', 'label', 'final', 'R', 'statist', 'insuffici', 'origin', 'true', 'str', 'float', 'instead', 'noncensord', 'met', '2005', 'number', 'censorship', 'maximum', 'fals', 'exceed', 'as_array', 'transform', 'transform_in', 'simpl', 'denni', 'valu', 'uncensor', 'data', 'int', 'line', 'ro', 'df', 'numpylog', 'helsel', 'appli', 'when', 'default', 'If', 'intermedi', 'column', '2', 'regress', 'substitut', 'bool', 'limit', 'substituion', 'requir', 'order', '08', '05', 'otherwis', 'nondetect', 'prior', 'substitution_fract', 'analysi', 'arraylik', 'contain', 'numpi', 'min_uncensor', 'bestfit', 'the', 'fit', 'observ', 'wiley', 'pandasdatafram', 'either', 'criterion', 'posit']}"
246,"{'func name': 'noop', 'comments': '', 'stemmed comments': []}"
247,"{'func name': 'no_show', 'comments': '', 'stemmed comments': []}"
248,"{'func name': 'symmetry_bowker', 'comments': 'Test for symmetry of a (k, k) square contingency table\n\nThis is an extension of the McNemar test to test the Null hypothesis that the contingency table is symmetric around the main diagonal, that is\n\nn_{i, j} = n_{j, i}\n\nfor all i, j\n\nParameters ---------- table : array_like, 2d, (k, k) a square contingency table that contains the count for k categories in rows and columns.\n##### Returns\n* **statistic **: float\n    chisquare test statistic\n\n* **p-value **: float\n    p-value of the test statistic based on chisquare distribution\n\n* **df **: int\n    degrees of freedom of the chisquare distribution\n\n', 'stemmed comments': ['k', 'null', 'hypothesi', 'count', 'array_lik', 'categori', 'n_', 'paramet', 'return', 'symmetr', 'thi', 'int', '=', 'diagon', 'base', 'around', 'distribut', 'degre', 'main', 'statist', 'contain', 'extens', 'j', 'float', 'pvalu', 'test', '{', 'df', 'freedom', '}', 'chisquar', 'mcnemar', '2d', 'conting', 'squar', 'symmetri', 'tabl', 'row', 'column']}"
249,"{'func name': 'S_white', 'comments': 'simple white heteroscedasticity robust covariance note: calculating this way is very inefficient, just for cross-checking\n\n\n', 'stemmed comments': ['simpl', 'calcul', 'note', 'white', 'heteroscedast', 'robust', 'ineffici', 'crosscheck', 'way', 'covari']}"
250,"{'func name': 'cov_nw_groupsum', 'comments': ""Driscoll and Kraay Panel robust covariance matrix\n\nRobust covariance matrix for panel data of Driscoll and Kraay.\n\nAssumes we have a panel of time series where the time index is available. The time index is assumed to represent equal spaced periods. At least one observation per period is required.\n\nParameters ---------- results : result instance result of a regression, uses results.model.exog and results.resid TODO: this should use wexog instead nlags : int or None Highest lag to include in kernel window. Currently, no default because the optimal length will depend on the number of observations per cross-sectional unit. time : ndarray of int this should contain the coding for the time period of each observation. time periods should be integers in range(maxT) where maxT is obs of i weights_func : callable weights_func is called with nlags as argument to get the kernel weights. default are Bartlett weights use_correction : 'cluster' or 'hac' or False If False, then no small sample correction is used. If 'hac' (default), then the same correction as in single time series, cov_hac is used. If 'cluster', then the same correction as in cov_cluster is used.\n##### Returns\n* **cov **: ndarray, (k_vars, k_vars)\n    HAC robust covariance matrix for parameter estimates\n\n* **Warning**: \n\n"", 'stemmed comments': ['warn', 'cluster', 'sampl', 'todo', 'per', 'callabl', 'space', 'instanc', 'kraay', 'correct', 'use_correct', 'code', 'get', 'index', 'least', 'optim', 'ob', 'k_var', 'window', 'use', 'cov', 'highest', 'paramet', 'return', 'argument', 'hac', 'weight', 'equal', 'estim', 'nlag', 'bartlett', 'lag', 'result', 'time', 'instead', 'call', 'number', 'maxt', 'fals', 'length', 'integ', 'singl', 'wexog', 'crosssect', 'current', 'assum', 'data', 'int', 'small', 'includ', 'panel', 'weights_func', 'kernel', 'repres', 'covari', 'robust', 'resultsmodelexog', 'none', 'default', 'If', 'period', 'regress', 'cov_clust', 'seri', 'driscol', 'avail', 'cov_hac', 'At', 'requir', 'matrix', 'contain', 'unit', 'rang', 'the', 'depend', 'one', 'observ', 'ndarray', 'resultsresid']}"
251,"{'func name': 'mad', 'comments': 'The Median Absolute Deviation along given axis of an array\n\nParameters ---------- a : array_like Input array. c : float, optional The normalization constant.\n\nDefined as scipy.stats.norm.ppf(3/4.), which is approximately .6745. axis : int, optional The default is 0. Can also be None. center : callable or float If a callable is provided, such as the default `np.median` then it is expected to be called center(a). The axis argument will be applied via np.apply_over_axes. Otherwise, provide a float.\n##### Returns\n* **mad **: float\n    `mad` = median(abs(`a` - center))/`c`\n\n', 'stemmed comments': ['normal', 'array_lik', 'deviat', '3/4', 'array', 'paramet', 'defin', 'axi', 'argument', 'approxim', 'return', 'npapply_over_ax', 'callabl', '6745', 'absolut', 'int', 'via', 'otherwis', '=', 'can', 'c', 'provid', 'ab', 'mad', 'constant', '/', 'expect', 'float', 'option', 'also', 'appli', 'the', 'median', 'scipystatsnormppf', '0', 'call', 'default', 'along', 'none', 'If', 'npmedian', 'center', 'given', 'input']}"
252,"{'func name': '_lazywhere', 'comments': 'np.where(cond, x, fillvalue) always evaluates x even where cond is False. This one only evaluates f(arr1[cond], arr2[cond], ...). For example, >>> a, b = np.array([1, 2, 3, 4]), np.array([5, 6, 7, 8]) >>> def f(a, b): return a*b >>> _lazywhere(a > 2, (a, b), f, np.nan) array([ nan,  nan,  21.,  32.]) Notice it assumes that all `arrays` are of the same shape, or can be broadcasted together.\n\n\n', 'stemmed comments': ['npwhere', 'fals', 'x', '2', '5', '7', 'npnan', '8', '3', ']', 'array', 'even', '6', 'broadcast', '4', 'return', 'notic', 'fillvalu', 'arr2', 'thi', 'nan', 'assum', 'evalu', 'def', '=', '_lazywher', 'for', 'nparray', 'shape', 'b', 'alway', '1', 'cond', 'arr1', 'togeth', '>', '[', 'one', '32', 'f', '21', 'exampl']}"
253,"{'func name': 'seasonal_decompose', 'comments': 'Seasonal decomposition using moving averages.\n\nParameters ---------- x : array_like Time series. If 2d, individual series are in columns. x must contain 2 complete cycles. model : {""additive"", ""multiplicative""}, optional Type of seasonal component. Abbreviations are accepted. filt : array_like, optional The filter coefficients for filtering out the seasonal component. The concrete moving average method used in filtering is determined by two_sided. period : int, optional Period of the series. Must be used if x is not a pandas object or if the index of x does not have\n\na frequency. Overrides default periodicity of x if x is a pandas object with a timeseries index. two_sided : bool, optional The moving average method used in filtering. If True (default), a centered moving average is computed using the filt. If False, the filter coefficients are for past values only. extrapolate_trend : int or \'freq\', optional If set to > 0, the trend resulting from the convolution is linear least-squares extrapolated on both ends (or the single one if two_sided is False) considering this many (+1) closest points. If set to \'freq\', use `freq` closest points. Setting this parameter results in no NaN values in trend or resid components.\n##### Returns\n', 'stemmed comments': ['comput', 'fals', 'x', '2', 'coeffici', 'array_lik', 'singl', 'resid', 'object', 'frequenc', 'use', 'multipl', 'compon', 'bool', 'determin', 'filter', 'set', 'trend', 'seri', 'paramet', 'return', 'decomposit', 'method', 'leastsquar', 'addit', 'extrapol', 'cycl', 'nan', 'valu', 'int', 'model', 'closest', 'point', 'accept', 'two_sid', 'timeseri', 'result', 'overrid', 'type', 'season', 'time', 'contain', 'panda', 'true', 'end', '1', 'option', '{', 'concret', 'move', 'averag', 'must', 'complet', 'the', '}', 'index', '>', 'one', '0', 'consid', 'default', 'If', '2d', 'freq', 'abbrevi', 'linear', 'individu', 'period', 'center', 'extrapolate_trend', 'convolut', 'filt', 'column', 'mani', 'past']}"
254,"{'func name': 'halton', 'comments': 'Halton sequence.\n\nPseudo-random number generator that generalize the Van der Corput sequence for multiple dimensions. Halton sequence use base-two Van der Corput sequence for the first dimension, base-three for its second and base-n for its n-dimension.\n\nParameters ---------- dim : int Dimension of the parameter space. n_sample : int Number of samples to generate in the parametr space. bounds : tuple or array_like ([min, k_vars], [max, k_vars]) Desired range of transformed data. The transformation apply the bounds on the sample and not the theoretical space, unit cube. Thus min and max values of the sample will coincide with the bounds. start_index : int Index to start the sequence from.\n##### Returns\n* **sequence **: array_like (n_samples, k_vars)\n    Sequence of Halton.\n\n', 'stemmed comments': ['k_var', 'array_lik', 'transform', 'basen', 'parametr', 'first', ']', 'start', 'multipl', 'use', 'ndimens', 'sampl', 'paramet', 'return', 'start_index', 'pseudorandom', 'n_sampl', 'min', 'valu', 'corput', 'van', 'int', 'desir', 'data', 'second', 'space', 'halton', 'basethre', 'coincid', 'basetwo', 'unit', 'sequenc', 'appli', 'rang', 'der', 'the', 'thu', 'cube', 'index', 'dim', '[', 'bound', 'dimens', 'max', 'theoret', 'tupl', 'number', 'gener']}"
255,"{'func name': 'process_tempita', 'comments': 'Runs pyx.in files through tempita is needed\n\n\n', 'stemmed comments': ['pyxin', 'run', 'need', 'file', 'tempita']}"
256,"{'func name': 'make_poly_basis', 'comments': 'given a vector x returns poly=(1, x, x^2, ..., x^degree) and its first and second derivative\n\n\n', 'stemmed comments': ['x^degre', 'x', 'deriv', '1', 'poly=', 'first', 'second', 'given', 'vector', 'return', 'x^2']}"
257,"{'func name': '_lowess_bisquare', 'comments': 'The bisquare function applied to a numpy array. The bisquare function is (1-t**2)**2.\n\nParameters ---------- t : ndarray array bisquare function is applied to, element-wise and in-place.\n##### Returns\n', 'stemmed comments': ['ndarray', '2', '1t', 'bisquar', 'elementwis', 'paramet', 'array', 'numpi', 'appli', 'return', 'function', 'the', 'inplac']}"
258,"{'func name': 'lowess', 'comments': ""LOWESS (Locally Weighted Scatterplot Smoothing)\n\nA lowess function that outs smoothed estimates of endog at the given exog values from points (exog, endog)\n\nParameters ---------- endog : 1-D numpy array The y-values of the observed points exog : 1-D numpy array The x-values of the observed points frac : float Between 0 and 1. The fraction of the data used when estimating each y-value. it : int The number of residual-based reweightings to perform. delta : float Distance within which to use linear-interpolation instead of weighted regression. is_sorted : bool If False (default), then the data will be sorted by exog before calculating lowess. If True, then it is assumed that the data is already sorted by exog. missing : str Available options are 'none', 'drop', and 'raise'. If 'none', no nan checking is done. If 'drop', any observations with nans are dropped. If 'raise', an error is raised. Default is 'drop'. return_sorted : bool If True (default), then the returned array is sorted by exog and has missing (nan or infinite) observations removed. If False, then the returned array is in the same length and the same sequence of observations as the input array.\n##### Returns\n* **out**: ndarray, float\n    The returned array is two-dimensional if return_sorted is True, and\n    one dimensional if return_sorted is False.\n    If return_sorted is True, then a numpy array with two columns. The\n    first column contains the sorted x (exog) values and the second column\n    the associated estimated y (endog) values.\n    If return_sorted is False, then only the fitted values are returned,\n    and the observations will be in the same order as the input arrays.\n\n* **Association 74 (368)**: 829-836.\n\n"", 'stemmed comments': ['array', 'function', 'endog', 'calcul', 'fraction', 'rais', 'check', 'option', 'associ', 'remov', 'done', 'given', 'within', '368', 'use', 'paramet', 'infinit', 'return', 'weight', 'estim', 'two', 'linearinterpol', 'out', 'alreadi', 'true', 'str', 'float', 'instead', 'exog', 'lowess', 'number', 'fals', 'length', 'first', 'scatterplot', 'nan', 'valu', 'assum', 'data', 'int', 'second', 'point', 'reweight', 'xvalu', 'sort', 'twodimension', '0', 'default', 'If', 'none', '74', 'local', 'input', 'column', 'A', 'x', 'regress', 'bool', 'error', 'delta', 'avail', 'order', 'residualbas', 'yvalu', 'between', '1D', '829836', 'contain', '1', 'drop', 'numpi', 'sequenc', 'distanc', 'is_sort', 'the', 'return_sort', 'fit', 'one', 'observ', 'ndarray', 'smooth', 'frac', 'miss', 'dimension', 'perform']}"
259,"{'func name': 'load_pickle', 'comments': 'Load a previously saved object\n\n.. warning::\n\nLoading pickled models is not secure against erroneous or maliciously constructed data. Never unpickle data received from an untrusted or unauthenticated source.\n\nParameters ---------- fname : str Filename to unpickle\n\nNotes ----- This method can be used to load *both* models and results.\n', 'stemmed comments': ['previous', 'note', 'warn', 'load', 'save', 'object', 'use', 'unpickl', 'sourc', 'paramet', 'method', 'construct', 'receiv', 'thi', 'data', 'model', 'fname', 'never', 'erron', 'unauthent', 'result', 'filenam', 'str', 'pickl', 'secur', 'untrust', 'malici']}"
260,"{'func name': 'printresults', 'comments': ""calculate and print(Bootstrap or Monte Carlo result\n\nParameters ---------- sample : ndarray original sample data arg : float\n\n (for general case will be array) bres : ndarray parameter estimates from Bootstrap or Monte Carlo run kind : {'bootstrap', 'montecarlo'} output is printed for Mootstrap (default) or Monte Carlo\n##### Returns\n* **made correction**: reference point for bootstrap is estimated parameter\n\n* **not clear**: I'm not doing any ddof adjustment in estimation of variance, do we\n    need ddof>0 ?\n\n* **todo**: return results and string instead of printing\n\n"", 'stemmed comments': ['adjust', 'run', 'array', 'string', 'sampl', 'paramet', 'm', 'todo', 'clear', 'return', 'bootstrap', 'bre', 'estim', 'data', 'kind', 'mootstrap', 'point', 'varianc', 'output', 'result', 'case', 'calcul', 'carlo', 'origin', 'correct', 'refer', 'float', 'ddof', '{', 'instead', 'need', 'I', 'mont', '}', '>', '0', 'default', 'ndarray', 'made', 'print', '?', 'gener', 'montecarlo', 'arg']}"
261,"{'func name': 'mean_forecast_err', 'comments': '', 'stemmed comments': []}"
262,"{'func name': 'compute_coincident_index', 'comments': '', 'stemmed comments': []}"
263,"{'func name': 'simulate_seasonal_term', 'comments': '', 'stemmed comments': []}"
264,"{'func name': 'statespace', 'comments': ""Estimate SARIMAX parameters using state space methods.\n\nParameters ---------- endog : array_like Input time series array. order : tuple, optional The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters. Default is (0, 0, 0). seasonal_order : tuple, optional The (P,D,Q,s) order of the seasonal component of the model for the AR parameters, differences, MA parameters, and periodicity. Default is (0, 0, 0, 0). include_constant : bool, optional Whether to add a constant term in `exog` if it's not already there. The estimate of the constant will then appear as one of the `exog` parameters. If `exog` is None, then the constant will represent the mean of the process. enforce_stationarity : bool, optional Whether or not to transform the AR parameters to enforce stationarity in the autoregressive component of the model. Default is True. enforce_invertibility : bool, optional Whether or not to transform the MA parameters to enforce invertibility in the moving average component of the model. Default is True. concentrate_scale : bool, optional Whether or not to concentrate the scale (variance of the error term) out of the likelihood. This reduces the number of parameters estimated by maximum likelihood by one. start_params : array_like, optional Initial guess of the solution for the loglikelihood maximization. The AR polynomial must be stationary. If `enforce_invertibility=True` the MA poylnomial must be invertible. If not provided, default starting parameters are computed using the Hannan-Rissanen method. fit_kwargs : dict, optional Arguments to pass to the state space model's `fit` method.\n##### Returns\n* **parameters **: SARIMAXParams object\n\n* **other_results **: Bunch\n    Includes two components, `spec`, containing the `SARIMAXSpecification`\n    instance corresponding to the input arguments; and\n    `state_space_results`, corresponding to the results from the underlying\n    state space model and Kalman filter / smoother.\n\n* **.. [1] Durbin, James, and Siem Jan Koopman. 2012.\n   Time Series Analysis by State Space Methods**: Second Edition.\n   Oxford University Press.\n\n"", 'stemmed comments': ['concentr', 'object', 'array', 'p', 's', ';', 'underli', 'start_param', 'press', 'P', 'space', 'provid', '2012', 'concentrate_scal', 'endog', 'kalman', 'poylnomi', 'instanc', 'constant', 'seasonal_ord', '/', 'autoregress', 'option', 'sarimaxspecif', 'add', 'averag', 'univers', 'must', '[', 'pass', 'D', 'edit', 'comput', 'invert', 'use', 'compon', 'filter', 'paramet', 'jame', 'correspond', 'argument', 'Q', 'method', 'return', 'siem', 'estim', 'scale', 'two', 'alreadi', 'result', 'sarimax', 'season', 'time', 'true', 'whether', 'spec', 'include_const', 'solut', 'sarimaxparam', 'koopman', 'differ', 'enforce_stationar', 'process', 'exog', 'enforc', 'number', 'maximum', 'initi', 'transform', 'smoother', 'start', 'AR', 'maxim', 'state_space_result', 'model', 'second', 'includ', 'loglikelihood', 'mean', 'move', 'state', 'repres', 'durbin', '0', 'appear', 'default', 'If', 'none', 'enforce_invertibility=tru', 'period', 'tupl', 'input', 'array_lik', 'likelihood', ']', 'bool', 'stationar', 'error', 'seri', 'order', 'thi', 'polynomi', 'fit_kwarg', 'varianc', 'analysi', 'stationari', 'jan', 'contain', '1', 'reduc', 'term', 'MA', 'bunch', 'the', 'hannanrissanen', 'fit', 'one', 'enforce_invert', 'dict', 'q', 'other_result', 'oxford', 'guess']}"
265,"{'func name': 'empiricalcdf', 'comments': 'Return the empirical cdf.\n\nMethods available: Hazen:\n\n\n\n\n\n (i-0.5)/N Weibull:\n\n\n\n i/(N+1) Chegodayev:\n\n(i-.3)/(N+.4) Cunnane:\n\n\n\n (i-.4)/(N+.2) Gringorten:\n\n(i-.44)/(N+.12) California:\n\n(i-1)/N\n\nWhere i goes from 1 to N.\n', 'stemmed comments': ['N', 'california', 'avail', 'return', 'method', 'gringorten', 'i4', 'n12', 'i/', 'N4', 'N1', 'hazen', 'weibul', 'i3', '/', 'goe', '1', 'cdf', 'i44', '/N', 'cunnan', 'i1', 'chegodayev', 'N2', 'empir', 'where', 'i05']}"
266,"{'func name': 'edf_normal_inverse_transformed', 'comments': 'rank based normal inverse transformed cdf\n\n\n', 'stemmed comments': ['normal', 'rank', 'transform', 'invers', 'cdf', 'base']}"
267,"{'func name': '_kpss_autolag', 'comments': 'Computes the number of lags for covariance matrix estimation in KPSS test using method of Hobijn et al (1998). See also Andrews (1991), Newey & West (1994), and Schwert (1989). Assumes Bartlett / Newey-West kernel.\n\n\n', 'stemmed comments': ['comput', 'see', 'kpss', 'use', '&', 'andrew', 'method', 'matrix', '1994', 'estim', 'assum', 'schwert', 'neweywest', 'bartlett', 'lag', '1991', 'hobijn', '/', 'test', 'also', '1998', 'kernel', 'newey', 'et', 'covari', '1989', 'west', 'al', 'number']}"
268,"{'func name': 'medcouple', 'comments': 'Calculate the medcouple robust measure of skew.\n\nParameters ---------- y : array_like Data to compute use in the estimator. axis : {int, None} Axis along which the medcouple statistic is computed.\n\nIf `None`, the entire array is used.\n##### Returns\n* **mc **: ndarray\n    The medcouple statistic with the same shape as `y`, with the specified\n    axis removed.\n\n', 'stemmed comments': ['comput', 'mc', 'array_lik', 'array', 'use', 'paramet', 'medcoupl', 'axi', 'return', 'measur', 'entir', 'estim', 'data', 'int', 'shape', 'calcul', 'statist', '{', 'remov', 'the', '}', 'robust', 'none', 'along', 'If', 'ndarray', 'skew', 'specifi']}"
269,"{'func name': 'summary_return', 'comments': '', 'stemmed comments': []}"
270,"{'func name': '_simple_tables', 'comments': '', 'stemmed comments': []}"
271,"{'func name': 'plot_survfunc', 'comments': 'Plot one or more survivor functions.\n\nParameters ---------- survfuncs : object or array_like A single SurvfuncRight object, or a list or SurvfuncRight objects that are plotted together.\n##### Returns\n* **Add a legend**: \n\n* **>>> df = data.loc[data.sex == ""F"", **: ]\n\n* **Change the line colors**: \n\n', 'stemmed comments': ['A', 'plot', 'array_lik', 'singl', 'object', ']', 'datasex', 'paramet', '==', 'return', 'survivor', 'survfunc', 'dataloc', '=', 'function', 'line', 'F', 'color', 'legend', 'df', 'togeth', 'add', '>', 'one', '[', 'survfuncright', 'chang', 'list']}"
272,"{'func name': 'svar_ckerr', 'comments': '', 'stemmed comments': []}"
273,"{'func name': 'cdf', 'comments': 'Return the cumulative density function as an expression in x\n\n\n', 'stemmed comments': ['cumul', 'x', 'densiti', 'function', 'express', 'return']}"
274,"{'func name': 'get_output_format', 'comments': '', 'stemmed comments': []}"
275,"{'func name': '_get_data', 'comments': '', 'stemmed comments': []}"
276,"{'func name': 'test_adf_autolag', 'comments': '', 'stemmed comments': []}"
277,"{'func name': 'test_mean_diff_plot', 'comments': '', 'stemmed comments': []}"
278,"{'func name': 'test_repeated_measures_aggregate_compare_with_ezANOVA', 'comments': '', 'stemmed comments': []}"
279,"{'func name': 'test_autoreg_forecast_period_index', 'comments': '', 'stemmed comments': []}"
280,"{'func name': 'test_lpol2index_index2lpol', 'comments': '', 'stemmed comments': []}"
281,"{'func name': 'test_arma_exog_const_trend_nc', 'comments': '', 'stemmed comments': []}"
282,"{'func name': 'test_innovations_nonstationary', 'comments': '', 'stemmed comments': []}"
283,"{'func name': 'test_holdertuple2', 'comments': '', 'stemmed comments': []}"
284,"{'func name': 'test_get_predict_start_end', 'comments': '', 'stemmed comments': []}"
285,"{'func name': 'test_mi_formula', 'comments': '', 'stemmed comments': []}"
286,"{'func name': 'test_doc_examples', 'comments': '', 'stemmed comments': []}"
287,"{'func name': 'test_beanplot_legend_text', 'comments': '', 'stemmed comments': []}"
288,"{'func name': 'test_misc', 'comments': '', 'stemmed comments': []}"
289,"{'func name': 'test_cancorr', 'comments': '', 'stemmed comments': []}"
290,"{'func name': 'test_add_indep', 'comments': '', 'stemmed comments': []}"
291,"{'func name': 'test_dfm', 'comments': '', 'stemmed comments': []}"
292,"{'func name': 'test_tvpvar', 'comments': '', 'stemmed comments': []}"
293,"{'func name': 'test_invalid', 'comments': '', 'stemmed comments': []}"
294,"{'func name': 'test_coint_johansen_0lag', 'comments': '', 'stemmed comments': []}"
295,"{'func name': 'test_dfm_missing', 'comments': '', 'stemmed comments': []}"
296,"{'func name': 'test_concentrated_scale_univariate', 'comments': '', 'stemmed comments': []}"
297,"{'func name': 'test_conditional_mnlogit_3d', 'comments': '', 'stemmed comments': []}"
298,"{'func name': 'test_invalid_fittedvalues_resid_predict', 'comments': '', 'stemmed comments': []}"
299,"{'func name': 'junk', 'comments': '', 'stemmed comments': []}"
300,"{'func name': 'test_cochranq', 'comments': '', 'stemmed comments': []}"
301,"{'func name': 'test_constraints', 'comments': '', 'stemmed comments': []}"
302,"{'func name': 'test_kernel_covariance', 'comments': '', 'stemmed comments': []}"
303,"{'func name': 'test_plot_corr_grid', 'comments': '', 'stemmed comments': []}"
304,"{'func name': 'test_corrpsd_threshold', 'comments': '', 'stemmed comments': []}"
305,"{'func name': 'test_HC_use', 'comments': '', 'stemmed comments': []}"
306,"{'func name': 'test_regression_with_arma_errors', 'comments': '', 'stemmed comments': []}"
307,"{'func name': 'test_raise_nonfinite_exog', 'comments': '', 'stemmed comments': []}"
308,"{'func name': 'test_dataset', 'comments': '', 'stemmed comments': []}"
309,"{'func name': 'test_patsy_577', 'comments': '', 'stemmed comments': []}"
310,"{'func name': 'test_dates_from_range', 'comments': '', 'stemmed comments': []}"
311,"{'func name': 'test_deprecated_alias', 'comments': '', 'stemmed comments': []}"
312,"{'func name': 'test_denton_quarterly2', 'comments': '', 'stemmed comments': []}"
313,"{'func name': 'test_sign_test', 'comments': '', 'stemmed comments': []}"
314,"{'func name': 'test_drop', 'comments': '', 'stemmed comments': []}"
315,"{'func name': 'test_small_skip', 'comments': '', 'stemmed comments': []}"
316,"{'func name': 'test_covreduce', 'comments': '', 'stemmed comments': []}"
317,"{'func name': 'test_t_test', 'comments': '', 'stemmed comments': []}"
318,"{'func name': 'test_debiased_v_average', 'comments': '', 'stemmed comments': []}"
319,"{'func name': 'test_multiple_sig', 'comments': '', 'stemmed comments': []}"
320,"{'func name': 'test_all', 'comments': '', 'stemmed comments': []}"
321,"{'func name': 'test_misc', 'comments': '', 'stemmed comments': []}"
322,"{'func name': 'test_start_params_nans', 'comments': '', 'stemmed comments': []}"
323,"{'func name': 'check_distribution_rvs', 'comments': '', 'stemmed comments': []}"
324,"{'func name': 'test_iqr_axis', 'comments': '', 'stemmed comments': []}"
325,"{'func name': 'test_nondiagonal_obs_cov', 'comments': '', 'stemmed comments': []}"
326,"{'func name': 'test_parameterless_model', 'comments': '', 'stemmed comments': []}"
327,"{'func name': 'test_convergence_simple', 'comments': '', 'stemmed comments': []}"
328,"{'func name': 'test_skewt', 'comments': '', 'stemmed comments': []}"
329,"{'func name': 'test_factor_scoring', 'comments': '', 'stemmed comments': []}"
330,"{'func name': 'test_pandas_freq_decorator', 'comments': '', 'stemmed comments': []}"
331,"{'func name': 'test_score_shape', 'comments': '', 'stemmed comments': []}"
332,"{'func name': 'test_append_multistep', 'comments': '', 'stemmed comments': []}"
333,"{'func name': 'test_datetime_roundtrip', 'comments': '', 'stemmed comments': []}"
334,"{'func name': 'test_formula_predict_series_exog', 'comments': '', 'stemmed comments': []}"
335,"{'func name': 'test_fboxplot_rainbowplot', 'comments': '', 'stemmed comments': []}"
336,"{'func name': 'test_cov_params', 'comments': '', 'stemmed comments': []}"
337,"{'func name': 'test_grid_ar', 'comments': '', 'stemmed comments': []}"
338,"{'func name': 'compare_waldres', 'comments': '', 'stemmed comments': []}"
339,"{'func name': 'test_poisson_residuals', 'comments': '', 'stemmed comments': []}"
340,"{'func name': 'test_int_exog', 'comments': '', 'stemmed comments': []}"
341,"{'func name': 'test_arma_kwargs', 'comments': '', 'stemmed comments': []}"
342,"{'func name': 'test_GLSARlag', 'comments': '', 'stemmed comments': []}"
343,"{'func name': 'moment_exponential_mult', 'comments': '', 'stemmed comments': []}"
344,"{'func name': 'test_gmm_basic', 'comments': '', 'stemmed comments': []}"
345,"{'func name': 'test_loop_vectorized_batch_equivalence', 'comments': '', 'stemmed comments': []}"
346,"{'func name': 'test_chisquare_effectsize', 'comments': '', 'stemmed comments': []}"
347,"{'func name': 'test_invalid_dist_config', 'comments': '', 'stemmed comments': []}"
348,"{'func name': 'test_dummy_sparse', 'comments': '', 'stemmed comments': []}"
349,"{'func name': 'test_unbiased_error', 'comments': '', 'stemmed comments': []}"
350,"{'func name': 'test_forecast_index', 'comments': '', 'stemmed comments': []}"
351,"{'func name': 'test_pandas_anchor', 'comments': '', 'stemmed comments': []}"
352,"{'func name': '_check_looo', 'comments': '', 'stemmed comments': []}"
353,"{'func name': 'test_invalid', 'comments': '', 'stemmed comments': []}"
354,"{'func name': 'test_innovations_mle_invalid', 'comments': '', 'stemmed comments': []}"
355,"{'func name': 'test_aggregate_raters', 'comments': '', 'stemmed comments': []}"
356,"{'func name': 'test_combinations', 'comments': '', 'stemmed comments': []}"
357,"{'func name': 'test_stationary_initialization', 'comments': '', 'stemmed comments': []}"
358,"{'func name': 'test_kde_bw_positive', 'comments': '', 'stemmed comments': []}"
359,"{'func name': 'test_all_kernels', 'comments': '', 'stemmed comments': []}"
360,"{'func name': 'test_invalid_kernel', 'comments': '', 'stemmed comments': []}"
361,"{'func name': 'test_sim', 'comments': '', 'stemmed comments': []}"
362,"{'func name': 'test_ksstat', 'comments': '', 'stemmed comments': []}"
363,"{'func name': 'test_stationary_solve_2d', 'comments': '', 'stemmed comments': []}"
364,"{'func name': 'test_invlogit_stability', 'comments': '', 'stemmed comments': []}"
365,"{'func name': 'test_get_distribution', 'comments': '', 'stemmed comments': []}"
366,"{'func name': 'test_returns_inputs', 'comments': '', 'stemmed comments': []}"
367,"{'func name': 'test_endog_1D_array', 'comments': '', 'stemmed comments': []}"
368,"{'func name': 'test_conditional_loglikelihoods', 'comments': '', 'stemmed comments': []}"
369,"{'func name': 'test_exog_tvtp', 'comments': '', 'stemmed comments': []}"
370,"{'func name': 'test_partials_logistic', 'comments': '', 'stemmed comments': []}"
371,"{'func name': 'test_surv', 'comments': '', 'stemmed comments': []}"
372,"{'func name': 'test_micedata_miss1', 'comments': '', 'stemmed comments': []}"
373,"{'func name': 'test_2factor', 'comments': '# R code: r = 0.4 p = 6 ii = seq(0, p-1) ii = outer(ii, ii, ""-"") ii = abs(ii) cm = r^ii factanal(covmat=cm, factors=2)\n\n\n', 'stemmed comments': ['R', 'p1', '04', 'ii', 'r^ii', 'covmat=cm', 'r', '=', 'code', '6', 'p', '0', 'factan', 'factors=2', 'outer', 'ab', 'cm', 'seq']}"
374,"{'func name': 'test_states_index_rangeindex', 'comments': '', 'stemmed comments': []}"
375,"{'func name': 'test_nonstationary_gls_error', 'comments': '', 'stemmed comments': []}"
376,"{'func name': 'test_large_kposdef', 'comments': '', 'stemmed comments': []}"
377,"{'func name': 'test_moment_conversion_types', 'comments': '', 'stemmed comments': []}"
378,"{'func name': 'test_missing_category', 'comments': '', 'stemmed comments': []}"
379,"{'func name': 'test_null_constrained', 'comments': '', 'stemmed comments': []}"
380,"{'func name': 'test_tukey_pvalues', 'comments': '', 'stemmed comments': []}"
381,"{'func name': 'test_affine_hypothesis', 'comments': '', 'stemmed comments': []}"
382,"{'func name': 'test_time_varying_model', 'comments': '', 'stemmed comments': []}"
383,"{'func name': 'test_cov_oneway', 'comments': '', 'stemmed comments': []}"
384,"{'func name': 'test_invalid', 'comments': '', 'stemmed comments': []}"
385,"{'func name': 'test_runstest_2sample', 'comments': '', 'stemmed comments': []}"
386,"{'func name': 'test_notebook', 'comments': '', 'stemmed comments': []}"
387,"{'func name': 'test_dtypes', 'comments': '', 'stemmed comments': []}"
388,"{'func name': 'test_simulate_equivalence', 'comments': '', 'stemmed comments': []}"
389,"{'func name': 'test_minimize_scipy_slsqp', 'comments': '', 'stemmed comments': []}"
390,"{'func name': 'test_reset_stata', 'comments': '', 'stemmed comments': []}"
391,"{'func name': 'test_docstring_optimization_compat', 'comments': '', 'stemmed comments': []}"
392,"{'func name': 'get_thsd', 'comments': '', 'stemmed comments': []}"
393,"{'func name': 'test_panel_robust_cov', 'comments': '', 'stemmed comments': []}"
394,"{'func name': 'test_parallel', 'comments': '', 'stemmed comments': []}"
395,"{'func name': 'test_repr_str', 'comments': '', 'stemmed comments': []}"
396,"{'func name': 'test_gls_warning', 'comments': '', 'stemmed comments': []}"
397,"{'func name': 'test_pca_svd', 'comments': '', 'stemmed comments': []}"
398,"{'func name': 'test_r', 'comments': '', 'stemmed comments': []}"
399,"{'func name': 'test_representation_pickle', 'comments': '', 'stemmed comments': []}"
400,"{'func name': 'test_pickle', 'comments': '', 'stemmed comments': []}"
401,"{'func name': 'test_power_solver_warn', 'comments': '', 'stemmed comments': []}"
402,"{'func name': 'pctl', 'comments': '', 'stemmed comments': []}"
403,"{'func name': 'test_predict_se', 'comments': '', 'stemmed comments': []}"
404,"{'func name': 'test_memory_no_predicted', 'comments': '', 'stemmed comments': []}"
405,"{'func name': 'test_score_numdiff', 'comments': '', 'stemmed comments': []}"
406,"{'func name': 'test_power_2indep', 'comments': '', 'stemmed comments': []}"
407,"{'func name': 'test_formula', 'comments': '', 'stemmed comments': []}"
408,"{'func name': 'test_qstrung', 'comments': '', 'stemmed comments': []}"
409,"{'func name': 'read_ch', 'comments': '', 'stemmed comments': []}"
410,"{'func name': 'test_alpha_summary', 'comments': '', 'stemmed comments': []}"
411,"{'func name': 'test_short_panel', 'comments': '', 'stemmed comments': []}"
412,"{'func name': 'test_alternative', 'comments': '', 'stemmed comments': []}"
413,"{'func name': 'test_fix_params', 'comments': '', 'stemmed comments': []}"
414,"{'func name': 'test_summary_no_constant', 'comments': '', 'stemmed comments': []}"
415,"{'func name': 'close_or_save', 'comments': '', 'stemmed comments': []}"
416,"{'func name': 'test_fit', 'comments': '', 'stemmed comments': []}"
417,"{'func name': 'test_missing', 'comments': '', 'stemmed comments': []}"
418,"{'func name': 'test_bad_criterion', 'comments': '', 'stemmed comments': []}"
419,"{'func name': 'test_cov_type_fixed_scale', 'comments': '', 'stemmed comments': []}"
420,"{'func name': 'test_expanding', 'comments': '', 'stemmed comments': []}"
421,"{'func name': 'test_brentq_expanding', 'comments': '', 'stemmed comments': []}"
422,"{'func name': 'test__do_ros', 'comments': '', 'stemmed comments': []}"
423,"{'func name': 'test_hac_simple', 'comments': '', 'stemmed comments': []}"
424,"{'func name': 'test_invalid_seasonal_order', 'comments': '', 'stemmed comments': []}"
425,"{'func name': 'test_existing_pickle', 'comments': '', 'stemmed comments': []}"
426,"{'func name': 'test_next_regular', 'comments': '', 'stemmed comments': []}"
427,"{'func name': 'test_glmgaussian_screening', 'comments': '', 'stemmed comments': []}"
428,"{'func name': 'test_seasonal_decompose_plot', 'comments': '', 'stemmed comments': []}"
429,"{'func name': 'test_halton', 'comments': '', 'stemmed comments': []}"
430,"{'func name': 'check_pickle', 'comments': '', 'stemmed comments': []}"
431,"{'func name': 'test_time_varying_selection', 'comments': '', 'stemmed comments': []}"
432,"{'func name': 'test_simulation_smoothing_state_intercept_diffuse', 'comments': '', 'stemmed comments': []}"
433,"{'func name': 'test_multivariate_polynomial_basis', 'comments': '', 'stemmed comments': []}"
434,"{'func name': 'test_news_invalid', 'comments': '', 'stemmed comments': []}"
435,"{'func name': 'test_invalid_estimator', 'comments': '', 'stemmed comments': []}"
436,"{'func name': 'test_start_params', 'comments': '', 'stemmed comments': []}"
437,"{'func name': 'test_durbin_watson_pandas', 'comments': '', 'stemmed comments': []}"
438,"{'func name': 'test_pacf_nlags_error', 'comments': '', 'stemmed comments': []}"
439,"{'func name': 'test_default_trend', 'comments': '', 'stemmed comments': []}"
440,"{'func name': 'test_apply_results', 'comments': '', 'stemmed comments': []}"
441,"{'func name': 'test_regression_summary', 'comments': '', 'stemmed comments': []}"
442,"{'func name': 'test_wrong_len_xname', 'comments': '', 'stemmed comments': []}"
443,"{'func name': 'test_summary_col_r2', 'comments': '', 'stemmed comments': []}"
444,"{'func name': 'test_kernel_cumincidence2', 'comments': '', 'stemmed comments': []}"
445,"{'func name': 'custom_labeller', 'comments': '', 'stemmed comments': []}"
446,"{'func name': 'test_tabledist', 'comments': '', 'stemmed comments': []}"
447,"{'func name': 'test_holder', 'comments': '', 'stemmed comments': []}"
448,"{'func name': 'test_forecast_errors', 'comments': '', 'stemmed comments': []}"
449,"{'func name': 'test_copy_index_vector', 'comments': '', 'stemmed comments': []}"
450,"{'func name': 'test_validate_basic', 'comments': '', 'stemmed comments': []}"
451,"{'func name': 'test_categorical_errors', 'comments': '', 'stemmed comments': []}"
452,"{'func name': 'test_tost_transform_paired', 'comments': '', 'stemmed comments': []}"
453,"{'func name': 'foldnorm_stats', 'comments': '', 'stemmed comments': []}"
454,"{'func name': 'test_standardize_ols', 'comments': '', 'stemmed comments': []}"
455,"{'func name': 'test_get_index_loc_quarterly', 'comments': '', 'stemmed comments': []}"
456,"{'func name': 'test_freq_to_period', 'comments': '', 'stemmed comments': []}"
457,"{'func name': 'test_seasonal_plot', 'comments': '', 'stemmed comments': []}"
458,"{'func name': 'test_time_varying_transition', 'comments': '', 'stemmed comments': []}"
459,"{'func name': 'test_webuse_pandas', 'comments': '', 'stemmed comments': []}"
460,"{'func name': 'test_not_bool_like', 'comments': '', 'stemmed comments': []}"
461,"{'func name': 'test_exceptions', 'comments': '', 'stemmed comments': []}"
462,"{'func name': 'test_from_formula', 'comments': '', 'stemmed comments': []}"
463,"{'func name': 'test_var_c_2exog', 'comments': '', 'stemmed comments': []}"
464,"{'func name': 'test_vma1_exog', 'comments': '', 'stemmed comments': []}"
465,"{'func name': 'test_VECM_seasonal_forecast', 'comments': '', 'stemmed comments': []}"
466,"{'func name': 'test_ztest_ztost', 'comments': '', 'stemmed comments': []}"
467,"{'func name': 'test_x13_arima_plot', 'comments': '', 'stemmed comments': []}"
468,"{'func name': 'test_invalid_xfail', 'comments': '', 'stemmed comments': []}"
469,"{'func name': 'assert_equal', 'comments': '', 'stemmed comments': []}"
470,"{'func name': 'extend_index', 'comments': '', 'stemmed comments': []}"
471,"{'func name': 'pcasvd', 'comments': 'principal components with svd\n\nParameters ---------- data : ndarray, 2d data with observations by rows and variables in columns keepdim : int number of eigenvectors to keep if keepdim is zero, then all eigenvectors are included demean : bool if true, then the column mean is subtracted from the data\n##### Returns\n* **xreduced **: ndarray, 2d, (nobs, nvars)\n    projection of the data x on the kept eigenvectors\n\n* **factors **: ndarray, 2d, (nobs, nfactors)\n    factor matrix, given by np.dot(x, evecs)\n\n* **evals **: ndarray, 2d, (nobs, nfactors)\n    eigenvalues\n\n* **evecs **: ndarray, 2d, (nobs, nfactors)\n    eigenvectors, normalized if normalize is true\n\n* **pca **: principal component analysis using eigenvector decomposition\n\n', 'stemmed comments': ['subtract', 'evec', 'normal', 'x', 'keep', 'keepdim', 'xreduc', 'use', 'compon', 'bool', 'eval', 'project', 'paramet', 'factor', 'return', 'decomposit', 'npdot', 'matrix', 'data', 'int', 'nob', 'nfactor', 'analysi', 'svd', 'princip', 'includ', 'true', 'demean', 'variabl', 'eigenvalu', 'mean', 'zero', 'observ', 'ndarray', '2d', 'pca', 'kept', 'eigenvector', 'given', 'row', 'nvar', 'number', 'column']}"
472,"{'func name': 'prepare_trend_data', 'comments': '', 'stemmed comments': []}"
473,"{'func name': 'locscale_grad', 'comments': 'derivative of log-likelihood with respect to location and scale\n\nParameters ---------- y : array_like data points of random variable at which loglike is evaluated loc : float location parameter of distribution scale : float scale parameter of distribution dlldy : function derivative of loglikelihood fuction wrt. random variable x args : array_like shape parameters of log-likelihood function\n##### Returns\n* **dlldloc **: ndarray\n    derivative of loglikelihood wrt location evaluated at the\n    points given in y\n\n* **dlldscale **: ndarray\n    derivative of loglikelihood wrt scale evaluated at the\n    points given in y\n\n', 'stemmed comments': ['x', 'array_lik', 'deriv', 'loglik', 'paramet', 'return', 'loc', 'fuction', 'scale', 'evalu', 'data', 'locat', 'wrt', 'shape', 'function', 'point', 'respect', 'distribut', 'loglikelihood', 'variabl', 'float', 'dlldi', 'dlldloc', 'ndarray', 'random', 'given', 'dlldscale', 'arg']}"
474,"{'func name': 'validate_basic', 'comments': 'Validate parameter vector for basic correctness.\n\nParameters ---------- params : array_like Array of parameters to validate. length : int Expected length of the parameter vector. allow_infnan : bool, optional Whether or not to allow `params` to contain -np.Inf, np.Inf, and np.nan. Default is False. title : str, optional Description of the parameters (e.g. ""autoregressive"") to use in error messages.\n##### Returns\n* **params **: ndarray\n    Array of validated parameters.\n\n', 'stemmed comments': ['fals', 'length', 'npnan', 'array_lik', 'array', 'messag', 'use', 'bool', 'npinf', 'vector', 'error', 'paramet', 'return', 'basic', 'valid', 'eg', 'int', 'contain', 'correct', 'expect', 'str', 'autoregress', 'option', 'whether', 'allow', 'default', 'titl', 'descript', 'ndarray', 'param', 'allow_infnan']}"
475,"{'func name': 'matrix_rank', 'comments': 'Matrix rank calculation using QR or SVD\n\nParameters ---------- m : array_like A 2-d array-like object to test tol : float, optional The tolerance to use when testing the matrix rank. If not provided an appropriate value is selected. method : {""ip"", ""qr"", ""svd""} The method used. ""ip"" uses the inner-product of a normalized version of m and then computes the rank using NumPy\'s matrix_rank. ""qr"" uses a QR decomposition and is the default. ""svd"" defers to NumPy\'s matrix_rank.\n##### Returns\n', 'stemmed comments': ['comput', 'A', 'normal', 'array_lik', 'object', 'use', 'defer', 'qr', 'paramet', 'QR', 's', 'return', 'method', 'ip', 'decomposit', 'matrix', 'valu', 'tol', 'toler', 'provid', 'appropri', 'version', 'select', 'matrix_rank', 'arraylik', 'svd', 'calcul', 'float', 'option', 'test', '{', 'numpi', 'innerproduct', 'the', '}', 'default', 'If', '2d', 'rank']}"
476,"{'func name': 'absfunc', 'comments': '', 'stemmed comments': []}"
477,"{'func name': 'getnodes', 'comments': 'walk tree to get list of branches and list of leaves\n\nParameters ---------- tree : list of tuples tree as defined for RU2NMNL\n##### Returns\n* **branch **: list\n    list of all branch names\n\n* **leaves **: list\n    list of all leaves names\n\n', 'stemmed comments': ['name', 'ru2nmnl', 'defin', 'tree', 'walk', 'branch', 'get', 'paramet', 'tupl', 'return', 'list', 'leav']}"
478,"{'func name': 'armaloop', 'comments': 'get arma recursion in simple loop\n\nfor simplicity assumes that ma polynomial is not longer than the ar-polynomial\n\nParameters ---------- arcoefs : array_like autoregressive coefficients in right hand side parameterization macoefs : array_like moving average coefficients, without leading 1\n##### Returns\n* **y **: ndarray\n    predicted values, initial values are the same as the observed values\n\n* **e **: ndarray\n    predicted residuals, zero for initial observations\n\n', 'stemmed comments': ['coeffici', 'initi', 'array_lik', 'side', 'macoef', 'parameter', 'paramet', 'return', 'simpl', 'simplic', 'polynomi', 'valu', 'assum', 'longer', 'residu', 'arcoef', '1', 'autoregress', 'hand', 'move', 'averag', 'get', 'zero', 'lead', 'arma', 'predict', 'observ', 'arpolynomi', 'ndarray', 'recurs', 'e', 'right', 'loop', 'without']}"
479,"{'func name': 'groupsstats_dummy', 'comments': '', 'stemmed comments': []}"
480,"{'func name': 'adf20', 'comments': '', 'stemmed comments': []}"
481,"{'func name': 'getpoly', 'comments': '', 'stemmed comments': []}"
482,"{'func name': 'dropname', 'comments': 'drop names from a list of strings, names to drop are in space delimited list does not change original list\n\n\n', 'stemmed comments': ['origin', 'chang', 'drop', 'space', 'string', 'name', 'list', 'delimit']}"
483,"{'func name': 'mean_residual_life', 'comments': 'empirical mean residual life or expected shortfall\n\nParameters ---------- x : 1-dimensional array_like frac : list[float], optional All entries must be between 0 and 1 alpha : float, default 0.05 FIXME: not actually used.\n\nTODO: check formula for std of mean does not include case for all observations last observations std is zero vectorize loop using cumsum frac does not work yet\n', 'stemmed comments': ['x', 'array_lik', 'all', ']', 'use', 'fixm', 'formula', 'paramet', '1dimension', 'todo', 'vector', 'yet', 'cumsum', 'actual', 'last', 'case', 'residu', 'includ', 'alpha', '005', 'expect', '1', 'check', 'float', 'option', 'mean', 'life', 'must', 'work', 'zero', '[', '0', 'default', 'observ', 'std', 'frac', 'entri', 'empir', 'shortfal', 'loop', 'list']}"
484,"{'func name': 'cohen_es', 'comments': '', 'stemmed comments': []}"
485,"{'func name': 'branch2', 'comments': 'walking a tree bottom-up based on dictionary\n\n\n', 'stemmed comments': ['dictionari', 'tree', 'walk', 'bottomup', 'base']}"
486,"{'func name': 'get_thsd', 'comments': '', 'stemmed comments': []}"
487,"{'func name': 'maxabs', 'comments': '', 'stemmed comments': []}"
488,"{'func name': 'mean_forecast_err', 'comments': '', 'stemmed comments': []}"
489,"{'func name': 'acovf_fft', 'comments': 'autocovariance function with call to fftconvolve, biased\n\nParameters ---------- x : array_like timeseries, signal demean : bool If true, then demean time series\n##### Returns\n* **acovf **: ndarray\n    autocovariance for data, same length as x\n\n', 'stemmed comments': ['length', 'x', 'array_lik', 'bool', 'seri', 'paramet', 'return', 'autocovari', 'data', 'acovf', 'function', 'timeseri', 'signal', 'true', 'time', 'demean', 'call', 'If', 'ndarray', 'bias', 'fftconvolv']}"
490,"{'func name': 'quarter_plot', 'comments': ""Seasonal plot of quarterly data\n\nParameters ---------- x : array_like Seasonal data to plot. If dates is None, x must be a pandas object with a PeriodIndex or DatetimeIndex with a monthly frequency. dates : array_like, optional If `x` is not a pandas object, then dates must be supplied. ylabel : str, optional The label for the y-axis. Will attempt to use the `name` attribute of the Series. ax : matplotlib.axes, optional Existing axes instance.\n##### Returns\n* **>>> dates = pd.to_datetime(list(map(lambda x**: '-'.join(x) + '-1',\n\n* **.. plot**: \n\n"", 'stemmed comments': ['suppli', 'attempt', 'plot', 'x', 'periodindex', 'monthli', 'array_lik', 'object', 'frequenc', 'use', 'exist', 'paramet', 'ylabel', 'seri', 'return', 'will', 'matplotlibax', 'data', '=', 'name', 'label', 'ax', 'instanc', 'axe', 'season', 'panda', 'str', 'attribut', '1', 'datetimeindex', 'option', 'yaxi', 'pdto_datetim', 'must', 'quarterli', 'the', '>', 'join', 'none', 'If', 'lambda', 'map', 'date', 'list']}"
491,"{'func name': 'freq_to_period', 'comments': 'Convert a pandas frequency to a periodicity\n\nParameters ---------- freq : str or offset Frequency to convert\n##### Returns\n* **period **: int\n    Periodicity of freq\n\n', 'stemmed comments': ['offset', 'convert', 'panda', 'freq', 'str', 'int', 'frequenc', 'period', 'paramet', 'return']}"
492,"{'func name': 'tukeyplot', 'comments': '', 'stemmed comments': []}"
493,"{'func name': 'seasonal_dummies', 'comments': 'Parameters ---------- n_seasons : int >= 0 Number of seasons (e.g. 12 for monthly data and 4 for quarterly data). len_endog : int >= 0 Total number of observations. first_period : int, default: 0 Season of the first observation. As an example, suppose we have monthly data and the first observation is in March (third month of the year). In this case we pass 2 as first_period. (0 for the first season, 1 for the second, ..., n_seasons-1 for the last season). An integer greater than n_seasons-1 are treated in the same way as the integer modulo n_seasons. centered : bool, default: False If True, center (demean) the dummy variables. That is useful in order to get seasonal dummies that are orthogonal to the vector of constant dummy variables (a vector of ones).\n\n\n##### Returns\n* **seasonal_dummies **: ndarray (len_endog x n_seasons-1)\n\n', 'stemmed comments': ['fals', 'monthli', 'year', '2', 'march', 'way', 'x', 'integ', 'first', 'use', 'bool', '4', 'vector', 'paramet', '12', 'return', 'seasonal_dummi', 'dummi', 'order', 'n_season', 'An', 'eg', 'int', 'data', '=', 'suppos', 'last', 'second', 'exampl', 'In', 'modulo', 'greater', 'case', 'orthogon', 'constant', 'that', 'season', 'As', 'true', 'month', '1', 'demean', 'len_endog', 'variabl', 'quarterli', 'get', '>', 'one', '0', 'observ', 'default', 'If', 'ndarray', 'pass', 'n_seasons1', 'center', 'third', 'treat', 'number', 'first_period', 'total']}"
494,"{'func name': 'annotate_axes', 'comments': 'Annotate Axes with labels, points, offset_points according to the given index.\n\n\n', 'stemmed comments': ['axe', 'offset_point', 'point', 'given', 'accord', 'index', 'annot', 'label']}"
495,"{'func name': 'as_numpy_dataset', 'comments': 'Convert a pandas dataset to a NumPy dataset\n\n\n', 'stemmed comments': ['dataset', 'convert', 'numpi', 'panda']}"
496,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
497,"{'func name': 'dict_like', 'comments': 'Check if dict_like (dict, Mapping) or raise if not\n\nParameters ---------- value : object Value to verify name : str Variable name for exceptions optional : bool Flag indicating whether None is allowed strict : bool If True, then only allow dict. If False, allow any Mapping-like object.\n##### Returns\n* **converted **: dict_like\n    value\n\n', 'stemmed comments': ['fals', 'object', 'except', 'bool', 'strict', 'paramet', 'return', 'valu', 'name', 'verifi', 'dict_lik', 'convert', 'true', 'str', 'rais', 'check', 'variabl', 'option', 'whether', 'flag', 'none', 'allow', 'If', 'map', 'dict', 'mappinglik', 'indic']}"
498,"{'func name': '_acovs_to_acorrs', 'comments': '', 'stemmed comments': []}"
499,"{'func name': 'plot_fevd', 'comments': '', 'stemmed comments': []}"
500,"{'func name': 'ar2lhs', 'comments': 'convert full (rhs) lagpolynomial into a reduced, left side lagpoly array\n\nthis is mainly a reminder about the definition\n', 'stemmed comments': ['lagpolynomi', 'convert', 'mainli', 'lagpoli', 'reduc', 'array', 'left', 'side', 'rh', 'remind', 'definit', 'full']}"
501,"{'func name': 'VARMA', 'comments': 'multivariate linear filter\n\nx (TxK) B (PxKxK)\n\nxhat(t,i) = sum{_p}sum{_k} { x(t-P:t,:) .* B(:,:,i) } + sum{_q}sum{_k} { e(t-Q:t,:) .* C(:,:,i) }for all i = 0,K-1\n', 'stemmed comments': ['x', 'xhat', 'B', '_k', 'filter', 'txk', 'C', '=', 'tP', 'multivari', 'tQ', '_p', 'sum', '{', '_q', '}', 'i', '0', 'pxkxk', 'linear', 'e', 'K1']}"
502,"{'func name': 'coint_johansen', 'comments': ""Johansen cointegration test of the cointegration rank of a VECM\n\nParameters ---------- endog : array_like (nobs_tot x neqs) Data to test det_order : int * -1\n\n- no deterministic terms * 0\n\n- constant term * 1\n\n- linear trend k_ar_diff : int, nonnegative Number of lagged differences in the model.\n##### Returns\n* **result **: JohansenTestResult\n    An object containing the test's results. The most important attributes\n    of the result class are\n\n"", 'stemmed comments': ['x', 'array_lik', 'object', 'trend', 'paramet', 'nobs_tot', 'return', 's', 'class', 'An', 'johansen', 'data', 'int', 'model', 'import', 'johansentestresult', 'lag', 'endog', 'neq', 'result', 'cointegr', 'determinist', 'constant', 'contain', '1', 'attribut', 'term', 'test', 'vecm', 'the', '0', 'det_ord', 'rank', 'differ', 'linear', 'number', 'k_ar_diff', 'nonneg']}"
503,"{'func name': 'scan_setup_py', 'comments': ""Validate the contents of setup.py against Versioneer's expectations.\n\n\n"", 'stemmed comments': ['valid', 'expect', 'content', 'version', 'setuppi', 's']}"
504,"{'func name': 'webdoc', 'comments': ""Opens a browser and displays online documentation\n\nParameters ---------- func : {str, callable} Either a string to search the documentation or a function stable : bool Flag indicating whether to use the stable documentation (True) or the development documentation (False).\n\nIf not provided, opens the stable documentation if the current version of statsmodels is a release\n\nExamples -------- >>> import statsmodels.api as sm\n\nDocumentation site\n\n>>> sm.webdoc()\n\nSearch for glm in docs\n\n>>> sm.webdoc('glm')\n\nGo to current generated help for OLS\n\n>>> sm.webdoc(sm.OLS, stable=False)\n\nNotes ----- By default, open stable documentation if the current version of statsmodels is a release.\n\nOtherwise opens the development documentation.\n\nUses the default system browser.\n"", 'stemmed comments': ['fals', 'note', 'doc', 'sm', 'use', 'bool', 'string', 'paramet', 'By', 'browser', 'statsmodel', 'current', 'smwebdoc', 'releas', 'onlin', 'open', 'document', 'callabl', 'help', 'otherwis', 'develop', 'import', 'function', 'provid', 'version', 'site', 'search', 'true', 'str', 'statsmodelsapi', '{', 'Go', 'whether', 'ol', 'flag', '>', '}', 'glm', 'stable=fals', 'display', 'default', 'If', 'either', 'smol', 'indic', 'system', 'stabl', 'gener', 'exampl', 'func']}"
505,"{'func name': 'ztost', 'comments': ""Equivalence test based on normal distribution\n\nParameters ---------- x1 : array_like one sample or first sample for 2 independent samples low, upp : float equivalence interval low < m1\n\n- m2 < upp x1 : array_like or None second sample for 2 independent samples test. If None, then a one-sample test is performed. usevar : str, 'pooled' If `pooled`, then the standard deviation of the samples is assumed to be the same. Only `pooled` is currently implemented.\n##### Returns\n* **pvalue **: float\n    pvalue of the non-equivalence test\n\n* **t1, pv1 **: tuple of floats\n    test statistic and pvalue for lower threshold test\n\n* **t2, pv2 **: tuple of floats\n    test statistic and pvalue for upper threshold test\n\n"", 'stemmed comments': ['pv2', 'x1', 'normal', 'pv1', '2', 'array_lik', 'deviat', 't1', 'first', 'sampl', 'paramet', 'independ', 'return', '<', 'current', 'usevar', 'assum', 'standard', 'second', 'lower', 'onesampl', 'threshold', 'base', 'distribut', 't2', 'upper', 'statist', 'str', 'float', 'm1', 'pvalu', 'test', 'nonequival', 'interv', 'implement', 'onli', 'one', 'low', 'none', 'If', 'pool', 'm2', 'upp', 'tupl', 'perform', 'equival']}"
506,"{'func name': 'index_trim_outlier', 'comments': 'returns indices to residual array with k outliers removed\n\nParameters ---------- resid : array_like, 1d data vector, usually residuals of a regression k : int number of outliers to remove\n##### Returns\n* **trimmed_index **: ndarray, 1d\n    index array with k outliers removed\n\n* **outlier_index **: ndarray, 1d\n    index array of k outliers\n\n', 'stemmed comments': ['k', 'regress', 'array_lik', 'resid', 'array', 'vector', 'paramet', 'return', 'data', 'int', 'trimmed_index', 'usual', 'residu', 'outlier_index', '1d', 'remov', 'outlier', 'index', 'ndarray', 'indic', 'number']}"
507,"{'func name': 'populate_wrapper', 'comments': '', 'stemmed comments': []}"
508,"{'func name': 'x13_arima_select_order', 'comments': 'Perform automatic seasonal ARIMA order identification using x12/x13 ARIMA.\n\nParameters ---------- endog : array_like, pandas.Series The series to model. It is best to use a pandas object with a DatetimeIndex or PeriodIndex. However, you can pass an array-like object. If your object does not have a dates index then ``start`` and ``freq`` are not optional. maxorder : tuple The maximum order of the regular and seasonal ARMA polynomials to examine during the model identification. The order for the regular polynomial must be greater than zero and no larger than 4. The order for the seasonal polynomial may be 1 or 2. maxdiff : tuple The maximum orders for regular and seasonal differencing in the automatic differencing procedure. Acceptable inputs for regular differencing are 1 and 2. The maximum order for seasonal differencing is 1. If ``diff`` is specified then ``maxdiff`` should be None. Otherwise, ``diff`` will be ignored. See also ``diff``. diff : tuple Fixes the orders of differencing for the regular and seasonal differencing. Regular differencing may be 0, 1, or 2. Seasonal differencing may be 0 or 1. ``maxdiff`` must be None, otherwise ``diff`` is ignored. exog : array_like Exogenous variables. log : bool or None If None, it is automatically determined whether to log the series or not. If False, logs are not taken. If True, logs are taken. outlier : bool Whether or not outliers are tested for and corrected, if detected. trading : bool Whether or not trading day effects are tested for. forecast_periods : int Number of forecasts produced. The default is None. start : str, datetime Must be given if ``endog`` does not have date information in its index. Anything accepted by pandas.DatetimeIndex for the start value. freq : str Must be givein if ``endog`` does not have date information in its index. Anything accepted by pandas.DatetimeIndex for the freq value. print_stdout : bool The stdout from X12/X13 is suppressed. To print it out, set this to True. Default is False. x12path : str or None The path to x12 or x13 binary. If None, the program will attempt to find x13as or x12a on the PATH or by looking at X13PATH or X12PATH depending on the value of prefer_x13. prefer_x13 : bool If True, will look for x13as first and will fallback to the X13PATH environmental variable. If False, will look for x12a first and will fallback to the X12PATH environmental variable. If x12path points to the path for the X12/X13 binary, it does nothing.\n##### Returns\n* **Bunch\n    A bunch object containing the listed attributes.\n    - order **: tuple\n      The regular order.\n    - sorder\n\n', 'stemmed comments': ['pandasseri', 'periodindex', 'object', 'determin', 'look', '4', 'maxord', 'trade', 'may', 'regular', 'endog', 'exogen', 'x12', 'panda', 'correct', 'detect', 'option', 'test', 'must', 'index', 'zero', 'pass', 'forecast_period', 'date', 'given', 'print', 'specifi', 'environment', 'see', 'anyth', 'identif', 'use', 'path', 'set', 'paramet', 'ignor', 'x13a', 'return', 'To', 'x12/x13', 'examin', 'stdout', 'accept', 'differenc', 'season', 'true', 'str', 'attribut', 'variabl', 'datetimeindex', 'program', 'datetim', 'whether', 'outlier', 'arma', 'fix', 'sorder', 'exog', 'fallback', 'number', 'greater', 'maximum', 'attempt', 'fals', 'noth', 'first', 'start', 'valu', 'int', 'model', 'x13path', 'point', 'inform', 'effect', '0', 'none', 'If', 'default', 'automat', 'binari', 'howev', 'log', 'taken', 'forecast', 'tupl', 'procedur', 'input', 'A', 'arima', '2', 'array_lik', 'print_stdout', 'prefer_x13', 'bool', 'seri', 'maxdiff', 'givein', 'order', 'suppress', 'polynomi', 'otherwis', 'x12path', 'x12a', 'diff', 'arraylik', 'contain', '1', 'pandasdatetimeindex', 'also', 'find', 'bunch', 'the', 'depend', 'best', 'produc', 'freq', 'It', 'x13', 'larger', 'perform', 'list', 'day']}"
509,"{'func name': 'yule_walker', 'comments': 'Estimate AR parameters using Yule-Walker equations.\n\nParameters ---------- endog : array_like or SARIMAXSpecification Input time series array, assumed to be stationary. ar_order : int, optional Autoregressive order. Default is 0. demean : bool, optional Whether to estimate and remove the mean from the process prior to fitting the autoregressive coefficients. Default is True. adjusted : bool, optional Whether to use the adjusted autocovariance estimator, which uses n\n\n- h degrees of freedom rather than n. For some processes this option may\n\nresult in a non-positive definite autocovariance matrix. Default is False.\n##### Returns\n* **parameters **: SARIMAXParams object\n    Contains the parameter estimates from the final iteration.\n\n* **other_results **: Bunch\n    Includes one component, `spec`, which is the `SARIMAXSpecification`\n    instance corresponding to the input arguments.\n\n', 'stemmed comments': ['adjust', 'fals', 'coeffici', 'array_lik', 'object', 'array', 'n', 'use', 'bool', 'compon', 'yulewalk', 'seri', 'paramet', 'AR', 'correspond', 'return', 'equat', 'autocovari', 'rather', 'order', 'matrix', 'iter', 'argument', 'estim', 'assum', 'int', 'may', 'for', 'prior', 'ar_ord', 'degre', 'stationari', 'endog', 'result', 'nonposit', 'definit', 'final', 'instanc', 'includ', 'true', 'time', 'contain', 'demean', 'sarimaxspecif', 'option', 'autoregress', 'mean', 'whether', 'h', 'remov', 'freedom', 'spec', 'bunch', 'fit', 'sarimaxparam', 'one', '0', 'default', 'process', 'other_result', 'input']}"
