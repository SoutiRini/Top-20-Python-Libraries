0,"{'func name': 'pyarrow_array_to_numpy_and_mask', 'comments': 'Convert a primitive pyarrow.Array to a numpy array and boolean mask based on the buffers of the Array.\n\nParameters ---------- arr : pyarrow.Array dtype : numpy.dtype\n##### Returns\n', 'stemmed comments': ['dtype', 'numpydtyp', 'paramet', 'boolean', 'pyarrowarray', 'numpi', 'base', 'array', 'convert', 'buffer', 'mask', 'return', 'primit', 'arr']}"
1,"{'func name': 'read_excel', 'comments': '', 'stemmed comments': []}"
2,"{'func name': '_get_plot_backend', 'comments': 'Return the plotting backend to use (e.g. `pandas.plotting._matplotlib`).\n\nThe plotting system of pandas has been using matplotlib, but the idea here is that it can also work with other third-party backends. In the future, this function will return the backend from a pandas option, and all the rest of the code in this file will use the backend specified there for the plotting.\n\nThe backend is imported lazily, as matplotlib is a soft dependency, and pandas can be used without it being installed.\n', 'stemmed comments': ['work', 'use', 'also', 'option', 'plot', 'depend', 'the', 'eg', 'lazili', 'system', 'matplotlib', 'instal', 'without', 'futur', 'import', 'soft', 'panda', 'return', 'code', 'thirdparti', 'rest', 'pandasplotting_matplotlib', 'function', 'file', 'In', 'backend', 'specifi', 'idea']}"
3,"{'func name': 'indent', 'comments': '', 'stemmed comments': []}"
4,"{'func name': 'rewrite_exception', 'comments': 'Rewrite the message of an exception.\n\n\n', 'stemmed comments': ['messag', 'except', 'rewrit']}"
5,"{'func name': 'read_json', 'comments': 'Convert a JSON string to pandas object.\n\nParameters ---------- path_or_buf : a valid JSON str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: ``file://localhost/path/to/table.json``.\n\nIf you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\nBy file-like object, we refer to objects with a ``read()`` method, such as a file handler (e.g. via builtin ``open`` function) or ``StringIO``. orient : str Indication of expected JSON string format. Compatible JSON strings can be produced by ``to_json()`` with a corresponding orient value. The set of possible orients is:\n\n- ``\'split\'`` : dict like ``{index -> [index], columns -> [columns], data -> [values]}``\n\n- ``\'records\'`` : list like ``[{column -> value}, ... , {column -> value}]``\n\n- ``\'index\'`` : dict like ``{index -> {column -> value}}``\n\n- ``\'columns\'`` : dict like ``{column -> {index -> value}}``\n\n- ``\'values\'`` : just the values array\n\nThe allowed and default values depend on the value of the `typ` parameter.\n\n* when ``typ == \'series\'``,\n\n- allowed orients are ``{\'split\',\'records\',\'index\'}``\n\n- default is ``\'index\'``\n\n- The Series index must be unique for orient ``\'index\'``.\n\n* when ``typ == \'frame\'``,\n\n- allowed orients are ``{\'split\',\'records\',\'index\', \'columns\',\'values\', \'table\'}``\n\n- default is ``\'columns\'``\n\n- The DataFrame index must be unique for orients ``\'index\'`` and ``\'columns\'``.\n\n- The DataFrame columns must be unique for orients ``\'index\'``, ``\'columns\'``, and ``\'records\'``.\n\n.. versionadded:: 0.23.0 \'table\' as an allowed value for the ``orient`` argument\n\ntyp : {\'frame\', \'series\'}, default \'frame\' The type of object to recover.\n\ndtype : bool or dict, default None If True, infer dtypes; if a dict of column to dtype, then use those; if False, then don\'t infer dtypes at all, applies only to the data.\n\nFor all ``orient`` values except ``\'table\'``, default is True.\n\n.. versionchanged:: 0.25.0\n\nNot applicable for ``orient=\'table\'``.\n\nconvert_axes : bool, default None Try to convert the axes to the proper dtypes.\n\nFor all ``orient`` values except ``\'table\'``, default is True.\n\n.. versionchanged:: 0.25.0\n\nNot applicable for ``orient=\'table\'``.\n\nconvert_dates : bool or list of str, default True List of columns to parse for dates. If True, then try to parse datelike columns. A column label is datelike if\n\n* it ends with ``\'_at\'``,\n\n* it ends with ``\'_time\'``,\n\n* it begins with ``\'timestamp\'``,\n\n* it is ``\'modified\'``, or\n\n* it is ``\'date\'``.\n\nkeep_default_dates : bool, default True If parsing dates, then parse the default datelike columns.\n\nnumpy : bool, default False Direct decoding to numpy arrays. Supports numeric data only, but non-numeric column and index labels are supported. Note also that the JSON ordering MUST be the same for each term if numpy=True.\n\n.. deprecated:: 1.0.0\n\nprecise_float : bool, default False Set to enable usage of higher precision (strtod) function when decoding string to double values. Default (False) is to use fast but less precise builtin functionality.\n\ndate_unit : str, default None The timestamp unit to detect if converting dates. The default behaviour is to try and detect the correct precision, but if this is not desired then pass one of \'s\', \'ms\', \'us\' or \'ns\' to force parsing only seconds, milliseconds, microseconds or nanoseconds respectively.\n\nencoding : str, default is \'utf-8\' The encoding to use to decode py3 bytes.\n\nlines : bool, default False Read the file as a json object per line.\n\nchunksize : int, optional Return JsonReader object for iteration. See the `line-delimited json docs <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#line-delimited-json>`_ for more information on ``chunksize``. This can only be passed if `lines=True`. If this is None, the file will be read into memory all at once.\n\n.. versionadded:: 0.21.0\n\ncompression : {\'infer\', \'gzip\', \'bz2\', \'zip\', \'xz\', None}, default \'infer\' For on-the-fly decompression of on-disk data. If \'infer\', then use gzip, bz2, zip or xz if path_or_buf is a string ending in \'.gz\', \'.bz2\', \'.zip\', or \'xz\', respectively, and no decompression otherwise. If using \'zip\', the ZIP file must contain only one data file to be read in. Set to None for no decompression.\n\n.. versionadded:: 0.21.0\n##### Returns\n* **DataFrame.to_json **: Convert a DataFrame to a JSON string.\n\n* **Series.to_json **: Convert a Series to a JSON string.\n\n* **Specific to ``orient=\'table\'``, if a **: class\n\n* ****: func\n\n* **subsequent read operation will incorrectly set the **: class\n\n* **``None``. This is because `index` is also used by **: func\n\n* **to denote a missing **: class\n\n* **limitation is encountered with a **: class\n\n* **Encoding/decoding a Dataframe using ``\'split\'`` formatted JSON**: \n\n* **\'{""columns""**: [""col 1"",""col 2""],\n  ""index""\n\n* **Encoding/decoding a Dataframe using ``\'index\'`` formatted JSON**: \n\n* **\'{""row 1""**: {""col 1""\n\n* **\'[{""col 1""**: ""a"",""col 2""\n\n* **\'{""schema""**: {""fields""\n\n', 'stemmed comments': ['ftp', 'set', 'end', 'order', 'begin', 'local', 'see', 'millisecond', 'class', 'ani', 'modifi', 'behaviour', 'string', 'encod', 'doubl', 'valu', 'via', 'label', 'indic', 'want', 'datafram', '==', 'appli', 'versionchang', 'split', 'per', 'str', 'A', '100', 'less', 'unit', 'jsonread', '<', '_at', 'panda', 'data', 'dtype', 'byte', 'proper', '0210', '>', 'list', 'not', 'bool', 'encoding/decod', 'ondisk', 'thi', 'correct', 'onthefli', 'desir', 'read', 'path_or_buf', 'use', 'ospathlik', 'specif', 'infer', 'convert', 'option', 'xz', 'type', 'utf8', 'py3', 'keep_default_d', 'bz2', 'nanosecond', 'decod', '//pandaspydataorg/pandasdocs/stable/user_guide/iohtml', 'method', 'compress', 'denot', 'enabl', 'orient', 'higher', 'date_unit', 'encount', 'forc', 'seri', 'int', 'expect', 'return', ']', 'ms', 'iter', 'argument', 'url', 'must', 'index', '0250', 'numpy=tru', 'strtod', 'function', 'recov', 'subsequ', 'file', 'If', 'term', 'handler', 'precis', 'respect', 'chunksiz', 'correspond', 'accept', 'object', 'typ', 'otherwis', 'timestamp', 'second', 'line', 'compat', 'also', 'allow', 'nonnumer', 'microsecond', 'includ', '_time', 'uniqu', 'miss', 'seriesto_json', 'func', '1', 'column', 'path', 'lines=tru', 'us', 'eg', 'schema', 'produc', 'fals', 'could', 'ns', 'row', 'http', 'filelik', 'frame', 'doc', 'incorrectli', 'field', 'default', 'usag', 'contain', 'orient=t', 'builtin', 'axe', 'true', 'oper', 'date', 'deprec', 'zip', '{', 'dataframeto_json', 'pass', 'numer', 'record', '0230', 'dict', 'refer', 'paramet', 'fast', 'By', 'one', 'tri', 'versionad', 'valid', 'none', 'gzip', 'scheme', 'like', 'convert_d', 'convert_ax', 'host', 'pars', 'to_json', ';', 'depend', 'the', '_', 'numpi', 'stringio', 'except', 'open', 'array', '//localhost/path/to/tablejson', 'detect', 'datelik', 'linedelimit', 'memori', 's3', 'note', 'nt', 'linedelimitedjson', 'inform', 'limit', 'gz', '2', 'applic', 'for', 'direct', 'possibl', 'tabl', 'support', '[', 'decompress', 's', 'col', 'precise_float', '}', 'json', 'format']}"
6,"{'func name': 'autocorrelation_plot', 'comments': 'Autocorrelation plot for time series.\n\nParameters ---------- series : Time series ax : Matplotlib axis object, optional **kwargs Options to pass to matplotlib plotting method.\n##### Returns\n* **class**: `matplotlib.axis.Axes`\n\n', 'stemmed comments': ['plot', 'paramet', 'matplotlib', 'seri', 'method', 'kwarg', 'time', 'matplotlibaxisax', 'axi', 'pass', 'option', 'autocorrel', 'return', 'ax', 'object', 'class']}"
7,"{'func name': '_json_normalize', 'comments': ""Normalize semi-structured JSON data into a flat table.\n\nParameters ---------- data : dict or list of dicts Unserialized JSON objects. record_path : str or list of str, default None Path in each object to list of records. If not passed, data will be assumed to be an array of records. meta : list of paths (str or list of str), default None Fields to use as metadata for each record in resulting table. meta_prefix : str, default None If True, prefix records with dotted (?) path, e.g. foo.bar.field if meta is ['foo', 'bar']. record_prefix : str, default None If True, prefix records with dotted (?) path, e.g. foo.bar.field if path to records is ['foo', 'bar']. errors : {'raise', 'ignore'}, default 'raise' Configures error handling.\n\n* 'ignore' : will ignore KeyError if keys listed in meta are not always present. * 'raise' : will raise KeyError if keys listed in meta are not always present. sep : str, default '.' Nested records will generate names separated by sep. e.g., for sep='.', {'foo': {'bar': 0}} -> foo.bar. max_level : int, default None Max number of levels(depth of dict) to normalize. if None, normalizes all levels.\n\n.. versionadded:: 0.25.0\n"", 'stemmed comments': ['paramet', 'use', 'name', 'sep', 'depth', '?', 'configur', 'number', 'metadata', 'versionad', 'none', 'semistructur', 'max', 'level', 'error', 'gener', 'record_prefix', 'alway', 'assum', 'path', 'array', 'keyerror', 'present', 'sep=', 'json', 'eg', 'flat', 'unseri', 'foobarfield', 'ignor', 'max_level', 'field', 'meta', 'default', '0', 'int', 'str', 'foo', 'record_path', 'key', ']', 'meta_prefix', 'data', 'true', 'separ', 'nest', 'handl', '0250', 'prefix', 'foobar', 'tabl', '>', 'normal', 'list', 'dot', 'If', '[', '{', 'pass', 'result', 'bar', 'rais', 'record', '}', 'object', 'dict']}"
8,"{'func name': 'import_optional_dependency', 'comments': 'Import an optional dependency.\n\nBy default, if a dependency is missing an ImportError with a nice message will be raised. If a dependency is present, but too old, we raise.\n\nParameters ---------- name : str The module name. This should be top-level only, so that the version may be checked. extra : str Additional text to include in the ImportError message. raise_on_missing : bool, default True Whether to raise if the optional dependency is not found. When False and the module is not present, None is returned. on_version : str {\'raise\', \'warn\'} What to do when a dependency\'s version is too old.\n\n* raise : Raise an ImportError * warn : Warn that the version is too old. Returns None * ignore: Return the module, even if the version is too old. It\'s expected that users validate the version locally when using ``on_version=""ignore""`` (see. ``io/html.py``)\n##### Returns\n* **maybe_module **: Optional[ModuleType]\n    The imported module, when found and the version is correct.\n    None is returned when the package is not found and `raise_on_missing`\n    is False, or when the package\'s version is too old and `on_version`\n    is ``\'warn\'``.\n\n', 'stemmed comments': ['messag', 'importerror', 'paramet', 'found', 'It', 'text', 'name', 'By', 'use', 'option', 'version', 'valid', 'none', 'nice', 'includ', 'user', 'local', 'see', 'miss', 'io/htmlpi', 'moduletyp', 'depend', 'on_vers', 'the', 'present', 'extra', 'fals', 'ignor', 'default', 'import', 'str', 'whether', 'warn', 'toplevel', 'return', 'expect', 'may', 'modul', ']', 'packag', 'true', 'raise_on_miss', 'what', 'on_version=', 'maybe_modul', 'addit', '[', '{', 'If', 's', 'bool', 'check', 'even', 'thi', 'rais', 'correct', 'when', '}', 'old']}"
9,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
10,"{'func name': '_generate_range_overflow_safe_signed', 'comments': 'A special case for _generate_range_overflow_safe where `periods * stride` can be calculated without overflowing int64 bounds.\n\n\n', 'stemmed comments': ['without', 'A', '_generate_range_overflow_saf', 'calcul', 'bound', 'stride', 'case', 'special', 'period', 'int64', 'overflow']}"
11,"{'func name': 'parse_table_schema', 'comments': 'Builds a DataFrame from a given schema\n\nParameters ---------- json : A JSON table schema precise_float : boolean Flag controlling precision when decoding string to double values, as dictated by ``read_json``\n##### Returns\n* **df **: DataFrame\n\n', 'stemmed comments': ['paramet', 'dictat', 'decod', 'string', 'doubl', 'df', 'schema', 'valu', 'datafram', 'boolean', 'A', 'flag', 'return', 'build', 'given', 'tabl', 'control', 'read_json', 'precis', 'precise_float', 'json']}"
12,"{'func name': 'async_mark', 'comments': '', 'stemmed comments': []}"
13,"{'func name': 'test', 'comments': '', 'stemmed comments': []}"
14,"{'func name': 'convert_rows_list_to_csv_str', 'comments': 'Convert list of CSV rows to single CSV-formatted string for current OS.\n\nThis method is used for creating expected value of to_csv() method.\n\nParameters ---------- rows_list : List[str] Each element represents the row of csv.\n##### Returns\n', 'stemmed comments': ['paramet', 'use', 'convert', 'csvformat', 'OS', 'method', 'each', 'string', 'repres', 'row', 'valu', 'csv', 'singl', 'current', 'rows_list', 'element', 'str', 'expect', 'return', ']', 'to_csv', 'list', 'creat', '[', 'thi']}"
15,"{'func name': '_pop_header_name', 'comments': 'Pop the header name for MultiIndex parsing.\n\nParameters ---------- row : list The data row to parse for the header name. index_col : int, list The index columns for our data. Assumed to be non-null.\n##### Returns\n* **header_name **: str\n    The extracted header name.\n\n* **trimmed_row **: list\n    The original data row with the header name removed.\n\n', 'stemmed comments': ['paramet', 'name', 'nonnul', 'trimmed_row', 'header_nam', 'pars', 'assum', 'the', 'column', 'row', 'index_col', 'header', 'multiindex', 'int', 'str', 'pop', 'extract', 'return', 'data', 'index', 'list', 'remov', 'origin']}"
16,"{'func name': 'validate_percentile', 'comments': 'Validate percentiles (used by describe and quantile).\n\nThis function checks if the given float oriterable of floats is a valid percentile otherwise raises a ValueError.\n\nParameters ---------- q: float or iterable of floats A single percentile or an iterable of percentiles.\n##### Returns\n', 'stemmed comments': ['paramet', 'otherwis', 'use', 'valid', 'float', 'describ', 'valueerror', 'singl', 'oriter', 'A', 'return', 'quantil', 'iter', 'given', 'percentil', 'function', 'check', 'thi', 'rais', 'q']}"
17,"{'func name': 'get_versions', 'comments': '', 'stemmed comments': []}"
18,"{'func name': 'register_index_accessor', 'comments': '', 'stemmed comments': []}"
19,"{'func name': 'safe_sort', 'comments': 'Sort ``values`` and reorder corresponding ``codes``.\n\n``values`` should be unique if ``codes`` is not None. Safe for use with mixed types (int, str), orders ints before strs.\n\nParameters ---------- values : list-like Sequence; must be unique if ``codes`` is not None. codes : list_like, optional Indices to ``values``. All out of bound indices are treated as ""not found"" and will be masked with ``na_sentinel``. na_sentinel : int, default -1 Value in ``codes`` to mark ""not found"". Ignored when ``codes`` is None. assume_unique : bool, default False When True, ``values`` are assumed to be unique, which can speed up the calculation. Ignored when ``codes`` is None. verify : bool, default True Check if codes are out of bound for the values and put out of bound codes equal to na_sentinel. If ``verify=False``, it is assumed there are no out of bound codes. Ignored when ``codes`` is None.\n\n.. versionadded:: 0.25.0\n##### Returns\n* **ordered **: ndarray\n    Sorted ``values``\n\n* **new_codes **: ndarray\n    Reordered ``codes``; returned when ``codes`` is not None.\n\n', 'stemmed comments': ['paramet', 'found', 'when', 'sort', 'ndarray', 'use', 'order', 'bound', 'speed', 'reorder', 'option', 'versionad', 'none', 'type', 'mask', 'uniqu', 'verifi', 'sequenc', 'assume_uniqu', ';', 'assum', '1', 'verify=fals', 'treat', 'mark', 'valu', 'fals', 'ignor', 'indic', 'put', 'default', 'int', 'mix', 'str', 'return', 'code', 'true', 'listlik', 'list_lik', 'new_cod', 'must', '0250', 'If', 'calcul', 'bool', 'equal', 'check', 'na_sentinel', 'safe', 'all', 'correspond']}"
20,"{'func name': 'reconstruct_object', 'comments': 'Reconstruct an object given its type, raw value, and possibly empty (None) axes.\n\nParameters ---------- typ : object A type obj : object The value to use in the type constructor axes : dict The axes to use to construct the resulting pandas object\n##### Returns\n* **ret **: typ\n    An object of type ``typ`` with the value `obj` and possible axes\n    `axes`.\n\n', 'stemmed comments': ['paramet', 'typ', 'reconstruct', 'use', 'none', 'type', 'empti', 'the', 'constructor', 'An', 'obj', 'raw', 'valu', 'A', 'panda', 'return', 'ret', 'axe', 'given', 'construct', 'possibl', 'result', 'object', 'dict']}"
21,"{'func name': 'all_indexes_same', 'comments': 'Determine if all indexes contain the same elements.\n\nParameters ---------- indexes : list of Index objects\n##### Returns\n', 'stemmed comments': ['index', 'paramet', 'element', 'object', 'list', 'contain', 'return', 'determin']}"
22,"{'func name': 'frame_apply', 'comments': 'construct and return a row or column based frame apply object\n\n\n', 'stemmed comments': ['construct', 'column', 'base', 'return', 'row', 'frame', 'object', 'appli']}"
23,"{'func name': 'get_array_op', 'comments': 'Return a binary array operation corresponding to the given operator op.\n\nParameters ---------- op : function Binary operator from operator or roperator module. str_rep : str or None, default None str_rep to pass to arithmetic_op\n##### Returns\n', 'stemmed comments': ['default', 'paramet', 'function', 'binari', 'str', 'arithmetic_op', 'roper', 'str_rep', 'array', 'pass', 'return', 'none', 'modul', 'given', 'op', 'correspond', 'oper']}"
24,"{'func name': 'make_data', 'comments': '', 'stemmed comments': []}"
25,"{'func name': 'make_data', 'comments': '', 'stemmed comments': []}"
26,"{'func name': '_make_index', 'comments': '', 'stemmed comments': []}"
27,"{'func name': 'make_data', 'comments': '', 'stemmed comments': []}"
28,"{'func name': '_try_convert_to_int_array', 'comments': 'Attempt to convert an array of data into an integer array.\n\nParameters ---------- data : The data to convert. copy : bool Whether to copy the data or not. dtype : np.dtype\n##### Returns\n* **int_array **: data converted to either an ndarray[int64] or ndarray[uint64]\n\n', 'stemmed comments': ['paramet', 'ndarray', 'convert', 'copi', 'attempt', 'the', 'array', 'int_array', 'npdtype', 'either', 'whether', 'return', 'integ', ']', 'data', 'uint64', 'dtype', '[', 'bool', 'int64']}"
29,"{'func name': 'try_cast_to_ea', 'comments': 'Call to `_from_sequence` that returns the object unchanged on Exception.\n\nParameters ---------- cls_or_instance : ExtensionArray subclass or instance obj : arraylike Values to pass to cls._from_sequence dtype : ExtensionDtype, optional\n##### Returns\n', 'stemmed comments': ['paramet', 'option', '_from_sequ', 'extensiondtyp', 'cls_from_sequ', 'except', 'obj', 'valu', 'return', 'cls_or_inst', 'arraylik', 'instanc', 'call', 'unchang', 'dtype', 'pass', 'extensionarray', 'subclass', 'object']}"
30,"{'func name': '_putmask_smart', 'comments': 'Return a new ndarray, try to preserve dtype if possible.\n\nParameters ---------- v : `values`, updated in-place (array like) mask : np.ndarray Applies to both sides (array like). n : `new values` either scalar or an array like aligned with `values`\n##### Returns\n* **values **: ndarray with updated values\n    this *may* be a copy of the original\n\n', 'stemmed comments': ['new', 'paramet', 'ndarray', 'side', 'tri', 'mask', 'like', 'scalar', 'n', 'copi', 'array', 'updat', 'valu', 'appli', 'either', 'npndarray', 'return', 'preserv', 'may', 'dtype', 'v', 'possibl', 'align', 'inplac', 'origin']}"
31,"{'func name': 'coerce_to_array', 'comments': 'Coerce the input values array to numpy arrays with a mask.\n\nParameters ---------- values : 1D list-like mask : bool 1D array, optional copy : bool, default False if True, copy the input\n##### Returns\n', 'stemmed comments': ['default', 'paramet', '1D', 'numpi', 'array', 'bool', 'option', 'coerc', 'mask', 'return', 'valu', 'fals', 'input', 'true', 'listlik', 'copi']}"
32,"{'func name': 'boxplot_frame_groupby', 'comments': '', 'stemmed comments': []}"
33,"{'func name': 'maybe_cast_to_integer_array', 'comments': 'Takes any dtype and returns the casted version, raising for when data is incompatible with integer/unsigned integer dtypes.\n\n.. versionadded:: 0.24.0\n\nParameters ---------- arr : array-like The array to cast. dtype : str, np.dtype The integer dtype to cast the array to. copy: bool, default False Whether to make a copy of the array before returning.\n##### Returns\n* **int_arr **: ndarray\n    An array of integer or unsigned integer dtype\n\n', 'stemmed comments': ['paramet', 'ndarray', 'unsign', 'versionad', 'version', 'arr', 'copi', 'take', 'the', 'array', 'An', 'fals', 'npdtype', 'make', 'cast', 'default', 'str', 'whether', 'integer/unsign', 'return', 'integ', 'arraylik', 'data', 'incompat', 'int_arr', 'dtype', 'bool', '0240', 'rais']}"
34,"{'func name': 'recode_from_groupby', 'comments': 'Reverse the codes_to_groupby to account for sort / observed.\n\nParameters ---------- c : Categorical sort : boolean The value of the sort parameter groupby was called with. ci : CategoricalIndex The codes / categories to recode\n##### Returns\n', 'stemmed comments': ['paramet', 'sort', 'groupbi', 'c', 'recod', 'revers', 'categor', 'categoricalindex', 'the', 'valu', 'ci', 'boolean', 'account', 'categori', '/', 'return', 'code', 'call', 'observ', 'codes_to_groupbi']}"
35,"{'func name': 'factorize_from_iterables', 'comments': 'A higher-level wrapper over `factorize_from_iterable`.\n\n*This is an internal function*\n\nParameters ---------- iterables : list-like of list-likes\n##### Returns\n* **codes_list **: list of ndarrays\n\n* **categories_list **: list of Indexes\n\n', 'stemmed comments': ['index', 'wrapper', 'paramet', 'factorize_from_iter', 'intern', 'function', 'A', 'codes_list', 'list', 'ndarray', 'return', 'thi', 'categories_list', 'iter', 'listlik', 'higherlevel']}"
36,"{'func name': 'to_clipboard', 'comments': 'Attempt to write text representation of object to the system clipboard The clipboard can be then pasted into Excel for example.\n\nParameters ---------- obj : the object to write to the clipboard excel : boolean, defaults to True if True, use the provided separator, writing in a csv format for allowing easy pasting into excel. if False, write a string representation of the object to the clipboard sep : optional, defaults to tab other keywords are passed to to_csv\n\nNotes ----- Requirements for your platform\n\n- Linux: xclip, or xsel (with PyQt4 modules)\n\n- Windows:\n\n- OS X\n', 'stemmed comments': ['clipboard', 'paramet', 'exampl', 'xsel', 'text', 'use', 'sep', 'write', 'option', 'allow', 'pyqt4', 'OS', 'tab', 'attempt', 'platform', 'linux', 'the', 'string', 'obj', 'requir', 'fals', 'system', 'default', 'csv', 'window', 'boolean', 'provid', 'note', 'easi', 'X', 'modul', 'keyword', 'separ', 'true', 'to_csv', 'xclip', 'excel', 'past', 'pass', 'represent', 'object', 'format']}"
37,"{'func name': '_maybe_remove', 'comments': 'For tests using tables, try removing the table to be sure there is no content from previous tests using the same table name.\n\n\n', 'stemmed comments': ['sure', 'test', 'for', 'content', 'tabl', 'use', 'name', 'tri', 'previou', 'remov']}"
38,"{'func name': 'assert_invalid_comparison', 'comments': 'Assert that comparison operations with mismatched types behave correctly.\n\nParameters ---------- left : np.ndarray, ExtensionArray, Index, or Series right : object box : {pd.DataFrame, pd.Series, pd.Index, tm.to_array}\n', 'stemmed comments': ['correctli', 'paramet', 'mismatch', 'left', 'type', 'behav', 'pddatafram', 'tmto_array', 'pdseri', 'comparison', 'seri', 'npndarray', 'oper', 'box', 'index', '{', 'extensionarray', 'right', 'assert', 'pdindex', '}', 'object']}"
39,"{'func name': 'check_binary_ew_min_periods', 'comments': '', 'stemmed comments': []}"
40,"{'func name': '_check_mixed_int', 'comments': '', 'stemmed comments': []}"
41,"{'func name': 'pandas_dtype', 'comments': 'Convert input into a pandas only dtype object or a numpy dtype object.\n\nParameters ---------- dtype : object to be converted\n##### Returns\n', 'stemmed comments': ['dtype', 'paramet', 'numpi', 'convert', 'panda', 'return', 'input', 'object']}"
42,"{'func name': '_unpack_zerodim_and_defer', 'comments': 'Boilerplate for pandas conventions in arithmetic and comparison methods.\n\nEnsure method returns NotImplemented when operating against ""senior"" classes.\n\nEnsure zero-dimensional ndarrays are always unpacked.\n\nParameters ---------- method : binary method name : str\n##### Returns\n', 'stemmed comments': ['ensur', 'paramet', 'ndarray', 'binari', 'name', 'class', 'method', 'zerodimension', 'alway', 'senior', 'comparison', 'convent', 'boilerpl', 'notimpl', 'str', 'arithmet', 'panda', 'return', 'oper', 'unpack']}"
43,"{'func name': 'get_handle', 'comments': ""Get file handle for given path/buffer and mode.\n\nParameters ---------- path_or_buf : str or file handle File path or object. mode : str Mode to open path_or_buf with. encoding : str or None Encoding to use. compression : str or dict, default None If string, specifies compression mode. If dict, value at key 'method' specifies compression mode. Compression mode must be one of {'infer', 'gzip', 'bz2', 'zip', 'xz', None}. If compression mode is 'infer' and `filepath_or_buffer` is path-like, then detect compression from the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no compression). If dict and compression mode is 'zip' or inferred as 'zip', other entries passed as additional compression options.\n\n.. versionchanged:: 1.0.0\n\nMay now be a dict with key 'method' as compression mode and other keys as compression options if compression mode is 'zip'.\n\nmemory_map : boolean, default False See parsers._parser_params for more information. is_text : boolean, default True whether file/buffer is in text format (csv, json, etc.), or in binary mode (pickle, etc.).\n##### Returns\n* **f **: file-like\n    A file-like object.\n\n* **handles **: list of file-like objects\n    A list of file-like object that were opened in this function.\n\n"", 'stemmed comments': ['paramet', 'path_or_buf', 'use', 'filepath_or_buff', 'otherwis', 'one', 'memory_map', 'text', 'infer', 'binari', 'f', 'option', 'xz', 'none', 'gzip', 'see', 'bz2', 'follow', 'method', 'compress', 'pickl', 'path', 'open', 'encod', 'string', 'json', 'entri', 'valu', 'fals', 'format', 'detect', 'etc', 'filelik', 'versionchang', 'default', 'csv', 'boolean', 'is_text', 'extens', 'str', '100', 'file/buff', 'A', 'whether', 'key', 'inform', 'return', 'may', 'given', 'true', 'gz', 'handl', 'must', 'function', 'mode', 'parsers_parser_param', 'zip', 'file', 'pathlik', '{', 'If', 'addit', 'list', 'pass', 'path/buff', 'specifi', 'get', '}', 'object', 'dict']}"
44,"{'func name': 'get_rename_function', 'comments': 'Returns a function that will map names/labels, dependent if mapper is a dict, Series or just a function.\n\n\n', 'stemmed comments': ['names/label', 'seri', 'function', 'depend', 'map', 'return', 'mapper', 'dict']}"
45,"{'func name': 'assert_is_on_offset', 'comments': '', 'stemmed comments': []}"
46,"{'func name': 'curpath', 'comments': '', 'stemmed comments': []}"
47,"{'func name': 'result_type_many', 'comments': 'Wrapper around numpy.result_type which overcomes the NPY_MAXARGS (32) argument limit.\n\n\n', 'stemmed comments': ['wrapper', 'numpyresult_typ', 'npy_maxarg', 'around', '32', 'overcom', 'limit', 'argument']}"
48,"{'func name': '_axify', 'comments': '', 'stemmed comments': []}"
49,"{'func name': 'get_weighted_roll_func', 'comments': '', 'stemmed comments': []}"
50,"{'func name': '_mpl_version', 'comments': '', 'stemmed comments': []}"
51,"{'func name': '_concat_sparse', 'comments': 'provide concatenation of an sparse/dense array of arrays each of which is a single dtype\n\nParameters ---------- to_concat : array of arrays axis : axis to provide concatenation typs : set of to_concat dtypes\n##### Returns\n', 'stemmed comments': ['dtype', 'set', 'singl', 'paramet', 'typ', 'sparse/dens', 'provid', 'axi', 'array', 'concaten', 'return', 'to_concat']}"
52,"{'func name': '_make_concat_multiindex', 'comments': '', 'stemmed comments': []}"
53,"{'func name': 'combine_concat_plans', 'comments': 'Combine multiple concatenation plans into one.\n\nexisting_plan is updated in-place.\n', 'stemmed comments': ['multipl', 'one', 'existing_plan', 'concaten', 'updat', 'inplac', 'combin', 'plan']}"
54,"{'func name': 'register_converter_cb', 'comments': '', 'stemmed comments': []}"
55,"{'func name': 'is_callable', 'comments': 'Parameters ---------- `obj` - the object to be checked\n\n\n##### Returns\n', 'stemmed comments': ['paramet', 'obj', 'return', 'check', 'object']}"
56,"{'func name': 'zero', 'comments': '', 'stemmed comments': []}"
57,"{'func name': 'multiindex_year_month_day_dataframe_random_data', 'comments': 'DataFrame with 3 level MultiIndex (year, month, day) covering first 100 business days from 2000-01-01 with random data\n\n\n', 'stemmed comments': ['busi', '3', 'multiindex', 'data', '100', 'month', 'year', 'first', 'level', 'random', 'day', '20000101', 'datafram', 'cover']}"
58,"{'func name': 'multiindex_year_month_day_dataframe_random_data', 'comments': 'DataFrame with 3 level MultiIndex (year, month, day) covering first 100 business days from 2000-01-01 with random data\n\n\n', 'stemmed comments': ['busi', '3', 'multiindex', 'data', '100', 'month', 'year', 'first', 'level', 'random', 'day', '20000101', 'datafram', 'cover']}"
59,"{'func name': 'series_and_frame', 'comments': 'Fixture for parametrization of Series and DataFrame with date_range, period_range and timedelta_range indexes\n\n\n', 'stemmed comments': ['parametr', 'index', 'seri', 'timedelta_rang', 'period_rang', 'fixtur', 'datafram', 'date_rang']}"
60,"{'func name': 'min_periods', 'comments': '', 'stemmed comments': []}"
61,"{'func name': 'frame_of_index_cols', 'comments': ""Fixture for DataFrame of columns that can be used for indexing\n\nColumns are ['A', 'B', 'C', 'D', 'E', ('tuple', 'as', 'label')]; 'A' & 'B' contain duplicates (but are jointly unique), the rest are unique.\n\nA\n\n\n\n\n\nB\n\nC\n\n\n\n\n\n\n\n D\n\n\n\n\n\n\n\n E\n\n(tuple, as, label) 0\n\nfoo\n\n\n\none\n\na\n\n0.608477 -0.012500\n\n\n\n\n\n\n\n\n\n -1.664297 1\n\nfoo\n\n\n\ntwo\n\nb -0.633460\n\n0.249614\n\n\n\n\n\n\n\n\n\n -0.364411 2\n\nfoo\n\nthree\n\nc\n\n0.615256\n\n2.154968\n\n\n\n\n\n\n\n\n\n -0.834666 3\n\nbar\n\n\n\none\n\nd\n\n0.234246\n\n1.085675\n\n\n\n\n\n\n\n\n\n\n\n0.718445 4\n\nbar\n\n\n\ntwo\n\ne\n\n0.533841 -0.005702\n\n\n\n\n\n\n\n\n\n -3.533912\n"", 'stemmed comments': ['tupl', 'B', 'use', 'jointli', 'c', 'one', '0718445', 'three', '1085675', '4', 'uniqu', '1664297', '0012500', '0234246', '3', ';', 'C', '1', 'column', '0533841', 'label', 'datafram', '3533912', 'e', '0', '0364411', 'A', 'foo', '0615256', '0005702', 'E', '0608477', 'contain', 'D', ']', 'fixtur', 'rest', '0633460', '0249614', '2', 'index', 'as', '2154968', '[', 'two', '0834666', 'bar', 'b', 'duplic', '&']}"
62,"{'func name': 'orient', 'comments': 'Fixture for orients excluding the table format.\n\n\n', 'stemmed comments': ['tabl', 'orient', 'fixtur', 'exclud', 'format']}"
63,"{'func name': 'month_classes', 'comments': 'Fixture for month based datetime offsets available for a time series.\n\n\n', 'stemmed comments': ['datetim', 'offset', 'seri', 'time', 'month', 'base', 'avail', 'fixtur']}"
64,"{'func name': 'check_for_file_leaks', 'comments': 'Fixture to run around every test to ensure that we are not leaking files.\n\nSee also -------- _test_decorators.check_file_leaks\n', 'stemmed comments': ['run', 'test', 'ensur', 'everi', 'leak', 'file', '_test_decoratorscheck_file_leak', 'around', 'also', 'fixtur', 'see']}"
65,"{'func name': 'non_mapping_dict_subclass', 'comments': 'Fixture for a non-mapping dictionary subclass.\n\n\n', 'stemmed comments': ['dictionari', 'nonmap', 'subclass', 'fixtur']}"
66,"{'func name': 'as_array', 'comments': 'Boolean fixture to support ExtensionDtype _from_sequence method testing.\n\n\n', 'stemmed comments': ['method', 'boolean', 'test', 'support', '_from_sequ', 'fixtur', 'extensiondtyp']}"
67,"{'func name': 'numeric_indexing_engine_type_and_dtype', 'comments': '', 'stemmed comments': []}"
68,"{'func name': 's3_resource', 'comments': 'Fixture for mocking S3 interaction.\n\nThe primary bucket name is ""pandas-test"". The following datasets are loaded.\n\n- tips.csv\n\n- tips.csv.gz\n\n- tips.csv.bz2\n\n- items.jsonl\n\nA private bucket ""cant_get_it"" is also created. The boto3 s3 resource is yielded by the fixture.\n', 'stemmed comments': ['tipscsv', 'name', 'mock', 'also', 'yield', 'tipscsvbz2', 'interact', 'load', 'follow', 'tipscsvgz', 'the', 'privat', 'bucket', 'dataset', 'cant_get_it', 's3', 'A', 'pandastest', 'fixtur', 'primari', 'resourc', 'S3', 'creat', 'boto3', 'itemsjsonl']}"
69,"{'func name': 'check_categorical', 'comments': '', 'stemmed comments': []}"
70,"{'func name': 'setup_mode', 'comments': 'Reset testing mode fixture\n\n\n', 'stemmed comments': ['test', 'reset', 'mode', 'fixtur']}"
71,"{'func name': 'allow_fill', 'comments': ""Boolean 'allow_fill' parameter for Categorical.take\n\n\n"", 'stemmed comments': ['categoricaltak', 'paramet', 'boolean', 'allow_fil']}"
72,"{'func name': 'wide_multi_index', 'comments': 'Return a MultiIndex that is wider than the display (>80 characters).\n\n\n', 'stemmed comments': ['multiindex', 'display', '>', 'charact', 'wider', '80', 'return']}"
73,"{'func name': 'engine_and_raw', 'comments': 'engine and raw keyword arguments for rolling.apply\n\n\n', 'stemmed comments': ['engin', 'rollingappli', 'raw', 'keyword', 'argument']}"
74,"{'func name': 'box_with_array', 'comments': 'Fixture to test behavior for Index, Series, DataFrame, and pandas Array classes\n\n\n', 'stemmed comments': ['index', 'test', 'seri', 'behavior', 'array', 'panda', 'fixtur', 'datafram', 'class']}"
75,"{'func name': 'encoding_fmt', 'comments': 'Fixture for all possible string formats of a UTF encoding.\n\n\n', 'stemmed comments': ['possibl', 'string', 'encod', 'fixtur', 'format', 'utf']}"
76,"{'func name': 'object_series', 'comments': 'Fixture for Series of dtype object with Index of unique strings\n\n\n', 'stemmed comments': ['dtype', 'index', 'seri', 'string', 'fixtur', 'uniqu', 'object']}"
77,"{'func name': 'groupby_func', 'comments': 'yields both aggregation and transformation functions.\n\n\n', 'stemmed comments': ['aggreg', 'yield', 'function', 'transform']}"
78,"{'func name': 'in_ipython_frontend', 'comments': ""Check if we're inside an an IPython zmq frontend.\n\n\n##### Returns\n"", 'stemmed comments': ['re', 'check', 'return', 'ipython', 'insid', 'frontend', 'zmq']}"
79,"{'func name': 'sanitize_index', 'comments': 'Sanitize an index type to return an ndarray of the underlying, pass through a non-Index.\n\n\n', 'stemmed comments': ['index', 'ndarray', 'sanit', 'underli', 'pass', 'return', 'type', 'nonindex']}"
80,"{'func name': 'create_series_with_explicit_dtype', 'comments': 'Helper to pass an explicit dtype when instantiating an empty Series.\n\nThis silences a DeprecationWarning described in GitHub-17261.\n\nParameters ---------- data : Mirrored from Series.__init__ index : Mirrored from Series.__init__ dtype : Mirrored from Series.__init__ name : Mirrored from Series.__init__ copy : Mirrored from Series.__init__ fastpath : Mirrored from Series.__init__ dtype_if_empty : str, numpy.dtype, or ExtensionDtype This dtype will be passed explicitly if an empty Series will be instantiated.\n##### Returns\n', 'stemmed comments': ['numpydtyp', 'paramet', 'instanti', 'name', 'empti', 'copi', 'extensiondtyp', 'describ', 'github17261', 'seri', 'fastpath', 'silenc', 'deprecationwarn', 'str', 'helper', 'return', 'data', 'mirror', 'dtype', 'index', 'explicitli', 'dtype_if_empti', 'pass', 'thi', 'series__init__', 'explicit']}"
81,"{'func name': 'get_finder', 'comments': '', 'stemmed comments': []}"
82,"{'func name': '_side_expander', 'comments': '', 'stemmed comments': []}"
83,"{'func name': '_check_columns', 'comments': '', 'stemmed comments': []}"
84,"{'func name': 'maybe_infer_freq', 'comments': 'Comparing a DateOffset to the string ""infer"" raises, so we need to be careful about comparisons.  Make a dummy variable `freq_infer` to signify the case where the given freq is ""infer"" and set freq to None to avoid comparison trouble later on.\n\nParameters ---------- freq : {DateOffset, None, str}\n##### Returns\n* **freq **: {DateOffset, None}\n\n* **freq_infer **: bool\n\n', 'stemmed comments': ['set', 'paramet', 'variabl', 'infer', 'care', 'none', 'signifi', 'dummi', 'troubl', 'later', 'string', 'dateoffset', 'need', 'comparison', 'make', 'avoid', 'str', 'return', 'case', 'given', 'freq', 'compar', '{', 'bool', 'rais', 'freq_inf', '}']}"
85,"{'func name': '_join_i8_wrapper', 'comments': 'Create the join wrapper methods.\n\n\n', 'stemmed comments': ['join', 'wrapper', 'creat', 'method']}"
86,"{'func name': 'to_time', 'comments': 'Parse time strings to time objects using fixed strptime formats (""%H:%M"", ""%H%M"", ""%I:%M%p"", ""%I%M%p"", ""%H:%M:%S"", ""%H%M%S"", ""%I:%M:%S%p"", ""%I%M%S%p"")\n\nUse infer_time_format if all the strings are in the same format to speed up conversion.\n\nParameters ---------- arg : string in time format, datetime.time, list, tuple, 1-d array,\n\nSeries format : str, default None Format used to convert arg into a time object.\n\nIf None, fixed formats are used. infer_time_format: bool, default False Infer the time format based on the first non-NaN element.\n\nIf all strings are in the same format, this will speed up conversion. errors : {\'ignore\', \'raise\', \'coerce\'}, default \'raise\'\n\n- If \'raise\', then invalid parsing will raise an exception\n\n- If \'coerce\', then invalid parsing will be set as None\n\n- If \'ignore\', then invalid parsing will return the input\n##### Returns\n', 'stemmed comments': ['fix', 'paramet', 'tupl', 'set', 'use', 'time', 'p', 'infer', 'speed', 'convert', 'S', 'none', 'strptime', '%', 'error', 'pars', '1d', 'invalid', 'first', 'infer_time_format', 'base', 'string', 'array', 'except', 'fals', 'ignor', 'I', 'default', 'element', 'seri', 'str', 'M', 'datetimetim', 'coerc', 'return', 'arg', 'list', '{', 'H', 'If', 'bool', 'nonnan', 'rais', 'input', '}', 'object', 'format', 'convers']}"
87,"{'func name': '_time_to_micros', 'comments': '', 'stemmed comments': []}"
88,"{'func name': '_maybe_localize_point', 'comments': 'Localize a start or end Timestamp to the timezone of the corresponding start or end Timestamp\n\nParameters ---------- ts : start or end Timestamp to potentially localize is_none : argument that should be None is_not_none : argument that should not be None freq : Tick, DateOffset, or None tz : str, timezone object or None ambiguous: str, localization behavior for ambiguous times nonexistent: str, localization behavior for nonexistent times\n##### Returns\n* **ts **: Timestamp\n\n', 'stemmed comments': ['paramet', 'end', 'timestamp', 'time', 'potenti', 'timezon', 'none', 'ts', 'local', 'tz', 'nonexist', 'dateoffset', 'ambigu', 'start', 'tick', 'behavior', 'str', 'return', 'argument', 'freq', 'is_non', 'is_not_non', 'correspond', 'object']}"
89,"{'func name': 'dispatch_to_extension_op', 'comments': 'Assume that left or right is a Series backed by an ExtensionArray, apply the operator defined by op.\n\nParameters ---------- op : binary operator left : ExtensionArray or np.ndarray right : object\n##### Returns\n', 'stemmed comments': ['paramet', 'seri', 'assum', 'binari', 'object', 'left', 'defin', 'npndarray', 'right', 'extensionarray', 'return', 'op', 'back', 'oper', 'appli']}"
90,"{'func name': 'detect_console_encoding', 'comments': 'Try to find the most capable encoding supported by the console. slightly modified from the way IPython handles the same issue.\n\n\n', 'stemmed comments': ['handl', 'capabl', 'issu', 'consol', 'support', 'modifi', 'tri', 'encod', 'ipython', 'way', 'slightli', 'find']}"
91,"{'func name': '_make_flex_doc', 'comments': ""Make the appropriate substitutions for the given operation and class-typ into either _flex_doc_SERIES or _flex_doc_FRAME to return the docstring to attach to a generated method.\n\nParameters ---------- op_name : str {'__add__', '__sub__', ... '__eq__', '__ne__', ...} typ : str {series, 'dataframe']}\n##### Returns\n* **doc **: str\n\n"", 'stemmed comments': ['paramet', 'typ', 'gener', 'method', '__sub__', 'docstr', '}', '_flex_doc_seri', 'datafram', 'doc', 'make', 'seri', 'either', 'str', 'substitut', '__eq__', 'classtyp', 'return', ']', 'given', 'oper', '__ne__', '__add__', '_flex_doc_fram', '{', 'attach', 'op_nam', 'appropri']}"
92,"{'func name': 'register_extension_dtype', 'comments': 'Register an ExtensionType with pandas as class decorator.\n\n.. versionadded:: 0.24.0\n\nThis enables operations like ``.astype(name)`` for the name of the ExtensionDtype.\n##### Returns\n* **... class MyExtensionDtype(ExtensionDtype)**: \n\n', 'stemmed comments': ['astyp', 'myextensiondtyp', 'name', 'extensiondtyp', 'decor', 'enabl', 'versionad', 'panda', '0240', 'return', 'thi', 'extensiontyp', 'like', 'oper', 'regist', 'class']}"
93,"{'func name': '_check_ne_builtin_clash', 'comments': 'Attempt to prevent foot-shooting in a helpful way.\n\nParameters ---------- terms : Term Terms can contain\n', 'stemmed comments': ['paramet', 'footshoot', 'term', 'help', 'contain', 'attempt', 'way', 'prevent']}"
94,"{'func name': 'eval', 'comments': ""Evaluate a Python expression as a string using various backends.\n\nThe following arithmetic operations are supported: ``+``, ``-``, ``*``, ``/``, ``**``, ``%``, ``//`` (python engine only) along with the following boolean operations: ``|`` (or), ``&`` (and), and ``~`` (not). Additionally, the ``'pandas'`` parser allows the use of :keyword:`and`, :keyword:`or`, and :keyword:`not` with the same semantics as the corresponding bitwise operators.\n\n:class:`~pandas.Series` and :class:`~pandas.DataFrame` objects are supported and behave as they would with plain ol' Python evaluation.\n\nParameters ---------- expr : str The expression to evaluate. This string cannot contain any Python `statements <https://docs.python.org/3/reference/simple_stmts.html#simple-statements>`__, only Python `expressions <https://docs.python.org/3/reference/simple_stmts.html#expression-statements>`__. parser : {'pandas', 'python'}, default 'pandas' The parser to use to construct the syntax tree from the expression. The default of ``'pandas'`` parses code slightly different than standard Python. Alternatively, you can parse an expression using the ``'python'`` parser to retain strict Python semantics.\n\nSee the :ref:`enhancing performance <enhancingperf.eval>` documentation for more details. engine : {'python', 'numexpr'}, default 'numexpr'\n\nThe engine used to evaluate the expression. Supported engines are\n\n- None\n\n\n\n\n\n\n\n : tries to use ``numexpr``, falls back to ``python``\n\n- ``'numexpr'``: This default engine evaluates pandas objects using numexpr for large speed ups in complex expressions with large frames.\n\n- ``'python'``: Performs operations as if you had ``eval``'d in top level python. This engine is generally not that useful.\n\nMore backends may be available in the future.\n\ntruediv : bool, optional Whether to use true division, like in Python >= 3. deprecated:: 1.0.0\n\nlocal_dict : dict or None, optional A dictionary of local variables, taken from locals() by default. global_dict : dict or None, optional A dictionary of global variables, taken from globals() by default. resolvers : list of dict-like or None, optional A list of objects implementing the ``__getitem__`` special method that you can use to inject an additional collection of namespaces to use for variable lookup. For example, this is used in the :meth:`~DataFrame.query` method to inject the ``DataFrame.index`` and ``DataFrame.columns`` variables that refer to their respective :class:`~pandas.DataFrame` instance attributes. level : int, optional The number of prior stack frames to traverse and add to the current scope. Most users will **not** need to change this parameter. target : object, optional, default None This is the target object for assignment. It is used when there is variable assignment in the expression. If so, then `target` must support item assignment with string keys, and if a copy is being returned, it must also support `.copy()`. inplace : bool, default False If `target` is provided, and the expression mutates `target`, whether to modify `target` inplace. Otherwise, return a copy of `target` with the mutation.\n##### Returns\n"", 'stemmed comments': ['global', 'target', 'It', 'bitwis', 'variabl', 'parser', 'avail', 'dictlik', 'ol', 'special', 'behav', 'local', 'dataframecolumn', '%', 'see', 'most', 'stack', 'class', 'modifi', 'string', 'd', 'semant', 'str', 'eval', '<', '100', 'A', 'expr', 'panda', 'key', 'keyword', 'altern', 'instanc', 'mutat', 'inject', 'slightli', '>', 'retain', 'fall', 'list', 'bool', 'namespac', 'thi', 'top', 'tree', 'lookup', 'taken', 'use', 'python', 'perform', 'number', 'option', 'detail', 'back', 'method', 'enhancingperfev', '//docspythonorg/3/reference/simple_stmtshtml', 'complex', 'larg', 'attribut', 'current', 'boolean', 'futur', 'int', 'express', 'whether', 'arithmet', 'return', 'numexpr', 'may', 'standard', 'meth', 'must', 'dataframeindex', 'If', 'differ', 'respect', 'correspond', 'object', 'collect', 'otherwis', 'resolv', 'speed', 'also', 'allow', '|', 'evalu', 'gener', 'copi', 'global_dict', '~pandasseri', 'prior', 'dictionari', 'statement', 'ref', 'need', 'fals', 'divis', 'would', 'http', 'frame', 'local_dict', 'default', 'truediv', 'provid', '//', 'implement', '/', 'contain', 'code', 'scope', 'true', 'more', 'oper', 'add', 'plain', 'deprec', 'enhanc', '{', 'strict', 'syntax', 'dict', 'item', 'refer', 'paramet', 'exampl', '=', '~dataframequeri', 'variou', 'tri', 'none', '__getitem__', 'like', 'user', 'level', '__', 'follow', 'pars', '3', 'up', 'travers', 'the', 'along', 'chang', 'expressionstat', '~pandasdatafram', '~', 'assign', 'engin', 'construct', 'for', 'support', 'addit', 'inplac', 'backend', 'simplestat', '}', '&', 'document']}"
95,"{'func name': 'add_ops', 'comments': 'Decorator to add default implementation of ops.\n\n\n', 'stemmed comments': ['default', 'add', 'implement', 'decor', 'op']}"
96,"{'func name': 'get_test_result', 'comments': 'get test result and reset test_results\n\n\n', 'stemmed comments': ['test', 'result', 'reset', 'test_result', 'get']}"
97,"{'func name': '_maybe_unwrap_index', 'comments': 'If operating against another Index object, we need to unwrap the underlying data before deferring to the DatetimeArray/TimedeltaArray/PeriodArray implementation, otherwise we will incorrectly return NotImplemented.\n\nParameters ---------- obj : object\n##### Returns\n', 'stemmed comments': ['index', 'notimpl', 'paramet', 'otherwis', 'object', 'implement', 'anoth', 'If', 'underli', 'obj', 'datetimearray/timedeltaarray/periodarray', 'defer', 'need', 'return', 'data', 'unwrap', 'incorrectli', 'oper']}"
98,"{'func name': 'read_feather', 'comments': 'Load a feather-format object from the file path.\n\nParameters ---------- path : str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: ``file://localhost/path/to/table.feather``.\n\nIf you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\nBy file-like object, we refer to objects with a ``read()`` method, such as a file handler (e.g. via builtin ``open`` function) or ``StringIO``. columns : sequence, default None If not provided, all columns are read.\n\n.. versionadded:: 0.24.0 use_threads : bool, default True Whether to parallelize reading using multiple threads.\n\n.. versionadded:: 0.24.0\n##### Returns\n', 'stemmed comments': ['ftp', 'paramet', 'featherformat', 'refer', 'thread', 'use', 'object', 'ospathlik', 'By', 'versionad', 'valid', 'none', 'includ', 'scheme', 'local', 'parallel', 'load', 'host', 'sequenc', '//localhost/path/to/tablefeath', 'method', 'ani', 'the', 'column', 'stringio', 'path', 'string', 'open', 'eg', 'could', 'via', 'http', 'filelik', 'want', 'default', 'use_thread', 's3', 'provid', 'str', 'A', 'multipl', 'whether', 'expect', 'panda', 'builtin', 'return', 'true', 'url', 'for', 'function', 'file', 'If', 'pass', 'bool', '0240', 'handler', 'accept', 'read']}"
99,"{'func name': 'buffer_put_lines', 'comments': 'Appends lines to a buffer.\n\nParameters ---------- buf The buffer to write to lines The lines to append.\n', 'stemmed comments': ['paramet', 'buf', 'the', 'line', 'append', 'write', 'buffer']}"
100,"{'func name': '_put_str', 'comments': '', 'stemmed comments': []}"
101,"{'func name': '_maybe_add_count', 'comments': '', 'stemmed comments': []}"
102,"{'func name': 'validate_minmax_axis', 'comments': 'Ensure that the axis argument passed to min, max, argmin, or argmax is zero or None, as otherwise it will be incorrectly ignored.\n\nParameters ---------- axis : int or None\n', 'stemmed comments': ['ensur', 'argmax', 'zero', 'argmin', 'otherwis', 'paramet', 'int', 'axi', 'max', 'pass', 'none', 'ignor', 'min', 'argument', 'incorrectli']}"
103,"{'func name': 'to_gbq', 'comments': '', 'stemmed comments': []}"
104,"{'func name': 'get_filepath_or_buffer', 'comments': '', 'stemmed comments': []}"
105,"{'func name': 'write_legacy_file', 'comments': '', 'stemmed comments': []}"
106,"{'func name': '_recast_datetimelike_result', 'comments': 'If we have date/time like in the original, then coerce dates as we are stacking can easily have object dtypes here.\n\nParameters ---------- result : DataFrame\n##### Returns\n', 'stemmed comments': ['dtype', 'date/tim', 'easili', 'paramet', 'date', 'If', 'origin', 'result', 'coerc', 'return', 'like', 'datafram', 'object', 'stack']}"
107,"{'func name': 'create_pandas_abc_type', 'comments': '', 'stemmed comments': []}"
108,"{'func name': '_make_logical_function', 'comments': '', 'stemmed comments': []}"
109,"{'func name': 'get_groupby', 'comments': '', 'stemmed comments': []}"
110,"{'func name': '_convert_grouper', 'comments': '', 'stemmed comments': []}"
111,"{'func name': '_hash_scalar', 'comments': 'Hash scalar value.\n\nParameters ---------- val : scalar encoding : str, default ""utf8"" hash_key : str, default _default_hash_key\n##### Returns\n', 'stemmed comments': ['default', 'paramet', '_default_hash_key', 'str', 'encod', 'hash', 'hash_key', 'return', 'valu', 'utf8', 'scalar', 'val']}"
112,"{'func name': 'hist_frame', 'comments': '', 'stemmed comments': []}"
113,"{'func name': 'HolidayCalendarFactory', 'comments': '', 'stemmed comments': []}"
114,"{'func name': 'read_html', 'comments': 'Read HTML tables into a ``list`` of ``DataFrame`` objects.\n\nParameters ---------- io : str, path object or file-like object A URL, a file-like object, or a raw string containing HTML. Note that lxml only accepts the http, ftp and file url protocols. If you have a URL that starts with ``\'https\'`` you might try removing the ``\'s\'``.\n\nmatch : str or compiled regular expression, optional The set of tables containing text matching this regex or string will be returned. Unless the HTML is extremely simple you will probably need to pass a non-empty string here. Defaults to \'.+\' (match any non-empty string). The default value will return all tables contained on a page. This value is converted to a regular expression so that there is consistent behavior between Beautiful Soup and lxml.\n\nflavor : str or None The parsing engine to use. \'bs4\' and \'html5lib\' are synonymous with each other, they are both there for backwards compatibility. The default of ``None`` tries to use ``lxml`` to parse and if that fails it falls back on ``bs4`` + ``html5lib``.\n\nheader : int or list-like or None, optional The row (or list of rows for a :class:`~pandas.MultiIndex`) to use to make the columns headers.\n\nindex_col : int or list-like or None, optional The column (or list of columns) to use to create the index.\n\nskiprows : int or list-like or slice or None, optional Number of rows to skip after parsing the column integer. 0-based. If a sequence of integers or a slice is given, will skip the rows indexed by that sequence.\n\nNote that a single element sequence means \'skip the nth row\' whereas an integer means \'skip n rows\'.\n\nattrs : dict or None, optional This is a dictionary of attributes that you can pass to use to identify the table in the HTML. These are not checked for validity before being passed to lxml or Beautiful Soup. However, these attributes must be valid HTML table attributes to work correctly. For example, ::\n\nattrs = {\'id\': \'table\'}\n\nis a valid attribute dictionary because the \'id\' HTML tag attribute is a valid HTML attribute for *any* HTML tag as per `this document <http://www.w3.org/TR/html-markup/global-attributes.html>`__. ::\n\nattrs = {\'asdf\': \'table\'}\n\nis *not* a valid attribute dictionary because \'asdf\' is not a valid HTML attribute even if it is a valid XML attribute.\n\nValid HTML 4.01 table attributes can be found `here <http://www.w3.org/TR/REC-html40/struct/tables.html#h-11.2>`__. A working draft of the HTML 5 spec can be found `here <http://www.w3.org/TR/html-markup/table.html>`__. It contains the latest information on table attributes for the modern web.\n\nparse_dates : bool, optional See :func:`~read_csv` for more details.\n\nthousands : str, optional Separator to use to parse thousands. Defaults to ``\',\'``.\n\nencoding : str or None, optional The encoding used to decode the web page. Defaults to ``None``.``None`` preserves the previous encoding behavior, which depends on the underlying parser library (e.g., the parser library will try to use the encoding provided by the document).\n\ndecimal : str, default \'.\' Character to recognize as decimal point (e.g. use \',\' for European data).\n\nconverters : dict, default None Dict of functions for converting values in certain columns. Keys can either be integers or column labels, values are functions that take one input argument, the cell (not column) content, and return the transformed content.\n\nna_values : iterable, default None Custom NA values.\n\nkeep_default_na : bool, default True If na_values are specified and keep_default_na is False the default NaN values are overridden, otherwise they\'re appended to.\n\ndisplayed_only : bool, default True Whether elements with ""display: none"" should be parsed.\n##### Returns\n* **Before using this function you should read the **: ref\n\n* **the body (by putting rows with only ``<th>`` elements into the header).\n    .. versionadded**: \n\n* **Similar to **: func\n\n* **This function will *always* return a list of **: class\n\n* **See the **: ref\n\n', 'stemmed comments': ['ftp', 'set', 'It', 'text', 'might', 'parser', 'web', 'skip', 'content', 'see', 'n', 'class', 'cell', 'identifi', 'alway', 'xml', '//wwww3org/tr/htmlmarkup/tablehtml', 'howev', 'string', 'nonempti', 'encod', 'thousand', 'na_valu', 'valu', 'label', 'datafram', 'make', 'index_col', 'header', 'singl', 'element', 'per', 'str', 'A', '<', 'key', 'keep_default_na', 'regular', 'data', '//wwww3org/tr/htmlmarkup/globalattributeshtml', 'asdf', 'similar', '>', 'fall', 'list', 'charact', 'append', 'bool', 'thi', 'input', 'specifi', 'read', '401', 'protocol', 'found', '//wwww3org/tr/rechtml40/struct/tableshtml', 'display', 'use', 'number', 'option', 'convert', 'detail', 'soup', 'back', 'decod', 'take', 'attribut', 'slice', 'flavor', 'put', 'probabl', 'mean', 'backward', 'int', 'express', 'whether', 'lxml', 'return', 'iter', 'given', 'listlik', 'separ', 'url', 'argument', 'must', 'bs4', 'index', 'function', 'wherea', 'NA', 'file', 'If', 'th', 'object', 'accept', 'latest', 'correctli', '0base', 'work', 'unless', 'otherwis', 'spec', 'compat', 'underli', 'attr', 'decim', 'befor', 'tag', 'h112', 'sequenc', 'page', 'func', 'nan', 'column', 'dictionari', 'these', 'path', 'ref', 'need', 'eg', 'raw', 'row', 'recogn', 'fals', 'modern', 'filelik', 'http', 'compil', 'default', 'provid', 'either', 'contain', 'preserv', 'true', 'synonym', 'creat', '{', 'pass', 'check', 'even', 'transform', '~read_csv', 'dict', 'certain', 'paramet', 'exampl', '=', '~pandasmultiindex', 'extrem', 'one', 'tri', 'displayed_onli', 'versionad', 'valid', 'none', '__', 'html', 'html5lib', 'pars', 'point', 'beauti', 'depend', 'the', 'overridden', 'nth', 'draft', 'id', 'start', '5', 'skiprow', 'librari', 'simpl', 'european', 'io', 'behavior', 'note', 'previou', 're', 'consist', 'inform', 'engin', 'integ', 'bodi', 'fail', 'for', 'regex', 'parse_d', 'tabl', 's', 'custom', 'remov', '}', 'match', 'document']}"
115,"{'func name': 'check_array_indexer', 'comments': ""Check if `indexer` is a valid array indexer for `array`.\n\nFor a boolean mask, `array` and `indexer` are checked to have the same length. The dtype is validated, and if it is an integer or boolean ExtensionArray, it is checked if there are missing values present, and it is converted to the appropriate numpy array. Other dtypes will raise an error.\n\nNon-array indexers (integer, slice, Ellipsis, tuples, ..) are passed through as is.\n\n.. versionadded:: 1.0.0\n\nParameters ---------- array : array-like The array that is being indexed (only used for the length). indexer : array-like or list-like The array-like that's used to index. List-like input that is not yet a numpy array or an ExtensionArray is converted to one. Other input types are passed through as is\n##### Returns\n"", 'stemmed comments': ['paramet', 'tupl', 'use', 'one', 'versionad', 'convert', 'valid', 'mask', 'type', 'error', 'length', 'miss', 'the', 'numpi', 'nonarray', 'array', 'present', 'slice', 'valu', 'boolean', '100', 'yet', 'return', 'integ', 'arraylik', 'other', 'listlik', 'ellipsi', 'dtype', 'index', 'for', 's', 'pass', 'extensionarray', 'check', 'rais', 'input', 'appropri']}"
116,"{'func name': '_can_do_equal_len', 'comments': 'Returns ------- bool True if we have an equal len settable.\n\n\n', 'stemmed comments': ['settabl', 'len', 'bool', 'equal', 'return', 'true']}"
117,"{'func name': 'is_sequence', 'comments': 'Check if the object is a sequence of objects. String types are not included as sequences here.\n\nParameters ---------- obj : The object to check\n##### Returns\n* **is_sequence **: bool\n    Whether `obj` is a sequence of objects.\n\n', 'stemmed comments': ['sequenc', 'paramet', 'is_sequ', 'the', 'whether', 'string', 'obj', 'bool', 'check', 'return', 'type', 'includ', 'object']}"
118,"{'func name': 'coerce_to_array', 'comments': 'Coerce the input values array to numpy arrays with a mask\n\nParameters ---------- values : 1D list-like dtype : integer dtype mask : bool 1D array, optional copy : bool, default False if True, copy the input\n##### Returns\n', 'stemmed comments': ['dtype', 'default', 'paramet', '1D', 'numpi', 'array', 'bool', 'option', 'coerc', 'mask', 'integ', 'valu', 'fals', 'input', 'return', 'true', 'listlik', 'copi']}"
119,"{'func name': 'interval_range', 'comments': ""Return a fixed frequency IntervalIndex.\n\nParameters ---------- start : numeric or datetime-like, default None Left bound for generating intervals. end : numeric or datetime-like, default None Right bound for generating intervals. periods : int, default None Number of periods to generate. freq : numeric, str, or DateOffset, default None The length of each interval. Must be consistent with the type of start and end, e.g. 2 for numeric, or '5H' for datetime-like.\n\nDefault is 1 for numeric and 'D' for datetime-like. name : str, default None Name of the resulting IntervalIndex. closed : {'left', 'right', 'both', 'neither'}, default 'right' Whether the intervals are closed on the left-side, right-side, both or neither.\n##### Returns\n* **IntervalIndex **: An Index of intervals that are all closed on the same side.\n\n* **<https**: //pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\n"", 'stemmed comments': ['fix', 'paramet', 'end', 'side', 'rightsid', 'name', 'left', 'close', 'bound', 'number', '5H', 'both', 'intervalindex', 'none', 'type', '//pandaspydataorg/pandasdocs/stable/user_guide/timeserieshtml', '__', 'neither', 'gener', 'length', 'leftsid', 'the', '1', 'offsetalias', 'datetimelik', 'An', 'dateoffset', 'eg', 'interv', 'http', 'start', 'default', 'int', 'str', '<', 'whether', 'consist', 'return', 'D', 'freq', '2', 'must', 'frequenc', 'index', '>', '{', 'result', 'right', 'numer', 'period', '}']}"
120,"{'func name': 'maybe_convert_platform_interval', 'comments': 'Try to do platform conversion, with special casing for IntervalArray. Wrapper around maybe_convert_platform that alters the default return dtype in certain cases to be compatible with IntervalArray.  For example, empty lists return with integer dtype instead of object dtype, which is prohibited for IntervalArray.\n\nParameters ---------- values : array-like\n##### Returns\n', 'stemmed comments': ['paramet', 'exampl', 'tri', 'compat', 'empti', 'special', 'platform', 'instead', 'valu', 'default', 'alter', 'wrapper', 'around', 'return', 'integ', 'arraylik', 'case', 'dtype', 'for', 'list', 'intervalarray', 'prohibit', 'maybe_convert_platform', 'object', 'certain', 'convers']}"
121,"{'func name': 'make_invalid_op', 'comments': 'Return a binary method that always raises a TypeError.\n\nParameters ---------- name : str\n##### Returns\n* **invalid_op **: function\n\n', 'stemmed comments': ['method', 'paramet', 'alway', 'function', 'binari', 'name', 'str', 'return', 'rais', 'typeerror', 'invalid_op']}"
122,"{'func name': 'get_locales', 'comments': 'Get all the locales that are available on the system.\n\nParameters ---------- prefix : str If not ``None`` then return only those locales with the prefix provided. For example to get all English language locales (those that start with ``""en""``), pass ``prefix=""en""``. normalize : bool Call ``locale.normalize`` on the resulting list of available locales. If ``True``, only locales that can be set without throwing an ``Exception`` are returned. locale_getter : callable The function to use to retrieve the current locales. This should return a string with each locale separated by a newline character.\n##### Returns\n* **locales **: list of strings\n    A list of locale strings that can be set with ``locale.setlocale()``.\n    For example\n\n', 'stemmed comments': ['set', 'paramet', 'exampl', 'use', 'localenorm', 'avail', 'none', 'en', 'local', 'locale_gett', 'the', 'except', 'string', 'start', 'system', 'current', 'callabl', 'without', 'newlin', 'provid', 'str', 'A', 'return', 'languag', 'true', 'retriev', 'separ', 'call', 'localesetlocal', 'prefix=', 'throw', 'for', 'prefix', 'function', 'normal', 'list', 'charact', 'If', 'pass', 'english', 'bool', 'result', 'thi', 'get']}"
123,"{'func name': 'concatenate_block_managers', 'comments': 'Concatenate block managers into one.\n\nParameters ---------- mgrs_indexers : list of (BlockManager, {axis: indexer,...}) tuples axes : list of Index concat_axis : int copy : bool\n', 'stemmed comments': ['index', 'paramet', 'blockmanag', 'tupl', 'concat_axi', 'int', 'one', 'list', '{', 'concaten', 'axi', 'bool', 'block', 'axe', '}', 'manag', 'copi', 'mgrs_index']}"
124,"{'func name': 'raise_for_nan', 'comments': '', 'stemmed comments': []}"
125,"{'func name': 'wide_to_long', 'comments': 'Wide panel to long format. Less flexible but more user-friendly than melt.\n\nWith stubnames [\'A\', \'B\'], this function expects to find one or more group of columns with format A-suffix1, A-suffix2,..., B-suffix1, B-suffix2,... You specify what you want to call this suffix in the resulting long format with `j` (for example `j=\'year\'`)\n\nEach row of these wide variables are assumed to be uniquely identified by `i` (can be a single column name or a list of column names)\n\nAll remaining variables in the data frame are left intact.\n\nParameters ---------- df : DataFrame The wide-format DataFrame. stubnames : str or list-like The stub name(s). The wide format variables are assumed to start with the stub names. i : str or list-like Column(s) to use as id variable(s). j : str The name of the sub-observation variable. What you wish to name your suffix in the long format. sep : str, default """" A character indicating the separation of the variable names in the wide format, to be stripped from the names in the long format. For example, if your column names are A-suffix1, A-suffix2, you can strip the hyphen by specifying `sep=\'-\'`. suffix : str, default \'\\\\d+\' A regular expression capturing the wanted suffixes. \'\\\\d+\' captures numeric suffixes. Suffixes with no numbers could be specified with the negated character class \'\\\\D+\'. You can also further disambiguate suffixes, for example, if your wide variables are of the form A-one, B-two,.., and you have an unrelated column A-rating, you can ignore the last one by specifying `suffix=\'(!?one|two)\'`.\n\n.. versionchanged:: 0.23.0 When all suffixes are numeric, they are cast to int64/float64.\n##### Returns\n* **>>> df = pd.DataFrame({""A1970"" **: {0\n\n* **...                    ""A1980"" **: {0\n\n* **...                    ""B1970"" **: {0\n\n* **...                    ""B1980"" **: {0\n\n* **...                    ""X""     **: dict(zip(range(3), np.random.randn(3)))\n\n* **... # doctest**: +NORMALIZE_WHITESPACE\n                  ht\n\n* **...     \'famid\'**: [1, 1, 1, 2, 2, 2, 3, 3, 3],\n\n* **...     \'birth\'**: [1, 2, 3, 1, 2, 3, 1, 2, 3],\n\n* **...     \'ht1\'**: [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],\n\n* **...     \'ht2\'**: [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]\n\n* **>>> df = pd.DataFrame({\'A(weekly)-2010\'**: np.random.rand(3),\n\n* **...                    \'A(weekly)-2011\'**: np.random.rand(3),\n\n* **...                    \'B(weekly)-2010\'**: np.random.rand(3),\n\n* **...                    \'B(weekly)-2011\'**: np.random.rand(3),\n\n* **...                    \'X\' **: np.random.randint(3, size=3)})\n\n* **>>> df # doctest**: +NORMALIZE_WHITESPACE, +ELLIPSIS\n   A(weekly)-2010  A(weekly)-2011  B(weekly)-2010  B(weekly)-2011  X  id\n\n* **...     \'ht_one\'**: [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],\n\n* **...     \'ht_two\'**: [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]\n\n', 'stemmed comments': ['B', 'variabl', '24', 'panel', '19', 'bsuffix1', 'class', 'nprandomrand', 'identifi', 'normalize_whitespac', 'sep=', 'indic', 'want', 'datafram', 'versionchang', 'rang', 'you', 'singl', '38', 'less', 'A', 'asuffix1', 'str', 'hyphen', '32', 'X', 'melt', 'data', 'regular', 'group', 'ht_one', 'stub', '>', 'list', 'charact', 'last', 'result', 'int64/float64', 'with', 'specifi', 'all', 'famid', 'use', '34', 'left', '22', 'number', 'ht', 'nprandomrandn', 'ht2', 'doctest', 'size=3', 'pddatafram', '33', 'wish', 'df', '2010', 'asuffix2', 'express', '28', 'expect', 'return', ']', 'wideformat', 'separ', 'listlik', 'ellipsi', 'intact', 'function', 'one|two', 'a1980', 'strip', 'name', 'sep', 'also', '21', 'uniqu', 'remain', 'find', 'ht_two', 'subobserv', 'assum', '1', 'column', 'row', 'suffix', 'could', 'ignor', 'frame', 'negat', 'form', 'default', '0', 'bsuffix2', 'userfriendli', 'aon', 'disambigu', 'b1970', 'zip', '{', 'j=year', 'numer', '18', 'wide', '0230', '?', 'dict', 'paramet', 'exampl', '=', 'nprandomrandint', 'one', 'stubnam', '23', 'suffix=', 'flexibl', 'unrel', 'ht1', '!', '3', 'the', 'a1970', 'each', 'id', '29', 'weekli', 'start', 'j', 'cast', '\\\\d', 'btwo', 'captur', '2011', 'birth', 'b1980', 'call', '2', 'arat', 'what', 'for', 'long', '[', 'when', '}', 'format']}"
126,"{'func name': '_items_overlap_with_suffix', 'comments': 'If two indices overlap, add suffixes to overlapping entries.\n\nIf corresponding suffix is empty, the entry is simply converted to string.\n', 'stemmed comments': ['add', 'simpli', 'two', 'If', 'string', 'convert', 'entri', 'empti', 'overlap', 'suffix', 'indic', 'correspond']}"
127,"{'func name': '_add_methods', 'comments': '', 'stemmed comments': []}"
128,"{'func name': 'autocorrelation_plot', 'comments': '', 'stemmed comments': []}"
129,"{'func name': 'is_valid_nat_for_dtype', 'comments': 'isna check that excludes incompatible dtypes\n\nParameters ---------- obj : object dtype : np.datetime64, np.timedelta64, DatetimeTZDtype, or PeriodDtype\n##### Returns\n', 'stemmed comments': ['dtype', 'paramet', 'datetimetzdtyp', 'obj', 'check', 'npdatetime64', 'return', 'nptimedelta64', 'exclud', 'incompat', 'isna', 'perioddtyp', 'object']}"
130,"{'func name': 'dispatch_fill_zeros', 'comments': 'Call fill_zeros with the appropriate fill value depending on the operation, with special logic for divmod and rdivmod.\n\nParameters ---------- op : function (operator.add, operator.div, ...) left : object (np.ndarray for non-reversed ops) right : object (np.ndarray for reversed ops) result : ndarray\n##### Returns\n* **result **: np.ndarray\n\n', 'stemmed comments': ['paramet', 'operatordiv', 'ndarray', 'divmod', 'left', 'revers', 'special', 'depend', 'valu', 'op', 'fill', 'rdivmod', 'operatoradd', 'npndarray', 'fill_zero', 'return', 'nonrevers', 'call', 'oper', 'function', 'logic', 'result', 'right', 'appropri', 'object']}"
131,"{'func name': '_rolling_window', 'comments': '[True, True, False, True, False], 2 ->\n\n[ [True,\n\nTrue], [True, False], [False, True], [True, False], ]\n', 'stemmed comments': ['>', '[', ']', 'fals', 'true', '2']}"
132,"{'func name': '_coerce_indexer_frozen', 'comments': 'Coerce the array_like indexer to the smallest integer dtype that can encode all of the given categories.\n\nParameters ---------- array_like : array-like categories : array-like copy : bool\n##### Returns\n', 'stemmed comments': ['dtype', 'index', 'smallest', 'paramet', 'array_lik', 'categori', 'encod', 'bool', 'coerc', 'integ', 'arraylik', 'return', 'given', 'copi']}"
133,"{'func name': 'nanpercentile', 'comments': 'Wrapper for np.percentile that skips missing values.\n\nParameters ---------- values : array over which to find quantiles q : scalar or array of quantile indices to find axis : {0, 1} na_value : scalar value to return for empty or all-null values mask : ndarray[bool] locations in values that should be considered missing ndim : {1, 2} interpolation : str\n##### Returns\n* **quantiles **: scalar or array\n\n', 'stemmed comments': ['paramet', 'ndarray', 'skip', 'empti', 'mask', 'ndim', 'scalar', 'miss', 'find', 'locat', '1', 'array', 'na_valu', 'valu', 'indic', 'interpol', '0', 'wrapper', 'nppercentil', 'str', 'allnul', 'return', 'quantil', ']', '2', 'consid', '[', '{', 'axi', 'bool', 'q', '}']}"
134,"{'func name': 'generate_numba_apply_func', 'comments': ""Generate a numba jitted apply function specified by values from engine_kwargs.\n\n1. jit the user's function 2. Return a rolling apply function with the jitted function inline\n\nConfigurations specified in engine_kwargs apply to both the user's function _AND_ the rolling apply function.\n\nParameters ---------- args : tuple *args to be passed into the function kwargs : dict **kwargs to be passed into the function func : function function to be applied to each window and will be JITed engine_kwargs : dict dictionary of arguments to be passed into numba.jit\n##### Returns\n"", 'stemmed comments': ['paramet', 'engine_kwarg', 'tupl', 'configur', 'numba', 'user', 'numbajit', 'gener', 'func', '1', 'dictionari', 'jit', 'valu', 'appli', 'window', 'kwarg', 'return', 'roll', 'argument', '2', 'function', 'jite', 'arg', '_and_', 's', 'pass', 'inlin', 'specifi', 'dict']}"
135,"{'func name': 'to_numeric', 'comments': ""Convert argument to a numeric type.\n\nThe default return dtype is `float64` or `int64` depending on the data supplied. Use the `downcast` parameter to obtain other dtypes.\n\nPlease note that precision loss may occur if really large numbers are passed in. Due to the internal limitations of `ndarray`, if numbers smaller than `-9223372036854775808` (np.iinfo(np.int64).min) or larger than `18446744073709551615` (np.iinfo(np.uint64).max) are passed in, it is very likely they will be converted to float so that they can stored in an `ndarray`. These warnings apply similarly to `Series` since it internally leverages `ndarray`.\n\nParameters ---------- arg : scalar, list, tuple, 1-d array, or Series errors : {'ignore', 'raise', 'coerce'}, default 'raise'\n\n- If 'raise', then invalid parsing will raise an exception.\n\n- If 'coerce', then invalid parsing will be set as NaN.\n\n- If 'ignore', then invalid parsing will return the input. downcast : {'integer', 'signed', 'unsigned', 'float'}, default None If not None, and if the data has been successfully cast to a numerical dtype (or if the data was numeric to begin with), downcast that resulting data to the smallest numerical dtype possible according to the following rules:\n\n- 'integer' or 'signed': smallest signed int dtype (min.: np.int8)\n\n- 'unsigned': smallest unsigned int dtype (min.: np.uint8)\n\n- 'float': smallest float dtype (min.: np.float32)\n\nAs this behaviour is separate from the core conversion to numeric values, any errors raised during the downcasting will be surfaced regardless of the value of the 'errors' input.\n\nIn addition, downcasting will only occur if the size of the resulting data's dtype is strictly larger than the dtype it is to be cast to, so if none of the dtypes checked satisfy that specification, no downcasting will be performed on the data.\n##### Returns\n* **ret **: numeric if parsing succeeded.\n    Return type depends on input.  Series if Series, otherwise ndarray.\n\n* **DataFrame.astype **: Cast argument to a specified dtype.\n\n* **to_datetime **: Convert argument to datetime.\n\n* **to_timedelta **: Convert argument to timedelta.\n\n* **numpy.ndarray.astype **: Cast a numpy array to a specified type.\n\n* **convert_dtypes **: Convert dtypes.\n\n* **dtype**: float64\n\n"", 'stemmed comments': ['set', 'tupl', 'obtain', 'begin', 'sign', 'surfac', 'strictli', 'max', 'invalid', 'behaviour', 'suppli', 'similarli', 'valu', 'appli', 'npint8', 'warn', 'occur', 'coerc', 'ret', 'data', 'dtype', 'datetim', 'realli', 'intern', 'float64', 'list', 'result', 'In', 'dataframeastyp', 'rais', 'input', 'specifi', 'sinc', 'convers', 'ndarray', 'use', 'pleas', 'specif', 'npiinfo', 'perform', 'number', 'convert', 'type', 'due', 'to_timedelta', 'store', 'npuint64', 'accord', '1d', 'larg', 'leverag', 'seri', 'int', 'return', 'may', 'regardless', 'separ', 'argument', 'smallest', 'If', '9223372036854775808', 'precis', 'As', 'otherwis', 'unsign', 'rule', 'nan', 'these', 'numpyndarrayastyp', 'ignor', 'default', 'loss', 'min', 'downcast', 'arg', '{', 'pass', 'check', 'numer', 'paramet', 'success', '18446744073709551615', 'convert_dtyp', 'none', 'core', 'like', 'scalar', 'error', 'pars', 'float', 'follow', 'npuint8', 'depend', 'larger', 'the', 'npfloat32', 'numpi', 'except', 'array', 'timedelta', 'satisfi', 'cast', 'note', 'smaller', 'succeed', 'integ', 'to_datetim', 'limit', 'possibl', 'npint64', 'addit', 's', 'size', 'int64', '}']}"
136,"{'func name': 'generate_range', 'comments': 'Generates a sequence of dates corresponding to the specified time offset. Similar to dateutil.rrule except uses pandas DateOffset objects to represent time increments.\n\nParameters ---------- start : datetime, (default None) end : datetime, (default None) periods : int, (default None) offset : DateOffset, (default BDay())\n\nNotes ----- * This method is faster for generating weekdays than dateutil.rrule * At least two of (start, end, periods) must be specified. * If both start and end are specified, the returned dates will satisfy start <= date <= end.\n##### Returns\n* **dates **: generator object\n\n', 'stemmed comments': ['paramet', 'end', 'offset', '=', 'use', 'time', 'none', 'gener', 'bday', 'sequenc', 'method', 'except', 'dateutilrrul', 'dateoffset', 'satisfi', 'repres', 'faster', 'start', 'default', 'int', '<', 'note', 'weekday', 'panda', 'return', 'must', 'similar', 'datetim', 'date', 'two', 'If', 'increment', 'thi', 'period', 'specifi', 'At', 'correspond', 'object', 'least']}"
137,"{'func name': 'get_splitter', 'comments': '', 'stemmed comments': []}"
138,"{'func name': 'isnumeric', 'comments': '', 'stemmed comments': []}"
139,"{'func name': 'read_orc', 'comments': 'Load an ORC object from the file path, returning a DataFrame.\n\n.. versionadded:: 1.0.0\n\nParameters ---------- path : str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: ``file://localhost/path/to/table.orc``.\n\nIf you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\nBy file-like object, we refer to objects with a ``read()`` method, such as a file handler (e.g. via builtin ``open`` function) or ``StringIO``. columns : list, default None If not None, only these columns will be read from the file. **kwargs Any additional kwargs are passed to pyarrow.\n##### Returns\n', 'stemmed comments': ['ftp', 'paramet', 'refer', 'object', 'ospathlik', 'By', 'versionad', 'valid', 'none', 'includ', 'scheme', 'local', 'load', 'host', 'method', 'ani', 'the', 'column', 'stringio', 'path', 'string', 'open', 'eg', 'pyarrow', 'could', 'via', 'orc', 'datafram', 'filelik', 'http', 'want', 'default', 's3', 'kwarg', '100', 'str', 'A', 'expect', 'return', 'panda', 'builtin', 'url', 'for', '//localhost/path/to/tableorc', 'function', 'file', 'list', 'addit', 'If', 'pass', 'handler', 'accept', 'read']}"
140,"{'func name': 'read_parquet', 'comments': ""Load a parquet object from the file path, returning a DataFrame.\n\n.. versionadded:: 0.21.0\n\nParameters ---------- path : str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: ``file://localhost/path/to/table.parquet``. A file URL can also be a path to a directory that contains multiple partitioned parquet files. Both pyarrow and fastparquet support paths to directories as well as file URLs. A directory path could be: ``file://localhost/path/to/tables``\n\nIf you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\nBy file-like object, we refer to objects with a ``read()`` method, such as a file handler (e.g. via builtin ``open`` function) or ``StringIO``. engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto' Parquet library to use. If 'auto', then the option ``io.parquet.engine`` is used. The default ``io.parquet.engine`` behavior is to try 'pyarrow', falling back to 'fastparquet' if 'pyarrow' is unavailable. columns : list, default=None If not None, only these columns will be read from the file.\n\n.. versionadded:: 0.21.1 **kwargs Any additional kwargs are passed to the engine.\n##### Returns\n"", 'stemmed comments': ['ftp', 'local', 'ani', 'string', 'pyarrow', 'via', 'datafram', 'want', 'auto', 'str', 'A', 'panda', '0210', 'fall', 'list', 'read', 'use', 'ospathlik', 'option', 'fastparquet', '//localhost/path/to/t', 'partit', 'load', 'back', '0211', 'method', 'well', 'expect', 'return', 'parquet', 'url', 'function', 'default=non', 'ioparquetengin', 'file', 'If', 'handler', 'unavail', 'directori', 'object', 'accept', 'also', 'includ', 'column', 'path', 'eg', 'could', '//localhost/path/to/tableparquet', 'filelik', 'http', 'default', 'kwarg', 'contain', 'builtin', '{', 'pass', 'refer', 'paramet', 'By', 'tri', 'both', 'versionad', 'valid', 'none', 'scheme', 'host', 'the', 'stringio', 'open', 'librari', 's3', 'behavior', 'multipl', 'engin', 'for', 'support', 'addit', '}']}"
141,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
142,"{'func name': '_get_col_names', 'comments': '', 'stemmed comments': []}"
143,"{'func name': 'tokenize_string', 'comments': 'Tokenize a Python source code string.\n\nParameters ---------- source : str The Python source code string.\n##### Returns\n* **tok_generator **: Iterator[Tuple[int, str]]\n    An iterator yielding all tokens with only toknum and tokval (Tuple[ing, str]).\n\n', 'stemmed comments': ['paramet', 'tupl', 'token', 'python', 'yield', 'sourc', 'the', 'string', 'An', 'tok_gener', 'int', 'toknum', 'str', 'return', ']', 'code', 'iter', 'ing', '[', 'tokval']}"
144,"{'func name': 'period_range', 'comments': 'Return a fixed frequency PeriodIndex.\n\nThe day (calendar) is the default frequency.\n\nParameters ---------- start : str or period-like, default None Left bound for generating periods. end : str or period-like, default None Right bound for generating periods. periods : int, default None Number of periods to generate. freq : str or DateOffset, optional Frequency alias. By default the freq is taken from `start` or `end` if those are Period objects. Otherwise, the default is ``""D""`` for daily frequency. name : str, default None Name of the resulting PeriodIndex.\n##### Returns\n* **Of the three parameters**: ``start``, ``end``, and ``periods``, exactly two\n\n* **<https**: //pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\n', 'stemmed comments': ['fix', 'paramet', 'end', 'taken', 'otherwis', 'name', 'By', 'left', 'bound', 'number', 'option', 'Of', 'three', 'none', '//pandaspydataorg/pandasdocs/stable/user_guide/timeserieshtml', '__', 'gener', 'the', 'offsetalias', 'daili', 'dateoffset', 'day', 'exactli', 'start', 'http', 'default', 'int', 'str', '<', 'return', 'D', 'freq', 'calendar', 'frequenc', 'periodlik', '>', 'two', 'result', 'right', 'periodindex', 'period', 'alia', 'object']}"
145,"{'func name': '_make_field_arrays', 'comments': '', 'stemmed comments': []}"
146,"{'func name': 'load', 'comments': 'Load a pickle, with a provided encoding,\n\nParameters ---------- fh : a filelike object encoding : an optional encoding is_verbose : show exception output\n', 'stemmed comments': ['paramet', 'provid', 'pickl', 'show', 'except', 'encod', 'is_verbos', 'option', 'fh', 'output', 'filelik', 'load', 'object']}"
147,"{'func name': 'read_pickle', 'comments': 'Load pickled pandas object (or any object) from file.\n\n.. warning::\n\nLoading pickled data received from untrusted sources can be unsafe. See `here <https://docs.python.org/3/library/pickle.html>`__.\n\nParameters ---------- filepath_or_buffer : str, path object or file-like object File path, URL, or buffer where the pickled object will be loaded from.\n\n.. versionchanged:: 1.0.0 Accept URL. URL is not limited to S3 and GCS.\n\ncompression : {\'infer\', \'gzip\', \'bz2\', \'zip\', \'xz\', None}, default \'infer\' If \'infer\' and \'path_or_url\' is path-like, then detect compression from the following extensions: \'.gz\', \'.bz2\', \'.zip\', or \'.xz\' (otherwise no compression) If \'infer\' and \'path_or_url\' is not path-like, then use None (= no decompression).\n##### Returns\n* **unpickled **: same type as object stored in file\n\n* **DataFrame.to_pickle **: Pickle (serialize) DataFrame object to file.\n\n* **Series.to_pickle **: Pickle (serialize) Series object to file.\n\n* **read_hdf **: Read HDF5 file into a DataFrame.\n\n* **read_sql **: Read SQL query or database table into a DataFrame.\n\n* **read_parquet **: Load a parquet object, returning a DataFrame.\n\n* **>>> original_df = pd.DataFrame({""foo""**: range(5), ""bar""\n\n', 'stemmed comments': ['paramet', 'sql', '=', 'original_df', 'filepath_or_buff', 'otherwis', 'use', 'path_or_url', 'read_parquet', 'infer', 'serial', '//docspythonorg/3/library/picklehtml', 'seriesto_pickl', 'xz', 'none', 'type', 'read_sql', 'gzip', 'see', '__', 'load', 'sourc', 'bz2', 'store', 'pddatafram', 'follow', 'compress', 'pickl', 'accept', 'path', 'detect', 'http', 'filelik', 'datafram', '5', 'gc', 'versionchang', 'default', 'rang', 'seri', 'extens', 'str', '100', '<', 'read_hdf', 'foo', 'warn', 'hdf5', 'panda', 'return', 'queri', 'databas', 'data', 'limit', 'parquet', 'gz', 'url', 'untrust', 'tabl', '>', 'zip', 'file', 'S3', '{', 'If', 'pathlik', 'unsaf', 'decompress', 'unpickl', 'dataframeto_pickl', 'buffer', 'bar', 'receiv', '}', 'object', 'read']}"
148,"{'func name': '_get_names', 'comments': '', 'stemmed comments': []}"
149,"{'func name': 'format_object_attrs', 'comments': ""Return a list of tuples of the (attr, formatted_value) for common attrs, including dtype, name, length\n\nParameters ---------- obj : object must be iterable include_dtype : bool If False, dtype won't be in the returned list\n##### Returns\n"", 'stemmed comments': ['paramet', 'tupl', 'include_dtyp', 'name', 'common', 'attr', 'includ', 'length', 'obj', 'fals', 'wo', 'nt', 'return', 'iter', 'must', 'dtype', 'list', 'If', 'bool', 'formatted_valu', 'object']}"
150,"{'func name': 'maybe_expression', 'comments': 'loose checking if s is a pytables-acceptable expression\n\n\n', 'stemmed comments': ['express', 'loos', 'check', 'pytablesaccept']}"
151,"{'func name': '_get_data_and_dtype_name', 'comments': 'Convert the passed data into a storable form and a dtype string.\n\n\n', 'stemmed comments': ['dtype', 'string', 'pass', 'convert', 'storabl', 'data', 'form']}"
152,"{'func name': 'asfreq', 'comments': 'Utility frequency conversion method for Series/DataFrame.\n\n\n', 'stemmed comments': ['frequenc', 'method', 'series/datafram', 'util', 'convers']}"
153,"{'func name': '_reorder_for_extension_array_stack', 'comments': 'Re-orders the values when stacking multiple extension-arrays.\n\nThe indirect stacking method used for EAs requires a followup take to get the order correct.\n\nParameters ---------- arr : ExtensionArray n_rows, n_columns : int The number of rows and columns in the original DataFrame.\n##### Returns\n* **taken **: ExtensionArray\n    The original `arr` with elements re-ordered appropriately\n\n', 'stemmed comments': ['paramet', 'taken', 'followup', 'use', 'order', 'indirect', 'number', 'reorder', 'n_row', 'ea', 'arr', 'stack', 'method', 'take', 'the', 'column', 'requir', 'valu', 'row', 'datafram', 'element', 'int', 'multipl', 'return', 'extensionarray', 'correct', 'origin', 'get', 'appropri', 'n_column']}"
154,"{'func name': 'rxor', 'comments': '', 'stemmed comments': []}"
155,"{'func name': 'get_filepath_or_buffer', 'comments': '', 'stemmed comments': []}"
156,"{'func name': '_parse_float_vec', 'comments': 'Parse a vector of float values representing IBM 8 byte floats into native 8 byte floats.\n\n\n', 'stemmed comments': ['byte', 'pars', 'float', 'nativ', 'repres', 'valu', '8', 'ibm', 'vector']}"
157,"{'func name': 'read_sas', 'comments': ""Read SAS files stored as either XPORT or SAS7BDAT format files.\n\nParameters ---------- filepath_or_buffer : str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: ``file://localhost/path/to/table.sas``.\n\nIf you want to pass in a path object, pandas accepts any ``os.PathLike``.\n\nBy file-like object, we refer to objects with a ``read()`` method, such as a file handler (e.g. via builtin ``open`` function) or ``StringIO``. format : str {'xport', 'sas7bdat'} or None If None, file format is inferred from file extension. If 'xport' or 'sas7bdat', uses the corresponding format. index : identifier of index column, defaults to None Identifier of column that should be used as index of the DataFrame. encoding : str, default is None Encoding for text data.\n\nIf None, text data are stored as raw bytes. chunksize : int Read file `chunksize` lines at a time, returns iterator. iterator : bool, defaults to False If True, returns an iterator for reading the file incrementally.\n##### Returns\n"", 'stemmed comments': ['ftp', 'xport', 'paramet', 'refer', 'filepath_or_buff', 'use', 'text', 'object', 'read', 'ospathlik', 'By', 'infer', 'line', 'time', 'valid', 'none', 'includ', 'scheme', 'local', 'store', 'sa', 'host', 'method', 'identifi', 'ani', 'the', 'column', 'stringio', 'path', 'string', 'open', 'encod', 'eg', 'raw', 'correspond', 'fals', 'could', 'via', 'filelik', 'http', 'want', 'datafram', 'default', 's3', 'int', 'extens', 'either', 'str', 'A', 'expect', 'panda', 'builtin', 'return', 'data', 'iter', 'true', 'url', 'byte', 'index', 'for', 'function', 'file', '{', '//localhost/path/to/tablesa', 'If', 'pass', 'bool', 'increment', 'sas7bdat', 'handler', 'chunksiz', '}', 'accept', 'format']}"
158,"{'func name': '_coo_to_sparse_series', 'comments': 'Convert a scipy.sparse.coo_matrix to a SparseSeries.\n\nParameters ---------- A : scipy.sparse.coo.coo_matrix dense_index : bool, default False\n##### Returns\n', 'stemmed comments': ['default', 'paramet', 'A', 'scipysparsecoo_matrix', 'bool', 'convert', 'return', 'dense_index', 'scipysparsecoocoo_matrix', 'fals', 'sparseseri']}"
159,"{'func name': '_get_pretty_string', 'comments': 'Return a prettier version of obj.\n\nParameters ---------- obj : object Object to pretty print\n##### Returns\n', 'stemmed comments': ['paramet', 'print', 'obj', 'version', 'return', 'prettier', 'pretti', 'object']}"
160,"{'func name': '_coerce_method', 'comments': 'Install the scalar coercion methods.\n\n\n', 'stemmed comments': ['scalar', 'method', 'instal', 'coercion']}"
161,"{'func name': '_reorder_by_uniques', 'comments': '', 'stemmed comments': []}"
162,"{'func name': 'read_spss', 'comments': 'Load an SPSS file from the file path, returning a DataFrame.\n\n.. versionadded:: 0.25.0\n\nParameters ---------- path : string or Path File path. usecols : list-like, optional Return a subset of the columns. If None, return all columns. convert_categoricals : bool, default is True Convert categorical columns into pd.Categorical.\n##### Returns\n', 'stemmed comments': ['paramet', 'categor', 'versionad', 'option', 'convert', 'none', 'load', 'column', 'path', 'string', 'usecol', 'spss', 'pdcategor', 'datafram', 'default', 'return', 'true', 'listlik', '0250', 'file', 'If', 'bool', 'subset', 'convert_categor']}"
163,"{'func name': 'get_schema', 'comments': 'Get the SQL db table schema for the given frame.\n\nParameters ---------- frame : DataFrame name : string name of SQL table keys : string or sequence, default: None columns to use a primary key con: an open SQL database connection object or a SQLAlchemy connectable Using SQLAlchemy makes it possible to use any DB supported by that library, default: None If a DBAPI2 object, only sqlite3 is supported. dtype : dict of column name to SQL type, default None Optional specifying the datatype for columns. The SQL type should be a SQLAlchemy type, or a string for sqlite3 fallback connection.\n', 'stemmed comments': ['sql', 'paramet', 'use', 'name', 'db', 'option', 'none', 'type', 'sequenc', 'the', 'column', 'sqlite3', 'string', 'open', 'schema', 'datafram', 'frame', 'librari', 'make', 'default', 'DB', 'key', 'databas', 'fallback', 'primari', 'sqlalchemi', 'given', 'dtype', 'connect', 'dbapi2', 'tabl', 'possibl', 'support', 'If', 'con', 'datatyp', 'specifi', 'get', 'object', 'dict']}"
164,"{'func name': '_pad_bytes_new', 'comments': ""Takes a bytes instance and pads it with null bytes until it's length chars.\n\n\n"", 'stemmed comments': ['byte', 'take', 's', 'char', 'length', 'pad', 'instanc', 'null']}"
165,"{'func name': 'copy', 'comments': 'Copy a docstring from another source function (if present)\n\n\n', 'stemmed comments': ['function', 'anoth', 'docstr', 'present', 'sourc', 'copi']}"
166,"{'func name': '_get_standard_colors', 'comments': '', 'stemmed comments': []}"
167,"{'func name': '_maybe_wrap_formatter', 'comments': '', 'stemmed comments': []}"
168,"{'func name': 'test_groupby_aggregate_period_frame', 'comments': '', 'stemmed comments': []}"
169,"{'func name': 'test_diff', 'comments': '', 'stemmed comments': []}"
170,"{'func name': 'test_int64_add_overflow', 'comments': '', 'stemmed comments': []}"
171,"{'func name': 'test_drop_non_empty_list', 'comments': '', 'stemmed comments': []}"
172,"{'func name': 'assert_bool_op_api', 'comments': 'Check that API for boolean operator opname works as advertised on frame\n\nParameters ---------- opname : string Name of the operator to test on frame float_frame : DataFrame DataFrame with columns of type float float_string_frame : DataFrame DataFrame with both float and string columns has_bool_only : bool, default False Whether the method ""opname"" has the kwarg ""bool_only""\n', 'stemmed comments': ['paramet', 'work', 'name', 'type', 'float', 'method', 'bool_onli', 'column', 'string', 'float_fram', 'fals', 'datafram', 'frame', 'default', 'test', 'boolean', 'kwarg', 'whether', 'advertis', 'float_string_fram', 'oper', 'has_bool_onli', 'bool', 'check', 'opnam', 'api']}"
173,"{'func name': 'test_numpy_type_funcs', 'comments': '', 'stemmed comments': []}"
174,"{'func name': 'test_np', 'comments': '', 'stemmed comments': []}"
175,"{'func name': 'test_namespace', 'comments': '', 'stemmed comments': []}"
176,"{'func name': 'test_rolling_apply_args_kwargs', 'comments': '', 'stemmed comments': []}"
177,"{'func name': 'zip_frames', 'comments': ""take a list of frames, zip them together under the assumption that these all have the first frames' index/columns.\n\n\n##### Returns\n* **new_frame **: DataFrame\n\n"", 'stemmed comments': ['take', 'new_fram', 'zip', 'list', 'first', 'togeth', 'assumpt', 'return', 'index/column', 'datafram', 'frame']}"
178,"{'func name': 'test_apply_function_returns_non_pandas_non_scalar', 'comments': '', 'stemmed comments': []}"
179,"{'func name': '_permute', 'comments': '', 'stemmed comments': []}"
180,"{'func name': 'test_pow_nan_with_zero', 'comments': '', 'stemmed comments': []}"
181,"{'func name': 'test_unary_op', 'comments': '', 'stemmed comments': []}"
182,"{'func name': 'test_datetime_subclass', 'comments': '', 'stemmed comments': []}"
183,"{'func name': 'test_map_missing', 'comments': '', 'stemmed comments': []}"
184,"{'func name': 'test_array_not_registered', 'comments': '', 'stemmed comments': []}"
185,"{'func name': 'date_range_frame', 'comments': ""Fixture for DataFrame of ints with date_range index\n\nColumns are ['A', 'B'].\n"", 'stemmed comments': ['index', 'B', 'int', 'A', 'column', '[', ']', 'fixtur', 'datafram', 'date_rang']}"
186,"{'func name': 'test_assert_almost_equal_iterable_values_mismatch', 'comments': '', 'stemmed comments': []}"
187,"{'func name': 'test_categorical_equal_object_override', 'comments': '', 'stemmed comments': []}"
188,"{'func name': 'test_assert_extension_array_equal_non_extension_array', 'comments': '', 'stemmed comments': []}"
189,"{'func name': 'test_frame_equal_unicode', 'comments': '', 'stemmed comments': []}"
190,"{'func name': 'test_index_equal_category_mismatch', 'comments': '', 'stemmed comments': []}"
191,"{'func name': 'test_interval_array_equal_start_mismatch', 'comments': '', 'stemmed comments': []}"
192,"{'func name': 'test_numpy_array_equal_different_na', 'comments': '', 'stemmed comments': []}"
193,"{'func name': 'test_assert_produces_warning_honors_filter', 'comments': '', 'stemmed comments': []}"
194,"{'func name': 'test_series_equal_categorical_mismatch', 'comments': '', 'stemmed comments': []}"
195,"{'func name': 'test_astype_category', 'comments': '', 'stemmed comments': []}"
196,"{'func name': 'test_extra_kinds_ok', 'comments': '', 'stemmed comments': []}"
197,"{'func name': 'test_win_type_not_implemented', 'comments': '', 'stemmed comments': []}"
198,"{'func name': 'test_resample_quantile', 'comments': '', 'stemmed comments': []}"
199,"{'func name': 'test_validate_1d_input', 'comments': '', 'stemmed comments': []}"
200,"{'func name': 'test_group_ohlc', 'comments': '', 'stemmed comments': []}"
201,"{'func name': 'test_is_bool_dtype', 'comments': '', 'stemmed comments': []}"
202,"{'func name': 'test_diff', 'comments': '', 'stemmed comments': []}"
203,"{'func name': 'test_mask_inplace', 'comments': '', 'stemmed comments': []}"
204,"{'func name': 'data_for_grouping', 'comments': '', 'stemmed comments': []}"
205,"{'func name': 'test_unix_style_breaks', 'comments': '', 'stemmed comments': []}"
206,"{'func name': 'test_calendar_2031', 'comments': '', 'stemmed comments': []}"
207,"{'func name': 'test_setitem_other_callable', 'comments': '', 'stemmed comments': []}"
208,"{'func name': 'data_for_grouping', 'comments': '', 'stemmed comments': []}"
209,"{'func name': 'test_groupby_agg_non_numeric', 'comments': '', 'stemmed comments': []}"
210,"{'func name': 'test_get_day_of_year_dt', 'comments': '', 'stemmed comments': []}"
211,"{'func name': 'test_indexer_caching', 'comments': '', 'stemmed comments': []}"
212,"{'func name': 'test_pass_through_non_array_likes', 'comments': '', 'stemmed comments': []}"
213,"{'func name': 'test_raw_roundtrip', 'comments': '', 'stemmed comments': []}"
214,"{'func name': 'check_comprehensiveness', 'comments': '', 'stemmed comments': []}"
215,"{'func name': 'test_comment_first_line', 'comments': '', 'stemmed comments': []}"
216,"{'func name': 'test_integer_precision', 'comments': '', 'stemmed comments': []}"
217,"{'func name': 'test_is_extension_array_dtype', 'comments': '', 'stemmed comments': []}"
218,"{'func name': 'mmap_file', 'comments': '', 'stemmed comments': []}"
219,"{'func name': 'test_astype_object_preserves_datetime_na', 'comments': '', 'stemmed comments': []}"
220,"{'func name': 'test_version_tag', 'comments': '', 'stemmed comments': []}"
221,"{'func name': 'test_rich_comparison_with_unsupported_type', 'comments': '', 'stemmed comments': []}"
222,"{'func name': 'pytables_hdf5_file', 'comments': 'Use PyTables to create a simple HDF5 file.\n\n\n', 'stemmed comments': ['simpl', 'use', 'file', 'creat', 'hdf5', 'pytabl']}"
223,"{'func name': 'test_pickle_compat_construction', 'comments': '', 'stemmed comments': []}"
224,"{'func name': 'test_invalid_numexpr_version', 'comments': '', 'stemmed comments': []}"
225,"{'func name': 'test_complex_append', 'comments': '', 'stemmed comments': []}"
226,"{'func name': 'test_invalid_compression', 'comments': '', 'stemmed comments': []}"
227,"{'func name': 'test_to_json_compression', 'comments': '', 'stemmed comments': []}"
228,"{'func name': 'test_with_missing_lzma_runtime', 'comments': 'Tests if RuntimeError is hit when calling lzma without having the module available.\n\n\n', 'stemmed comments': ['runtimeerror', 'test', 'without', 'lzma', 'avail', 'hit', 'modul', 'call']}"
229,"{'func name': 'test_get_dtype_kinds_period', 'comments': '', 'stemmed comments': []}"
230,"{'func name': 'test_concat_sparse', 'comments': '', 'stemmed comments': []}"
231,"{'func name': 'test_detect_console_encoding_fallback_to_default', 'comments': '', 'stemmed comments': []}"
232,"{'func name': 'test_cast_1d_array_like_from_scalar_categorical', 'comments': '', 'stemmed comments': []}"
233,"{'func name': 'test_construct_1d_ndarray_preserving_na', 'comments': '', 'stemmed comments': []}"
234,"{'func name': 'test_cast_1d_array_invalid_scalar', 'comments': '', 'stemmed comments': []}"
235,"{'func name': 'test_td_constructor_value_error', 'comments': '', 'stemmed comments': []}"
236,"{'func name': 'test_index_equal_empty_iterable', 'comments': '', 'stemmed comments': []}"
237,"{'func name': 'name', 'comments': '', 'stemmed comments': []}"
238,"{'func name': 'test_isin_multi_index_with_missing_value', 'comments': '', 'stemmed comments': []}"
239,"{'func name': 'test_to_numpy_kwargs_raises', 'comments': '', 'stemmed comments': []}"
240,"{'func name': 'test_localize_pydatetime_dt_types', 'comments': '', 'stemmed comments': []}"
241,"{'func name': 'test_to_flat_index', 'comments': '', 'stemmed comments': []}"
242,"{'func name': 'test_maybe_convert_objects_copy', 'comments': '', 'stemmed comments': []}"
243,"{'func name': 'test_timtetonum_accepts_unicode', 'comments': '', 'stemmed comments': []}"
244,"{'func name': 'test_converter_index_col_bug', 'comments': '', 'stemmed comments': []}"
245,"{'func name': 'test_copy_method_kwargs', 'comments': '', 'stemmed comments': []}"
246,"{'func name': 'test_css_relative_font_size', 'comments': '', 'stemmed comments': []}"
247,"{'func name': '_check_accum_op', 'comments': '', 'stemmed comments': []}"
248,"{'func name': 'test_cut_nullable_integer', 'comments': '', 'stemmed comments': []}"
249,"{'func name': 'test_cython_with_timestamp_and_nat', 'comments': '', 'stemmed comments': []}"
250,"{'func name': 'test_parse_all_fields', 'comments': '', 'stemmed comments': []}"
251,"{'func name': 'test_date_range_with_custom_holidays', 'comments': '', 'stemmed comments': []}"
252,"{'func name': 'test_resample_apply_product', 'comments': '', 'stemmed comments': []}"
253,"{'func name': 'test_multiindex_period_datetime', 'comments': '', 'stemmed comments': []}"
254,"{'func name': 'test_setitem_tuple_with_datetimetz', 'comments': '', 'stemmed comments': []}"
255,"{'func name': 'na_value', 'comments': '', 'stemmed comments': []}"
256,"{'func name': 'test_shift_months', 'comments': '', 'stemmed comments': []}"
257,"{'func name': 'test_to_numpy_extra', 'comments': '', 'stemmed comments': []}"
258,"{'func name': '_check_plot_works', 'comments': '', 'stemmed comments': []}"
259,"{'func name': 'test_to_numpy_keyword', 'comments': '', 'stemmed comments': []}"
260,"{'func name': 'test_deprecate_keyword', 'comments': '', 'stemmed comments': []}"
261,"{'func name': 'test_deprecate_wrong_docstring', 'comments': '', 'stemmed comments': []}"
262,"{'func name': 'test_dialect_conflict_delimiter', 'comments': '', 'stemmed comments': []}"
263,"{'func name': 'test_datetime_with_timezone', 'comments': '', 'stemmed comments': []}"
264,"{'func name': 'test_missing_required_dependency', 'comments': '', 'stemmed comments': []}"
265,"{'func name': 'test_drop_duplicates_bool', 'comments': '', 'stemmed comments': []}"
266,"{'func name': 'test_drop_duplicates_ignore_index', 'comments': '', 'stemmed comments': []}"
267,"{'func name': 'test_drop_with_non_unique_datetime_index_and_invalid_keys', 'comments': '', 'stemmed comments': []}"
268,"{'func name': 'test_update_dtype_raises', 'comments': '', 'stemmed comments': []}"
269,"{'func name': 'test_boolean_dtype', 'comments': '', 'stemmed comments': []}"
270,"{'func name': '_check_cast', 'comments': 'Check if all dtypes of df are equal to v\n\n\n', 'stemmed comments': ['dtype', 'v', 'df', 'equal', 'check']}"
271,"{'func name': 'test_is_dtype_no_warning', 'comments': '', 'stemmed comments': []}"
272,"{'func name': 'test_duplicated_nan_none', 'comments': '', 'stemmed comments': []}"
273,"{'func name': 'test_duplicated_on_empty_frame', 'comments': '', 'stemmed comments': []}"
274,"{'func name': 'test_is_unique_class_ne', 'comments': '', 'stemmed comments': []}"
275,"{'func name': 'test_duplicated2', 'comments': '', 'stemmed comments': []}"
276,"{'func name': 'test_encoding_named_temp_file', 'comments': '', 'stemmed comments': []}"
277,"{'func name': 'test_multiindex_compare', 'comments': '', 'stemmed comments': []}"
278,"{'func name': 'test_AbstractMethodError_classmethod', 'comments': '', 'stemmed comments': []}"
279,"{'func name': 'test_negate_lt_eq_le', 'comments': '', 'stemmed comments': []}"
280,"{'func name': 'test_expanding_count_default_min_periods_with_null_values', 'comments': '', 'stemmed comments': []}"
281,"{'func name': 'test_duplicate_index', 'comments': '', 'stemmed comments': []}"
282,"{'func name': 'test_duplicate_index', 'comments': '', 'stemmed comments': []}"
283,"{'func name': 'test_concat_axis1', 'comments': '', 'stemmed comments': []}"
284,"{'func name': 'test_memorial_day', 'comments': '', 'stemmed comments': []}"
285,"{'func name': 'test_fields_readonly', 'comments': '', 'stemmed comments': []}"
286,"{'func name': 'test_filter_dropna_with_empty_groups', 'comments': '', 'stemmed comments': []}"
287,"{'func name': 'test_period_dtype_mismatch', 'comments': '', 'stemmed comments': []}"
288,"{'func name': 'test_fy5253qtr_onoffset_last', 'comments': '', 'stemmed comments': []}"
289,"{'func name': 'test_filepath_or_buffer_bad_arg_raises', 'comments': '', 'stemmed comments': []}"
290,"{'func name': 'test_repr_max_seq_item_setting', 'comments': '', 'stemmed comments': []}"
291,"{'func name': 'test_isoformat', 'comments': '', 'stemmed comments': []}"
292,"{'func name': 'test_to_native_types', 'comments': '', 'stemmed comments': []}"
293,"{'func name': 'test_to_native_types', 'comments': '', 'stemmed comments': []}"
294,"{'func name': '_generate_4_axes_via_gridspec', 'comments': '', 'stemmed comments': []}"
295,"{'func name': 'test_get_code_invalid', 'comments': '', 'stemmed comments': []}"
296,"{'func name': 'test_apply_to_nullable_integer_returns_float', 'comments': '', 'stemmed comments': []}"
297,"{'func name': 'test_read_gbq_progress_bar_type_kwarg', 'comments': '', 'stemmed comments': []}"
298,"{'func name': 'test_gcs_not_present_exception', 'comments': '', 'stemmed comments': []}"
299,"{'func name': 'test_setattr_warnings', 'comments': '', 'stemmed comments': []}"
300,"{'func name': 'test_set_levels_with_iterable', 'comments': '', 'stemmed comments': []}"
301,"{'func name': 'test_frame_mi_access_returns_frame', 'comments': '', 'stemmed comments': []}"
302,"{'func name': 'test_groupby_list_level', 'comments': '', 'stemmed comments': []}"
303,"{'func name': 'test_hash_object_none_key', 'comments': '', 'stemmed comments': []}"
304,"{'func name': 'test_read_csv_multiindex_columns', 'comments': '', 'stemmed comments': []}"
305,"{'func name': 'test_both_offset_observance_raises', 'comments': '', 'stemmed comments': []}"
306,"{'func name': 'test_same_ordering', 'comments': '', 'stemmed comments': []}"
307,"{'func name': 'test_getitem_iloc', 'comments': '', 'stemmed comments': []}"
308,"{'func name': 'test_iloc_nonunique', 'comments': '', 'stemmed comments': []}"
309,"{'func name': 'test_grouper_index_level_as_string_series', 'comments': '', 'stemmed comments': []}"
310,"{'func name': 'test_no_multi_index_level_names_empty', 'comments': '', 'stemmed comments': []}"
311,"{'func name': 'test_large_mi_dataframe_indexing', 'comments': '', 'stemmed comments': []}"
312,"{'func name': 'test_object_casting_indexing_wraps_datetimelike', 'comments': '', 'stemmed comments': []}"
313,"{'func name': 'test_series_at', 'comments': '', 'stemmed comments': []}"
314,"{'func name': 'test_setitem_with_bool_mask_and_values_matching_n_trues_in_length', 'comments': '', 'stemmed comments': []}"
315,"{'func name': 'test_slice_locs_with_missing_value', 'comments': '', 'stemmed comments': []}"
316,"{'func name': 'test_getitem_2d_no_warning', 'comments': '', 'stemmed comments': []}"
317,"{'func name': 'test_maybe_infer_to_datetimelike_ser_construct', 'comments': '', 'stemmed comments': []}"
318,"{'func name': 'test_cast_scalar_to_array', 'comments': '', 'stemmed comments': []}"
319,"{'func name': 'test_ms_vs_capital_ms', 'comments': '', 'stemmed comments': []}"
320,"{'func name': 'test_ensure_categorical', 'comments': '', 'stemmed comments': []}"
321,"{'func name': 'data_for_grouping', 'comments': '', 'stemmed comments': []}"
322,"{'func name': 'test_array_setitem', 'comments': '', 'stemmed comments': []}"
323,"{'func name': 'test_nlevels', 'comments': '', 'stemmed comments': []}"
324,"{'func name': 'test_hasnans_unchached_for_series', 'comments': '', 'stemmed comments': []}"
325,"{'func name': 'test_dataframe_not_equal', 'comments': '', 'stemmed comments': []}"
326,"{'func name': 'name', 'comments': '', 'stemmed comments': []}"
327,"{'func name': 'tree', 'comments': '', 'stemmed comments': []}"
328,"{'func name': 'data_for_grouping', 'comments': '', 'stemmed comments': []}"
329,"{'func name': 'test_arrow_table_roundtrip', 'comments': '', 'stemmed comments': []}"
330,"{'func name': 'create_series_categorical_intervals', 'comments': '', 'stemmed comments': []}"
331,"{'func name': 'interval', 'comments': '', 'stemmed comments': []}"
332,"{'func name': 'test_dir', 'comments': '', 'stemmed comments': []}"
333,"{'func name': 'test_suppress_future_warning_with_sort_kw', 'comments': '', 'stemmed comments': []}"
334,"{'func name': 'test_join_multi_wrong_order', 'comments': '', 'stemmed comments': []}"
335,"{'func name': 'test_merge_join_categorical_multiindex', 'comments': '', 'stemmed comments': []}"
336,"{'func name': 'data_for_grouping', 'comments': '', 'stemmed comments': []}"
337,"{'func name': 'test_drop_labels_or_levels_series', 'comments': '', 'stemmed comments': []}"
338,"{'func name': 'test_cache_readonly_preserve_docstrings', 'comments': '', 'stemmed comments': []}"
339,"{'func name': 'test_assert_aliases_deprecated', 'comments': '', 'stemmed comments': []}"
340,"{'func name': 'test_roll_convention', 'comments': '', 'stemmed comments': []}"
341,"{'func name': 'check_cases', 'comments': '', 'stemmed comments': []}"
342,"{'func name': 'test_to_csv', 'comments': '', 'stemmed comments': []}"
343,"{'func name': 'test_loc_period_string_indexing', 'comments': '', 'stemmed comments': []}"
344,"{'func name': 'test_loc_axis_1_slice', 'comments': '', 'stemmed comments': []}"
345,"{'func name': 'test_basic_setitem_with_labels', 'comments': '', 'stemmed comments': []}"
346,"{'func name': 'test_encoding_detected', 'comments': '', 'stemmed comments': []}"
347,"{'func name': 'test_mangled_unnamed_placeholders', 'comments': '', 'stemmed comments': []}"
348,"{'func name': 'test_join_indexes_and_columns_on', 'comments': '', 'stemmed comments': []}"
349,"{'func name': 'test_merge_datetime_upcast_dtype', 'comments': '', 'stemmed comments': []}"
350,"{'func name': 'test_get_accessor_args', 'comments': '', 'stemmed comments': []}"
351,"{'func name': 'test_iter_readonly', 'comments': '', 'stemmed comments': []}"
352,"{'func name': 'test_na_value_for_dtype', 'comments': '', 'stemmed comments': []}"
353,"{'func name': 'interp_methods_ind', 'comments': ""Fixture that returns a (method name, required kwargs) pair to be tested for various Index types.\n\nThis fixture does not include methods\n\n- 'time', 'index', 'nearest', 'values' as a parameterization\n"", 'stemmed comments': ['nearest', 'pair', 'method', 'test', 'index', 'kwarg', 'name', 'time', 'variou', 'includ', 'return', 'requir', 'type', 'thi', 'valu', 'fixtur', 'parameter']}"
354,"{'func name': 'test_nan_stays_float', 'comments': '', 'stemmed comments': []}"
355,"{'func name': '_rolling_consistency_cases', 'comments': '', 'stemmed comments': []}"
356,"{'func name': 'test_searchsorted_monotonic', 'comments': '', 'stemmed comments': []}"
357,"{'func name': 'test_multi_thread_path_multipart_read_csv', 'comments': '', 'stemmed comments': []}"
358,"{'func name': 'idx_cols_multi', 'comments': '', 'stemmed comments': []}"
359,"{'func name': 'test_na_treated_as_false', 'comments': '', 'stemmed comments': []}"
360,"{'func name': 'test_pickle_roundtrip_containers', 'comments': '', 'stemmed comments': []}"
361,"{'func name': 'test_str_nan_dropped', 'comments': '', 'stemmed comments': []}"
362,"{'func name': 'test_setting_names_from_levels_raises', 'comments': '', 'stemmed comments': []}"
363,"{'func name': 'test_nanops_independent_of_mask_param', 'comments': '', 'stemmed comments': []}"
364,"{'func name': 'test_nat_comparisons', 'comments': '', 'stemmed comments': []}"
365,"{'func name': 'tips_df', 'comments': 'DataFrame with the tips dataset.\n\n\n', 'stemmed comments': ['tip', 'datafram', 'dataset']}"
366,"{'func name': 'assert_check_nselect_boundary', 'comments': '', 'stemmed comments': []}"
367,"{'func name': 'df_main_dtypes', 'comments': '', 'stemmed comments': []}"
368,"{'func name': 'test_normalize_date_sub_types', 'comments': '', 'stemmed comments': []}"
369,"{'func name': 'max_level_test_input_data', 'comments': 'input data to test json_normalize with max_level param\n\n\n', 'stemmed comments': ['input', 'test', 'json_norm', 'param', 'data', 'max_level']}"
370,"{'func name': 'test_nth_nan_in_grouper', 'comments': '', 'stemmed comments': []}"
371,"{'func name': 'test_dataframe_div_silenced', 'comments': '', 'stemmed comments': []}"
372,"{'func name': 'test_non_coerce_uint64_conflict', 'comments': '', 'stemmed comments': []}"
373,"{'func name': 'test_getitem_int64', 'comments': '', 'stemmed comments': []}"
374,"{'func name': 'test_uint_index_does_not_convert_to_float64', 'comments': '', 'stemmed comments': []}"
375,"{'func name': 'test_elementwise_comparison_warning', 'comments': '', 'stemmed comments': []}"
376,"{'func name': 'test_setitem_preserves_views', 'comments': '', 'stemmed comments': []}"
377,"{'func name': 'skip_numpy_object', 'comments': ""Tests for PandasArray with nested data. Users typically won't create these objects via `pd.array`, but they can show up through `.array` on a Series with nested data. Many of the base tests fail, as they aren't appropriate for nested data.\n\nThis fixture allows these tests to be skipped when used as a usefixtures marker to either an individual test or a test class.\n"", 'stemmed comments': ['use', 'allow', 'skip', 'user', 'pandasarray', 'marker', 'class', 'base', 'array', 'pdarray', 'mani', 'via', 'test', 'wo', 'seri', 'either', 'show', 'usefixtur', 'nt', 'data', 'fixtur', 'nest', 'individu', 'fail', 'creat', 'typic', 'thi', 'appropri', 'object']}"
378,"{'func name': 'test_index_ops_defer_to_unknown_subclasses', 'comments': '', 'stemmed comments': []}"
379,"{'func name': 'test_after_nearest_workday', 'comments': '', 'stemmed comments': []}"
380,"{'func name': 'test_nonexistent_sheetname_raises', 'comments': '', 'stemmed comments': []}"
381,"{'func name': 'test_shift_across_dst', 'comments': '', 'stemmed comments': []}"
382,"{'func name': 'test_week_add_invalid', 'comments': '', 'stemmed comments': []}"
383,"{'func name': 'test_to_excel_with_openpyxl_engine', 'comments': '', 'stemmed comments': []}"
384,"{'func name': 'start_shift', 'comments': 'Fixture for generating intervals of different types from a start value and a shift value that can be added to start to generate an endpoint.\n\n\n', 'stemmed comments': ['ad', 'differ', 'shift', 'type', 'interv', 'valu', 'fixtur', 'endpoint', 'start', 'gener']}"
385,"{'func name': 'start_shift', 'comments': 'Fixture for generating intervals of types from a start value and a shift value that can be added to start to generate an endpoint\n\n\n', 'stemmed comments': ['ad', 'shift', 'type', 'interv', 'valu', 'fixtur', 'endpoint', 'start', 'gener']}"
386,"{'func name': 'test_no_version_raises', 'comments': '', 'stemmed comments': []}"
387,"{'func name': 'test_orc_reader_snappy_compressed', 'comments': '', 'stemmed comments': []}"
388,"{'func name': 'test_groupby_agg_err_catching', 'comments': '', 'stemmed comments': []}"
389,"{'func name': 'assert_json_roundtrip_equal', 'comments': '', 'stemmed comments': []}"
390,"{'func name': 'test_cross_engine_fp_pa', 'comments': '', 'stemmed comments': []}"
391,"{'func name': 'test_hypothesis_delimited_date', 'comments': '', 'stemmed comments': []}"
392,"{'func name': 'test_parsers_iso8601_leading_space', 'comments': '', 'stemmed comments': []}"
393,"{'func name': 'test_parse_time_string_check_instance_type_raise_exception', 'comments': '', 'stemmed comments': []}"
394,"{'func name': 'test_partial_string_timestamp_multiindex', 'comments': '', 'stemmed comments': []}"
395,"{'func name': 'test_loc_getitem_partial_both_axis', 'comments': '', 'stemmed comments': []}"
396,"{'func name': 'test_pct_change_with_duplicated_indices', 'comments': '', 'stemmed comments': []}"
397,"{'func name': 'test_pct_change_with_duplicated_indices', 'comments': '', 'stemmed comments': []}"
398,"{'func name': 'test_period_ordinal_business_day', 'comments': '', 'stemmed comments': []}"
399,"{'func name': '_series_name', 'comments': '', 'stemmed comments': []}"
400,"{'func name': 'test_arrow_table_roundtrip', 'comments': '', 'stemmed comments': []}"
401,"{'func name': 'na_value', 'comments': '', 'stemmed comments': []}"
402,"{'func name': '_permute', 'comments': '', 'stemmed comments': []}"
403,"{'func name': 'test_small_year_parsing', 'comments': '', 'stemmed comments': []}"
404,"{'func name': 'test_maybe_convert_timedelta', 'comments': '', 'stemmed comments': []}"
405,"{'func name': 'test_pickle_s3url_roundtrip', 'comments': '', 'stemmed comments': []}"
406,"{'func name': 'interval_values', 'comments': '', 'stemmed comments': []}"
407,"{'func name': 'test_repr_binary_type', 'comments': '', 'stemmed comments': []}"
408,"{'func name': 'test_maybe_promote_dimensions', 'comments': '', 'stemmed comments': []}"
409,"{'func name': 'test_pytables_raises', 'comments': '', 'stemmed comments': []}"
410,"{'func name': 'test_malformed_skipfooter', 'comments': '', 'stemmed comments': []}"
411,"{'func name': 'test_qcut_nullable_integer', 'comments': '', 'stemmed comments': []}"
412,"{'func name': 'skip_if_no_pandas_parser', 'comments': '', 'stemmed comments': []}"
413,"{'func name': 'test_unbalanced_quoting', 'comments': '', 'stemmed comments': []}"
414,"{'func name': 'test_pct_max_many_rows', 'comments': '', 'stemmed comments': []}"
415,"{'func name': 'test_rank_zero_div', 'comments': '', 'stemmed comments': []}"
416,"{'func name': 'test_fwf_compression', 'comments': '', 'stemmed comments': []}"
417,"{'func name': 'read_ext', 'comments': '', 'stemmed comments': []}"
418,"{'func name': 'test_readjson_unicode', 'comments': '', 'stemmed comments': []}"
419,"{'func name': 'get_objs', 'comments': '', 'stemmed comments': []}"
420,"{'func name': 'test_raises_attribute_error', 'comments': '', 'stemmed comments': []}"
421,"{'func name': 'test_reindex_non_unique', 'comments': '', 'stemmed comments': []}"
422,"{'func name': 'test_replace', 'comments': '', 'stemmed comments': []}"
423,"{'func name': 'mix_abc', 'comments': '', 'stemmed comments': []}"
424,"{'func name': 'test_resample_agg_readonly', 'comments': '', 'stemmed comments': []}"
425,"{'func name': 'test_median_duplicate_columns', 'comments': '', 'stemmed comments': []}"
426,"{'func name': 'test_unstacking_multi_index_df', 'comments': '', 'stemmed comments': []}"
427,"{'func name': 'test_delete_base', 'comments': '', 'stemmed comments': []}"
428,"{'func name': 'test_rolling_count_default_min_periods_with_null_values', 'comments': '', 'stemmed comments': []}"
429,"{'func name': 'test_streaming_s3_objects', 'comments': '', 'stemmed comments': []}"
430,"{'func name': 'test_safe_import_dummy', 'comments': '', 'stemmed comments': []}"
431,"{'func name': 'test_zero_variables', 'comments': '', 'stemmed comments': []}"
432,"{'func name': 'test_frame_setitem_copy_no_write', 'comments': '', 'stemmed comments': []}"
433,"{'func name': 'test_setops_disallow_true', 'comments': '', 'stemmed comments': []}"
434,"{'func name': 'empty_index', 'comments': '', 'stemmed comments': []}"
435,"{'func name': 'test_union_dtypes', 'comments': '', 'stemmed comments': []}"
436,"{'func name': '_permute', 'comments': '', 'stemmed comments': []}"
437,"{'func name': 'test_skip_rows_bad_callable', 'comments': '', 'stemmed comments': []}"
438,"{'func name': 'test_sort_column_level_and_index_label', 'comments': '', 'stemmed comments': []}"
439,"{'func name': 'test_argsort', 'comments': '', 'stemmed comments': []}"
440,"{'func name': 'test_decons', 'comments': '', 'stemmed comments': []}"
441,"{'func name': 'data_for_grouping', 'comments': '', 'stemmed comments': []}"
442,"{'func name': 'test_spss_usecols', 'comments': '', 'stemmed comments': []}"
443,"{'func name': 'tquery', 'comments': 'Replace removed sql.tquery function\n\n\n', 'stemmed comments': ['function', 'sqltqueri', 'replac', 'remov']}"
444,"{'func name': 'parsed_114', 'comments': '', 'stemmed comments': []}"
445,"{'func name': 'test_value_counts_na', 'comments': '', 'stemmed comments': []}"
446,"{'func name': 'test_constructor_from_list', 'comments': '', 'stemmed comments': []}"
447,"{'func name': 'data_for_grouping', 'comments': '', 'stemmed comments': []}"
448,"{'func name': 'test_string_array_extract', 'comments': '', 'stemmed comments': []}"
449,"{'func name': 'test_from_custom_template', 'comments': '', 'stemmed comments': []}"
450,"{'func name': 'test_styler_to_excel', 'comments': '', 'stemmed comments': []}"
451,"{'func name': 'dtype_fill_out_dtype', 'comments': '', 'stemmed comments': []}"
452,"{'func name': 'assert_array_dicts_equal', 'comments': '', 'stemmed comments': []}"
453,"{'func name': 'test_compare_ticks_to_strs', 'comments': '', 'stemmed comments': []}"
454,"{'func name': 'test_upsample_sum', 'comments': '', 'stemmed comments': []}"
455,"{'func name': 'test_truthiness', 'comments': '', 'stemmed comments': []}"
456,"{'func name': 'test_resample_timedelta_values', 'comments': '', 'stemmed comments': []}"
457,"{'func name': 'test_delta_to_nanoseconds_error', 'comments': '', 'stemmed comments': []}"
458,"{'func name': 'assert_range_equal', 'comments': '', 'stemmed comments': []}"
459,"{'func name': 'close_open_fixture', 'comments': '', 'stemmed comments': []}"
460,"{'func name': 'test_dt_subclass_add_timedelta', 'comments': '', 'stemmed comments': []}"
461,"{'func name': 'test_py2_created_with_datetimez', 'comments': '', 'stemmed comments': []}"
462,"{'func name': 'test_infer_tz_mismatch', 'comments': '', 'stemmed comments': []}"
463,"{'func name': 'test_css_to_excel_bad_colors', 'comments': '', 'stemmed comments': []}"
464,"{'func name': 'test_html_repr_min_rows', 'comments': '', 'stemmed comments': []}"
465,"{'func name': 'test_no_buf', 'comments': '', 'stemmed comments': []}"
466,"{'func name': 'test_anchored_shortcuts', 'comments': '', 'stemmed comments': []}"
467,"{'func name': 'test_na_to_datetime', 'comments': '', 'stemmed comments': []}"
468,"{'func name': 'test_transform_lambda_indexing', 'comments': '', 'stemmed comments': []}"
469,"{'func name': 'test_outer', 'comments': '', 'stemmed comments': []}"
470,"{'func name': 'numpy', 'comments': '', 'stemmed comments': []}"
471,"{'func name': 'python_engine', 'comments': '', 'stemmed comments': []}"
472,"{'func name': 'test_upcast_datetime', 'comments': '', 'stemmed comments': []}"
473,"{'func name': 'test_usecols_subset_names_mismatch_orig_columns', 'comments': '', 'stemmed comments': []}"
474,"{'func name': 'test_rng_context', 'comments': '', 'stemmed comments': []}"
475,"{'func name': 'test_validation', 'comments': '', 'stemmed comments': []}"
476,"{'func name': 'test_validation', 'comments': '', 'stemmed comments': []}"
477,"{'func name': 'test_validate_bool_kwarg', 'comments': '', 'stemmed comments': []}"
478,"{'func name': 'dataframe', 'comments': '', 'stemmed comments': []}"
479,"{'func name': 'test_series_groupby_value_counts_with_grouper', 'comments': '', 'stemmed comments': []}"
480,"{'func name': 'test_all_methods_categorized', 'comments': '', 'stemmed comments': []}"
481,"{'func name': 'set_engine', 'comments': 'Fixture to set engine for use in each test case.\n\nRather than requiring `engine=...` to be provided explicitly as an argument in each test, this fixture sets a global option to dictate which engine should be used to write Excel files. After executing the test it rolls back said change to the global option.\n', 'stemmed comments': ['set', 'global', 'use', 'dictat', 'write', 'option', 'back', 'engine=', 'said', 'chang', 'requir', 'test', 'provid', 'engin', 'case', 'fixtur', 'roll', 'after', 'argument', 'execut', 'explicitli', 'excel', 'file', 'rather']}"
482,"{'func name': 'test_excel_table_sheet_by_index', 'comments': '', 'stemmed comments': []}"
483,"{'func name': 'test_write_append_mode_raises', 'comments': '', 'stemmed comments': []}"
484,"{'func name': 'test_write_append_mode_raises', 'comments': '', 'stemmed comments': []}"
485,"{'func name': 'numeric_as_float', 'comments': '', 'stemmed comments': []}"
486,"{'func name': 'test_series_getitem_multiindex_xs_by_label', 'comments': '', 'stemmed comments': []}"
487,"{'func name': 'test_on_offset', 'comments': '', 'stemmed comments': []}"
488,"{'func name': '_infer_precision', 'comments': 'Infer an appropriate precision for _round_frac\n\n\n', 'stemmed comments': ['_round_frac', 'precis', 'appropri', 'infer']}"
489,"{'func name': '_infer_precision', 'comments': 'Infer an appropriate precision for _round_frac\n\n\n', 'stemmed comments': ['_round_frac', 'precis', 'appropri', 'infer']}"
490,"{'func name': '_convert_listlike', 'comments': 'Convert a list of objects to a timedelta index object.\n\n\n', 'stemmed comments': ['index', 'list', 'timedelta', 'convert', 'object']}"
491,"{'func name': '_generate_regular_range', 'comments': '', 'stemmed comments': []}"
492,"{'func name': 'timedelta_range', 'comments': ""Return a fixed frequency TimedeltaIndex, with day as the default frequency.\n\nParameters ---------- start : str or timedelta-like, default None Left bound for generating timedeltas. end : str or timedelta-like, default None Right bound for generating timedeltas. periods : int, default None Number of periods to generate. freq : str or DateOffset, default 'D' Frequency strings can have multiples, e.g. '5H'. name : str, default None Name of the resulting TimedeltaIndex. closed : str, default None Make the interval closed with respect to the given frequency to the 'left', 'right', or both sides (None).\n##### Returns\n* **rng **: TimedeltaIndex\n\n* **<https**: //pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\n* **TimedeltaIndex(['1 days 00**: 00\n\n"", 'stemmed comments': ['fix', 'paramet', 'end', 'side', 'name', 'rng', 'left', 'close', 'bound', 'number', '5H', '//pandaspydataorg/pandasdocs/stable/user_guide/timeserieshtml', 'none', '__', 'gener', 'timedeltalik', 'timedeltaindex', 'offsetalias', '1', 'string', 'timedelta', 'dateoffset', 'day', 'eg', 'interv', 'http', 'start', 'make', 'default', 'int', 'str', 'multipl', '<', '00', 'return', 'D', 'given', 'freq', 'frequenc', '>', '[', 'result', 'right', 'respect', 'period']}"
493,"{'func name': 'format_dateaxis', 'comments': 'Pretty-formats the date axis (x-axis).\n\nMajor and minor ticks are automatically set for the frequency of the current underlying series.\n\nAs the dynamic mode is activated by default, changing the limits of the x axis will intelligently change the positions of the ticks.\n', 'stemmed comments': ['set', 'As', 'underli', 'x', 'dynam', 'chang', 'major', 'default', 'tick', 'current', 'seri', 'minor', 'limit', 'frequenc', 'automat', 'prettyformat', 'mode', 'date', 'axi', 'activ', 'intellig', 'posit', 'xaxi']}"
494,"{'func name': '_get_xlim', 'comments': '', 'stemmed comments': []}"
495,"{'func name': 'cartesian_product', 'comments': 'Numpy version of itertools.product. Sometimes faster (for large inputs)...\n\nParameters ---------- X : list-like of list-likes\n##### Returns\n* **product **: list of ndarrays\n\n* **itertools.product **: Cartesian product of input iterables.  Equivalent to\n    nested for-loops.\n\n', 'stemmed comments': ['paramet', 'ndarray', 'cartesian', 'product', 'version', 'itertoolsproduct', 'numpi', 'larg', 'sometim', 'faster', 'equival', 'return', 'X', 'iter', 'listlik', 'nest', 'forloop', 'list', 'input']}"
