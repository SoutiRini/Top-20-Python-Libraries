0,"{'func name': 'parallel_info', 'comments': 'Returns detailed string with parallelization settings\n\n\n', 'stemmed comments': ['set', 'detail', 'parallel', 'string', 'return']}"
1,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
2,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
3,"{'func name': '_parseFile', 'comments': '', 'stemmed comments': []}"
4,"{'func name': '_get_win_folder_with_jna', 'comments': '', 'stemmed comments': []}"
5,"{'func name': 'wait', 'comments': 'Forces completion of a `torch.jit.Future[T]` asynchronous task, returning the result of the task. See :func:`~fork` for docs and examples. Arguments: func (torch.jit.Future[T]): an asynchronous task reference, created through `torch.jit.fork` Returns: `T`: the return value of the the completed task\n\n\n', 'stemmed comments': ['refer', 'complet', 'T', 'see', 'argument', 'exampl', 'valu', ']', 'torchjitfork', '[', 'asynchron', 'result', 'func', 'torchjitfutur', 'return', 'task', 'doc', 'creat', 'forc', '~fork']}"
6,"{'func name': 'register_after_fork', 'comments': 'Register a callable to be executed in the child process after a fork.\n\nNote: In python < 3.7 this will only work with processes created using the ``multiprocessing`` module. In python >= 3.7 it also works with ``os.fork()``.\n\nArguments: func (function): Function taking no arguments to be called in the child after fork\n', 'stemmed comments': ['python', 'regist', '>', '<', 'note', 'also', 'argument', 'fork', 'multiprocess', 'osfork', 'callabl', '37', '=', 'call', 'function', 'func', 'work', 'execut', 'take', 'process', 'child', 'In', 'creat', 'modul', 'use']}"
7,"{'func name': '_find_builtin', 'comments': '', 'stemmed comments': []}"
8,"{'func name': 'protos_to_graph_def', 'comments': 'Convert a set of Caffe2 net definitions to a Tensorflow graph.\n\n\n##### Args\n* **net_defs**: List of caffe2_pb2.NetDef protobufs representing computation\n    graphs.\n\n* **shapes**: Dictionary mapping blob names to their shapes/dimensions.\n\n##### Returns\n', 'stemmed comments': ['comput', 'shapes/dimens', 'caffe2_pb2netdef', 'net', 'net_def', 'protobuf', 'dictionari', 'definit', 'caffe2', 'map', 'convert', 'shape', 'list', 'return', 'blob', 'arg', 'set', 'repres', 'tensorflow', 'graph', 'name']}"
9,"{'func name': '_prepare_caffe2', 'comments': '', 'stemmed comments': []}"
10,"{'func name': 'hash_build_arguments', 'comments': '', 'stemmed comments': []}"
11,"{'func name': 'make_mat', 'comments': '', 'stemmed comments': []}"
12,"{'func name': 'freeze', 'comments': ""Freezing a :class:`ScriptModule` will clone it and attempt to inline the cloned module's submodules, parameters, and attributes as constants in the TorchScript IR Graph. By default, `forward` will be preserved, as well as attributes & methods specified in `preserved_attrs`. Additionally, any attribute that is modified within a preserved method will be preserved.\n\nFreezing currently only accepts ScriptModules that are in eval mode.\n\nArguments: mod (:class:`ScriptModule`): a module to be frozen\n\npreserved_attrs (Optional[List[str]]): a list of attributes to preserve in addition to the forward method.\n##### Attributes\n##### Returns\n* **Frozen **: class\n\n* **ple (Freezing a simple module with a Parameter)**: \n\n* **estcode**: \n\n* **class MyModule(torch.nn.Module)**: def __init__(self, N, M)\n\n* **class MyModule2(torch.nn.Module)**: def __init__(self)\n\n* ****: \n\n"", 'stemmed comments': ['accept', 'submodul', 'specifi', 'within', 'mymodul', 'scriptmodul', 'argument', 'str', 'By', 'mod', 'current', 'estcod', ']', 'self', 'method', 'addit', 'attribut', 'frozen', '[', 'simpl', '&', 'paramet', 'mymodule2', 'list', 'torchscript', 'preserved_attr', 'return', 'forward', 'attempt', 'well', 'option', '__init__', 'mode', 'modifi', 'inlin', 'torchnnmodul', 'M', 's', 'constant', 'default', 'clone', 'eval', 'modul', 'IR', 'graph', 'class', 'freez', 'def', 'ple', 'N', 'preserv']}"
13,"{'func name': '_get_stream', 'comments': 'Gets a background stream for copying between CPU and GPU\n\n\n', 'stemmed comments': ['copi', 'cpu', 'background', 'gpu', 'stream', 'get']}"
14,"{'func name': '_graph_for', 'comments': '', 'stemmed comments': []}"
15,"{'func name': '_TensorCPU_reshape', 'comments': '', 'stemmed comments': []}"
16,"{'func name': '_disable_emit_hooks_decorator', 'comments': '', 'stemmed comments': []}"
17,"{'func name': 'symeig', 'comments': 'Return eigenpairs of A with specified ordering.\n\n\n', 'stemmed comments': ['A', 'specifi', 'order', 'eigenpair', 'return']}"
18,"{'func name': 'LOBPCG_call_tracker', 'comments': '', 'stemmed comments': []}"
19,"{'func name': 'pca_lowrank', 'comments': 'Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.\n\nThis function returns a namedtuple ``(U, S, V)`` which is the nearly optimal approximation of a singular value decomposition of a centered matrix :math:`A` such that :math:`A = U diag(S) V^T`.\n\n.. note:: The relation of ``(U, S, V)`` to PCA is as follows:\n\n- :math:`A` is a data matrix with ``m`` samples and ``n`` features\n\n- the :math:`V` columns represent the principal directions\n\n- :math:`S ** 2 / (m\n\n- 1)` contains the eigenvalues of :math:`A^T A / (m\n\n- 1)` which is the covariance of ``A`` when ``center=True`` is provided.\n\n- ``matmul(A, V[:, :k])`` projects data to the first k principal components\n\n.. note:: Different from the standard SVD, the size of returned matrices depend on the specified rank and q values as follows:\n\n- :math:`U` is m x q matrix\n\n- :math:`S` is q-vector\n\n- :math:`V` is n x q matrix\n\n.. note:: To obtain repeatable results, reset the seed for the pseudorandom number generator\n\nArguments:\n\nA (Tensor): the input tensor of size :math:`(*, m, n)`\n\nq (int, optional): a slightly overestimated rank of :math:`A`. By default, ``q = min(6, m, n)``.\n\ncenter (bool, optional): if True, center the input tensor, otherwise, assume that the input is centered.\n\nniter (int, optional): the number of subspace iterations to conduct; niter must be a nonnegative integer, and defaults to 2.\n\nReferences::\n\n- Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions, arXiv:0909.4061 [math.NA; math.PR], 2009 (available at `arXiv <http://arxiv.org/abs/0909.4061>`_).\n', 'stemmed comments': ['refer', 'direct', 'approxim', 'optim', 'By', 'valu', 'perform', 'rank', '=', 'follow', 'diag', 'n', 'mathna', 'svd', 'batch', 'integ', 'a^t', 'obtain', 'repres', 'reset', 'differ', 'algorithm', 'otherwis', '//arxivorg/abs/09094061', '2', 'http', 'math', '<', 'pseudorandom', '>', '1', 'covari', 'halko', 'input', 'nearli', 'x', 'arxiv', 'true', 'sampl', 'arxiv09094061', 'featur', 'function', 'pergunnar', 'conduct', 'thi', ';', 'iter', 'v^t', 'k', 'gener', 'structur', 'spars', 'option', 'repeat', 'int', 'slightli', 'pca', 'first', 'provid', 'specifi', 'joel', 'note', 'matmul', 'contain', 'q', 'construct', 'martinsson', 'size', 'V', 'the', 'A', 'depend', 'result', 'nonneg', 'must', 'S', 'overestim', 'project', '_', 'decomposit', 'nathan', 'find', 'bool', 'seed', 'center', '6', 'avail', 'default', 'lowrank', 'singular', 'min', 'niter', 'tropp', 'matrix', 'subspac', 'qvector', 'eigenvalu', 'linear', 'probabilist', 'number', 'column', 'compon', 'argument', 'data', ']', 'To', '[', 'relat', '2009', 'center=tru', 'namedtupl', 'return', 'standard', 'mathpr', 'assum', 'analysi', 'random', 'princip', 'U', 'matric', '/', 'tensor']}"
20,"{'func name': 'update_names', 'comments': ""There are two usages\n\ntensor.rename(*names) returns a view on tensor with named dims `names`. `names` must be of length `tensor.dim()`; otherwise, if '...' is in `names`, then it is expanded greedily to be equal to the corresponding names from `tensor.names`.\n\nFor example, ``` >>> x = torch.empty(2, 3, 5, 7, names=('N', 'C', 'H', 'W')) >>> x.rename('...', 'height', 'width').names ('N', 'C', 'height', 'width')\n\n>>> x.rename('batch', '...', 'width').names ('batch', 'C', 'H', 'width') ```\n\ntensor.rename(**rename_map) returns a view on tensor that has rename dims as specified in the mapping `rename_map`.\n\nFor example, ``` >>> x = torch.empty(2, 3, 5, 7, names=('N', 'C', 'H', 'W')) >>> x.rename(W='width', H='height').names ('N', 'C', 'height', 'width') ```\n\nFinally, tensor.rename has an in-place version called tensor.rename_.\n"", 'stemmed comments': ['tensornam', '2', '>', 'correspond', 'specifi', 'H', 'two', 'w=width', 'xrenam', 'renam', 'version', '5', 'inplac', 'exampl', 'x', 'view', 'tensorrename_', '3', 'h=height', '7', 'map', 'height', '=', 'names=', 'final', 'usag', 'call', 'torchempti', 'length', 'for', 'W', 'expand', 'dim', 'tensordim', 'rename_map', 'must', ';', 'equal', 'width', 'batch', 'there', 'return', 'C', 'tensorrenam', 'greedili', 'tensor', 'N', 'name', 'otherwis']}"
21,"{'func name': 'compare_model_outputs', 'comments': ""Compare output activations between float and quantized models at corresponding locations for the same input. Return a dict with key corresponding to quantized module names and each entry being a dictionary with two keys 'float' and 'quantized', containing the activations of quantized model and float model at matching locations. This dict can be used to compare and compute the propagation quantization error.\n\nExample usage: act_compare_dict = compare_model_outputs(float_model, qmodel, data) for key in act_compare_dict: print(key, compute_error(act_compare_dict[key]['float'], act_compare_dict[key]['quantized'].dequantize()))\n##### Args\n* **float_model**: float model used to generate the q_model\n\n* **q_model**: model quantized from float_model\n\n* **data**: input data used to run the prepared float_model and q_model\n\n* **Logger**: type of logger to be attached to float_module and q_module\n\n* **white_list**: list of module types to attach logger\n\n* **rn**: \n\n* **act_compare_dict**: dict with key corresponding to quantized module names\n\n"", 'stemmed comments': ['activ', 'comput', 'type', 'correspond', 'two', 'act_compare_dict', 'qmodel', 'run', 'rn', 'contain', 'input', 'data', 'q_modul', 'output', 'exampl', 'logger', 'dequant', 'dict', ']', 'float', 'compare_model_output', 'quantiz', 'white_list', 'dictionari', 'q_model', 'error', 'propag', 'use', '=', 'compute_error', 'usag', '[', 'prepar', 'float_model', 'compar', 'attach', 'thi', 'match', 'entri', 'locat', 'list', 'gener', 'return', 'model', 'arg', 'print', 'modul', 'key', 'name', 'float_modul']}"
22,"{'func name': 'parse', 'comments': '', 'stemmed comments': []}"
23,"{'func name': 'dl_open_guard', 'comments': 'Context manager to set the RTLD_GLOBAL dynamic linker flag while we open a shared library to load custom operators.\n\n\n', 'stemmed comments': ['flag', 'set', 'rtld_global', 'load', 'oper', 'manag', 'custom', 'context', 'dynam', 'librari', 'open', 'share', 'linker']}"
24,"{'func name': 'get_overridable_functions', 'comments': 'List functions that are overridable via __torch_function__\n\n\n##### Returns\n', 'stemmed comments': ['overrid', 'function', '__torch_function__', 'list', 'via', 'return']}"
25,"{'func name': 'restore_type_tag', 'comments': '', 'stemmed comments': []}"
26,"{'func name': 'node_proto', 'comments': 'Creates an object matching https://github.com/tensorflow/tensorboard/blob/master/tensorboard/compat/proto/node_def.proto\n\n\n', 'stemmed comments': ['//githubcom/tensorflow/tensorboard/blob/master/tensorboard/compat/proto/node_defproto', 'http', 'match', 'creat', 'object']}"
27,"{'func name': 'graph', 'comments': 'This method processes a PyTorch model and produces a `GraphDef` proto that can be logged to TensorBoard.\n\n\n##### Args\n* **model (PyTorch module)**: The model to be parsed.\n\n* **args (tuple)**: input tensor[s] for the model.\n\n* **verbose (bool)**: Whether to print out verbose information while\n  processing.\n\n', 'stemmed comments': ['verbos', 'pars', 'inform', 'input', ']', 'proto', 'method', '[', 'the', 'thi', 'pytorch', 'model', 'arg', 'process', 'print', 'tupl', 'bool', 'log', 'graphdef', 'modul', 'produc', 'whether', 'tensor', 'tensorboard']}"
28,"{'func name': 'lazy_bind', 'comments': 'Returns a function that lazily binds `unbound_method` to a provided Module IValue, then invokes the method. We do this so that any Python shenanigans that will poison type sharing are impossible at compile time.\n\n\n', 'stemmed comments': ['type', 'python', 'share', 'compil', 'time', 'invok', 'method', 'poison', 'function', 'bind', 'We', 'return', 'lazili', 'unbound_method', 'imposs', 'modul', 'ivalu', 'shenanigan', 'provid']}"
29,"{'func name': 'legacy_get_enum', 'comments': '', 'stemmed comments': []}"
30,"{'func name': '_unwrap_optional', 'comments': '', 'stemmed comments': []}"
31,"{'func name': 'validate_map_location', 'comments': '', 'stemmed comments': []}"
32,"{'func name': 'bind_method', 'comments': '', 'stemmed comments': []}"
33,"{'func name': '_set_jit_function_cache', 'comments': '', 'stemmed comments': []}"
34,"{'func name': 'add_docstr_all', 'comments': '', 'stemmed comments': []}"
35,"{'func name': 'add_docstr_all', 'comments': '', 'stemmed comments': []}"
36,"{'func name': '_str', 'comments': '', 'stemmed comments': []}"
37,"{'func name': 'visualize_rec', 'comments': 'Recursive part of visualize (basically skips setting up the input and output nodes).\n\n\n', 'stemmed comments': ['node', 'set', 'recurs', 'visual', 'skip', 'basic', 'part', 'input', 'output']}"
38,"{'func name': 'merge_dicts', 'comments': '', 'stemmed comments': []}"
39,"{'func name': '_get_trace_graph', 'comments': '.. warning:: This function is internal-only and should only be used by the ONNX exporter. If you are trying to get a graph through tracing, please go through the public API instead\n\ntrace = torch.jit.trace(nn.LSTMCell(), (input, hidden)) trace_graph = trace.graph\n\nTrace a function or model, returning a tuple consisting of the both the *trace* of an execution, as well as the original return value. If return_inputs, also returns the trace inputs as part of the tuple\n\nTracing is guaranteed not to change the semantics of the function/module that is traced.\n\nArguments: f (torch.nn.Module or function): the function or module to be traced. args (tuple or Tensor): the positional arguments to pass to the function/module to be traced.\n\nA non-tuple is assumed to be a single positional argument to be passed to the model. kwargs (dict): the keyword arguments to pass to the function/module to be traced.\n\nExample (trace a cell):\n\n.. testcode::\n\ntrace = torch.jit.trace(nn.LSTMCell(), (input, hidden))\n', 'stemmed comments': ['posit', 'onnx', 'cell', 'guarante', 'also', 'function/modul', 'part', 'input', 'argument', 'pass', 'valu', 'exampl', 'dict', 'tensor', 'chang', 'nontupl', 'testcod', 'trace_graph', '=', 'pleas', 'A', 'singl', 'kwarg', 'function', 'return_input', 'thi', 'origin', 'public', 'keyword', 'consist', 'torchjittrac', 'return', 'model', 'warn', 'execut', 'well', 'arg', 'internalonli', 'assum', 'torchnnmodul', 'tupl', 'If', 'tracegraph', 'f', 'instead', 'tri', 'semant', 'graph', 'api', 'nnlstmcell', 'export', 'trace', 'hidden', 'modul', 'use', 'get', 'go']}"
40,"{'func name': 'get_source_lines_and_file', 'comments': 'Wrapper around inspect.getsourcelines and inspect.getsourcefile.\n\n\n', 'stemmed comments': ['inspectgetsourcefil', 'wrapper', 'around', 'inspectgetsourcelin']}"
41,"{'func name': 'convert_to_HWC', 'comments': '', 'stemmed comments': []}"
42,"{'func name': '_get_device_index', 'comments': ""Gets the device index from :attr:`device`, which can be a torch.device object, a Python integer, or ``None``.\n\nIf :attr:`device` is a torch.device object, returns the device index if it has index. Note that for a device without a specified index, i.e., ``torch.device('xxx')``, this will return the current default device of that type if :attr:`optional` is ``True``. If :attr:`allow_cpu` is ``True``, CPU devices will be accepted and ``-1`` will be returned in this case.\n\nIf :attr:`device` is a Python integer, it is returned as is.\n\nIf :attr:`device` is ``None``, this will return the current default device of the supported runtime platform if :attr:`optional` is ``True``. i.e., the current default CUDA device will be returned if CUDA runtime is supported.\n"", 'stemmed comments': ['type', 'python', 'none', 'accept', 'specifi', 'torchdevic', '1', 'note', 'cuda', 'support', 'case', 'true', 'current', 'devic', 'cpu', 'attr', 'object', 'index', 'runtim', 'ie', 'allow_cpu', 'integ', 'return', 'option', 'xxx', 'If', 'default', 'platform', 'get', 'without']}"
43,"{'func name': '_dummy_type', 'comments': '', 'stemmed comments': []}"
44,"{'func name': 'vmap', 'comments': 'vmap is the vectorizing map. Returns a new function that maps `func` over some dimension of the inputs. Semantically, vmap pushes the map into PyTorch operations called by `func`, effectively vectorizing those operations.\n\nvmap is useful for handling batch dimensions: one can write a function `func` that runs on examples and the lift it to a function that can take batches of examples with `vmap(func)`. Furthermore, it is possible to use vmap to obtain batched gradients when composed with autograd.\n##### Args\n* **func (function)**: A Python function that takes one or more arguments.\n    Must return one or more Tensors.\n\n* **in_dims (int or Tuple[Optional[int]])**: Specifies which dimension of the\n    inputs should be mapped over. If `in_dims` is a Tuple, then it should have\n    one element per input. If the `in_dim` for a particular input is\n    None, then that indicates there is no map dimension. Default\n\n* **out_dims (int or Tuple[int])**: Specifies where the mapped dimension\n    should appear in the outputs. If `out_dims` is a Tuple, then it should\n    have one element per output. Default\n\n##### Returns\n* **arning**: \n\n', 'stemmed comments': ['possibl', 'python', 'none', 'specifi', 'vector', 'lift', 'run', 'indic', 'element', 'effect', 'input', 'argument', 'output', 'exampl', 'tensor', 'handl', ']', 'particular', 'dimens', 'map', '[', 'new', 'call', 'gradient', 'one', 'A', 'push', 'function', 'vmap', 'must', 'write', 'func', 'furthermor', 'per', 'pytorch', 'out_dim', 'appear', 'batch', 'arn', 'return', 'take', 'option', 'arg', 'in_dim', 'obtain', 'tupl', 'If', 'int', 'compos', 'oper', 'default', 'autograd', 'semant', 'use']}"
45,"{'func name': 'adagrad_sparse_test_helper', 'comments': '', 'stemmed comments': []}"
46,"{'func name': 'batch_mat_mul', 'comments': '', 'stemmed comments': []}"
47,"{'func name': 'allcompare_process', 'comments': '', 'stemmed comments': []}"
48,"{'func name': 'ann_to_type', 'comments': '', 'stemmed comments': []}"
49,"{'func name': 'initialize_params_from_file', 'comments': '', 'stemmed comments': []}"
50,"{'func name': 'trainFun', 'comments': '', 'stemmed comments': []}"
51,"{'func name': 'rpc_async', 'comments': 'Make a non-blocking RPC call to run function ``func`` on worker ``to``. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe. This method will immediately return a :class:`~torch.futures.Future` that can be awaited on.\n\nArguments: to (str or WorkerInfo): id or name of the destination worker. func (callable): a callable function, such as Python callables, builtin operators (e.g. :meth:`~torch.add`) and annotated TorchScript functions. args (tuple): the argument tuple for the ``func`` invocation. kwargs (dict): is a dictionary of keyword arguments for the ``func`` invocation. timeout (float, optional): timeout in seconds to use for this RPC. If the RPC does not complete in this amount of time, an exception indicating it has timed out will be raised. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with ``_set_rpc_timeout`` is used.\n##### Returns\n* **``kwargs`` can be retrieved from the **: class\n\n* **arning **: \n\n* ****: class\n\n* **ple**: \n\n* **on both workers. Refer to **: meth\n\n* **Then run the following code in two different processes**: \n\n* **>>> # On worker 0**: \n\n* **>>> # On worker 1**: \n\n* **>>> # On both workers**: \n\n* **>>> def my_script_add(t1, t2)**: \n\n', 'stemmed comments': ['refer', 'python', 'workerinfo', 'annot', 'nonblock', 'messag', 'then', 'valu', 'dict', 'dictionari', 'error', 't1', 'await', 'call', 'follow', 'never', 'kwarg', 'immedi', 'func', 'differ', 'eg', 'float', 'tupl', 'infinit', '~torchfuturesfutur', '>', '1', 'timeout', 'str', 'time', 'meth', 'invoc', 'builtin', 'function', 'thi', 'keyword', 't2', 'arn', 'torchscript', 'rpc', 'option', 'arg', 'code', 'If', 'make', 'parallel', 'sent', 'On', 'ple', 'use', 'provid', 'two', 'run', 'my_script_add', 'receiv', 'id', 'destin', 'A', 'rais', 'ie', 'threadsaf', 'initi', 'amount', 'second', 'set', 'oper', 'default', 'except', 'def', 'complet', '0', 'indic', 'argument', 'retriev', 'method', 'callabl', 'worker', '~torchadd', '_set_rpc_timeout', 'return', 'execut', 'process', 'class', 'name']}"
52,"{'func name': 'calculate_ap', 'comments': '', 'stemmed comments': []}"
53,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
54,"{'func name': 'get_current_scope', 'comments': '', 'stemmed comments': []}"
55,"{'func name': 'depth_concat', 'comments': 'The old depth concat function - we should move to use concat.\n\n\n', 'stemmed comments': ['the', 'function', 'use', 'old', 'concat', 'move', 'depth']}"
56,"{'func name': 'apply_soft_coverage_attention', 'comments': '', 'stemmed comments': []}"
57,"{'func name': 'custom_bwd', 'comments': 'Helper decorator for backward methods of custom autograd functions (subclasses of :class:`torch.autograd.Function`). Ensures that ``backward`` executes with the same autocast state as ``forward``. See the :ref:`example page<amp-custom-examples>` for more detail.\n\n\n', 'stemmed comments': ['>', '<', 'see', 'state', 'torchautogradfunct', 'helper', 'backward', 'exampl', 'method', 'subclass', 'custom', 'function', 'ref', 'autocast', 'forward', 'execut', 'decor', 'detail', 'ampcustomexampl', 'autograd', 'class', 'page', 'ensur']}"
58,"{'func name': '_tensorpipe_init_backend_handler', 'comments': '', 'stemmed comments': []}"
59,"{'func name': 'convertAttributeProto', 'comments': 'Convert an ONNX AttributeProto into an appropriate Python object for the type.\n\nNB: Tensor attribute gets returned as the straight proto.\n', 'stemmed comments': ['type', 'onnx', 'python', 'tensor', 'straight', 'proto', 'object', 'attribut', 'NB', 'return', 'convert', 'attributeproto', 'appropri', 'get']}"
60,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
61,"{'func name': 'basic_rnn_reference', 'comments': '', 'stemmed comments': []}"
62,"{'func name': '_inputs', 'comments': '', 'stemmed comments': []}"
63,"{'func name': 'generate_rois_rotated', 'comments': '', 'stemmed comments': []}"
64,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
65,"{'func name': 'bench_group', 'comments': '', 'stemmed comments': []}"
66,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
67,"{'func name': 'generate_c2_gradient_test', 'comments': 'This function creates Caffe2 op test based on the given operator\n\n\n', 'stemmed comments': ['test', 'function', 'thi', 'creat', 'oper', 'caffe2', 'base', 'op', 'given']}"
68,"{'func name': '_build_test', 'comments': 'Generate PyTorch/Caffe2 tests of operators with different inputs. Args: configs: a dictionary that has the input shapes bench_op: a subclass of Caffe2BenchmarkBase/TorchBenchmarkBase which includes tensor creation and operator execution OperatorTestCase: a named tuple to save the metadata of an test run_backward: a bool parameter indicating backward path op_name_function: a dictionary includes operator name and function\n\n\n', 'stemmed comments': ['includ', 'path', 'indic', 'input', 'backward', 'run_backward', 'test', 'operatortestcas', 'dictionari', 'caffe2benchmarkbase/torchbenchmarkbas', 'subclass', 'shape', 'function', 'pytorch/caffe2', 'config', 'bench_op', 'paramet', 'gener', 'creation', 'execut', 'save', 'arg', 'tupl', 'bool', 'oper', 'differ', 'op_name_funct', 'tensor', 'name', 'metadata']}"
69,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
70,"{'func name': 'create_pytorch_op_test_case', 'comments': ""This method is used to generate est. func_name is a global unique string. For PyTorch add operator with M=8, N=2, K=1, tag = long, here are the values for the members in test_case: op.module_name: add framework: PyTorch test_config: TestConfig(test_name='add_M8_N2_K1', input_config='M: 8, N: 2, K: 1', tag='long', run_backward=False) func_name: addPyTorchTestConfig(test_name='add_M8_N2_K1', input_config='M: 8, N: 2, K: 1', tag='long', run_backward=False)\n\n\n"", 'stemmed comments': ['2', 'n=2', 'test_cas', '1', 'uniqu', 'string', 'K', 'test_config', 'valu', 'm=8', 'func_nam', 'method', 'test_name=add_m8_n2_k1', 'opmodule_nam', 'input_config=', '=', 'long', 'for', 'member', 'framework', 'addpytorchtestconfig', 'tag=long', 'thi', 'global', 'pytorch', 'gener', 'add', 'k=1', '8', 'testconfig', 'oper', 'est', 'tag', 'use', 'N', 'run_backward=fals', 'M']}"
71,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
72,"{'func name': 'generate_pt_tests_from_op_list', 'comments': 'This function creates pt op tests one by one from a list of dictionaries. ops_list is a list of dictionary. Each dictionary includes the name of the operator and the math operation. Here is an example of using this API: unary_ops_configs = op_bench.config_list( attrs=[...], attr_names=[""M"", ""N""], ) unary_ops_list = op_bench.op_list( attr_names=[""op_name"", ""op_func""], attrs=[ [""abs"", torch.abs], ], ) class UnaryOpBenchmark(op_bench.TorchBenchmarkBase): def init(self, M, N, op_name, op_func): ... def forward(self): ... op_bench.generate_pt_tests_from_op_list(unary_ops_list, unary_ops_configs, UnaryOpBenchmark)\n\n\n', 'stemmed comments': ['includ', 'attrs=', 'op_nam', 'math', 'op_benchgenerate_pt_tests_from_op_list', 'each', 'ops_list', 'unary_ops_config', 'exampl', 'torchab', 'test', ']', 'self', 'dictionari', 'op_benchtorchbenchmarkbas', '=', '[', 'one', 'function', 'unaryopbenchmark', 'thi', 'attr_names=', 'list', 'forward', 'here', 'pt', 'op_func', 'op_benchop_list', 'op_benchconfig_list', 'oper', 'creat', 'ab', 'api', 'def', 'class', 'init', 'unary_ops_list', 'op', 'use', 'N', 'name', 'M']}"
73,"{'func name': 'process_arg_list', 'comments': '', 'stemmed comments': []}"
74,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
75,"{'func name': 'register_benchmark_class', 'comments': '', 'stemmed comments': []}"
76,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
77,"{'func name': '_clamp_by_zero', 'comments': '', 'stemmed comments': []}"
78,"{'func name': 'gen_multiple_boxes', 'comments': '', 'stemmed comments': []}"
79,"{'func name': 'register_broadcast_ops', 'comments': '', 'stemmed comments': []}"
80,"{'func name': 'plot_function', 'comments': 'Plot a function on the current plot. The additional arguments may be used to specify color, alpha, etc.\n\n\n', 'stemmed comments': ['the', 'etc', 'current', 'specifi', 'alpha', 'function', 'may', 'color', 'addit', 'argument', 'use', 'plot']}"
81,"{'func name': 'is_hip_clang', 'comments': '', 'stemmed comments': []}"
82,"{'func name': 'build_caffe2', 'comments': '', 'stemmed comments': []}"
83,"{'func name': 'bundle_large_tensor', 'comments': 'Wrap a tensor to allow bundling regardless of size.\n\n\n', 'stemmed comments': ['wrap', 'regardless', 'allow', 'tensor', 'size', 'bundl']}"
84,"{'func name': 'add_blob', 'comments': '', 'stemmed comments': []}"
85,"{'func name': 'setUpModule', 'comments': '', 'stemmed comments': []}"
86,"{'func name': 'TranslateReduction', 'comments': '', 'stemmed comments': []}"
87,"{'func name': 'gen_forward_pass_builder_fun', 'comments': '', 'stemmed comments': []}"
88,"{'func name': 'gen_param_update_builder_fun', 'comments': '', 'stemmed comments': []}"
89,"{'func name': 'rnn_tanh_cell', 'comments': '', 'stemmed comments': []}"
90,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
91,"{'func name': 'check_bc', 'comments': '', 'stemmed comments': []}"
92,"{'func name': 'local_copy_op', 'comments': '', 'stemmed comments': []}"
93,"{'func name': 'epoch_limiter', 'comments': 'Creates a task that will output True when a given number of epochs has finished.\n\n\n', 'stemmed comments': ['true', 'task', 'finish', 'creat', 'number', 'output', 'given', 'epoch']}"
94,"{'func name': 'scoped_name', 'comments': '', 'stemmed comments': []}"
95,"{'func name': 'checkpoint_sequential', 'comments': ""A helper function for checkpointing sequential models.\n\nSequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will run in :func:`torch.no_grad` manner, i.e., not storing the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass.\n\nSee :func:`~torch.utils.checkpoint.checkpoint` on how checkpointing works.\n\n.. warning:: Checkpointing doesn't work with :func:`torch.autograd.grad`, but only with :func:`torch.autograd.backward`.\n\n.. warning: At least one of the inputs needs to have :code:`requires_grad=True` if grads are needed for model inputs, otherwise the checkpointed part of the model won't have gradients.\n\n.. warning: Since PyTorch 1.4, it allows only one Tensor as the input and intermediate outputs, just like :class:`torch.nn.Sequential`.\n##### Args\n* **functions**: A\n\n* **segments**: Number of chunks to create in the model\n\n* **input**: A Tensor that is input to\n\n* **preserve_rng_state(bool, optional, default=True)**: Omit stashing and restoring\n    the RNG state during each checkpoint.\n\n##### Returns\n* **Output of running **: attr\n\n* **ple**: \n\n"", 'stemmed comments': ['activ', 'store', '14', 'stash', 'rerun', 'see', 'need', 'sequenti', 'requires_grad=tru', 'number', 'run', 'checkpoint', 'state', 'part', 'input', 'torchautogradgrad', 'pass', 'helper', 'like', 'backward', 'output', 'preserve_rng_st', 'wo', 'ple', 'grad', 'torchno_grad', 'least', 'attr', 'variou', 'gradient', 'the', 'one', 'A', 'default=tru', 'function', 'divid', 'ie', 'omit', 'therefor', 'manner', 'segment', 'func', 'list', 'pytorch', 'allow', 'chunk', 'option', 'last', 'intermedi', 'model', 'execut', 'save', 'work', 'modules/funct', 'all', 'warn', 'code', 'sinc', 'arg', 'bool', 'torchnnsequenti', 'order', 'At', 'creat', 'except', 'nt', 'torchautogradbackward', 'restor', 'class', 'return', '~torchutilscheckpointcheckpoint', 'rng', 'tensor', 'otherwis']}"
96,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
97,"{'func name': 'get_and_check_clang_format', 'comments': ""Download a platform-appropriate clang-format binary if one doesn't already exist at the expected location and verify that it is the right binary by checking its SHA1 hash against the expected hash.\n\n\n"", 'stemmed comments': ['one', 'binari', 'hash', 'expect', 'right', 'sha1', 'nt', 'platformappropri', 'locat', 'download', 'check', 'verifi', 'clangformat', 'exist', 'alreadi']}"
98,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
99,"{'func name': 'clip_grad_value_', 'comments': 'Clips gradient of an iterable of parameters at specified value.\n\nGradients are modified in-place.\n\nArguments: parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a single Tensor that will have gradients normalized clip_value (float or int): maximum allowed value of the gradients. The gradients are clipped in the range :math:`\\left[\\text{-clip\\_value}, \\text{clip\\_value}\\right]`\n', 'stemmed comments': ['math', 'specifi', 'clip_valu', '\\text', 'clip\\_valu', 'argument', 'rang', 'inplac', 'valu', 'tensor', ']', '{', 'clip', '[', 'gradient', 'the', '\\right', 'singl', 'paramet', 'iter', 'allow', '}', 'maximum', 'modifi', '\\left', 'int', 'normal', 'float']}"
100,"{'func name': 'get_cmake_cache_variables_from_file', 'comments': 'Gets values in CMakeCache.txt into a dictionary.\n\nArguments: cmake_cache_file: A CMakeCache.txt file object.\n##### Returns\n* **dict**: A ``dict`` containing the value of cached CMake variables.\n\n', 'stemmed comments': ['file', 'dict', 'A', 'cmake', 'cach', 'cmakecachetxt', 'variabl', 'dictionari', 'object', 'return', 'argument', 'cmake_cache_fil', 'get', 'contain', 'valu']}"
101,"{'func name': 'default_collate', 'comments': 'Puts each data field into a tensor with outer dimension batch size\n\n\n', 'stemmed comments': ['field', 'outer', 'dimens', 'put', 'batch', 'data', 'tensor', 'size']}"
102,"{'func name': 'distribute_fpn_ref', 'comments': '', 'stemmed comments': []}"
103,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
104,"{'func name': 'gather', 'comments': 'Gathers tensors from multiple GPU devices.\n\nArguments: tensors (Iterable[Tensor]): an iterable of tensors to gather. Tensor sizes in all dimensions other than :attr:`dim` have to match. dim (int, optional): a dimension along which the tensors will be concatenated. Default: ``0``. destination (torch.device, str, or int, optional): the output device. Can be CPU or CUDA. Default: the current CUDA device. out (Tensor, optional, keyword-only): the tensor to store gather result. Its sizes must match those of :attr:`tensors`, except for :attr:`dim`, where the size must equal ``sum(tensor.size(dim) for tensor in tensors)``. Can be on CPU or CUDA.\n\n.. note:: :attr:`destination` must not be specified when :attr:`out` is specified.\n##### Returns\n* **- If **: attr\n\n', 'stemmed comments': ['store', 'specifi', 'torchdevic', '0', 'note', 'gpu', 'argument', 'str', 'cuda', 'it', 'output', 'multipl', 'current', ']', 'devic', 'cpu', 'dimens', 'attr', 'tensors', '[', 'sum', 'size', 'along', 'destin', 'result', 'dim', 'must', 'can', 'equal', 'match', 'iter', 'return', 'option', 'keywordonli', 'If', 'int', 'concaten', 'default', 'except', 'gather', 'tensor']}"
105,"{'func name': 'tf32_on_and_off', 'comments': '', 'stemmed comments': []}"
106,"{'func name': 'skipCUDAIfNoCudnn', 'comments': '', 'stemmed comments': []}"
107,"{'func name': 'simple_sparse_reduce_tests', 'comments': 'Generate a number of basic test cases for sparse reduction. These cover tensors with a varying number of sparse dimensions and a varying number of dense dimensions. The only reduction operation we support is sum.\n\n\n', 'stemmed comments': ['case', 'the', 'test', 'support', 'cover', 'vari', 'these', 'dimens', 'basic', 'dens', 'number', 'oper', 'reduct', 'gener', 'tensor', 'sum', 'spars']}"
108,"{'func name': 'exclude_tensor_method', 'comments': '', 'stemmed comments': []}"
109,"{'func name': 'padding3d_circular', 'comments': 'input: [[[[[ 0.,  1.,  2.], [ 3.,  4.,  5.]], [[ 6.,  7.,  8.], [ 9., 10., 11.]]]]] pad: (1, 2, 2, 1, 1, 2) output: [[[[[ 8.,  6.,  7.,  8.,  6.,  7.], [11.,  9., 10., 11.,  9., 10.], [ 8.,  6.,  7.,  8.,  6.,  7.], [11.,  9., 10., 11.,  9., 10.], [ 8.,  6.,  7.,  8.,  6.,  7.]],\n\n[[ 2.,\n\n0.,\n\n1.,\n\n2.,\n\n0.,\n\n1.], [ 5.,\n\n3.,\n\n4.,\n\n5.,\n\n3.,\n\n4.], [ 2.,\n\n0.,\n\n1.,\n\n2.,\n\n0.,\n\n1.], [ 5.,\n\n3.,\n\n4.,\n\n5.,\n\n3.,\n\n4.], [ 2.,\n\n0.,\n\n1.,\n\n2.,\n\n0.,\n\n1.]],\n\n[[ 8.,\n\n6.,\n\n7.,\n\n8.,\n\n6.,\n\n7.], [11.,\n\n9., 10., 11.,\n\n9., 10.], [ 8.,\n\n6.,\n\n7.,\n\n8.,\n\n6.,\n\n7.], [11.,\n\n9., 10., 11.,\n\n9., 10.], [ 8.,\n\n6.,\n\n7.,\n\n8.,\n\n6.,\n\n7.]],\n\n[[ 2.,\n\n0.,\n\n1.,\n\n2.,\n\n0.,\n\n1.], [ 5.,\n\n3.,\n\n4.,\n\n5.,\n\n3.,\n\n4.], [ 2.,\n\n0.,\n\n1.,\n\n2.,\n\n0.,\n\n1.], [ 5.,\n\n3.,\n\n4.,\n\n5.,\n\n3.,\n\n4.], [ 2.,\n\n0.,\n\n1.,\n\n2.,\n\n0.,\n\n1.]],\n\n[[ 8.,\n\n6.,\n\n7.,\n\n8.,\n\n6.,\n\n7.], [11.,\n\n9., 10., 11.,\n\n9., 10.], [ 8.,\n\n6.,\n\n7.,\n\n8.,\n\n6.,\n\n7.], [11.,\n\n9., 10., 11.,\n\n9., 10.], [ 8.,\n\n6.,\n\n7.,\n\n8.,\n\n6.,\n\n7.]]]]]\n', 'stemmed comments': ['2', '8', '3', ']', '10', 'pad', '1', '6', '0', '7', '4', '9', 'input', '[', '5', 'output', '11']}"
110,"{'func name': 'get_script_module', 'comments': '', 'stemmed comments': []}"
111,"{'func name': 'qengine_is_qnnpack', 'comments': '', 'stemmed comments': []}"
112,"{'func name': '_assertGradAndGradgradChecks', 'comments': '', 'stemmed comments': []}"
113,"{'func name': 'parse_header', 'comments': '', 'stemmed comments': []}"
114,"{'func name': 'set_torch_threads', 'comments': '', 'stemmed comments': []}"
115,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
116,"{'func name': '_tensor_splits', 'comments': 'Generates (axis, split_info, tensor_splits) tuples.\n\n\n', 'stemmed comments': ['split_info', 'tensor_split', 'gener', 'tupl', 'axi']}"
117,"{'func name': '_tensor_splits', 'comments': 'Generates (axis, split_info, tensor_splits) tuples.\n\n\n', 'stemmed comments': ['split_info', 'tensor_split', 'gener', 'tupl', 'axi']}"
118,"{'func name': 'get_concatenated_feature_to_index', 'comments': '', 'stemmed comments': []}"
119,"{'func name': 'setup', 'comments': '', 'stemmed comments': []}"
120,"{'func name': 'patched_make_field', 'comments': '', 'stemmed comments': []}"
121,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
122,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
123,"{'func name': 'pytest_configure', 'comments': '', 'stemmed comments': []}"
124,"{'func name': '_transform_to_stack', 'comments': '', 'stemmed comments': []}"
125,"{'func name': 'is_dependent', 'comments': '', 'stemmed comments': []}"
126,"{'func name': '_get_active_context', 'comments': '', 'stemmed comments': []}"
127,"{'func name': 'disambiguate_grad_if_op_output', 'comments': '', 'stemmed comments': []}"
128,"{'func name': 'add_while_op', 'comments': ""A helper function to add a While op to the net. Same rules for determining outer and inner blobs as for the 'If' operator apply for the 'While' operator loop and condition subnets. If specified, condition net is executed in a separate workspace before the first and after each iteration, the last operator must have a single scalar boolean output that is written into the condition blob. Inputs: while_net - net to add a While op to; cond_blob - scalar bool blob reference, used as a stop condition; lexical_scope - a set of outer blob names visible to the loop's body; loop_body_net - net to execute on each iteration; condition_body_net - net to compute condition value\n\n\n"", 'stemmed comments': ['refer', 'comput', 'rule', 'scalar', 'outer', 'specifi', 'subnet', 'inner', 'input', 'separ', 'helper', 'net', 'output', 'valu', 'loop', 'same', 'lexical_scop', 'A', 'determin', 'singl', 'function', 'must', 'while_net', ';', 'iter', 'written', 'last', 'blob', 'workspac', 'execut', 'stop', 'add', 'visibl', 'condit', 'while', 'set', 'bool', 'If', 'loop_body_net', 's', 'condition_body_net', 'oper', 'cond_blob', 'bodi', 'first', 'boolean', 'op', 'use', 'name', 'appli']}"
129,"{'func name': 'loop', 'comments': 'Loop\n\n\n', 'stemmed comments': ['loop']}"
130,"{'func name': 'IfNot', 'comments': 'If condition_blob_or_net returns false, executes true_nets_or_steps, otherwise executes false_nets_or_steps\n\n\n', 'stemmed comments': ['If', 'true_nets_or_step', 'fals', 'false_nets_or_step', 'condition_blob_or_net', 'return', 'execut', 'otherwis']}"
131,"{'func name': 'freeze_bn_stats', 'comments': '', 'stemmed comments': []}"
132,"{'func name': '_cudnn_convolution_algo_count', 'comments': '', 'stemmed comments': []}"
133,"{'func name': 'group_conv_deprecated', 'comments': ""GroupConvolution's deprecated interface.\n\nThis is used to simulate a group convolution via split and concat. You should always use the new group convolution in your new code.\n"", 'stemmed comments': ['code', 'convolut', 'group', 's', 'split', 'via', 'thi', 'deprec', 'simul', 'you', 'concat', 'alway', 'interfac', 'use', 'new', 'groupconvolut']}"
134,"{'func name': 'onnx_to_caffe2', 'comments': '', 'stemmed comments': []}"
135,"{'func name': '_check_param_device', 'comments': 'This helper function is to check if the parameters are located in the same device. Currently, the conversion between model parameters and single vector form is not supported for multiple allocations, e.g. parameters in different GPUs, or mixture of CPU/GPU.\n\nArguments: param ([Tensor]): a Tensor of a parameter of a model old_param_device (int): the device where the first parameter of a model is allocated.\n##### Returns\n* **old_param_device (int)**: report device for the first time\n\n', 'stemmed comments': ['vector', 'old_param_devic', 'cpu/gpu', 'gpu', 'argument', 'support', 'helper', 'multipl', 'current', ']', 'devic', 'time', 'form', 'convers', 'mixtur', '[', 'singl', 'function', 'alloc', 'paramet', 'thi', 'locat', 'return', 'model', 'report', 'int', 'differ', 'first', 'eg', 'param', 'check', 'tensor']}"
136,"{'func name': 'GetArgumentParser', 'comments': '', 'stemmed comments': []}"
137,"{'func name': 'GetArgumentParser', 'comments': '', 'stemmed comments': []}"
138,"{'func name': 'get_input_tensors', 'comments': '', 'stemmed comments': []}"
139,"{'func name': 'AddNogradient', 'comments': '', 'stemmed comments': []}"
140,"{'func name': '_extract_stacktrace', 'comments': 'This function extracts stacktrace without file system access by purely using sys._getframe() and removes part that belongs to this file (core.py). We are not using inspect module because its just a wrapper on top of sys._getframe() whose logic is based on accessing source files on disk - exactly what we are trying to avoid here. Same stands for traceback module\n\nThe reason for file system access avoidance is that if code is located on an NFS, file access might be slow\n\nFunction returns a list of tuples (file_name, line_number, function)\n', 'stemmed comments': ['logic', 'part', 'corepi', 'whose', 'inspect', 'file', 'pure', 'extract', 'stacktrac', 'same', 'the', 'sourc', 'function', 'top', 'reason', 'thi', 'sys_getfram', 'locat', 'We', 'base', 'list', 'wrapper', 'might', 'return', 'exactli', 'avoid', 'code', 'belong', 'stand', 'nf', 'tupl', 'file_nam', 'remov', 'access', 'system', 'modul', 'disk', 'slow', 'line_numb', 'traceback', 'use', 'tri', 'without']}"
141,"{'func name': 'gen_covered_ops', 'comments': '', 'stemmed comments': []}"
142,"{'func name': '_is_cuda_file', 'comments': '', 'stemmed comments': []}"
143,"{'func name': 'apply_crf', 'comments': '', 'stemmed comments': []}"
144,"{'func name': 'unjoined_sigmoid_cross_entropy_grad', 'comments': '', 'stemmed comments': []}"
145,"{'func name': 'softmax', 'comments': '', 'stemmed comments': []}"
146,"{'func name': 'parse', 'comments': 'A simple parser that parses the report of cuda-memcheck. This parser is meant to be simple and it only split the report into separate errors and a summary. Where each error is further splitted into error message and backtrace. No further details are parsed.\n\nA report contains multiple errors and a summary on how many errors are detected. It looks like:\n\n========= CUDA-MEMCHECK ========= Program hit cudaErrorInvalidValue (error 1) due to ""invalid argument"" on CUDA API call to cudaPointerGetAttributes. =========\n\n\n\n Saved host backtrace up to driver entry point at error =========\n\n\n\n Host Frame:/usr/lib/x86_64-linux-gnu/libcuda.so.1 [0x38c7b3] =========\n\n\n\n Host Frame:/usr/local/cuda/lib64/libcudart.so.10.1 (cudaPointerGetAttributes + 0x1a9) [0x428b9] =========\n\n\n\n Host Frame:/home/xgao/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so [0x5b778a9] =========\n\n\n\n ..... ========= ========= Program hit cudaErrorInvalidValue (error 1) due to ""invalid argument"" on CUDA API call to cudaGetLastError. =========\n\n\n\n Saved host backtrace up to driver entry point at error =========\n\n\n\n Host Frame:/usr/lib/x86_64-linux-gnu/libcuda.so.1 [0x38c7b3] =========\n\n\n\n Host Frame:/usr/local/cuda/lib64/libcudart.so.10.1 (cudaGetLastError + 0x163) [0x4c493] =========\n\n\n\n ..... ========= ========= ..... ========= ========= Program hit cudaErrorInvalidValue (error 1) due to ""invalid argument"" on CUDA API call to cudaGetLastError. =========\n\n\n\n Saved host backtrace up to driver entry point at error =========\n\n\n\n Host Frame:/usr/lib/x86_64-linux-gnu/libcuda.so.1 [0x38c7b3] =========\n\n\n\n ..... =========\n\n\n\n Host Frame:python (_PyEval_EvalFrameDefault + 0x6a0) [0x1d0ad0] =========\n\n\n\n Host Frame:python (_PyEval_EvalCodeWithName + 0xbb9) [0x116db9] ========= ========= ERROR SUMMARY: 4 errors\n', 'stemmed comments': ['invalid', 'pars', 'python', 'No', '1', 'split', 'mani', 'host', '0x38c7b3', '/usr/local/cuda/lib64/libcudartso101', '_pyeval_evalcodewithnam', 'due', '0x5b778a9', 'contain', 'messag', 'separ', 'like', 'argument', 'cuda', '0x6a0', 'cudapointergetattribut', 'meant', 'multipl', ']', '0x116db9', '0x1d0ad0', 'look', '0x4c493', '0xbb9', 'driver', 'error', 'hit', '=========', '/usr/lib/x86_64linuxgnu/libcudaso1', 'simpl', '[', 'detect', 'call', 'A', '0x1a9', 'cudamemcheck', 'thi', 'entri', '0x428b9', 'where', 'frame', 'backtrac', 'save', 'report', '/home/xgao/anaconda3/lib/python37/sitepackages/torch/lib/libtorchso', '_pyeval_evalframedefault', 'cudaerrorinvalidvalu', 'detail', 'point', 'summari', 'api', '4', 'parser', '0x163', 'cudagetlasterror', 'It', 'program']}"
147,"{'func name': 'test_script_stacked_lnlstm', 'comments': '', 'stemmed comments': []}"
148,"{'func name': 'process_declaration', 'comments': '', 'stemmed comments': []}"
149,"{'func name': '_GPUInterDeviceBatchNormalization', 'comments': '', 'stemmed comments': []}"
150,"{'func name': 'data_parallel', 'comments': 'Evaluates module(input) in parallel across the GPUs given in device_ids.\n\nThis is the functional version of the DataParallel module.\n##### Args\n* **module (Module)**: the module to evaluate in parallel\n\n* **inputs (Tensor)**: inputs to the module\n\n* **device_ids (list of int or torch.device)**: GPU ids on which to replicate module\n\n* **output_device (list of int or torch.device)**: GPU location of the output  Use -1 to indicate the CPU.\n    (default\n\n##### Returns\n', 'stemmed comments': ['device_id', 'torchdevic', '1', 'gpu', 'indic', 'input', 'evalu', 'version', 'output', 'given', 'cpu', 'across', 'id', 'use', 'function', 'thi', 'locat', 'list', 'return', 'arg', 'int', 'parallel', 'default', 'replic', 'modul', 'output_devic', 'tensor', 'dataparallel']}"
151,"{'func name': 'dummy_fetcher_rnn', 'comments': '', 'stemmed comments': []}"
152,"{'func name': 'enqueuer', 'comments': '', 'stemmed comments': []}"
153,"{'func name': 'make_destination_dataset', 'comments': '', 'stemmed comments': []}"
154,"{'func name': 'CountUntil', 'comments': '', 'stemmed comments': []}"
155,"{'func name': '_dataset', 'comments': '', 'stemmed comments': []}"
156,"{'func name': 'random_split', 'comments': 'Randomly split a dataset into non-overlapping new datasets of given lengths. Optionally fix the generator for reproducible results, e.g.\n\n>>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n\nArguments: dataset (Dataset): Dataset to be split lengths (sequence): lengths of splits to be produced generator (Generator): Generator used for the random permutation.\n', 'stemmed comments': ['>', 'split', 'generator=torchgener', 'manual_se', 'argument', 'rang', 'given', '3', ']', '10', 'nonoverlap', '7', 'reproduc', '[', 'new', '42', 'fix', 'length', 'result', 'dataset', 'random_split', 'gener', 'option', 'random', 'randomli', 'produc', 'eg', 'use', 'sequenc', 'permut']}"
157,"{'func name': 'execution_step_with_progress', 'comments': '', 'stemmed comments': []}"
158,"{'func name': 'db_input', 'comments': '', 'stemmed comments': []}"
159,"{'func name': 'weights_init', 'comments': '', 'stemmed comments': []}"
160,"{'func name': 'set_shutdown_signal', 'comments': '', 'stemmed comments': []}"
161,"{'func name': 'run_embed_params', 'comments': 'This is only a helper debug function so we can test embed_params=False case as well on pytorch front This should likely be removed from the release version of the code\n\n\n', 'stemmed comments': ['case', 'code', 'test', 'function', 'remov', 'thi', 'embed_params=fals', 'debug', 'pytorch', 'front', 'version', 'like', 'helper', 'releas', 'well']}"
162,"{'func name': '_conv_2d_shuffle_offsets', 'comments': '', 'stemmed comments': []}"
163,"{'func name': 'dense_vector_to_id_list_ref', 'comments': '', 'stemmed comments': []}"
164,"{'func name': 'approx_heatmap_keypoint', 'comments': 'Mask R-CNN uses bicubic upscaling before taking the maximum of the heat map for keypoints. We are using bilinear upscaling, which means we can approximate the maximum coordinate with the low dimension maximum coordinates. We would like to avoid bicubic upscaling, because it is computationally expensive. Brown and Lowe  (Invariant Features from Interest Point Groups, 2002) uses a method  for fitting a 3D quadratic function to the local sample points to determine the interpolated location of the maximum of scale space, and his experiments showed that this provides a substantial improvement to matching and stability for keypoint extraction. This approach uses the Taylor expansion (up to the quadratic terms) of the scale-space function. It is equivalent with the Newton method. This efficient method were used in many keypoint estimation algorithms like SIFT, SURF etc...\n\nThe implementation of Newton methods with numerical analysis is straight forward and super simple, though we need a linear solver.\n', 'stemmed comments': ['mean', 'approxim', 'mani', 'substanti', 'like', 'heat', 'extract', 'invari', 'interpol', 'upscal', 'show', 'forward', 'take', 'point', 'algorithm', 'taylor', 'space', 'expens', 'bicub', 'solver', 'brown', 'fit', 'dimens', 'sampl', 'surf', 'super', 'keypoint', 'implement', 'featur', 'etc', 'function', 'equival', 'thi', 'We', 'newton', 'approach', 'use', 'It', 'provid', 'though', 'scale', '2002', 'map', 'bilinear', 'simpl', 'the', 'scalespac', 'interest', 'locat', 'avoid', 'term', 'straight', 'quadrat', 'comput', 'coordin', 'low', 'sift', 'experi', 'would', 'need', 'linear', 'stabil', 'numer', 'group', 'estim', 'method', 'determin', 'local', 'match', 'rcnn', '3D', 'improv', 'maximum', 'analysi', 'mask', 'expans', 'effici']}"
165,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
166,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
167,"{'func name': '_Dirichlet_backward', 'comments': '', 'stemmed comments': []}"
168,"{'func name': 'fork_add', 'comments': '', 'stemmed comments': []}"
169,"{'func name': '_run_trainer_torchscript', 'comments': '', 'stemmed comments': []}"
170,"{'func name': 'rpc_async_method', 'comments': 'Call rpc.rpc_async on a method in a remote object.\n\n\n##### Args\n* **method**: the method (for example, Class.method)\n\n* **obj_rref (RRef)**: remote reference to the object\n\n* **args**: positional arguments to pass to the method\n\n* **kwargs**: keyword arguments to pass to the method\n\n', 'stemmed comments': ['arg', 'refer', 'posit', 'rpcrpc_async', 'kwarg', 'classmethod', 'method', 'keyword', 'object', 'obj_rref', 'argument', 'remot', 'pass', 'rref', 'exampl', 'call']}"
171,"{'func name': 'get_function_event', 'comments': 'Returns the first event that matches partial_event_name in the provided function_events. These function_events should be the output of torch.autograd.profiler.function_events().\n\n\n##### Args\n* **function_events**: function_events returned by the profiler.\n\n* **event_name (str)**: partial key that the event was profiled with.\n\n', 'stemmed comments': ['arg', 'partial_event_nam', 'event_nam', 'partial', 'these', 'torchautogradprofilerfunction_ev', 'profil', 'match', 'first', 'event', 'function_ev', 'str', 'key', 'return', 'output', 'provid']}"
172,"{'func name': 'new_group', 'comments': 'Creates a new distributed group.\n\nThis function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.\n\nArguments: ranks (list[int]): List of ranks of group members. timeout (timedelta, optional): Timeout for operations executed against the process group. Default value equals 30 minutes. This is only applicable for the ``gloo`` backend. backend (str or Backend, optional): The backend to use. Depending on build-time configurations, valid values are ``gloo`` and ``nccl``. By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., ``""gloo""``), which can also be accessed via :class:`Backend` attributes (e.g., ``Backend.GLOO``).\n##### Returns\n', 'stemmed comments': ['backendgloo', 'backend', 'string', 'requir', 'also', 'timeout', 'part', 'argument', 'job', 'str', 'By', 'nccl', 'lowercas', 'valu', 'given', ']', 'distribut', 'group', '30', 'addit', 'valid', 'attribut', 'even', 'rank', 'minut', '[', 'new', 'use', 'via', 'the', 'field', 'depend', 'member', 'configur', 'function', 'ie', 'thi', 'equal', 'global', 'enter', 'applic', 'list', 'return', 'buildtim', 'execut', 'option', 'process', 'order', 'int', 'timedelta', 'oper', 'creat', 'default', 'access', 'main', 'eg', 'class', 'gloo', 'go']}"
173,"{'func name': '_dump_DDP_relevant_env_vars', 'comments': '', 'stemmed comments': []}"
174,"{'func name': 'run_conv_or_fc', 'comments': '', 'stemmed comments': []}"
175,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
176,"{'func name': 'validModelName', 'comments': '', 'stemmed comments': []}"
177,"{'func name': 'dropout', 'comments': 'dropout\n\n\n', 'stemmed comments': ['dropout']}"
178,"{'func name': 'dump', 'comments': '', 'stemmed comments': []}"
179,"{'func name': '_init_impl', 'comments': '', 'stemmed comments': []}"
180,"{'func name': 'elementwise_linear', 'comments': '', 'stemmed comments': []}"
181,"{'func name': 'rowmux', 'comments': '', 'stemmed comments': []}"
182,"{'func name': 'register_element_ops', 'comments': '', 'stemmed comments': []}"
183,"{'func name': 'GetArgumentParser', 'comments': '', 'stemmed comments': []}"
184,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
185,"{'func name': '_dev_options', 'comments': '', 'stemmed comments': []}"
186,"{'func name': 'lib_paths_from_base', 'comments': '', 'stemmed comments': []}"
187,"{'func name': 'run_resnet50_epoch', 'comments': '', 'stemmed comments': []}"
188,"{'func name': 'replace_string_literal', 'comments': 'Replace a triple quoted string literal with new contents. Only handles printable ASCII correctly at the moment.  This will preserve the quote style of the original string, and makes a best effort to preserve raw-ness (unless it is impossible to do so.)\n\n\n##### Returns\n', 'stemmed comments': ['string', 'handl', 'best', 'unless', 'effort', 'onli', 'new', 'content', 'quot', 'printabl', 'thi', 'origin', 'return', 'style', 'replac', 'imposs', 'tripl', 'make', 'ascii', 'raw', 'liter', 'correctli', 'moment', 'preserv']}"
189,"{'func name': 'resnet_imagenet_create_model', 'comments': '', 'stemmed comments': []}"
190,"{'func name': 'gen_param_update_builder_fun', 'comments': '', 'stemmed comments': []}"
191,"{'func name': 'collect_generated_testcases', 'comments': '', 'stemmed comments': []}"
192,"{'func name': 'convert_tests', 'comments': '', 'stemmed comments': []}"
193,"{'func name': 'DlopenGuard', 'comments': '', 'stemmed comments': []}"
194,"{'func name': 'lstm_factory_multilayer', 'comments': '', 'stemmed comments': []}"
195,"{'func name': 'enable_observer', 'comments': '', 'stemmed comments': []}"
196,"{'func name': '_faulty_process_group_init_backend_handler', 'comments': '', 'stemmed comments': []}"
197,"{'func name': 'get_fc_predictor_version', 'comments': '', 'stemmed comments': []}"
198,"{'func name': 'fc_sparse', 'comments': 'FC_Sparse: Only takes in allocated weights\n\n\n', 'stemmed comments': ['fc_spars', 'alloc', 'weight', 'onli', 'take']}"
199,"{'func name': 'get_fc_predictor_version', 'comments': '', 'stemmed comments': []}"
200,"{'func name': '_fill_diagonal', 'comments': '', 'stemmed comments': []}"
201,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
202,"{'func name': 'find_before', 'comments': '', 'stemmed comments': []}"
203,"{'func name': 'mse', 'comments': '', 'stemmed comments': []}"
204,"{'func name': 'create_derived', 'comments': '', 'stemmed comments': []}"
205,"{'func name': '_unflatten', 'comments': '', 'stemmed comments': []}"
206,"{'func name': 'build_cpp_tests', 'comments': '', 'stemmed comments': []}"
207,"{'func name': '_tensor_splits', 'comments': 'Generates (axis, split_info, tensor_splits) tuples.\n\n\n', 'stemmed comments': ['split_info', 'tensor_split', 'gener', 'tupl', 'axi']}"
208,"{'func name': 'hvp', 'comments': ""Function that computes the dot product between the Hessian of a given scalar function and a vector ``v`` at the point given by the inputs.\n\n\n##### Args\n* **func (function)**: a Python function that takes Tensor inputs and returns\n    a Tensor with a single element.\n\n* **inputs (tuple of Tensors or Tensor)**: inputs to the function ``func``.\n\n* **v (tuple of Tensors or Tensor)**: The vector for which the Hessian vector\n    product is computed. Must be the same size as the input of\n    ``func``. This argument is optional when ``func``'s input contains\n    a single element and (if it is not provided) will be set as a\n    Tensor containing a single ``1``.\n\n* **create_graph (bool, optional)**: If ``True``, both the output and result will be\n    computed in a differentiable way. Note that when ``strict`` is\n    ``False``, the result can not require gradients or be disconnected\n    from the inputs.  Defaults to ``False``.\n\n* **strict (bool, optional)**: If ``True``, an error will be raised when we\n    detect that there exists an input such that all the outputs are\n    independent of it. If ``False``, we return a Tensor of zeros as the\n    hvp for said inputs, which is the expected mathematical value.\n    Defaults to ``False``.\n\n##### Returns\n* **func_output (tuple of Tensors or Tensor)**: output of ``func(inputs)``\n    hvp (tuple of Tensors or Tensor)\n\n* **ple**: \n\n* **>>> def pow_reducer(x)**: \n\n* **>>> def pow_adder_reducer(x, y)**: \n\n* ****: \n\n"", 'stemmed comments': ['mathemat', 'comput', 'python', 'scalar', '>', 'vector', 'dot', '1', 'note', 'requir', 'func_output', 'hessian', 'element', 'input', 'argument', 'contain', 'output', 'given', 'valu', 'x', 'true', 'hvp', 'create_graph', 'expect', 'error', 'differenti', 'size', 'gradient', 'the', 'detect', 'result', 'singl', 'pow_adder_reduc', 'function', 'rais', 'must', 'thi', 'strict', 'func', 'zero', 'return', 'way', 'take', 'option', 'arg', 'said', 'set', 'product', 'tupl', 'bool', 's', 'If', 'independ', 'pow_reduc', 'default', 'point', 'fals', 'def', 'v', 'ple', 'tensor', 'disconnect', 'exist', 'provid']}"
209,"{'func name': 'multi_head_attention_forward', 'comments': 'Args: query, key, value: map a query and a set of key-value pairs to an output. See ""Attention Is All You Need"" for more details. embed_dim_to_check: total dimension of the model. num_heads: parallel attention heads. in_proj_weight, in_proj_bias: input projection weight and bias. bias_k, bias_v: bias of the key and value sequences to be added at dim=0. add_zero_attn: add a new batch of zeros to the key and value sequences at dim=1. dropout_p: probability of an element to be zeroed. out_proj_weight, out_proj_bias: the output projection weight and bias. training: apply dropout if is ``True``. key_padding_mask: if provided, specified padding elements in the key will be ignored by the attention. This is an binary mask. When the value is True, the corresponding value on the attention layer will be filled with -inf. need_weights: output attn_output_weights. attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all the batches while a 3D mask allows to specify a different mask for the entries of each batch. use_separate_proj_weight: the function accept the proj. weights for query, key, and value in different forms. If false, in_proj_weight will be used, which is a combination of q_proj_weight, k_proj_weight, v_proj_weight. q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias. static_k, static_v: static key and value used for attention operators.\n\nShape: Inputs:\n\n- query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is the embedding dimension.\n\n- key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is the embedding dimension.\n\n- value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is the embedding dimension.\n\n- key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length. If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n\n- attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length. 3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length, S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True`` are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.\n\n- static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length, N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n\n- static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length, N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n\nOutputs:\n\n- attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is the embedding dimension.\n\n- attn_output_weights: :math:`(N, L, S)` where N is the batch size, L is the target sequence length, S is the source sequence length.\n', 'stemmed comments': ['posit', 'Is', 'element', 'broadcast', 'valu', '2D', 'weight', 'num_head', 'embed', 'in_proj_weight', 'shape', 'length', 'attent', 'e/num_head', 'ad', 'entri', 'attend', 'attn_mask', 'batch', 'layer', 'fill', 'out_proj_weight', 'add_zero_attn', 'ignor', 'differ', 'q_proj_weight', 'fals', 'dim=0', 'dim=1', 'when', 'ensur', 'certain', 'accept', 'math', 'head', 'bytetensor', 'see', 'total', 'input', 'in_proj_bia', 'attn_output', 'true', 'pad', 'dimens', 'train', 'attn_output_weight', 'E', 'prevent', 'function', 'nonzero', 'v_proj_weight', 'thi', 'target', 'out_proj_bia', 'k_proj_weight', 'arg', 'all', 'If', 'detail', 'parallel', 'use', 'bias_k', 'N', 'sequenc', 'provid', 'static_k', 'specifi', 'correspond', 'static', 'output', 'queri', 'you', 'keyvalu', 'map', 'L', 'form', 'size', 'A', 'binari', 'S', 'project', 'allow', 'floattensor', 'dropout_p', 'model', 'add', 'set', 'unchang', 'dropout', 'oper', 'appli', 'booltensor', 'probabl', 'need', 'bias_v', 'inf', 'proj', 'embed_dim_to_check', 'unmask', 'bia', 'new', 'combin', 'sourc', 'zero', '3D', 'need_weight', 'static_v', 'mask', 'key_padding_mask', 'use_separate_proj_weight', 'pair', 'key']}"
210,"{'func name': 'align_tensors', 'comments': '', 'stemmed comments': []}"
211,"{'func name': 'namedtupledict', 'comments': '', 'stemmed comments': []}"
212,"{'func name': 'upsample_nearest', 'comments': ""Upsamples the input, using nearest neighbours' pixel values.\n\n.. warning:: This function is deprecated in favor of :func:`torch.nn.quantized.functional.interpolate`. This is equivalent with ``nn.quantized.functional.interpolate(..., mode='nearest')``.\n\n.. note:: The input quantization parameters propagate to the output.\n\n.. note:: Only 2D inputs are supported\n##### Args\n* **input (Tensor)**: quantized input\n\n* **size (int or Tuple[int, int] or Tuple[int, int, int])**: output spatial\n    size.\n\n* **scale_factor (int)**: multiplier for spatial size. Has to be an integer.\n\n"", 'stemmed comments': ['deprec', 'note', 'input', 'pixel', 'support', 'nnquantizedfunctionalinterpol', 'valu', 'output', '2D', 'tensor', 'spatial', ']', 'favor', 'quantiz', 'ha', 'propag', 'onli', '[', 'size', 'the', 'neighbour', 'function', 'equival', 'paramet', 'thi', 'multipli', 'func', 'integ', 'mode=nearest', 'warn', 'arg', 'upsampl', 'tupl', 'int', 'nearest', 'scale_factor', 'torchnnquantizedfunctionalinterpol', 'use']}"
213,"{'func name': 'async_execution', 'comments': 'A decorator for a function indicating that the return value of the function is guaranteed to be a :class:`~torch.futures.Future` object and this function can run asynchronously on the RPC callee. More specifically, the callee extracts the :class:`~torch.futures.Future` returned by the wrapped function and installs subsequent processing steps as a callback to that :class:`~torch.futures.Future`. The installed callback will read the value from the :class:`~torch.futures.Future` when completed and send the value back as the RPC response. That also means the returned :class:`~torch.futures.Future` only exists on the callee side and is never sent through RPC. This decorator is useful when the wrapped function\'s (``fn``) execution needs to pause and resume due to, e.g., containing :meth:`~torch.distributed.rpc.rpc_async` or waiting for other signals.\n\n.. note:: To enable asynchronous execution, applications must pass the function object returned by this decorator to RPC APIs. Otherwise, RPC will not be able to detect the attributes installed by this decorator. However, this does not mean this decorator has to be outmost one when defining a function. For example, when combined with ``@staticmethod`` or ``@classmethod``, ``@rpc.functions.async_execution`` needs to be the inner decorator to allow the target function be recognized as a static or class function. This target function can still execute asynchronously because, when accessed, the static or class method preserves attributes installed by ``@rpc.functions.async_execution``.\n\n.. warning:: `autograd profiler <https://pytorch.org/docs/stable/autograd.html#profiler>`_ does not work with ``async_execution`` functions.\n\nExample:: The returned :class:`~torch.futures.Future` object can come from ``rpc.rpc_async``, ``Future.then(cb)``, or :class:`~torch.futures.Future` constructor. The example below shows directly using the :class:`~torch.futures.Future` returned by ``Future.then(cb)``.\n\n>>> from torch.distributed import rpc >>> >>> # omitting setup and shutdown RPC >>> >>> # On all workers >>> @rpc.functions.async_execution >>> def async_add_chained(to, x, y, z): >>>\n\n\n\n # This function runs on ""worker1"" and returns immediately when >>>\n\n\n\n # the callback is installed through the `then(cb)` API. In the >>>\n\n\n\n # mean time, the `rpc_async` to ""worker2"" can run concurrently. >>>\n\n\n\n # When the return value of that `rpc_async` arrives at >>>\n\n\n\n # ""worker1"", ""worker1"" will run the lambda function accordinly >>>\n\n\n\n # and set the value for the previously returned `Future`, which >>>\n\n\n\n # will then trigger RPC to send the result back to ""worker0"". >>>\n\n\n\n return rpc.rpc_async(to, torch.add, args=(x, y)).then( >>>\n\n\n\n\n\n\n\n lambda fut: fut.wait() + z >>>\n\n\n\n ) >>> >>> # On worker0 >>> ret = rpc.rpc_sync( >>>\n\n\n\n ""worker1"", >>>\n\n\n\n async_add_chained, >>>\n\n\n\n args=(""worker2"", torch.ones(2), 1, 1) >>> ) >>> print(ret)\n\n# prints tensor([3., 3.])\n\nWhen combined with TorchScript decorators, this decorator must be the outmost one.\n\n>>> from torch import Tensor >>> from torch.futures import Future >>> from torch.distributed import rpc >>> >>> # omitting setup and shutdown RPC >>> >>> # On all workers >>> @torch.jit.script >>> def script_add(x: Tensor, y: Tensor) -> Tensor: >>>\n\n\n\n return x + y >>> >>> @rpc.functions.async_execution >>> @torch.jit.script >>> def async_add(to: str, x: Tensor, y: Tensor) -> Future[Tensor]: >>>\n\n\n\n return rpc.rpc_async(to, script_add, (x, y)) >>> >>> # On worker0 >>> ret = rpc.rpc_sync( >>>\n\n\n\n ""worker1"", >>>\n\n\n\n async_add, >>>\n\n\n\n args=(""worker2"", torch.ones(2), 1) >>> ) >>> print(ret)\n\n# prints tensor([2., 2.])\n\nWhen combined with static or class method, this decorator must be the inner one.\n\n>>> from torch.distributed import rpc >>> >>> # omitting setup and shutdown RPC >>> >>> # On all workers >>> class AsyncExecutionClass: >>> >>>\n\n\n\n @staticmethod >>>\n\n\n\n @rpc.functions.async_execution >>>\n\n\n\n def static_async_add(to, x, y, z): >>>\n\n\n\n\n\n\n\n return rpc.rpc_async(to, torch.add, args=(x, y)).then( >>>\n\n\n\n\n\n\n\n\n\n\n\n lambda fut: fut.wait() + z >>>\n\n\n\n\n\n\n\n ) >>> >>>\n\n\n\n @classmethod >>>\n\n\n\n @rpc.functions.async_execution >>>\n\n\n\n def class_async_add(cls, to, x, y, z): >>>\n\n\n\n\n\n\n\n ret_fut = torch.futures.Future() >>>\n\n\n\n\n\n\n\n rpc.rpc_async(to, torch.add, args=(x, y)).then( >>>\n\n\n\n\n\n\n\n\n\n\n\n lambda fut: ret_fut.set_result(fut.wait() + z) >>>\n\n\n\n\n\n\n\n ) >>>\n\n\n\n\n\n\n\n return ret_fut >>> >>> # On worker0 >>> ret = rpc.rpc_sync( >>>\n\n\n\n ""worker1"", >>>\n\n\n\n AsyncExecutionClass.static_async_add, >>>\n\n\n\n args=(""worker2"", torch.ones(2), 1, 2) >>> ) >>> print(ret)\n\n# prints tensor([4., 4.]) >>> >>> ret = rpc.rpc_sync( >>>\n\n\n\n ""worker1"", >>>\n\n\n\n AsyncExecutionClass.class_async_add, >>>\n\n\n\n args=(""worker2"", torch.ones(2), 1, 2) >>> ) >>> print(ret)\n\n# prints tensor([4., 4.])\n', 'stemmed comments': ['resum', 'mean', 'worker1', 'guarante', 'accordinli', 'specif', 'then', 'shutdown', 'valu', 'worker2', 'extract', 'send', 'ret', '//pytorchorg/docs/stable/autogradhtml', '=', 'futwait', 'async_execut', 'one', 'never', 'z', 'immedi', 'omit', 'classmethod', 'rpc_async', 'arriv', 'applic', 'show', 'static_async_add', 'rpcfunctionsasync_execut', 'subsequ', 'In', 'ret_futset_result', 'side', 'enabl', 'eg', 'when', 'torchdistribut', 'worker0', 'ret_fut', '~torchfuturesfutur', 'howev', 'otherwis', 'preserv', '2', 'http', '>', '<', 'asyncexecutionclassstatic_async_add', '1', 'inner', 'profil', 'str', 'x', '3', 'time', 'meth', 'paus', 'rpcrpc_sync', 'attribut', 'torchjitscript', 'asynchron', 'trigger', 'function', 'defin', 'cb', 'args=', 'cl', 'thi', 'target', 'recogn', 'that', 'torchscript', 'rpc', 'setup', 'callback', 'decor', 'previous', 'class_async_add', 's', 'sent', 'calle', 'autograd', 'On', '4', 'torchadd', 'use', 'static', 'torch', 'note', 'run', 'back', 'also', 'read', 'contain', 'pass', 'exampl', 'directli', 'instal', 'asyncexecutionclass', 'signal', 'detect', 'the', 'respons', 'A', 'import', 'result', 'must', '_', 'allow', 'abl', 'wrap', 'work', 'warn', 'set', 'print', 'futurethen', 'concurr', 'api', 'def', '@', 'access', 'torchfutur', 'fut', 'constructor', 'complet', 'torchfuturesfutur', 'staticmethod', 'torchon', 'need', 'come', 'due', 'indic', 'wait', 'async_add', ']', 'script_add', 'To', 'more', 'step', 'method', 'object', '[', 'worker', 'combin', 'still', 'for', 'futur', 'asyncexecutionclassclass_async_add', 'outmost', 'return', 'execut', 'process', 'rpcrpc_async', 'lambda', 'async_add_chain', 'class', '~torchdistributedrpcrpc_async', 'tensor', 'fn', 'exist']}"
214,"{'func name': 'fuse_modules', 'comments': 'Fuses a list of modules into a single module\n\nFuses only the following sequence of modules: conv, bn conv, bn, relu conv, relu linear, relu bn, relu All other sequences are left unchanged. For these sequences, replaces the first item in the list with the fused module, replacing the rest of the modules with identity.\n\nArguments: model: Model containing the modules to be fused modules_to_fuse: list of list of module names to fuse. Can also be a list of strings if there is only a single list of modules to fuse. inplace: bool specifying if fusion happens in place on the model, by default a new model is returned fuser_func: Function that takes in a list of modules and outputs a list of fused modules of the same length. For example, fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()] Defaults to torch.quantization.fuse_known_modules\n##### Returns\n* **ples**: \n\n', 'stemmed comments': ['fusion', 'specifi', 'relu', 'linear', 'string', 'nnident', 'also', 'rest', 'argument', 'contain', 'convmodul', 'inplac', 'output', 'exampl', ']', 'bn', 'ident', 'torchquantizationfuse_known_modul', '[', 'new', 'follow', 'conv', 'for', 'length', 'modules_to_fus', 'singl', 'convbnmodul', 'item', 'function', 'can', 'happen', 'fuse', 'list', 'return', 'model', 'take', 'bnmodul', 'all', 'unchang', 'replac', 'bool', 'default', 'place', 'modul', 'fuser_func', 'first', 'ple', 'left', 'name', 'sequenc']}"
215,"{'func name': 'fused_rowwise_8bit_quantize_dequantize_reference', 'comments': '', 'stemmed comments': []}"
216,"{'func name': 'ErrorThresholdRow', 'comments': '', 'stemmed comments': []}"
217,"{'func name': '_compress_uniform_simplified', 'comments': '', 'stemmed comments': []}"
218,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
219,"{'func name': 'fuse_conv_bn_weights', 'comments': '', 'stemmed comments': []}"
220,"{'func name': 'div_float_future', 'comments': '', 'stemmed comments': []}"
221,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
222,"{'func name': 'prod', 'comments': 'np.prod can overflow, so for sizes the product should be done in Python.\n\nEven though np.prod type promotes to int64, it can still overflow in which case the negative value will pass the size check and OOM when attempting to actually allocate the Tensor.\n', 'stemmed comments': ['type', 'python', 'though', 'npprod', 'pass', 'valu', 'case', 'tensor', 'even', 'promot', 'size', 'still', 'alloc', 'done', 'int64', 'attempt', 'neg', 'oom', 'check', 'actual', 'product', 'overflow']}"
223,"{'func name': '_standard_gamma', 'comments': '', 'stemmed comments': []}"
224,"{'func name': '_inputs', 'comments': '', 'stemmed comments': []}"
225,"{'func name': 'gather_ranges_to_dense_with_key', 'comments': '', 'stemmed comments': []}"
226,"{'func name': 'uses_single_grad', 'comments': '', 'stemmed comments': []}"
227,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
228,"{'func name': 'register_backend_select_methods', 'comments': '', 'stemmed comments': []}"
229,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
230,"{'func name': 'gen_transitive_closure', 'comments': '', 'stemmed comments': []}"
231,"{'func name': 'emit_assignments', 'comments': '', 'stemmed comments': []}"
232,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
233,"{'func name': 'op_name', 'comments': '', 'stemmed comments': []}"
234,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
235,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
236,"{'func name': 'process_function', 'comments': '', 'stemmed comments': []}"
237,"{'func name': 'dispatch_strategy', 'comments': ""How are we going to call the underlying implementation of a declaration?  There are two strategies\n\n- use_derived: we want to call the implementation on CPUDoubleType (or a similar, derived Type instance).\n\nBecause these derived instances deal in Tensors, not Variables (it's a completely different object, so it doesn't dispatch back to VariableType), code on this dispatch path needs to wrap/unwrap tensors.\n\nIf the derived implementation takes and returns tensors, the implementation is usually differentiable (although we also use the derived dispatch path for non-differentiable functions that we still want to dispatch on the derived Type instance; e.g., size())\n\n- use_type: we want to call the implementation on Type, because it is implemented concretely, and the functions it invokes will get dispatched back to VariableType (which will ensure that they are differentiable.)\n"", 'stemmed comments': ['type', 'use_deriv', 'complet', 'dispatch', 'two', 'underli', 'path', 'similar', 'need', 'back', 'wrap/unwrap', 'also', 'cpudoubletyp', 'declar', 'variabletyp', 'use_typ', 'invok', 'instanc', 'object', 'differenti', 'although', 'use', 'strategi', 'ensur', 'size', 'call', 'implement', 'usual', 'nondifferenti', 'still', '?', 'deal', 'function', ';', 'becaus', 'deriv', 'there', 'return', 'take', 'code', 'If', 's', 'variabl', 'want', 'concret', 'differ', 'nt', 'how', 'eg', 'tensor', 'get', 'go']}"
238,"{'func name': 'generate_outputs', 'comments': '', 'stemmed comments': []}"
239,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
240,"{'func name': 'scriptAndSave', 'comments': '', 'stemmed comments': []}"
241,"{'func name': 'get_model_input_fun', 'comments': '', 'stemmed comments': []}"
242,"{'func name': 'getCodeLink', 'comments': '', 'stemmed comments': []}"
243,"{'func name': '_glu_old_input', 'comments': '', 'stemmed comments': []}"
244,"{'func name': '_refresh_per_optimizer_state', 'comments': '', 'stemmed comments': []}"
245,"{'func name': 'conv3d_weight', 'comments': 'Computes the gradient of conv3d with respect to the weight of the convolution.\n\n\n##### Args\n* **input**: input tensor of shape (minibatch x in_channels x iT x iH x iW)\n\n* **weight_size **: Shape of the weight gradient tensor\n\n* **grad_output **: output gradient tensor (minibatch x out_channels x oT x oH x oW)\n\n* **stride (int or tuple, optional)**: Stride of the convolution. Default\n\n* **padding (int or tuple, optional)**: Zero-padding added to both sides of the input. Default\n\n* **dilation (int or tuple, optional)**: Spacing between kernel elements. Default\n\n* **groups (int, optional)**: Number of blocked connections from input channels to output channels. Default\n\n* **ples**: \n\n', 'stemmed comments': ['comput', 'respect', 'oT', 'number', 'kernel', 'iW', 'element', 'in_channel', 'input', 'oH', 'connect', 'output', 'x', 'pad', 'weight_siz', 'group', 'minibatch', 'weight', 'stride', 'shape', 'gradient', 'out_channel', 'iT', 'channel', 'ad', 'zeropad', 'dilat', 'conv3d', 'option', 'arg', 'convolut', 'tupl', 'oW', 'int', 'iH', 'side', 'default', 'grad_output', 'ple', 'tensor', 'space', 'block']}"
246,"{'func name': 'gradgradcheck', 'comments': ""Check gradients of gradients computed via small finite differences against analytical gradients w.r.t. tensors in :attr:`inputs` and :attr:`grad_outputs` that are of floating point or complex type and with ``requires_grad=True``.\n\nThis function checks that backpropagating through the gradients computed to the given :attr:`grad_outputs` are correct.\n\nThe check between numerical and analytical gradients uses :func:`~torch.allclose`.\n\n.. note:: The default values are designed for :attr:`input` and :attr:`grad_outputs` of double precision. This check will likely fail if they are of less precision, e.g., ``FloatTensor``.\n\n.. warning:: If any checked tensor in :attr:`input` and :attr:`grad_outputs` has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from :func:`torch.expand`), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.\n##### Args\n* **func (function)**: a Python function that takes Tensor inputs and returns\n    a Tensor or a tuple of Tensors\n\n* **inputs (tuple of Tensor or Tensor)**: inputs to the function\n\n* **grad_outputs (tuple of Tensor or Tensor, optional)**: The gradients with\n    respect to the function's outputs.\n\n* **eps (float, optional)**: perturbation for finite differences\n\n* **atol (float, optional)**: absolute tolerance\n\n* **rtol (float, optional)**: relative tolerance\n\n* **gen_non_contig_grad_outputs (bool, optional)**: if\n\n* **raise_exception (bool, optional)**: indicating whether to raise an exception if\n    the check fails. The exception gives more information about the\n    exact nature of the failure. This is helpful when debugging gradchecks.\n\n* **nondet_tol (float, optional)**: tolerance for non-determinism. When running\n    identical inputs through the differentiation, the results must either match\n    exactly (default, 0.0) or be within this tolerance. Note that a small amount\n    of nondeterminism in the gradient will lead to larger inaccuracies in\n    the second derivative.\n\n* **check_undefined_grad (bool, options)**: if True, check if undefined output grads\n    are supported and treated as zeros\n\n##### Returns\n"", 'stemmed comments': ['respect', 'python', 'within', 'atol', 'inform', 'natur', 'like', 'support', 'valu', 'fail', 'check_undefined_grad', 'finit', 'grad', 'attr', 'differenti', 'complex', '~torchallclos', 'failur', 'func', 'take', 'gen_non_contig_grad_output', 'lead', 'differ', 'point', '00', 'eg', 'when', 'check', 'tupl', 'float', 'toler', 'input', 'memori', 'true', 'debug', 'raise_except', 'precis', 'function', 'thi', 'treat', 'help', 'option', 'arg', 'If', 's', 'overlap', 'whether', 'use', 'rel', 'requires_grad=tru', 'note', 'doubl', 'run', 'undefin', 'output', 'share', 'nondetermin', 'the', 'result', 'rais', 'ie', 'must', 'inaccuraci', 'deriv', 'floattensor', 'analyt', 'warn', 'amount', 'second', 'bool', 'rtol', 'default', 'except', 'grad_output', 'type', 'comput', 'ep', 'address', 'nondet_tol', 'gradcheck', 'indic', 'numer', 'given', 'give', 'design', 'chang', 'ident', 'either', 'via', 'small', 'gradient', 'exact', 'torchexpand', 'match', 'less', 'wrt', 'perturb', 'backpropag', 'return', 'exactli', 'zero', 'correct', 'absolut', 'larger', 'tensor']}"
247,"{'func name': '_assert_close', 'comments': '', 'stemmed comments': []}"
248,"{'func name': '_prepare_gru_unit_op', 'comments': '', 'stemmed comments': []}"
249,"{'func name': 'c10_op_ref', 'comments': '', 'stemmed comments': []}"
250,"{'func name': 'benchmark_pytorch_model', 'comments': 'Run the model several times, and measure the execution time. Return the execution time per iteration (millisecond).\n\n\n', 'stemmed comments': ['time', 'execut', 'sever', 'millisecond', 'iter', 'run', 'per', 'return', 'model', 'measur']}"
251,"{'func name': 'run_in_hip', 'comments': '', 'stemmed comments': []}"
252,"{'func name': 'hipify', 'comments': '', 'stemmed comments': []}"
253,"{'func name': 'warn_if_has_hooks', 'comments': '', 'stemmed comments': []}"
254,"{'func name': 'generic', 'comments': '', 'stemmed comments': []}"
255,"{'func name': 'create_hierarchy', 'comments': '', 'stemmed comments': []}"
256,"{'func name': 'load_state_dict_from_url', 'comments': ""Loads the Torch serialized object at the given URL.\n\nIf downloaded file is a zip file, it will be automatically decompressed.\n\nIf the object is already present in `model_dir`, it's deserialized and returned. The default value of `model_dir` is ``<hub_dir>/checkpoints`` where `hub_dir` is the directory returned by :func:`~torch.hub.get_dir`.\n##### Args\n* **url (string)**: URL of the object to download\n\n* **model_dir (string, optional)**: directory in which to save the object\n\n* **map_location (optional)**: a function or a dict specifying how to remap storage locations (see torch.load)\n\n* **progress (bool, optional)**: whether or not to display a progress bar to stderr.\n    Default\n\n* **check_hash(bool, optional)**: If True, the filename part of the URL should follow the naming convention\n    ``filename-<sha256>.ext`` where ``<sha256>`` is the first eight or more\n    digits of the SHA256 hash of the contents of the file. The hash is used to\n    ensure unique names and to verify the contents of the file.\n    Default\n\n* **file_name (string, optional)**: name for the downloaded file. Filename from `url` will be used if not set.\n\n* **ple**: \n\n* **>>> state_dict = torch.hub.load_state_dict_from_url('https**: //s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n\n"", 'stemmed comments': ['http', '>', '<', 'specifi', 'torch', 'map_loc', 'see', 'string', 'filenam', 'uniqu', 'directori', 'torchhubload_state_dict_from_url', 'part', 'url', 'deseri', 'given', 'alreadi', 'file', 'valu', 'dict', 'true', 'storag', 'ple', 'display', 'torchload', 'zip', 'object', 'check_hash', 'ext', 'eight', 'use', '=', 'content', '~torchhubget_dir', 'stderr', 'the', 'follow', 'load', 'function', 'hub_dir', 'locat', 'decompress', 'func', 'digit', 'bar', '//s3amazonawscom/pytorch/models/resnet185c106cdepth', 'serial', 'verifi', 'return', 'automat', 'save', 'option', 'arg', 'set', 'bool', 'If', 'hash', 's', '/checkpoint', 'convent', 'sha256', 'file_nam', 'default', 'remap', 'present', 'state_dict', 'download', 'first', 'whether', 'model_dir', 'ensur', 'name', 'progress']}"
257,"{'func name': 'runOpOnInput', 'comments': '', 'stemmed comments': []}"
258,"{'func name': '_test_binary_broadcast', 'comments': '', 'stemmed comments': []}"
259,"{'func name': 'assert_deadline_disabled', 'comments': '', 'stemmed comments': []}"
260,"{'func name': 'gradient_checker_device_option', 'comments': '', 'stemmed comments': []}"
261,"{'func name': 'run_test', 'comments': '', 'stemmed comments': []}"
262,"{'func name': 'test_forward_only_fast_simplenet', 'comments': '', 'stemmed comments': []}"
263,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
264,"{'func name': 'run_training_net', 'comments': '', 'stemmed comments': []}"
265,"{'func name': 'run_training_net', 'comments': '', 'stemmed comments': []}"
266,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
267,"{'func name': '_make_deprecate', 'comments': '', 'stemmed comments': []}"
268,"{'func name': 'update_initializer', 'comments': 'A helper function to convert from operator_name_and_kwargs to new object of type initializer_class. This function serves two purposes\n\n1. Support for custom initialization operators being passed in 2. Allow user to specify a custom Initializer without overwriting default operators used for initialization\n\nIf initializer_class is None, creates a default initializer using the Initializer class and operator_name_and_kwargs provided\n\nIf operator_name_and_kwargs is None, uses default_operator_name_and_kwargs\n\nreturns an instantiated Initializer object\n', 'stemmed comments': ['type', '2', 'none', 'specifi', 'two', '1', 'pass', 'support', 'helper', 'initializer_class', 'object', 'operator_name_and_kwarg', 'purpos', 'convert', 'new', 'custom', 'A', 'function', 'thi', 'default_operator_name_and_kwarg', 'allow', 'instanti', 'serv', 'return', 'initi', 'If', 'user', 'overwrit', 'oper', 'creat', 'default', 'class', 'use', 'without', 'provid']}"
269,"{'func name': 'instantiate_non_scriptable_remote_module_template', 'comments': '', 'stemmed comments': []}"
270,"{'func name': '_start_record_function', 'comments': 'This function should be called from RPC/RRef functions to create a RecordFunction object for profiling. This function also runs the before callbacks that start the profiling, though the user is responsible for running the appropriate callbacks when the function to be profiled finishes.\n\nArguments: exec_type (RPCExecMode): Type of RPC/RRef call func_name (str): Name of function being profiled. current_worker_name (str): Name of current worker. dest_worker_name (str): Name of the destination worker.\n##### Returns\n', 'stemmed comments': ['type', 'though', 'profil', 'also', 'run', 'argument', 'str', 'dest_worker_nam', 'func_nam', 'current', 'current_worker_nam', 'rpc/rref', 'object', 'start', 'worker', 'call', 'respons', 'rpcexecmod', 'destin', 'function', 'thi', 'exec_typ', 'appropri', 'return', 'callback', 'user', 'recordfunct', 'finish', 'creat', 'name']}"
271,"{'func name': 'torch_sumall', 'comments': '', 'stemmed comments': []}"
272,"{'func name': 'get_all_nn_module_tests', 'comments': '', 'stemmed comments': []}"
273,"{'func name': 'attrs_with_prefix', 'comments': '', 'stemmed comments': []}"
274,"{'func name': 'jsd_grad', 'comments': '', 'stemmed comments': []}"
275,"{'func name': '_kl_cauchy_cauchy', 'comments': '', 'stemmed comments': []}"
276,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
277,"{'func name': 'generate_training_nets', 'comments': '', 'stemmed comments': []}"
278,"{'func name': '_layer_norm_grad_ref', 'comments': '', 'stemmed comments': []}"
279,"{'func name': 'is_request_only_scalar', 'comments': '', 'stemmed comments': []}"
280,"{'func name': '_strong_wolfe', 'comments': '', 'stemmed comments': []}"
281,"{'func name': 'compare_rowwise', 'comments': '', 'stemmed comments': []}"
282,"{'func name': 'FakeQuantization8BitsRowwise', 'comments': '', 'stemmed comments': []}"
283,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
284,"{'func name': 'match_declarations_with_differentiability_info', 'comments': 'Sets the ""derivative"" and ""output_differentiability"" key on declarations to matching differentiability info\n\nIn-place functions will use the out-of-place derivative definition if there is no in-place specific derivative.\n', 'stemmed comments': ['declar', 'outofplac', 'set', 'function', 'specif', 'use', 'output_differenti', 'match', 'definit', 'deriv', 'differenti', 'key', 'info', 'inplac']}"
285,"{'func name': '_batch_lowrank_mahalanobis', 'comments': 'Uses ""Woodbury matrix identity"":: inv(W @ W.T + D) = inv(D) - inv(D) @ W @ inv(C) @ W.T @ inv(D), where :math:`C` is the capacitance matrix :math:`I + W.T @ inv(D) @ W`, to compute the squared Mahalanobis distance :math:`x.T @ inv(W @ W.T + D) @ x`.\n\n\n', 'stemmed comments': ['matrix', 'comput', 'capacit', 'mahalanobi', 'math', 'squar', 'WT', 'x', 'ident', 'xT', 'D', '=', 'W', 'I', 'woodburi', 'distanc', 'inv', 'C', '@', 'use']}"
286,"{'func name': 'GetArgumentParser', 'comments': '', 'stemmed comments': []}"
287,"{'func name': 'Compare', 'comments': '', 'stemmed comments': []}"
288,"{'func name': 'count_blobs', 'comments': '', 'stemmed comments': []}"
289,"{'func name': 'collect_blob_sizes', 'comments': '', 'stemmed comments': []}"
290,"{'func name': 'convert_conv2d_weight_memory_format', 'comments': ""Convert ``memory_format`` of ``nn.Conv2d.weight`` to ``memory_format`` The conversion recursively applies to nested ``nn.Module``, including ``module``. Note that it only changes the memory_format, but not the semantics of each dimensions. This function is used to facilitate the computation to adopt NHWC kernels, which provides considerable speed up for fp16 data on CUDA devices with compute capability >= 7.0\n\n.. note:: Calling ``model.to(memory_format=torch.channels_last)`` is more aggressive than the utility function ``convert_conv2d_weight_memory_format``. Any layer with 4d weight will be affected by ``model.to``, which does not necessarily benefit from conversion to specified ``memory_format``. One place we are confident in is that NHWC(channels_last) conversion for convolution in cuDNN, As it is beneficial to run convolution in NHWC, even in cases where we have to apply permutation to input tensors.\n\nHence our strategy here is to convert only the weight of convolution to channels_last. This ensures that; 1. Fast convolution kernels will be used, the benefit of which could outweigh overhead of permutation (if input is not in the same format) 2. No unnecessary permutations are applied on layers that do not benefit from memory_format conversion.\n\nThe optimal case is that, layers between convolution layers are channels last compatible. Input tensor would be permuted to channels last when it encounters the first convolution layer and stay in that memory format. Hence following convolutions will not need to permute its input tensor.\n\nIn case where a channels last incompatible layer is between convolution layers, we need to permute the input tensor back to contiguous format for that layer. The input tensor will go through the remaining layers in contiguous format and be permuted to channels last when it encounters another convolution layer. There's no point in propagating that permutation to an earlier layer, as most layers are quite agnostic to ``memory_format``.\n\nThis claim might change when PyTorch supports fusion of permutation, as there might have been a better spot to fuse the permutation other than immediately before a convolution.\n##### Args\n* **module (nn.Module)**: ``nn.Conv2d`` & ``nn.ConvTranspose2d``  or container\n                    ``nn.Module``\n\n* **format**: user specified ``memory_format``,\n    e.g. ``torch.channels_last`` or ``torch.contiguous_format``\n\n##### Returns\n* **ple**: \n\n* **>>>  # This is identical to**: \n\n"", 'stemmed comments': ['recurs', 'optim', 'facilit', 'cuda', 'support', 'weight', 'capabl', 'convers', '=', 'strategi', 'format', 'call', 'follow', 'one', '&', 'immedi', 'speed', 'pytorch', 'nnmodul', 'layer', 'torchcontiguous_format', 'consider', 'In', 'benefici', 'place', 'modul', 'point', 'nest', 'contigu', 'eg', 'ensur', 'better', 'nnconv2d', '2', 'fusion', 'nnconvtranspose2d', '>', 'No', '1', 'input', 'memori', 'case', 'outweigh', 'dimens', 'propag', 'memory_format=torchchannels_last', 'even', 'henc', 'benefit', 'spot', 'function', 'affect', 'util', 'thi', ';', 'quit', 'arg', 'anoth', 'fp16', 'remain', 's', 'user', '70', 'first', 'ple', 'use', 'permut', 'provid', 'includ', 'specifi', 'note', 'nhwc', 'run', 'back', 'encount', 'contain', 'nnconv2dweight', 'memory_format', 'incompat', 'the', 'could', '4d', 'might', 'there', 'last', 'earlier', 'torchchannels_last', 'channels_last', 'unnecessari', 'appli', 'comput', 'stay', 'would', 'need', 'adopt', 'kernel', 'aggress', 'compat', 'data', 'cudnn', 'chang', 'devic', 'ident', 'necessarili', 'convert', 'overhead', 'fast', 'claim', 'channel', 'agnost', 'fuse', 'modelto', 'return', 'confid', 'convolut', 'As', 'ani', 'semant', 'convert_conv2d_weight_memory_format', 'tensor', 'go']}"
291,"{'func name': 'memory_summary', 'comments': 'Returns a human-readable printout of the current memory allocator statistics for a given device.\n\nThis can be useful to display periodically during training, or when handling out-of-memory exceptions.\n\nArguments: device (torch.device or int, optional): selected device. Returns printout for the current device, given by :func:`~torch.cuda.current_device`, if :attr:`device` is ``None`` (default). abbreviated (bool, optional): whether to return an abbreviated summary (default: False).\n\n.. note:: See :ref:`cuda-memory-management` for more details about GPU memory management.\n', 'stemmed comments': ['none', 'torchdevic', 'see', 'period', 'note', 'gpu', 'argument', 'memori', 'given', 'current', 'handl', 'devic', 'display', 'attr', 'train', 'select', 'outofmemori', 'alloc', 'thi', 'ref', 'func', 'cudamemorymanag', 'abbrevi', 'return', 'printout', 'option', '~torchcudacurrent_devic', 'bool', 'int', 'detail', 'humanread', 'default', 'manag', 'except', 'fals', 'summari', 'whether', 'use', 'statist']}"
292,"{'func name': 'merge_id_lists_ref', 'comments': '', 'stemmed comments': []}"
293,"{'func name': 'gradient_checker_device_option', 'comments': '', 'stemmed comments': []}"
294,"{'func name': 'to_mkldnn', 'comments': '', 'stemmed comments': []}"
295,"{'func name': 'Export', 'comments': 'Returns init_net and predict_net suitable for writing to disk and loading into a Predictor\n\n\n', 'stemmed comments': ['predictor', 'load', 'write', 'disk', 'predict_net', 'suitabl', 'init_net', 'return']}"
296,"{'func name': 'generate_mobile_module_lints', 'comments': 'Args: script_module: An instance of torch script module with type of ScriptModule\n\n\n##### Returns\n* **lint_map**: A list of dictionary that contains modules lints\n\n', 'stemmed comments': ['arg', 'type', 'A', 'lint_map', 'torch', 'script_modul', 'dictionari', 'instanc', 'modul', 'An', 'scriptmodul', 'list', 'contain', 'script', 'return', 'lint']}"
297,"{'func name': '_data', 'comments': '', 'stemmed comments': []}"
298,"{'func name': 'ExtractPredictorNet', 'comments': ""Takes a model net for training and returns a net which can be used for prediction. For example, all gradient operators and input operators are removed. @param net_proto protobuf of the net you want to process (net.Proto()) @param input_blobs list/set of blob names that are the inputs of predictor @param output_blobs list/set of blob names that are outputs of predictor @param device optional device option that is assigned @param renames dictionary of blob name to a new name (optional) @param disabled_inputs optional set of blobs that are 'switched off'. This will cause branches with those blobs as inputs to be removed\n\n\n"", 'stemmed comments': ['predictor', 'renam', 'input', 'net', 'output', 'exampl', 'switch', 'assign', 'devic', 'list/set', 'net_proto', 'protobuf', 'dictionari', 'predict', 'train', 'netproto', 'new', 'gradient', 'for', 'caus', 'thi', 'return', 'blob', 'model', 'take', 'option', 'branch', 'process', 'set', 'input_blob', 'disabled_input', 'remov', 'oper', 'want', 'param', '@', 'use', 'name', 'output_blob']}"
299,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
300,"{'func name': 'build_cpp_tests', 'comments': '', 'stemmed comments': []}"
301,"{'func name': 'import_module', 'comments': '', 'stemmed comments': []}"
302,"{'func name': 'register_module_backward_hook', 'comments': 'Registers a backward hook common to all the modules.\n\n.. warning :: This adds global state to the `nn.module` module and it is only intended for debugging/profiling purposes.\n\nThe current implementation will not have the presented behavior for complex :class:`Module` that perform many operations. In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only contain the gradients for a subset of the inputs and outputs. For such :class:`Module`, you should use :func:`torch.Tensor.register_hook` directly on a specific input or output to get the required gradients.\n\nThe hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature::\n\nhook(module, grad_input, grad_output) -> Tensor or None\n\nThe :attr:`grad_input` and :attr:`grad_output` may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments.\n\nGlobal hooks are called before hooks registered with `register_backward_hook`\n##### Returns\n* ****: class\n\n', 'stemmed comments': ['posit', 'respect', 'comput', 'regist', 'none', '>', 'correspond', 'may', 'specif', 'mani', 'state', 'register_backward_hook', 'requir', 'contain', 'subset', 'input', 'argument', 'signatur', 'backward', 'output', 'given', 'case', 'tensor', 'current', 'directli', 'time', 'multipl', 'torchtensorregister_hook', 'attr', 'behavior', 'perform', 'purpos', 'grad_input', 'intend', 'new', 'gradient', 'the', 'implement', 'for', 'call', 'complex', 'follow', 'hook', 'debugging/profil', 'thi', 'global', 'failur', 'func', 'nnmodul', 'common', 'return', 'warn', 'option', 'add', 'modifi', 'tupl', 'subsequ', 'In', 'everi', 'oper', 'place', 'modul', 'present', 'class', 'grad_output', 'use', 'get']}"
303,"{'func name': 'getClassFromModule', 'comments': '', 'stemmed comments': []}"
304,"{'func name': 'SetupMPI', 'comments': '', 'stemmed comments': []}"
305,"{'func name': 'AllreduceFallback', 'comments': 'A fallback option for Allreduce with no assumption on p2p.\n\nAlgorithm: a flat operation on gpu 0 0r <- 0 0r <- 0r + i for i in gpu_indices[1:] ir <- 0r for i in gpu_indices[1:]\n', 'stemmed comments': ['p2p', 'A', ']', '<', 'fallback', '0r', '1', 'assumpt', 'flat', 'oper', '0', 'allreduc', 'ir', 'gpu', 'algorithm', '[', 'gpu_indic', 'option']}"
306,"{'func name': 'benchmark_mul_gradient', 'comments': '', 'stemmed comments': []}"
307,"{'func name': '_precision_to_scale_tril', 'comments': '', 'stemmed comments': []}"
308,"{'func name': 'run', 'comments': '', 'stemmed comments': []}"
309,"{'func name': 'benchmark', 'comments': '', 'stemmed comments': []}"
310,"{'func name': 'reduce_scatter', 'comments': '', 'stemmed comments': []}"
311,"{'func name': '_test_if', 'comments': '', 'stemmed comments': []}"
312,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
313,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
314,"{'func name': 'example_job', 'comments': '', 'stemmed comments': []}"
315,"{'func name': 'debug_net', 'comments': 'Given a Net, produce another net that logs info about the operator call before each operator execution. Use for debugging purposes.\n\n\n', 'stemmed comments': ['anoth', 'execut', 'log', 'oper', 'debug', 'produc', 'purpos', 'use', 'info', 'net', 'given', 'call']}"
316,"{'func name': 'run', 'comments': '', 'stemmed comments': []}"
317,"{'func name': 'has_avx2', 'comments': '', 'stemmed comments': []}"
318,"{'func name': 'div_float_nofuture', 'comments': '', 'stemmed comments': []}"
319,"{'func name': 'transpose_network', 'comments': 'Convert all Convolutions operators which are in the NCHW order to NHWC order and also transform their inputs and outputs so that the rest of the graph is not affected.\n\n\n', 'stemmed comments': ['convolut', 'order', 'affect', 'oper', 'nhwc', 'also', 'graph', 'rest', 'nchw', 'input', 'convert', 'output', 'transform']}"
320,"{'func name': 'render', 'comments': '', 'stemmed comments': []}"
321,"{'func name': 'tanh', 'comments': 'Tanh.\n\n\n', 'stemmed comments': ['tanh']}"
322,"{'func name': 'moments_with_running_stats', 'comments': '', 'stemmed comments': []}"
323,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
324,"{'func name': 'build_test_net', 'comments': '', 'stemmed comments': []}"
325,"{'func name': 'mark', 'comments': 'Describe an instantaneous event that occurred at some point.\n\nArguments: msg (string): ASCII message to associate with the event.\n', 'stemmed comments': ['instantan', 'ascii', 'string', 'occur', 'point', 'event', 'msg', 'argument', 'messag', 'associ', 'describ']}"
326,"{'func name': '_with_args', 'comments': 'Wrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same constructor arguments, but different instances.\n\nExample::\n\n>>> Foo.with_args = classmethod(_with_args) >>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42) >>> foo_instance1 = foo_builder() >>> foo_instance2 = foo_builder() >>> id(foo_instance1) == id(foo_instance2) False\n', 'stemmed comments': ['foo_instance1', 'constructor', '>', '_with_arg', 'foo_build', 'need', 'argument', 'exampl', 'with_arg', '==', 'instanc', 'answer=42', 'b=4', 'id', '=', 'factori', 'classmethod', 'thi', 'allow', 'a=3', 'wrapper', 'creation', 'creat', 'differ', 'foowith_arg', 'fals', 'class', 'use', 'foo_instance2']}"
327,"{'func name': '_one_hots', 'comments': '', 'stemmed comments': []}"
328,"{'func name': 'onnxifi_caffe2_net', 'comments': 'Transform the caffe2_net by collapsing ONNXIFI-runnable nodes into Onnxifi c2 ops\n\n\n', 'stemmed comments': ['node', 'onnxifirunn', 'collaps', 'onnxifi', 'c2', 'op', 'transform', 'caffe2_net']}"
329,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
330,"{'func name': 'convert', 'comments': '', 'stemmed comments': []}"
331,"{'func name': 'gen_names', 'comments': '', 'stemmed comments': []}"
332,"{'func name': 'setThrowIfFpExceptions', 'comments': '', 'stemmed comments': []}"
333,"{'func name': 'reshape_from_tensor_shape', 'comments': '', 'stemmed comments': []}"
334,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
335,"{'func name': '_wait_for_all', 'comments': '', 'stemmed comments': []}"
336,"{'func name': 'build_rms_prop', 'comments': '', 'stemmed comments': []}"
337,"{'func name': 'assembleAllOutputs', 'comments': '', 'stemmed comments': []}"
338,"{'func name': 'run_testing_net', 'comments': '', 'stemmed comments': []}"
339,"{'func name': 'parallel_apply', 'comments': 'Applies each `module` in :attr:`modules` in parallel on arguments contained in :attr:`inputs` (positional) and :attr:`kwargs_tup` (keyword) on each of :attr:`devices`.\n\n\n##### Args\n* **modules (Module)**: modules to be parallelized\n\n* **inputs (tensor)**: inputs to the modules\n\n* **devices (list of int or torch.device)**: CUDA devices\n\n* **r**: `devices` (if given) should all have same length. Moreover, each\n\n* **ent of **: attr\n\n', 'stemmed comments': ['posit', 'torchdevic', 'argument', 'input', 'cuda', 'contain', 'given', 'devic', 'attr', 'ent', 'r', 'moreov', 'length', 'kwargs_tup', 'keyword', 'list', 'arg', 'int', 'parallel', 'modul', 'tensor', 'appli']}"
340,"{'func name': 'dequeue_value', 'comments': '', 'stemmed comments': []}"
341,"{'func name': 'run_worker', 'comments': '', 'stemmed comments': []}"
342,"{'func name': 'bmuf_process', 'comments': '', 'stemmed comments': []}"
343,"{'func name': 'ParameterSharing', 'comments': ""Helper function for sharing scopes. All the parameters within the shared_scopes, will be remapped with the respect of CurrentNamescope()\n\nI.e. if one calls ParameterSharing with {'scope_b': 'scope_'a'}, from the scope 'some_global_scope', it'll effectively mean, that all parameters from 'some_global_scope/scope_b' will shared with the parameters from 'some_global_scope/scope_a'\n"", 'stemmed comments': ['respect', 'mean', 'within', 'll', 'Ie', 'shared_scop', 'some_global_scope/scope_b', 'effect', 'helper', 'share', 'some_global_scope/scope_a', '{', 'call', 'one', 'parametershar', 'function', 'paramet', 'scope_b', 'scope', '}', 'scope_', 'all', 'some_global_scop', 'remap', 'currentnamescop']}"
344,"{'func name': 'parse_parity_tracker_table', 'comments': '', 'stemmed comments': []}"
345,"{'func name': 'pin_memory', 'comments': '', 'stemmed comments': []}"
346,"{'func name': '_pipe_step', 'comments': '\n\n\n', 'stemmed comments': []}"
347,"{'func name': 'clean_worker', 'comments': '', 'stemmed comments': []}"
348,"{'func name': 'max_pool_with_index', 'comments': 'Max pooling with an explicit index of max position\n\n\n', 'stemmed comments': ['posit', 'index', 'pool', 'explicit', 'max']}"
349,"{'func name': 'load_from_db', 'comments': '', 'stemmed comments': []}"
350,"{'func name': 'AddModelIdArg', 'comments': 'Takes the model_id from the predict_net of meta_net_def (if it is populated) and adds it to the net_def passed in. This is intended to be called on init_nets, as their model_id is not populated by default, but should be the same as that of the predict_net\n\n\n', 'stemmed comments': ['add', 'net_def', 'popul', 'meta_net_def', 'thi', 'model_id', 'default', 'init_net', 'pass', 'predict_net', 'intend', 'take', 'call']}"
351,"{'func name': 'run', 'comments': '', 'stemmed comments': []}"
352,"{'func name': 'parse_reports', 'comments': '', 'stemmed comments': []}"
353,"{'func name': 'insert', 'comments': '', 'stemmed comments': []}"
354,"{'func name': 'full_profile', 'comments': '', 'stemmed comments': []}"
355,"{'func name': 'run_profiler_benchmark_parallel', 'comments': '', 'stemmed comments': []}"
356,"{'func name': 'build_table', 'comments': 'Prints a summary of events (which can be a list of FunctionEvent or FunctionEventAvg).\n\n\n', 'stemmed comments': ['print', 'functioneventavg', 'functionev', 'summari', 'list', 'event']}"
357,"{'func name': 'profile', 'comments': '', 'stemmed comments': []}"
358,"{'func name': '_compute_norm', 'comments': ""Compute the L_n-norm across all entries in tensor `t` along all dimension except for the one identified by dim. Example: if `t` is of shape, say, 3x2x4 and dim=2 (the last dim), then norm will have Size [4], and each entry will represent the `L_n`-norm computed using the 3x2=6 entries for each of the 4 channels.\n\n\n##### Args\n* **t (torch.Tensor)**: tensor representing the parameter to prune\n\n* **n (int, float, inf, -inf, 'fro', 'nuc')**: See documentation of valid\n    entries for argument p in torch.norm\n\n* **dim (int)**: dim identifying the channels to prune\n\n##### Returns\n* **norm (torch.Tensor)**: L_n norm computed across all dimensions except\n    for `dim`. By construction, `norm.shape = t.shape[-1]`.\n\n"", 'stemmed comments': ['comput', 'torchnorm', '1', 'see', 'identifi', 'prune', '3x2=6', 'tshape', 'argument', 'l_n', 'By', 'construct', 'exampl', 'inf', ']', 'float', 'dimens', 'nuc', 'valid', 'across', 'p', 'use', 'shape', '[', '=', 'size', 'along', 'torchtensor', 'one', 'l_nnorm', 'n', 'dim', 'channel', 'paramet', 'entri', 'fro', 'last', 'return', '3x2x4', 'arg', 'document', 'normshap', 'say', 'repres', 'norm', 'int', 'except', '4', 'tensor', 'dim=2']}"
359,"{'func name': 'op_builder', 'comments': '', 'stemmed comments': []}"
360,"{'func name': 'PyTorchModule', 'comments': 'Embed an ONNX-exportable PyTorch Model into a Caffe2 model being built.\n\nArguments: helper (caffe2.python.core.ModelHelder): the model helper where this imported network should be inserted model (torch.nn.Module): the model to be exported sample_arguments (tuple of arguments): the inputs to the model, e.g., such that ``model(*args)`` is a valid invocation of the model.\n\nAny non-Variable arguments will be hard-coded into the exported model; any Variable arguments will become inputs of the exported model, in the order they occur in args.\n\nIf args is a Variable, this is equivalent to having called it with a 1-ary tuple of that Variable. (Note: passing keyword arguments to the model is not currently supported.\n\nGive us a shout if you need it.) caffe2_inputs (list of str or caffe2.python.core.BlobReference): the caffe2 Blobs that should be inputs to this network. Must be the same length as sample_arguments prefix_name: prefix name to add to each member of the blob, if None then a fresh prefix pytorch_input_N/ is used\n##### Returns\n', 'stemmed comments': ['none', '1ari', 'caffe2_input', 'us', 'need', 'note', 'fresh', 'input', 'argument', 'str', 'pass', 'helper', 'support', 'caffe2pythoncoreblobrefer', 'give', 'current', 'valid', 'caffe2', 'onnxexport', 'use', 'invoc', 'insert', 'call', 'length', 'import', 'member', 'equival', 'prefix_nam', 'must', 'sample_argu', ';', 'becom', 'keyword', 'occur', 'nonvari', 'built', 'pytorch', 'list', 'hardcod', 'return', 'blob', 'model', 'emb', 'arg', 'add', 'network', 'prefix', 'torchnnmodul', 'If', 'order', 'caffe2pythoncoremodelheld', 'variabl', 'ani', 'shout', 'export', 'eg', 'tupl', 'name', 'pytorch_input_n/']}"
361,"{'func name': 'get_default_qat_qconfig', 'comments': '', 'stemmed comments': []}"
362,"{'func name': 'quantize_dynamic_jit', 'comments': ""Quantize the input float TorchScript model with post training dynamic quantization. Currently only qint8 quantization of torch.nn.Linear is supported.\n\n\n##### Args\n* **`model`**: input float TorchScript model\n\n* **`qconfig_dict`**: qconfig_dict is a dictionary with names of sub modules as key and\n\n* **descriptions in **: func\n\n* **`inplace`**: carry out model transformations in-place, the original module is\n\n* **`debug`**: flag for producing a debug friendly model (preserve weight attribute)\n\n* **rn**: \n\n* **ple**: \n\n* **calibrate(model, data_loader)**: \n\n* **with torch.no_grad()**: for image, target in data_loader\n\n* **{''**: qconfig},\n\n"", 'stemmed comments': ['descript', 'imag', 'friendli', 'rn', 'input', 'support', 'inplac', 'current', 'float', 'torchno_grad', 'quantiz', 'weight', 'dictionari', 'train', 'debug', 'attribut', '{', 'carri', 'calibr', 'qconfig', 'torchnnlinear', 'origin', 'target', 'func', 'sub', 'torchscript', '}', 'model', 'post', 'arg', 'flag', 'qint8', 'qconfig_dict', 'modul', 'produc', 'data_load', 'dynam', 'ple', 'key', 'name', 'transform', 'preserv']}"
363,"{'func name': 'get_observer_dict', 'comments': 'Traverse the modules and save all observers into dict. This is mainly used for quantization accuracy debug Args: mod: the top module we want to save all observers prefix: the prefix for the current module target_dict: the dictionary used to save all the observers\n\n\n', 'stemmed comments': ['mainli', 'accuraci', 'mod', 'dict', 'current', 'dictionari', 'debug', 'use', 'travers', 'top', 'thi', 'save', 'arg', 'target_dict', 'prefix', 'observ', 'want', 'modul', 'quantiz']}"
364,"{'func name': 'quantize_rnn_modules', 'comments': '', 'stemmed comments': []}"
365,"{'func name': 'close_queue', 'comments': '', 'stemmed comments': []}"
366,"{'func name': 'parse_args', 'comments': '', 'stemmed comments': []}"
367,"{'func name': 'initial_seed', 'comments': 'Returns the current random seed of the current GPU.\n\n.. warning:: This function eagerly initializes CUDA.\n', 'stemmed comments': ['current', 'random', 'initi', 'function', 'seed', 'thi', 'gpu', 'cuda', 'return', 'warn', 'eagerli']}"
368,"{'func name': 'fork_rng', 'comments': 'Forks the RNG, so that when you return, the RNG is reset to the state that it was previously in.\n\nArguments: devices (iterable of CUDA IDs): CUDA devices for which to fork the RNG.\n\nCPU RNG state is always forked.\n\nBy default, :meth:`fork_rng` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed enabled (bool): if ``False``, the RNG is not forked.\n\nThis is a convenience argument for easily disabling the context manager without having to delete it and unindent your Python code under it.\n', 'stemmed comments': ['python', 'specifi', 'explicitli', 'conveni', 'state', 'run', 'unind', 'argument', 'cuda', 'By', 'fork', 'case', 'devic', 'emit', 'suppress', 'cpu', 'meth', 'disabl', 'id', 'alway', 'function', 'thi', 'delet', 'easili', 'iter', 'context', 'return', 'warn', 'sinc', 'code', 'previous', 'machin', 'reset', 'If', 'bool', 'oper', 'slowli', 'default', 'enabl', 'manag', 'fork_rng', 'fals', 'lot', 'rng', 'without']}"
369,"{'func name': 'primefac', 'comments': '', 'stemmed comments': []}"
370,"{'func name': 'run_bench', 'comments': '', 'stemmed comments': []}"
371,"{'func name': 'retrieve_step_blobs', 'comments': 'Retrieves blobs from step workspaces (which contain intermediate recurrent network computation for each timestep) and puts them in the global workspace. This allows access to the contents of this intermediate computation in python. Returns the list of extracted blob names.\n\nnet: the net from which the step workspace blobs should be extracted\n\nprefix: prefix to append to extracted blob names when placing them in the global workspace\n', 'stemmed comments': ['comput', 'python', 'recurr', 'contain', 'net', 'retriev', 'extract', 'step', 'timestep', 'append', 'content', 'global', 'thi', 'allow', 'list', 'return', 'intermedi', 'blob', 'workspac', 'network', 'prefix', 'place', 'put', 'access', 'name']}"
372,"{'func name': 'getNorm', 'comments': '', 'stemmed comments': []}"
373,"{'func name': 'init_reductions', 'comments': '', 'stemmed comments': []}"
374,"{'func name': 'create_scripted_module', 'comments': '', 'stemmed comments': []}"
375,"{'func name': '_create_module', 'comments': '', 'stemmed comments': []}"
376,"{'func name': 'gen_rendezvous_ctx', 'comments': '', 'stemmed comments': []}"
377,"{'func name': '_env_rendezvous_handler', 'comments': '', 'stemmed comments': []}"
378,"{'func name': 'pt_repeat_n_times', 'comments': '', 'stemmed comments': []}"
379,"{'func name': 'replicate', 'comments': '', 'stemmed comments': []}"
380,"{'func name': '_test_reshape', 'comments': '', 'stemmed comments': []}"
381,"{'func name': '_test_reshape_output_and_gradient', 'comments': '', 'stemmed comments': []}"
382,"{'func name': 'create_resnet50', 'comments': '', 'stemmed comments': []}"
383,"{'func name': 'complex_resnet', 'comments': '', 'stemmed comments': []}"
384,"{'func name': 'rewrite_model_helper_simple', 'comments': '', 'stemmed comments': []}"
385,"{'func name': '_prepare_rnn', 'comments': '', 'stemmed comments': []}"
386,"{'func name': 'prepare_mul_rnn', 'comments': '', 'stemmed comments': []}"
387,"{'func name': '_layered_LSTM', 'comments': '', 'stemmed comments': []}"
388,"{'func name': 'apply_permutation', 'comments': '', 'stemmed comments': []}"
389,"{'func name': 'apply_permutation', 'comments': '', 'stemmed comments': []}"
390,"{'func name': 'pack_sequence', 'comments': 'Packs a list of variable length Tensors\n\n``sequences`` should be a list of Tensors of size ``L x *``, where `L` is the length of a sequence and `*` is any number of trailing dimensions, including zero.\n\nFor unsorted sequences, use `enforce_sorted = False`. If ``enforce_sorted`` is ``True``, the sequences should be sorted in the order of decreasing length. ``enforce_sorted = True`` is only necessary for ONNX export.\n\n Example: >>> from torch.nn.utils.rnn import pack_sequence >>> a = torch.tensor([1,2,3]) >>> b = torch.tensor([4,5]) >>> c = torch.tensor([6]) >>> pack_sequence([a, b, c]) PackedSequence(data=tensor([ 1,\n\n4,\n\n6,\n\n2,\n\n5,\n\n3]), batch_sizes=tensor([ 3,\n\n2,\n\n1]))\n\n Arguments: sequences (list[Tensor]): A list of sequences of decreasing length. enforce_sorted (bool, optional): if ``True``, checks that the input contains sequences sorted by length in a decreasing order. If ``False``, this condition is not checked. Default: ``True``.\n##### Returns\n* **a **: class\n\n', 'stemmed comments': ['onnx', 'includ', '2', '>', '1', 'c', 'number', 'necessari', 'packedsequ', 'trail', 'argument', 'input', '123', '5', 'unsort', 'contain', 'exampl', 'x', 'tensor', 'true', '3', ']', 'dimens', 'pack_sequ', 'L', '=', '[', 'size', 'torchtensor', 'length', 'for', 'enforce_sort', 'import', 'A', 'torchnnutilsrnn', '45', 'list', 'zero', 'pack', 'return', 'b', 'sort', 'option', 'condit', 'bool', 'If', 'order', 'variabl', '6', 'default', 'fals', 'decreas', '4', 'batch_sizes=tensor', 'export', 'data=tensor', 'check', 'class', 'use', 'sequenc']}"
391,"{'func name': 'init_dropout_state', 'comments': '', 'stemmed comments': []}"
392,"{'func name': 'update_counter_ref', 'comments': '', 'stemmed comments': []}"
393,"{'func name': 'rpc_async_with_rref_arg', 'comments': '', 'stemmed comments': []}"
394,"{'func name': 'load_script_module_with_pickled_rref', 'comments': '', 'stemmed comments': []}"
395,"{'func name': 'return_future', 'comments': '', 'stemmed comments': []}"
396,"{'func name': '_invoke_rpc', 'comments': '', 'stemmed comments': []}"
397,"{'func name': 'is_cpu_only', 'comments': '', 'stemmed comments': []}"
398,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
399,"{'func name': 'get_nn_runners', 'comments': '', 'stemmed comments': []}"
400,"{'func name': 'sample_functional', 'comments': '', 'stemmed comments': []}"
401,"{'func name': 'gather', 'comments': 'Gathers tensors from different GPUs on a specified device (-1 means the CPU).\n\n\n', 'stemmed comments': ['mean', 'specifi', 'devic', '1', 'cpu', 'differ', 'gpu', 'gather', 'tensor']}"
402,"{'func name': 'attach_metadata_to_scalars', 'comments': '', 'stemmed comments': []}"
403,"{'func name': 'thread_runner', 'comments': '', 'stemmed comments': []}"
404,"{'func name': 'EmptyDeviceScope', 'comments': ""Allow users to 'disable' the device scope behaviour (so it can be controlled at a NetDef::DeviceOption level, not overridden at OperatorDef::DeviceOption level).\n\nThis sets the CurrentDeviceScope() to None, so that the field is not set in CreateOperator(...), etc.\n"", 'stemmed comments': ['none', 'operatordef', 'devic', 'createoper', 'disabl', 'deviceopt', 'overridden', 'control', 'netdef', 'field', 'etc', 'currentdevicescop', 'behaviour', 'thi', 'allow', 'scope', 'set', 'user', 'level']}"
405,"{'func name': 'recurrent_scaleshift', 'comments': '', 'stemmed comments': []}"
406,"{'func name': 'sparse_lengths_weighted_sum_grad_ref', 'comments': '', 'stemmed comments': []}"
407,"{'func name': 'output_projection', 'comments': '', 'stemmed comments': []}"
408,"{'func name': '_gather_padding_ref', 'comments': '', 'stemmed comments': []}"
409,"{'func name': 'deserialize_protobuf_struct', 'comments': '', 'stemmed comments': []}"
410,"{'func name': '_is_torchscript_zip', 'comments': '', 'stemmed comments': []}"
411,"{'func name': 'testWithArgs', 'comments': '', 'stemmed comments': []}"
412,"{'func name': 'print_box', 'comments': '', 'stemmed comments': []}"
413,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
414,"{'func name': 'create_shufflenet', 'comments': '', 'stemmed comments': []}"
415,"{'func name': '_set_SIGCHLD_handler', 'comments': '', 'stemmed comments': []}"
416,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
417,"{'func name': 'add_tensors_loop', 'comments': '', 'stemmed comments': []}"
418,"{'func name': 'benchmark_sparse_lengths_sum', 'comments': '', 'stemmed comments': []}"
419,"{'func name': 'benchmark_sparse_lengths_sum', 'comments': '', 'stemmed comments': []}"
420,"{'func name': '_is_id_score_list', 'comments': '', 'stemmed comments': []}"
421,"{'func name': 'test_reshape', 'comments': '', 'stemmed comments': []}"
422,"{'func name': 'netbuilder', 'comments': '', 'stemmed comments': []}"
423,"{'func name': 'spawn', 'comments': ""Spawns ``nprocs`` processes that run ``fn`` with ``args``.\n\nIf one of the processes exits with a non-zero exit status, the remaining processes are killed and an exception is raised with the cause of termination. In the case an exception was caught in the child process, it is forwarded and its traceback is included in the exception raised in the parent process.\n\nArguments: fn (function): Function is called as the entrypoint of the spawned process. This function must be defined at the top level of a module so it can be pickled and spawned. This is a requirement imposed by multiprocessing.\n\nThe function is called as ``fn(i, *args)``, where ``i`` is the process index and ``args`` is the passed through tuple of arguments.\n\nargs (tuple): Arguments passed to ``fn``. nprocs (int): Number of processes to spawn. join (bool): Perform a blocking join on all processes. daemon (bool): The spawned processes' daemon flag. If set to True, daemonic processes will be created. start_method (string): (deprecated) this method will always use ``spawn`` as the start method. To use a different start method use ``start_processes()``.\n##### Returns\n* ****: class\n\n"", 'stemmed comments': ['includ', 'termin', 'join', 'deprec', 'exit', 'number', 'requir', 'run', 'string', 'argument', 'block', 'pass', 'case', 'multiprocess', 'true', 'start_method', 'daemon', 'impos', 'To', 'method', 'perform', 'index', 'start', 'alway', 'use', 'call', 'the', 'one', 'caus', 'nonzero', 'rais', 'parent', 'function', 'kill', 'thi', 'must', 'defin', 'top', 'entrypoint', 'statu', 'start_process', 'nproc', 'return', 'forward', 'arg', 'process', 'caught', 'pickl', 'flag', 'child', 'bool', 'If', 'In', 'remain', 'int', 'level', 'set', 'creat', 'differ', 'except', 'modul', 'class', 'traceback', 'tupl', 'fn', 'spawn']}"
424,"{'func name': 'remove_spectral_norm', 'comments': 'Removes the spectral normalization reparameterization from a module.\n\n\n##### Args\n* **module (Module)**: containing module\n\n* **name (str, optional)**: name of weight parameter\n\n* **ple**: \n\n', 'stemmed comments': ['arg', 'reparameter', 'weight', 'remov', 'paramet', 'modul', 'normal', 'contain', 'str', 'ple', 'spectral', 'name', 'option']}"
425,"{'func name': 'grad', 'comments': '', 'stemmed comments': []}"
426,"{'func name': '_initialize_orthogonal', 'comments': '', 'stemmed comments': []}"
427,"{'func name': '_load_from_bytes', 'comments': '', 'stemmed comments': []}"
428,"{'func name': 'assign', 'comments': '', 'stemmed comments': []}"
429,"{'func name': '_string_lists', 'comments': '', 'stemmed comments': []}"
430,"{'func name': 'mesh', 'comments': 'Outputs a merged `Summary` protocol buffer with a mesh/point cloud.\n\n\n##### Args\n* **tag**: A name for this summary operation.\n\n* **vertices**: Tensor of shape `[dim_1, ..., dim_n, 3]` representing the 3D\n  coordinates of vertices.\n\n* **faces**: Tensor of shape `[dim_1, ..., dim_n, 3]` containing indices of\n  vertices within each triangle.\n\n* **colors**: Tensor of shape `[dim_1, ..., dim_n, 3]` containing colors for each\n  vertex.\n\n* **display_name**: If set, will be used as the display name in TensorBoard.\n  Defaults to `name`.\n\n* **description**: A longform readable description of the summary data. Markdown\n  is supported.\n\n* **config_dict**: Dictionary with ThreeJS classes names and configuration.\n\n##### Returns\n', 'stemmed comments': ['mesh/point', 'dim_n', 'coordin', 'within', 'buffer', 'merg', 'threej', 'indic', 'contain', 'data', 'vertex', 'vertic', 'support', 'output', 'display_nam', '3', ']', 'triangl', 'display', 'dictionari', 'use', 'shape', '[', 'A', 'configur', 'longform', 'dim_1', 'config_dict', 'markdown', '3D', 'return', 'arg', 'set', 'repres', 'color', 'If', 'readabl', 'oper', 'face', 'default', 'cloud', 'summari', 'class', 'descript', 'tag', 'protocol', 'tensor', 'tensorboard', 'name']}"
431,"{'func name': '_list_supported_ops', 'comments': '', 'stemmed comments': []}"
432,"{'func name': 'update_bn', 'comments': 'Updates BatchNorm running_mean, running_var buffers in the model.\n\nIt performs one pass over data in `loader` to estimate the activation statistics for BatchNorm layers in the model. Arguments: loader (torch.utils.data.DataLoader): dataset loader to compute the activation statistics on. Each data batch should be either a tensor, or a list/tuple whose first element is a tensor containing data. model (torch.nn.Module): model for which we seek to update BatchNorm statistics. device (torch.device, optional): If set, data will be trasferred to :attr:`device` before being passed into :attr:`model`.\n\nExample: >>> loader, model = ... >>> torch.optim.swa_utils.update_bn(loader, model)\n\n.. note:: The `update_bn` utility assumes that each data batch in :attr:`loader` is either a tensor or a list or tuple of tensors; in the latter case it is assumed that :meth:`model.forward()` should be called on the first element of the list or tuple corresponding to the data batch.\n', 'stemmed comments': ['activ', 'comput', '>', 'correspond', 'buffer', 'torchdevic', 'each', 'update_bn', 'torchoptimswa_utilsupdate_bn', 'note', 'element', 'argument', 'whose', 'data', 'pass', 'contain', 'exampl', 'case', 'devic', 'latter', 'estim', 'meth', 'attr', 'perform', 'torchutilsdatadataload', 'batchnorm', 'either', '=', 'running_var', 'call', 'the', 'one', 'dataset', 'util', ';', 'trasfer', 'list', 'loader', 'batch', 'list/tupl', 'layer', 'model', 'updat', 'option', 'set', 'assum', 'torchnnmodul', 'tupl', 'If', 'running_mean', 'modelforward', 'first', 'tensor', 'It', 'statist', 'seek']}"
433,"{'func name': 'sigmoid', 'comments': '', 'stemmed comments': []}"
434,"{'func name': '_cast_func_template', 'comments': '', 'stemmed comments': []}"
435,"{'func name': 'min', 'comments': '', 'stemmed comments': []}"
436,"{'func name': 'full_like', 'comments': '', 'stemmed comments': []}"
437,"{'func name': 'take', 'comments': '', 'stemmed comments': []}"
438,"{'func name': 'fake_quantize_per_tensor_affine', 'comments': '', 'stemmed comments': []}"
439,"{'func name': 'flatten', 'comments': '', 'stemmed comments': []}"
440,"{'func name': 'le', 'comments': '', 'stemmed comments': []}"
441,"{'func name': 'get_registered_op', 'comments': '', 'stemmed comments': []}"
442,"{'func name': 'final_output', 'comments': 'Adds an output to the current Task, or if no task is active, create a dummy task that returns the given blob or record to the client. This will return the value of the blob or record when the last task of the TaskGroup for a given node finishes.\n\n\n', 'stemmed comments': ['add', 'activ', 'task', 'current', 'record', 'last', 'node', 'valu', 'client', 'creat', 'thi', 'finish', 'taskgroup', 'dummi', 'return', 'blob', 'output', 'given']}"
443,"{'func name': 'get_engine', 'comments': '', 'stemmed comments': []}"
444,"{'func name': '_wrap_type_error_to_not_implemented', 'comments': '', 'stemmed comments': []}"
445,"{'func name': 'ops_to_graph_def', 'comments': '', 'stemmed comments': []}"
446,"{'func name': 'load_events', 'comments': '', 'stemmed comments': []}"
447,"{'func name': 'tensorboard_events', 'comments': '', 'stemmed comments': []}"
448,"{'func name': 'add_test', 'comments': '', 'stemmed comments': []}"
449,"{'func name': 'to_test_backend_multi', 'comments': '', 'stemmed comments': []}"
450,"{'func name': 'reference_spatialbn_test16', 'comments': '', 'stemmed comments': []}"
451,"{'func name': 'bundle_jpeg_image', 'comments': '', 'stemmed comments': []}"
452,"{'func name': 'save_and_load', 'comments': '', 'stemmed comments': []}"
453,"{'func name': 'create_tcp_store', 'comments': 'Creates a TCP store. Retries if the chosen port is already in use.\n\n\n', 'stemmed comments': ['store', 'retri', 'creat', 'port', 'chosen', 'tcp', 'use', 'alreadi']}"
454,"{'func name': 'run_generated_test', 'comments': '', 'stemmed comments': []}"
455,"{'func name': 'num_non_tensor_nodes', 'comments': '', 'stemmed comments': []}"
456,"{'func name': 'remove_build_path', 'comments': '', 'stemmed comments': []}"
457,"{'func name': 'get_cycles_per_ms', 'comments': 'Approximate number of cycles per millisecond for torch.cuda._sleep\n\n\n', 'stemmed comments': ['torchcuda_sleep', 'approxim', 'millisecond', 'number', 'per', 'cycl']}"
458,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
459,"{'func name': 'test_equality', 'comments': '', 'stemmed comments': []}"
460,"{'func name': 'canonical', 'comments': '', 'stemmed comments': []}"
461,"{'func name': 'worker_set_affinity', 'comments': '', 'stemmed comments': []}"
462,"{'func name': '_captured_output', 'comments': '', 'stemmed comments': []}"
463,"{'func name': 'is_all_nan', 'comments': 'Checks if all entries of a tensor is nan.\n\n\n', 'stemmed comments': ['check', 'tensor', 'nan', 'entri']}"
464,"{'func name': 'load_tests', 'comments': '', 'stemmed comments': []}"
465,"{'func name': 'add_one', 'comments': '', 'stemmed comments': []}"
466,"{'func name': 'benchmark', 'comments': '', 'stemmed comments': []}"
467,"{'func name': 'create_module', 'comments': '', 'stemmed comments': []}"
468,"{'func name': '_jit_disabled', 'comments': '', 'stemmed comments': []}"
469,"{'func name': 'warmup_forward', 'comments': '', 'stemmed comments': []}"
470,"{'func name': 'warmup_forward', 'comments': '', 'stemmed comments': []}"
471,"{'func name': 'normalize_check_ad', 'comments': '', 'stemmed comments': []}"
472,"{'func name': 'exportTest', 'comments': '', 'stemmed comments': []}"
473,"{'func name': 'test_nested', 'comments': '', 'stemmed comments': []}"
474,"{'func name': 'fs_sharing', 'comments': '', 'stemmed comments': []}"
475,"{'func name': 'out_fn', 'comments': '', 'stemmed comments': []}"
476,"{'func name': '_buildEquivalentAffineTransforms3d', 'comments': '', 'stemmed comments': []}"
477,"{'func name': 'check_onnx_opsets_operator', 'comments': '', 'stemmed comments': []}"
478,"{'func name': '_print_net', 'comments': '', 'stemmed comments': []}"
479,"{'func name': 'get_defaults', 'comments': '', 'stemmed comments': []}"
480,"{'func name': 'export_to_pb', 'comments': '', 'stemmed comments': []}"
481,"{'func name': 'drosenbrock', 'comments': '', 'stemmed comments': []}"
482,"{'func name': 'generate_tensor_like_override_tests', 'comments': '', 'stemmed comments': []}"
483,"{'func name': 'load_normalized_test_case', 'comments': '', 'stemmed comments': []}"
484,"{'func name': 'flatten', 'comments': '', 'stemmed comments': []}"
485,"{'func name': 'setup_rnn_tests', 'comments': '', 'stemmed comments': []}"
486,"{'func name': 'setup_rnn_tests', 'comments': '', 'stemmed comments': []}"
487,"{'func name': '_get_random_tensor_and_q_params', 'comments': '', 'stemmed comments': []}"
488,"{'func name': 'get_supported_device_types', 'comments': '', 'stemmed comments': []}"
489,"{'func name': 'cuda_only', 'comments': '', 'stemmed comments': []}"
490,"{'func name': 'write_proto', 'comments': '', 'stemmed comments': []}"
491,"{'func name': 'generate_not_implemented_tests', 'comments': '', 'stemmed comments': []}"
492,"{'func name': '_download_onnx_model', 'comments': '', 'stemmed comments': []}"
493,"{'func name': 'get_all_examples', 'comments': 'get_all_examples() -> str\n\nThis function grabs (hopefully all) examples from the torch documentation strings and puts them in one nonsensical module returned as a string.\n', 'stemmed comments': ['one', 'get_all_exampl', 'grab', 'document', '>', 'function', 'hope', 'torch', 'thi', 'string', 'nonsens', 'modul', 'put', 'str', 'return', 'exampl']}"
494,"{'func name': 'float_double_default_dtype', 'comments': '', 'stemmed comments': []}"
495,"{'func name': 'is_flaky_test_mode', 'comments': '', 'stemmed comments': []}"
496,"{'func name': 'print_net', 'comments': '', 'stemmed comments': []}"
497,"{'func name': 'sum_of_state_dict', 'comments': '', 'stemmed comments': []}"
498,"{'func name': '_get_buffer_ids', 'comments': 'Object addresses stay constant if and only if all modifications are in-place\n\n\n', 'stemmed comments': ['stay', 'address', 'constant', 'object', 'modif', 'inplac']}"
499,"{'func name': 'test_vl_py', 'comments': '', 'stemmed comments': []}"
500,"{'func name': 'drosenbrock', 'comments': '', 'stemmed comments': []}"
501,"{'func name': 'shutdown', 'comments': '', 'stemmed comments': []}"
502,"{'func name': '_get_thnn_function_backend', 'comments': '', 'stemmed comments': []}"
503,"{'func name': 'format_time', 'comments': 'Defines how to format time\n\n\n', 'stemmed comments': ['defin', 'format', 'time']}"
504,"{'func name': 'EuthanizeIfNecessary', 'comments': 'Call this if you have problem with process getting stuck at shutdown. It will kill the process if it does not terminate in timeout_secs.\n\n\n', 'stemmed comments': ['process', 'termin', 'kill', 'It', 'stuck', 'timeout_sec', 'problem', 'get', 'shutdown', 'call']}"
505,"{'func name': 'video_input', 'comments': '', 'stemmed comments': []}"
506,"{'func name': 'fused_rowwise_8bit_quantize_dequantize_reference', 'comments': '', 'stemmed comments': []}"
507,"{'func name': 'foo', 'comments': '', 'stemmed comments': []}"
508,"{'func name': 'add_weight_decay', 'comments': 'Adds a decay to weights in the model.\n\nThis is a form of L2 regularization.\n##### Args\n* **weight_decay**: strength of the regularization\n\n', 'stemmed comments': ['add', 'arg', 'regular', 'decay', 'weight', 'L2', 'thi', 'form', 'strength', 'model', 'weight_decay']}"
509,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
510,"{'func name': 'Optimize', 'comments': '', 'stemmed comments': []}"
511,"{'func name': 'transform_caffe2_net', 'comments': 'Transform the caffe2_net by collapsing TRT-runnable nodes into trt c2 ops\n\n\n', 'stemmed comments': ['node', 'trtrunnabl', 'collaps', 'c2', 'trt', 'op', 'transform', 'caffe2_net']}"
512,"{'func name': 'fuseConvBN', 'comments': '', 'stemmed comments': []}"
513,"{'func name': '_get_activation_fn', 'comments': '', 'stemmed comments': []}"
514,"{'func name': '_clipped_sigmoid', 'comments': '', 'stemmed comments': []}"
515,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
516,"{'func name': 'fc_net_to_tt_net', 'comments': '', 'stemmed comments': []}"
517,"{'func name': '_unique_ref', 'comments': '', 'stemmed comments': []}"
518,"{'func name': '_list_unsupported_tensor_ops', 'comments': '', 'stemmed comments': []}"
519,"{'func name': 'cleanup', 'comments': '', 'stemmed comments': []}"
520,"{'func name': 'onnx_verify', 'comments': '', 'stemmed comments': []}"
521,"{'func name': '_list_with_default', 'comments': '', 'stemmed comments': []}"
522,"{'func name': '_validate_dynamic_axes', 'comments': '', 'stemmed comments': []}"
523,"{'func name': 'check_onnx_broadcast', 'comments': '', 'stemmed comments': []}"
524,"{'func name': 'probs_to_logits', 'comments': 'Converts a tensor of probabilities into logits. For the binary case, this denotes the probability of occurrence of the event indexed by `1`. For the multi-dimensional case, the values along the last dimension denote the probabilities of occurrence of each of the events.\n\n\n', 'stemmed comments': ['case', 'for', 'probabl', 'binari', '1', 'dimens', 'occurr', 'multidimension', 'logit', 'index', 'event', 'denot', 'convert', 'tensor', 'last', 'valu', 'along']}"
525,"{'func name': 'benchmark_module', 'comments': '', 'stemmed comments': []}"
526,"{'func name': 'create_int8_bias_tensor_fill', 'comments': 'Similar to create_int8_given_tensor_fill, but for bias blobs to be stored as int32.\n\n\n', 'stemmed comments': ['int32', 'store', 'similar', 'create_int8_given_tensor_fil', 'bia', 'blob']}"
527,"{'func name': 'try_remove_folder', 'comments': '', 'stemmed comments': []}"
528,"{'func name': 'NCHW2NHWC', 'comments': '', 'stemmed comments': []}"
529,"{'func name': 'signature_without_args', 'comments': '', 'stemmed comments': []}"
530,"{'func name': '_ntuple_from_first', 'comments': 'Converts the argument to a tuple of size n with the first element repeated.\n\n\n', 'stemmed comments': ['n', 'repeat', 'first', 'element', 'argument', 'convert', 'tupl', 'size']}"
531,"{'func name': 'verify', 'comments': ""Export a model into ONNX, import it into a specified ONNX backend, and then on a few random inputs verify that PyTorch and the backend produced the same results.  Requires onnx to be installed.\n\nThis function may spuriously fail: some operators are implemented with different numerical precision in an ONNX backend, in which case an unstable network (e.g., Inception) may blow up these numerical instabilities.\n\nThis situation is less likely to happen if your model has been trained.\n\nHowever, if this is not the case, you may have found a bug!\n\nPlease report it to the PyTorch developers.\n\nYou can also debug the issue yourself by removing suffixes of operators from your model until verification passes.\n\nFor reproducibility, we recommend explicitly setting PyTorch's seed before invoking this function.\n\nArguments: model (torch.nn.Module): the model to be exported and verified args (tuple of arguments): the inputs to the model, e.g., such that ``model(*args)`` is a valid invocation of the model.\n\nAny non-Variable arguments will be hard-coded into the exported model; any Variable arguments will become inputs of the exported model, in the order they occur in args.\n\nIf args is a Variable, this is equivalent to having called it with a 1-ary tuple of that Variable. (Note: passing keyword arguments to the model is not currently supported.\n\nGive us a shout if you need it.) backend (onnx.backend module): ONNX backend to verify with verbose (bool, default False): if specified, we will print out a debug description of the trace being exported. training (bool, default False): export the model in training mode.\n\nAt the moment, ONNX is oriented towards exporting models for inference only, so you will generally not need to set this to True. rtol (float, default 1e-3): relative precision required test_args (int or iterable of args, default 2): either an integer specifying the number of random arguments to generate, or an iterable producing arguments to test under. opset_version (int, default None): the opset version of the model to export. If not specified, the default value in symboli_helper will be used in utils._export(). operator_export_type (enum, default OperatorExportTypes.ONNX): the operator export type to use when exporting the model. The default value converts all operators to ONNX ops.\n"", 'stemmed comments': ['verbos', 'utils_export', 'explicitli', 'may', 'version', 'like', 'support', 'valu', 'fail', 'orient', 'pleas', 'operatorexporttypesonnx', 'infer', 'call', '!', 'becom', 'happen', 'pytorch', 'integ', 'mode', 'remov', 'shout', 'differ', 'opset_vers', 'modul', 'produc', 'fals', 'export', 'eg', 'descript', 'trace', 'tupl', 'spurious', 'moment', 'float', 'op', 'howev', 'bug', '2', 'none', 'onnxbackend', 'backend', 'blow', 'requir', 'input', 'case', 'true', 'current', 'found', 'invok', 'train', 'debug', 'reproduc', 'precis', 'invoc', 'implement', 'unstabl', 'function', 'equival', 'thi', ';', 'keyword', 'iter', 'gener', 'verifi', 'arg', 'torchnnmodul', 'If', 's', 'int', 'variabl', '1e3', 'use', 'onnx', '1ari', 'specifi', 'rel', 'symboli_help', 'note', 'also', 'pass', 'test', 'instal', 'you', 'situat', 'the', 'result', 'import', 'occur', 'model', 'report', 'test_arg', 'set', 'suffix', 'print', 'bool', 'seed', 'At', 'rtol', 'oper', 'default', 'toward', 'type', 'verif', 'us', 'issu', 'need', 'number', 'enum', 'incept', 'argument', 'numer', 'give', 'opset', 'valid', 'either', 'convert', 'for', 'nonvari', 'operator_export_typ', 'less', 'hardcod', 'develop', 'network', 'random', 'recommend', 'order', 'ani', 'instabl']}"
532,"{'func name': 'ChannelLast', 'comments': 'Convert a CHW array to HWC.\n\n\n', 'stemmed comments': ['array', 'convert', 'chw', 'hwc']}"
533,"{'func name': '_rejection_sample', 'comments': '', 'stemmed comments': []}"
534,"{'func name': 'remove_weight_norm', 'comments': 'Removes the weight normalization reparameterization from a module.\n\n\n##### Args\n* **module (Module)**: containing module\n\n* **name (str, optional)**: name of weight parameter\n\n* **ple**: \n\n', 'stemmed comments': ['arg', 'reparameter', 'weight', 'remov', 'paramet', 'modul', 'normal', 'contain', 'str', 'ple', 'name', 'option']}"
535,"{'func name': 'wngrad_sparse_test_helper', 'comments': '', 'stemmed comments': []}"
536,"{'func name': '_worker_loop', 'comments': '', 'stemmed comments': []}"
537,"{'func name': '_Blob_to_torch', 'comments': '', 'stemmed comments': []}"
