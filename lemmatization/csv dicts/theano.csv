0,"{'func name': 'get_versions', 'comments': 'Get version information or return default if unable to do so.\n\n\n', 'stemmed comments': ['inform', 'default', 'unabl', 'return', 'version', 'get']}"
1,"{'func name': 'bilinear_upsampling', 'comments': 'Compute bilinear upsampling This function will build the symbolic graph for upsampling a tensor by the given ratio using bilinear interpolation.\n\nParameters ---------- input: symbolic 4D tensor mini-batch of feature map stacks, of shape (batch size, input channels, input rows, input columns) that will be upsampled. ratio: `int or Constant or Scalar Tensor of int* dtype` the ratio by which the input is upsampled in the 2D space (row and col size). frac_ratio: None, tuple of int or tuple of tuples of int The tuple defining the fractional ratio by which the input is upsampled in the 2D space. One fractional ratio should be represented as (numerator, denominator). If row and col ratios are different frac_ratio should be a tuple of fractional ratios, i.e a tuple of tuples. use_1D_kernel: bool if set to true, row and column will be upsampled separately by 1D kernels, otherwise they are upsampled together using a 2D kernel. The final result is the same, only the speed can differ, given factors such as upsampling ratio.\n##### Returns\n* ****: note\n\n', 'stemmed comments': ['graph', 'bilinear', 'shape', 'defin', 'one', 'col', 'differ', 'tensor', 'bool', 'return', 'row', 'paramet', 'final', 'numer', 'use', 'ratio', 'ie', 'channel', 'togeth', 'set', 'map', 'input', '2D', 'space', 'featur', 'the', 'use_1d_kernel', 'build', 'frac_ratio', 'true', 'If', 'thi', 'upsampl', 'otherwis', 'result', 'note', '4D', 'minibatch', 'symbol', 'comput', 'given', 'stack', 'dtype', 'tupl', 'fraction', 'factor', '1D', 'function', 'batch', 'none', 'denomin', 'column', 'speed', 'separ', 'constant', 'int', 'scalar', 'kernel', 'repres', 'size', 'interpol']}"
2,"{'func name': 'profile_printer', 'comments': '', 'stemmed comments': []}"
3,"{'func name': 'theano_dtype', 'comments': '', 'stemmed comments': []}"
4,"{'func name': 'handle_composite', 'comments': '', 'stemmed comments': []}"
5,"{'func name': 'dot', 'comments': 'Operation for efficiently calculating the dot product when one or all operands is sparse. Supported format are CSC and CSR. The output of the operation is dense.\n\nParameters ---------- x Sparse or dense matrix variable. y Sparse or dense matrix variable.\n##### Returns\n', 'stemmed comments': ['one', 'csr', 'support', 'dot', 'return', 'paramet', 'x', 'effici', 'format', 'calcul', 'matrix', 'the', 'dens', 'csc', 'product', 'oper', 'operand', 'spars', 'variabl', 'output']}"
6,"{'func name': 'choose', 'comments': 'Construct an array from an index array and a set of arrays to choose from.\n\nFirst of all, if confused or uncertain, definitely look at the Examples\n\n- in its full generality, this function is less simple than it might seem from the following code description (below ndi = numpy.lib.index_tricks):\n\nnp.choose(a,c) == np.array([c[a[I]][I] for I in ndi.ndindex(a.shape)]).\n\nBut this omits some subtleties. Here is a fully general summary:\n\nGiven an ``index`` array (a) of integers and a sequence of n arrays (choices), a and each choice array are first broadcast, as necessary, to arrays of a common shape; calling these Ba and Bchoices[i], i = 0,...,n-1 we have that, necessarily, Ba.shape == Bchoices[i].shape for each i. Then, a new array with shape Ba.shape is created as follows:\n\n- if mode=raise (the default), then, first of all, each element of a (and thus Ba) must be in the range [0, n-1]; now, suppose that i (in that range) is the value at the (j0, j1, ..., jm) position in Ba\n\n- then the value at the same position in the new array is the value in Bchoices[i] at that same position;\n\n- if mode=wrap, values in a (and thus Ba) may be any (signed) integer; modular arithmetic is used to map integers outside the range [0, n-1] back into that range; and then the new array is constructed as above;\n\n- if mode=clip, values in a (and thus Ba) may be any (signed) integer; negative integers are mapped to 0; values greater than n-1 are mapped to n-1; and then the new array is constructed as above.\n\nParameters ---------- a : int array This array must contain integers in [0, n-1], where n is the number of choices, unless mode=wrap or mode=clip, in which cases any integers are permissible. choices : sequence of arrays Choice arrays. a and all of the choices must be broadcastable to the same shape. If choices is itself an array (not recommended), then its outermost dimension (i.e., the one corresponding to choices.shape[0]) is taken as defining the ``sequence``. out : array, optional If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype. mode : {``raise`` (default), ``wrap``, ``clip``}, optional Specifies how indices outside [0, n-1] will be treated: ``raise`` : an exception is raised ``wrap`` : value becomes value mod n ``clip`` : values < 0 are mapped to 0, values > n-1 are mapped to n-1\n##### Returns\n', 'stemmed comments': ['I', 'must', 'defin', 'back', 'may', 'return', 'outsid', 'might', 'arithmet', 'specifi', 'valu', 'exampl', 'creat', 'descript', 'clip', 'If', 'sequenc', 'bchoic', 'suppos', 'summari', '{', 'dtype', 'new', 'function', 'broadcast', 'element', 'wrap', 'taken', 'nparray', 'gener', 'default', 'definit', 'follow', 'j0', 'jm', 'c', 'choic', 'common', 'simpl', 'rais', 'bashap', 'set', 'mode=rais', 'map', '==', 'thu', 'modular', 'code', 'result', 'integ', 'sign', '}', '<', 'necessari', 'ndindindex', 'confus', 'number', 'ndi', 'It', ']', 'outermost', 'less', 'numpylibindex_trick', 'index', 'contain', 'shape', 'dimens', 'appropri', 'seem', 'ie', 'greater', 'unless', 'posit', 'construct', 'provid', 'look', 'j1', 'thi', 'given', 'Ba', 'except', 'mod', 'n1', 'ashap', '[', 'fulli', 'choicesshap', 'full', 'necessarili', '=', 'then', 'insert', 'mode=wrap', 'subtleti', 'mode', 'call', 'one', 'becom', 'mode=clip', 'case', 'omit', 'uncertain', 'paramet', ';', 'correspond', '>', 'use', 'recommend', 'permiss', 'treat', 'n', 'array', 'indic', 'choos', 'here', 'first', '0', 'rang', 'option', 'npchoos', 'neg', 'int', 'but']}"
7,"{'func name': 'make_c_gemv_destructive', 'comments': '', 'stemmed comments': []}"
8,"{'func name': '____gemm_code', 'comments': '', 'stemmed comments': []}"
9,"{'func name': 'make_ger_destructive', 'comments': '', 'stemmed comments': []}"
10,"{'func name': 'local_inplace_gpuagemmbatch', 'comments': '', 'stemmed comments': []}"
11,"{'func name': 'local_print_as_we_go_along', 'comments': '', 'stemmed comments': []}"
12,"{'func name': 'sparse_block_dot', 'comments': 'Compute the dot product (plus bias) of the specified pieces of vectors and matrices. See SparseBlockGemv to get more information.\n\nThe parameter types are actually their expected shapes relative to each other.\n\nParameters ---------- W : iBlocks, oBlocks, iSize, oSize weight matrix h : batch, iWin, iSize input from lower layer (sparse) inputIdx : batch, iWin indexes of the input blocks b : oBlocks, oSize bias vector outputIdx : batch, oWin indexes of the output blocks\n##### Returns\n', 'stemmed comments': ['inform', 'sparseblockgemv', 'shape', 'plu', 'index', 'dot', 'b', 'return', 'paramet', 'expect', 'specifi', 'piec', 'matric', 'inputidx', 'matrix', 'input', 'the', 'W', 'osiz', 'block', 'oblock', 'rel', 'actual', 'layer', 'type', 'outputidx', 'isiz', 'comput', 'lower', 'product', 'batch', 'see', 'vector', 'iwin', 'owin', 'bia', 'spars', 'h', 'get', 'weight', 'iblock', 'output']}"
13,"{'func name': 'local_abstract_batch_norm_inference', 'comments': '', 'stemmed comments': []}"
14,"{'func name': 'filter_output', 'comments': '', 'stemmed comments': []}"
15,"{'func name': 'inline_ofg_expansion', 'comments': 'This optimization expands internal graph of OpFromGraph. Only performed if node.op.is_inline == True Doing so can improve optimization at the cost of compilation speed.\n\n\n', 'stemmed comments': ['graph', 'intern', 'do', 'speed', 'expand', 'nodeopis_inlin', 'perform', '==', 'opfromgraph', 'cost', 'true', 'improv', 'onli', 'optim', 'thi', 'compil']}"
16,"{'func name': 'burn', 'comments': '', 'stemmed comments': []}"
17,"{'func name': '_default_checker', 'comments': 'Default checker for DualLinker. This checks that the variables contain the same data using ==.\n\nParameters: ---------- x,y the variables to compare data\n', 'stemmed comments': ['default', 'contain', 'duallink', 'checker', '==', 'paramet', 'check', 'variabl', 'use', 'x', 'compar', 'data', 'thi']}"
18,"{'func name': 'test', 'comments': '', 'stemmed comments': []}"
19,"{'func name': 'test_true_half_config_support', 'comments': '', 'stemmed comments': []}"
20,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
21,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
22,"{'func name': 'icc_module_compile_str', 'comments': '', 'stemmed comments': []}"
23,"{'func name': 'basecompiledir_purge', 'comments': '', 'stemmed comments': []}"
24,"{'func name': 'refresh_lock', 'comments': ""'Refresh' an existing lock by re-writing the file containing the owner's unique id, using a new (randomly generated) id, which is also returned.\n\n\n"", 'stemmed comments': ['contain', 'randomli', 'rewrit', 'uniqu', 'file', 'return', 'lock', 'also', 'id', 'owner', 'use', 'new', 'exist', 'refresh', 'gener', 's']}"
25,"{'func name': 'linkcode_resolve', 'comments': '', 'stemmed comments': []}"
26,"{'func name': 'ref_cast', 'comments': '', 'stemmed comments': []}"
27,"{'func name': 'default_compiledir', 'comments': '', 'stemmed comments': []}"
28,"{'func name': 'BoolParam', 'comments': '', 'stemmed comments': []}"
29,"{'func name': 'local_inplace_DiagonalSubtensor', 'comments': 'Also work for IncDiagonalSubtensor.\n\n\n', 'stemmed comments': ['work', 'incdiagonalsubtensor', 'also']}"
30,"{'func name': 'conv2d', 'comments': ""signal.conv.conv2d performs a basic 2D convolution of the input with the given filters. The input parameter can be a single 2D image or a 3D tensor, containing a set of images. Similarly, filters can be a single 2D filter or a 3D tensor, corresponding to a set of 2D filters.\n\nShape parameters are optional and will result in faster execution.\n\nParameters ---------- input\n\n : Symbolic theano tensor for images to be filtered. Dimensions: ([num_images], image height, image width) filters : Symbolic theano tensor for convolution filter(s). Dimensions: ([num_filters], filter height, filter width) border_mode: {'valid', 'full'} See scipy.signal.convolve2d. subsample Factor by which to subsample output. image_shape : tuple of length 2 or 3 ([num_images,] image height, image width). filter_shape : tuple of length 2 or 3 ([num_filters,] filter height, filter width). kwargs See theano.tensor.nnet.conv.conv2d.\n##### Returns\n"", 'stemmed comments': ['contain', 'shape', 'dimens', 'tensor', 'return', 'paramet', 'signalconvconv2d', 'correspond', 'set', '2D', 'input', 'singl', 'convolut', 'the', 'theano', 'width', 'image_shap', 'theanotensornnetconvconv2d', 'subsampl', 'length', 'similarli', 'result', 'basic', '3D', 'execut', 'faster', 'height', 'valid', 'border_mod', 'symbol', 'scipysignalconvolve2d', '}', '{', 'given', 'tupl', 'kwarg', 'factor', '[', 'imag', 'num_imag', 'see', '2', 'filter_shap', 'full', 'option', 'num_filt', 'perform', ']', '3', 'filter', 'output']}"
31,"{'func name': 'gen_conv_code_unroll_batch_kern', 'comments': 'c_code for ConvOp that unroll the batch size loop.\n\n\n', 'stemmed comments': ['unrol', 'convop', 'c_code', 'loop', 'size', 'batch']}"
32,"{'func name': 'cpuCount', 'comments': 'Returns the number of CPUs in the system\n\n\n', 'stemmed comments': ['cpu', 'system', 'return', 'number']}"
33,"{'func name': 'local_gpu_ctc_no_grad', 'comments': '', 'stemmed comments': []}"
34,"{'func name': 'local_ctc_no_grad', 'comments': '', 'stemmed comments': []}"
35,"{'func name': 'get_definitions', 'comments': 'Return cuDNN definitions to be used by Theano for the given cuDNN version.\n\n``cudnn_version`` must be None or an integer (typically the version returned by :func:`theano.gpuarray.dnn.version`). if None, return definitions for the\n\nmost recent supported cuDNN version.\n', 'stemmed comments': ['must', 'definit', 'none', 'support', 'theanogpuarraydnnvers', 'typic', 'theano', 'return', 'integ', 'cudnn_vers', 'given', 'version', 'use', 'func', 'recent', 'cudnn']}"
36,"{'func name': 'compile_cutils', 'comments': 'Do just the compilation of cutils_ext.\n\n\n', 'stemmed comments': ['cutils_ext', 'Do', 'compil']}"
37,"{'func name': 'd3write', 'comments': 'Convert Theano graph to pydot graph and write to dot file.\n\nParameters ---------- fct : theano.compile.function_module.Function A compiled Theano function, variable, apply or a list of variables. path: str Path to output file\n\nNotes ----- This function accepts extra parameters which will be forwarded to :class:`theano.d3viz.formatting.PyDotFormatter`.\n', 'stemmed comments': ['graph', 'accept', 'dot', 'write', 'paramet', 'compil', 'forward', 'fct', 'convert', 'file', 'theano', 'list', 'A', 'thi', 'note', 'class', 'function', 'theanod3vizformattingpydotformatt', 'str', 'appli', 'extra', 'theanocompilefunction_modulefunct', 'variabl', 'pydot', 'path', 'output']}"
38,"{'func name': '_check_preallocated_output', 'comments': 'Try to apply thunk() on different output storages.\n\n\n', 'stemmed comments': ['differ', 'storag', 'tri', 'thunk', 'appli', 'output']}"
39,"{'func name': 'fast_inplace_check', 'comments': 'Return the variables in inputs that are posible candidate for as inputs of inplace operation.\n\nParameters ---------- inputs : list Inputs Variable that you want to use as inplace destination.\n', 'stemmed comments': ['candid', 'posibl', 'input', 'destin', 'inplac', 'return', 'list', 'paramet', 'want', 'variabl', 'use', 'oper']}"
40,"{'func name': 'disturb_mem', 'comments': '', 'stemmed comments': []}"
41,"{'func name': 'local_abstract_batch_norm_inference_cudnn', 'comments': '', 'stemmed comments': []}"
42,"{'func name': 'make_loop_careduce', 'comments': ""Make a nested loop over several arrays and associate specific code to each level of nesting.\n\nParameters ---------- loop_orders : list of N tuples of length M Each value of each tuple can be either the index of a dimension to loop over or the letter 'x' which means there is no looping to be done over that variable at that point (in other words we broadcast over that dimension). If an entry is an integer, it will become an alias of the entry of that rank. loop_tasks : list of M+1 pieces of code The ith loop_task is a pair of strings, the first string is code to be executed before the ith loop starts, the second one contains code to be executed just before going to the next element of the ith dimension. The last element if loop_tasks is a single string, containing code to be executed at the very end. sub: dictionary Maps 'lv#' to a suitable variable name. The 'lvi' variable corresponds to the ith element of loop_orders.\n"", 'stemmed comments': ['index', 'contain', 'one', 'word', 'dimens', 'each', 'becom', 'suitabl', 'second', 'loop', 'paramet', 'ith', 'correspond', 'x', 'point', 'specif', 'piec', 'valu', 'next', 'singl', 'done', 'loop_ord', 'array', 'map', 'the', 'list', 'alia', 'dictionari', 'mean', 'level', 'length', 'code', 'If', 'entri', 'rank', 'start', 'pair', 'make', 'letter', 'execut', 'string', 'integ', 'M', 'tupl', 'either', 'first', 'lvi', 'broadcast', 'element', 'N', 'M1', 'go', 'end', 'sub', 'name', 'sever', 'loop_task', 'last', 'associ', 'nest', 'variabl', 'lv']}"
43,"{'func name': 'runScript', 'comments': '', 'stemmed comments': []}"
44,"{'func name': 'ElemwiseOpTime', 'comments': '', 'stemmed comments': []}"
45,"{'func name': 'gpu_ca_reduce_cuda', 'comments': '', 'stemmed comments': []}"
46,"{'func name': 'TensorConstant', 'comments': '', 'stemmed comments': []}"
47,"{'func name': 'numpy_sub', 'comments': '', 'stemmed comments': []}"
48,"{'func name': 'local_gpua_cumop', 'comments': '', 'stemmed comments': []}"
49,"{'func name': 'ravel_multi_index', 'comments': ""Converts a tuple of index arrays into an array of flat indices, applying boundary modes to the multi-index.\n\nParameters ---------- multi_index : tuple of Theano or NumPy arrays A tuple of integer arrays, one array for each dimension. dims : tuple of ints The shape of array into which the indices from ``multi_index`` apply. mode : {'raise', 'wrap', 'clip'}, optional Specifies how out-of-bounds indices are handled.\n\nCan specify either one mode or a tuple of modes, one mode per index. * 'raise' -- raise an error (default) * 'wrap' -- wrap around * 'clip' -- clip to the range In 'clip' mode, a negative index which would normally wrap will clip to 0 instead. order : {'C', 'F'}, optional Determines whether the multi-index should be viewed as indexing in row-major (C-style) or column-major (Fortran-style) order.\n##### Returns\n* **raveled_indices **: Theano array\n    An array of indices into the flattened version of an array\n    of dimensions ``dims``.\n\n"", 'stemmed comments': ['index', 'default', 'one', 'flat', 'dimens', 'shape', 'normal', 'return', 'F', 'boundari', 'paramet', 'dim', 'order', 'rais', 'around', 'specifi', 'whether', 'per', 'error', 'An', 'convert', 'array', 'indic', 'the', 'theano', 'A', 'view', 'clip', 'version', 'rowmajor', 'In', 'cstyle', 'multiindex', 'multi_index', 'integ', '}', '{', 'tupl', 'handl', 'either', 'would', 'columnmajor', 'determin', 'appli', 'wrap', 'raveled_indic', '0', 'fortranstyl', 'outofbound', 'rang', 'option', 'neg', 'can', 'flatten', 'C', 'int', 'numpi', 'mode', 'instead']}"
50,"{'func name': '_unitary', 'comments': '', 'stemmed comments': []}"
51,"{'func name': '_unitary', 'comments': '', 'stemmed comments': []}"
52,"{'func name': 'dict_to_pdnode', 'comments': 'Create pydot node from dict.\n\n\n', 'stemmed comments': ['pydot', 'node', 'dict', 'creat']}"
53,"{'func name': 'dct_matrix', 'comments': ""Return a (rows x cols) matrix implementing a discrete cosine transform.\n\nThis algorithm is adapted from Dan Ellis' Rastmat spec2cep.m, lines 15-20.\n"", 'stemmed comments': ['implement', 'cosin', 'matrix', 'col', 'elli', 'adapt', 'line', '1520', 'return', 'transform', 'row', 'x', 'rastmat', 'algorithm', 'dan', 'spec2cepm', 'discret', 'thi']}"
54,"{'func name': 'write_w', 'comments': ""Return the function name to write data.\n\nThis should be used like this::\n\ncode = 'res = %s(oval)' % (write_w(output_type),)\n"", 'stemmed comments': ['oval', 'output_typ', 're', 'name', 'return', '=', 'write', 'like', 'write_w', 'use', 'function', 'data', 'code', 'thi', '%']}"
55,"{'func name': 'get_info_on_inputs', 'comments': 'Return a human-readable description of named and un-named inputs.\n\n\n', 'stemmed comments': ['input', 'name', 'return', 'humanread', 'descript', 'unnam']}"
56,"{'func name': 'function', 'comments': 'Return a :class:`callable object <theano.compile.function_module.Function>` that will calculate `outputs` from `inputs`.\n\nParameters ---------- inputs : list of either Variable or In instances. Function parameters, these are not allowed to be shared variables. outputs : list or dict of Variables or Out instances. If it is a dict, the keys must be strings. Expressions to compute. mode : string or `Mode` instance. Compilation mode. updates : iterable over pairs (shared_variable, new_expression). List, tuple or OrderedDict. Updates the values for SharedVariable inputs according to these expressions. givens : iterable over pairs (Var1, Var2) of Variables. List, tuple or dict. The Var1 and Var2 in each pair must have the same Type. Specific substitutions to make in the computation graph (Var2 replaces Var1). no_default_updates: either bool or list of Variables If True, do not perform any automatic update on Variables. If False (default), perform them all. Else, perform automatic updates on all Variables that are neither in ""updates"" nor in ""no_default_updates"". accept_inplace : bool True iff the graph can contain inplace operations prior to the optimization phase (default is False). *Note* this parameter is unsupported, and its use is not recommended. name : str An optional name for this function. The profile mode will print the time spent in this function. rebuild_strict : bool True (Default) is the safer and better tested setting, in which case `givens` must substitute new variables with the same Type as the variables they replace. False is a you-better-know-what-you-are-doing setting, that permits `givens` to replace variables with new variables of any Type. The consequence of changing a Type is that all results depending on that variable may have a different Type too (the graph is rebuilt from inputs to outputs). If one of the new types does not make sense for one of the Ops in the graph, an Exception will be raised. allow_input_downcast: bool or None True means that the values passed as inputs when calling the function can be silently down-casted to fit the dtype of the corresponding Variable, which may lose precision. False means that it will only be cast to a more general, or precise, type. None (default) is almost like False, but allows down-casting of Python float scalars to floatX. profile: None, True, or ProfileStats instance Accumulate profiling information into a given ProfileStats instance. If argument is `True` then a new ProfileStats instance will be used. If argument is a string, a new ProfileStats instance will be created with that string as its ``message`` attribute. This profiling object will be available via self.profile. on_unused_input What to do if a variable in the \'inputs\' list is not used in the graph. Possible values are \'raise\', \'warn\', \'ignore\' and None.\n##### Returns\n* ****: class\n\n* **Regarding givens**: Be careful to make sure that these\n\n* **Internal documentation**: What happens when you call theano.function?\n       1. RemoveShared\n\n', 'stemmed comments': ['must', 'shared_vari', 'Be', 'out', 'iff', 'may', 'return', 'object', 'sens', 'automat', 'valu', 'inplac', 'allow_input_downcast', 'list', 'creat', 'floatx', 'accept_inplac', 'mean', 'If', 'substitut', 'lose', 'In', 'new_express', 'dtype', 'new', 'function', 'share', 'callabl', 'theanofunct', 'warn', 'test', 'permit', 'theanocompilefunction_modulefunct', 'safer', 'gener', 'graph', 'default', 'inform', 'ordereddict', 'consequ', 'attribut', 'allow', 'phase', 'rais', 'calcul', 'set', 'print', 'fals', 'removeshar', 'fit', 'sharedvari', 'express', 'what', 'result', 'pair', 'avail', '<', 'comput', 'oper', 'sure', 'neither', 'chang', 'scalar', 'replac', 'var1', 'output', 'contain', 'instanc', 'downcast', 'spent', 'prior', 'bool', 'like', 'time', 'profilestat', 'var2', 'compil', '?', 'input', 'An', 'float', 'better', 'key', 'silent', 'the', 'dict', 'updat', 'via', 'no_default_upd', 'true', 'pass', 'thi', 'note', 'unsupport', 'op', 'happen', 'tupl', 'given', 'except', 'str', 'rebuilt', 'none', 'messag', 'intern', 'name', 'python', 'variabl', 'mode', 'depend', 'rebuild_strict', 'accord', 'call', 'possibl', 'one', 'differ', 'case', 'ignor', 'accumul', '>', 'paramet', 'document', 'correspond', 'use', 'recommend', 'specif', 'argument', 'on_unused_input', 'youbetterknowwhatyouaredo', '1', 'make', 'precis', 'string', 'type', 'care', 'either', 'class', 'els', 'optim', 'cast', 'profil', 'option', 'selfprofil', 'perform', 'almost', 'iter', 'regard']}"
57,"{'func name': 'get_pulls_list', 'comments': 'get pull request list\n\ngithub_api : version of github api to use\n', 'stemmed comments': ['request', 'api', 'github', 'list', 'use', 'version', 'pull', 'get', 'github_api']}"
58,"{'func name': 'grad_scale', 'comments': 'This op scale or inverse the gradient in the backpropagation.\n\nParameters ---------- x: The variable we want its gradient inputs scale multiplier: Scale of the gradient\n\nExamples -------- >>> x = theano.tensor.fscalar() >>> fx = theano.tensor.sin(x) >>> fp = theano.tensor.grad(fx, wrt=x) >>> fprime = theano.function([x], fp) >>> print(fprime(2))\n\n# doctest: +ELLIPSIS -0.416... >>> f_inverse=grad_scale(fx, -1.) >>> fpp = theano.tensor.grad(f_inverse, wrt=x) >>> fpprime = theano.function([x], fpp) >>> print(fpprime(2))\n\n# doctest: +ELLIPSIS 0.416...\n', 'stemmed comments': ['fprime', 'f_inverse=grad_scal', '>', 'paramet', 'x', 'fpp', 'input', 'exampl', 'print', 'scale', 'the', 'theanotensorfscalar', '0416', 'doctest', '1', 'thi', 'fx', 'op', 'gradient', 'fp', 'invers', 'theanotensorsin', '[', 'wrt=x', 'theanofunct', 'fpprime', 'theanotensorgrad', '=', ']', 'want', 'variabl', 'ellipsi', 'f_invers', 'backpropag', 'multipli', '2']}"
59,"{'func name': 'nodes_constructed', 'comments': 'A contextmanager that is used in inherit_stack_trace and keeps track of all the newly created varaible nodes inside an optimization. A list of new_nodes is instantiated but will be filled in a lazy manner (when Variable.notify_construction_observers is called).\n\n`observer` is the entity that updates the new_nodes list. construction_observers is a list inside Variable class and contains a list of observer functions. The observer functions inside construction_observers are only called when a variable node is instantiated (where Variable.notify_construction_observers is called). When the observer function is called, a new variable node is added to the new_nodes list.\n\n Parameters ---------- new_nodes A list of all the variable nodes that are created inside the optimization.\n\nyields new_nodes list.\n', 'stemmed comments': ['contain', 'newli', 'paramet', 'use', 'inherit_stack_trac', 'ad', 'track', 'the', 'list', 'creat', 'manner', 'A', 'updat', 'new_nod', 'varaibl', 'fill', 'observ', 'when', 'lazi', 'yield', 'class', 'new', 'function', 'optim', 'insid', 'node', 'construction_observ', 'keep', 'instanti', 'entiti', 'variabl', 'contextmanag', 'call', 'variablenotify_construction_observ']}"
60,"{'func name': 'cond_merge_random_op', 'comments': '', 'stemmed comments': []}"
61,"{'func name': 'inc_code', 'comments': '', 'stemmed comments': []}"
62,"{'func name': 'transpose_inplace', 'comments': 'Perform a transpose on a tensor without copying the underlying storage\n\n\n', 'stemmed comments': ['transpos', 'storag', 'tensor', 'without', 'perform', 'underli', 'copi']}"
63,"{'func name': 'mpi_tag_key', 'comments': 'Break MPI ties by using the variable tag - prefer lower tags first.\n\n\n', 'stemmed comments': ['prefer', 'mpi', 'tag', 'first', 'lower', 'variabl', 'use', 'tie', 'break']}"
64,"{'func name': 'inline_softmax_fixed_shared', 'comments': ""Generate code to perform softmax with a fixed amount of shared memory.\n\nOn entry, `buf` is assumed to be empty.\n\nOn exit, `buf[0]` contains the softmax, `buf2` contains un-normalized softmax.\n\nParameters ---------- N Length of the buffer, atleast waprSize(32). buf A shared memory buffer of size warpSize * sizeof(dtype). x A ptr to the gpu memory where the row is stored. stride_x The stride between each element in x. load_x Wrapper to read from x. sm A ptr to the gpu memory to store the result. sm_stride The stride between each sm element. write_sm Wrapper before writing to sm. threadPos Index of executing thread. threadCount Number of executing threads. b Optional, pointer to the bias. stride_b Optional, the stride of b if b is provided. load_b Optional, wrapper to read from b if b is provided. dtype Optional, the dtype of the softmax's output if not float32.\n\nNotes ----- `buf` should be in gpu shared memory, we access it many times.\n\nWe use tx as an int variable in a loop.\n"", 'stemmed comments': ['index', 'contain', 'thread', 'mani', 'b', 'loop', 'On', 'paramet', 'atleast', 'row', 'unnorm', 'x', 'write', 'assum', 'buffer', 'write_sm', 'store', 'time', 'use', '32', 'read', 'threadpo', 'stride', 'the', 'pointer', 'provid', 'exit', 'buf', 'A', 'load_x', 'empti', 'length', 'waprsiz', 'threadcount', 'gpu', 'entri', 'code', 'result', 'note', 'execut', 'amount', 'warpsiz', 'fix', 'sm', 'access', 'dtype', 'wrapper', 'sm_stride', 'number', '[', 'element', 'stride_b', 'N', 's', 'share', 'float32', 'We', 'stride_x', 'output', '0', 'ptr', 'sizeof', 'memori', 'option', 'softmax', 'bia', 'tx', 'perform', ']', 'int', 'size', 'load_b', 'variabl', 'gener', 'buf2']}"
65,"{'func name': 'try_reload', 'comments': '', 'stemmed comments': []}"
66,"{'func name': 'gpu_qr', 'comments': 'This function performs the QR on GPU.\n\nParameters ---------- complete : bool, optional If `False`, returns only r.\n##### Returns\n* **Q, R **: matrices\n\n', 'stemmed comments': ['matric', 'R', 'option', 'bool', 'complet', 'perform', 'fals', 'return', 'r', 'Q', 'paramet', 'function', 'gpu', 'If', 'thi', 'QR']}"
67,"{'func name': 'WrapLinkerMany', 'comments': 'Variant on WrapLinker that runs a series of wrapper functions instead of just one.\n\n\n', 'stemmed comments': ['one', 'variant', 'run', 'wrapper', 'seri', 'wraplink', 'function', 'instead']}"
68,"{'func name': 'inner_fct', 'comments': '', 'stemmed comments': []}"
69,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
70,"{'func name': 'may_share_memory', 'comments': '', 'stemmed comments': []}"
71,"{'func name': 'test_mlp', 'comments': 'Demonstrate stochastic gradient descent optimization for a multilayer perceptron\n\nThis is demonstrated on MNIST.\n\n:type learning_rate: float :param learning_rate: learning rate used (factor for the stochastic gradient\n\n:type n_epochs: int :param n_epochs: maximal number of epochs to run the optimizer\n\n:type dataset: string :param dataset: the path of the MNIST dataset file from http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n', 'stemmed comments': ['descent', 'multilay', 'learn', 'use', 'perceptron', 'dataset', 'float', 'file', 'epoch', 'rate', 'thi', 'gradient', 'string', 'n_epoch', 'type', 'maxim', 'factor', 'demonstr', 'number', '//wwwiroumontrealca/~lisa/deep/data/mnist/mnistpklgz', 'optim', 'http', 'learning_r', 'run', 'int', 'path', 'mnist', 'stochast', 'param']}"
72,"{'func name': 'register_mode', 'comments': 'Add a `Mode` which can be referred to by `name` in `function`.\n\n\n', 'stemmed comments': ['name', 'refer', 'mode', 'function', 'add']}"
73,"{'func name': 'detect_nan', 'comments': '', 'stemmed comments': []}"
74,"{'func name': 'local_gpua_multinomial_wor', 'comments': '', 'stemmed comments': []}"
75,"{'func name': 'f_compute', 'comments': '', 'stemmed comments': []}"
76,"{'func name': 'neibs2images', 'comments': 'Function :func:`neibs2images <theano.sandbox.neighbours.neibs2images>` performs the inverse operation of :func:`images2neibs <theano.sandbox.neigbours.neibs2images>`. It inputs the output of :func:`images2neibs <theano.sandbox.neigbours.neibs2images>` and reconstructs its input.\n\nParameters ---------- neibs : 2d tensor Like the one obtained by :func:`images2neibs <theano.sandbox.neigbours.neibs2images>`. neib_shape `neib_shape` that was used in :func:`images2neibs <theano.sandbox.neigbours.neibs2images>`. original_shape Original shape of the 4d tensor given to :func:`images2neibs <theano.sandbox.neigbours.neibs2images>`\n##### Returns\n* **object\n    Reconstructs the input of\n    **: func\n\n* ****: func\n\n* **.. code-block**: \n\n* **.. note**: \n\n', 'stemmed comments': ['images2neib', 'one', 'shape', 'theanosandboxneigboursneibs2imag', 'tensor', 'return', '4d', 'neib_shap', '>', 'paramet', 'like', 'origin', 'object', 'use', '2d', 'func', 'obtain', 'neibs2imag', 'codeblock', 'theanosandboxneighboursneibs2imag', 'input', 'reconstruct', 'original_shap', 'note', '<', 'neib', 'given', 'invers', 'function', 'oper', 'It', 'perform', 'output']}"
77,"{'func name': 'tensorsolve', 'comments': ""Theano utilization of numpy.linalg.tensorsolve. Does not run on GPU!\n\nSolve the tensor equation ``a x = b`` for x. It is assumed that all indices of `x` are summed over in the product, together with the rightmost indices of `a`, as is done in, for example, ``tensordot(a, x, axes=len(b.shape))``.\n\nParameters ---------- a : array_like Coefficient tensor, of shape ``b.shape + Q``. `Q`, a tuple, equals the shape of that sub-tensor of `a` consisting of the appropriate number of its rightmost indices, and must be such that ``prod(Q) == prod(b.shape)`` (in which sense `a` is said to be 'square'). b : array_like Right-hand tensor, which can be of any shape. axes : tuple of ints, optional Axes in `a` to reorder to the right, before inversion. If None (default), no reordering is done.\n##### Returns\n* **x **: ndarray, shape Q\n\n"", 'stemmed comments': ['must', 'default', 'shape', 'rightmost', 'tensor', 'numpylinalgtensorsolv', 'b', 'return', '!', 'paramet', 'appropri', 'x', 'assum', 'prod', 'ndarray', 'sens', 'togeth', 'squar', 'done', 'indic', 'exampl', 'bshape', 'theano', 'Q', '==', 'solv', 'gpu', 'If', 'consist', 'array_lik', 'axes=len', 'right', 'tupl', 'util', 'invers', 'number', 'sum', 'product', 'doe', 'coeffici', 'axe', 'tensordot', 'It', 'none', 'righthand', 'option', 'said', 'run', '=', 'subtensor', 'int', 'equat', 'reorder', 'equal']}"
78,"{'func name': 'confusion_matrix', 'comments': 'Computes the confusion matrix of given vectors containing actual observations and predicted observations.\n\nParameters ---------- actual : 1-d tensor variable pred : 1-d tensor variable\n##### Returns\n* **conf_mat **: Confusion matrix of actual and predictions observations as shown below.\n           | Predicted\n\n* **order **: 1-d array of order of entries in rows and columns\n\n', 'stemmed comments': ['contain', 'tensor', 'return', 'shown', 'row', 'paramet', 'order', 'matrix', 'array', 'predict', '|', 'entri', '1d', 'actual', 'observ', 'comput', 'given', 'confus', 'vector', 'column', 'conf_mat', 'variabl', 'pred']}"
79,"{'func name': 'apply_meth', 'comments': '', 'stemmed comments': []}"
80,"{'func name': 'spectral_radius_bound', 'comments': 'Returns upper bound on the largest eigenvalue of square symmetrix matrix X.\n\nlog2_exponent must be a positive-valued integer. The larger it is, the slower and tighter the bound. Values up to 5 should usually suffice. The algorithm works by multiplying X by itself this many times.\n\nFrom V.Pan, 1990. ""Estimating the Extremal Eigenvalues of a Symmetric Matrix"", Computers Math Applic. Vol 20 n. 2 pp 17-22. Rq: an efficient algorithm, not used here, is defined in this paper.\n', 'stemmed comments': ['must', 'defin', 'log2_expon', 'mani', 'return', 'applic', 'tighter', 'time', 'from', 'estim', 'use', 'effici', 'vpan', 'usual', 'work', 'squar', 'valu', 'matrix', 'math', 'n', 'positivevalu', 'the', 'X', '1990', 'symmetrix', '20', 'algorithm', 'larger', 'integ', 'comput', '1722', 'Rq', 'suffic', 'largest', 'vol', 'pp', 'upper', 'bound', 'eigenvalu', 'extrem', '5', 'paper', 'slower', 'symmetr', 'multipli', '2']}"
81,"{'func name': 'register_specify_shape_c_code', 'comments': ""Tell SpecifyShape how to generate C code for a Theano Type.\n\nParameters ---------- typ : Theano type It must be the Theano class itself and not an instance of the class. code : C code Checks the shape and returns a view for the Theano type 'typ'. Use %(iname)s and %(oname)s for the input and output C variable names respectively. %(shape)s is the vector of shape of %(iname)s. Check that its length is good. version A number indicating the version of the code, for cache. c_support_code_apply Extra code.\n"", 'stemmed comments': ['must', 'tell', 'instanc', 'shape', 'respect', 'return', 'paramet', 'use', '%', 'inam', 'input', 'indic', 'theano', 'A', 'view', 'onam', 'version', 'length', 'good', 'code', 'c_support_code_appli', 'type', 'cach', 'check', 'class', 'number', 'extra', 's', 'It', 'vector', 'name', 'typ', 'C', 'variabl', 'specifyshap', 'gener', 'output']}"
82,"{'func name': 'local_dimshuffle_subtensor', 'comments': 'If a subtensor is inside a dimshuffle which only drop broadcastable dimensions, scrap the dimshuffle and index the subtensor with 0\n\nx[i:j, :, k:l].dimshuffle(0, 2) => x[i:j, 0, k:l] if x.broadcastable == (False, True, False)\n', 'stemmed comments': ['dimshuffl', 'index', 'dimens', '>', 'drop', 'x', 'xbroadcast', 'scrap', 'fals', '==', 'true', 'If', 'j', 'k', 'l', 'broadcast', '[', 'insid', '0', 'subtensor', ']', '=', '2']}"
83,"{'func name': 'unpad_dims', 'comments': 'Reshapes the output after pad_dims.\n\nThis reverts the padding by `pad_dims`.\n', 'stemmed comments': ['revert', 'reshap', 'pad_dim', 'thi', 'pad', 'output']}"
84,"{'func name': 'check_stack_trace', 'comments': ""This function checks if the outputs of specific ops of a compiled graph have a stack.\n\nParameters ---------- f_or_fgraph: theano.compile.function_module.Function or theano.gof.fg.FunctionGraph The compiled function or the function graph to be analysed. ops_to_check: it can be of four different types:\n\n- classes or instances inheriting from theano.gof.Op\n\n- tuple/list of classes or instances inheriting from theano.gof.Op\n\n- string\n\n- function returning a boolean and taking as input an instance of theano.gof.Op.\n\n- if ops_to_check is a string, it should be either 'last' or 'all'. 'last' will check only the last op of the graph while 'all' will check all the ops of the graph.\n\n- if ops_to_check is an op or a tuple/list of ops, the function will check that all the outputs of their occurrences in the graph have a stack trace.\n\n- if ops_to_check is a function, it should take as input a theano.gof.Op and return a boolean indicating if the input op should be checked or not. bug_print: string belonging to {'raise', 'warn', 'ignore'} You can specify the behaviour of the function when the specified ops_to_check are not in the graph of f_or_fgraph: it can either raise an exception, write a warning or simply ignore it.\n##### Returns\n"", 'stemmed comments': ['graph', 'instanc', 'differ', 'analys', 'trace', 'bug_print', 'return', 'theanogofop', 'ignor', 'behaviour', 'write', 'paramet', 'take', 'rais', 'ops_to_check', 'compil', 'specifi', 'specif', 'tuple/list', 'input', 'indic', 'four', 'the', 'belong', 'thi', 'op', 'string', 'type', '}', '{', 'stack', 'either', 'except', 'check', 'class', 'function', 'boolean', 'all', 'inherit', 'you', 'theanogoffgfunctiongraph', 'warn', 'last', 'theanocompilefunction_modulefunct', 'simpli', 'f_or_fgraph', 'occurr', 'output']}"
85,"{'func name': 'local_sampling_dot_csr', 'comments': '', 'stemmed comments': []}"
86,"{'func name': 'typed_list_inplace_opt', 'comments': '', 'stemmed comments': []}"
87,"{'func name': 'local_useless_topk', 'comments': 'TopKOp generates two outputs by default This opt removes the useless ones\n\n\n', 'stemmed comments': ['default', 'one', 'remov', 'two', 'topkop', 'useless', 'thi', 'opt', 'gener', 'output']}"
88,"{'func name': 'local_gpu_ctc', 'comments': '', 'stemmed comments': []}"
89,"{'func name': 'local_abstractconv_check', 'comments': '', 'stemmed comments': []}"
90,"{'func name': 'check_deterministic', 'comments': '', 'stemmed comments': []}"
91,"{'func name': 'iter_over_pairs', 'comments': ""Return an iterator over pairs present in the 'pairs' input.\n\nParameters ---------- pairs : dictionary or iterable The pairs to iterate upon. These may be stored either as (key, value) items in a dictionary, or directly as pairs in any kind of iterable structure.\n##### Returns\n"", 'stemmed comments': ['may', 'return', 'paramet', 'present', 'structur', 'store', 'valu', 'input', 'key', 'the', 'dictionari', 'directli', 'pair', 'upon', 'item', 'either', 'kind', 'these', 'iter']}"
92,"{'func name': 'zipadd', 'comments': 'Calls a function with a file object, saving it to a zip file.\n\nparam func: The function to call. :type func: callable\n\n:param zip_file: The zip file that `func` should write its data to. :type zip_file: :class:`zipfile.ZipFile`\n\n:param name: The name of the file inside of the zipped archive that `func` should save its data to. :type name: str\n', 'stemmed comments': ['write', 'object', 'func', 'the', 'file', 'zipfilezipfil', 'zip_fil', 'archiv', 'zip', 'type', 'class', 'function', 'save', 'data', 'str', 'callabl', 'insid', 'name', 'call', 'param']}"
93,"{'func name': 'pool_3d', 'comments': ""Downscale the input by a specified factor\n\nTakes as input a N-D tensor, where N >= 3. It downscales the input image by the specified factor, by keeping only the maximum value of non-overlapping patches of size (ws[0],ws[1],ws[2])\n\nParameters ---------- input : N-D theano tensor of input images Input images. Max pooling will be done over the 3 last dimensions. ws : tuple of length 3 or theano vector of ints of size 3 Factor by which to downscale (vertical ws, horizontal ws, depth ws). (2,2,2) will halve the image in each dimension. ignore_border : bool (default None, will print a warning and set to False) When True, (5,5,5) input with ws=(2,2,2) will generate a (2,2,2) output. (3,3,3) otherwise. st : tuple of three ints or theano vector of ints of size 3 Stride size, which is the number of shifts over rows/cols/slices to get the next pool region. If st is None, it is considered equal to ws (no overlap on pooling regions). pad : tuple of two ints or theano vector of ints of size 3 (pad_h, pad_w, pad_d), pad zeros to extend beyond six borders of the images, pad_h is the size of the top and bottom margins, pad_w is the size of the left and right margins, and pad_d is the size of the front and back margins mode : {'max', 'sum', 'average_inc_pad', 'average_exc_pad'} Operation executed on each window. `max` and `sum` always exclude the padding in the computation. `average` gives you the choice to include or exclude it. ds *deprecated*, use parameter ws instead. st *deprecated*, use parameter st instead. padding *deprecated*, use parameter pad instead.\n"", 'stemmed comments': ['back', 'depth', 'vertic', '333', 'three', 'averag', 'specifi', 'valu', 'average_inc_pad', 'ignore_bord', 'region', 'theano', 'consid', 'If', 'otherwis', 'when', 'horizont', '{', 'warn', 'border', 'get', 'gener', 'size', 'default', 'choic', 'margin', 'take', '555', 'ws=', 'set', 'done', 'print', 'fals', 'length', 'st', '}', 'comput', 'number', 'oper', '222', 'It', 'extend', 'vector', ']', 'pad_w', 'shift', 'ws', 'output', 'dimens', 'bool', 'front', 'ds', 'max', 'pad', 'window', 'downscal', 'input', 'stride', 'true', 'two', 'rows/cols/slic', 'tupl', 'pad_d', 'pool', 'factor', 'right', '[', 'left', 'top', 'none', 'zero', '=', 'last', 'mode', 'instead', 'equal', '2', 'ND', 'tensor', '>', 'paramet', 'includ', 'use', 'give', 'bottom', 'next', 'alway', 'nonoverlap', '1', 'overlap', 'pad_h', 'execut', 'maximum', 'exclud', 'beyond', 'sum', 'N', 'six', 'halv', 'imag', '0', 'keep', 'average_exc_pad', 'deprec', '3', 'patch', 'int']}"
94,"{'func name': 'hex_digest', 'comments': 'Returns a short, mostly hexadecimal hash of a numpy ndarray\n\n\n', 'stemmed comments': ['short', 'return', 'mostli', 'numpi', 'hash', 'ndarray', 'hexadecim']}"
95,"{'func name': 'register_profiler_printer', 'comments': '', 'stemmed comments': []}"
96,"{'func name': 'random_make_inplace', 'comments': '', 'stemmed comments': []}"
97,"{'func name': 'getlspace', 'comments': '', 'stemmed comments': []}"
98,"{'func name': 'mrg_random_make_inplace', 'comments': '', 'stemmed comments': []}"
99,"{'func name': 'local_gpua_mrg', 'comments': '', 'stemmed comments': []}"
100,"{'func name': 'bias_weights', 'comments': 'theano shared variable for bias unit, given length\n\n\n', 'stemmed comments': ['share', 'unit', 'bia', 'theano', 'given', 'variabl', 'length']}"
101,"{'func name': 'run', 'comments': '', 'stemmed comments': []}"
102,"{'func name': '_asarray', 'comments': ""Convert the input to a Numpy array.\n\nThis function is almost identical to ``numpy.asarray``, but it should be used instead of its numpy counterpart when a data type is provided in order to perform type conversion if required. The reason is that ``numpy.asarray`` may not actually update the array's data type to the user-provided type. For more information see ticket http://projects.scipy.org/numpy/ticket/870.\n\nIn that case, we check that both dtype have the same string description (byte order, basic type, and number of bytes), and return a view with the desired dtype.\n\nThis function's name starts with a '_' to indicate that it is meant to be used internally. It is imported so as to be available directly through theano._asarray\n"", 'stemmed comments': ['userprovid', 'inform', 'ticket', 'convers', 'requir', 'case', 'ident', 'may', 'return', 'counterpart', 'reason', 'order', 'use', '_', 'input', 'convert', 'array', 'indic', 'the', 'provid', 'updat', 'descript', 'view', 'start', 'thi', 'byte', 'directli', 'In', 'basic', 'avail', 'actual', 'import', 'string', 'type', 'dtype', 'check', 'number', 'function', 'numpyasarray', 'for', 'data', 'desir', 's', 'http', 'intern', 'It', 'meant', 'see', 'name', 'perform', 'almost', 'numpi', 'theano_asarray', '//projectsscipyorg/numpy/ticket/870', 'instead']}"
103,"{'func name': 'scan_checkpoints', 'comments': 'Scan function that uses less memory, but is more restrictive.\n\nIn :func:`~theano.scan`, if you compute the gradient of the output with respect to the input, you will have to store the intermediate results at each time step, which can be prohibitively huge. This function allows to do ``save_every_N`` steps of forward computations without storing the intermediate results, and to recompute them during the gradient computation.\n\nNotes ----- Current assumptions:\n\n* Every sequence has the same length. * If ``n_steps`` is specified, it has the same value as the length of any sequence. * The value of ``save_every_N`` divides the number of steps the scan will run without remainder. * Only singly-recurrent and non-recurrent outputs are used. No multiple recurrences. * Only the last timestep of any output will ever be used.\n\nParameters ---------- fn ``fn`` is a function that describes the operations involved in one step of ``scan``. See the documentation of :func:`~theano.scan` for more information.\n\nsequences ``sequences`` is the list of Theano variables or dictionaries describing the sequences ``scan`` has to iterate over. All sequences must be the same length in this version of ``scan``.\n\noutputs_info ``outputs_info`` is the list of Theano variables or dictionaries describing the initial state of the outputs computed recurrently.\n\nnon_sequences ``non_sequences`` is the list of arguments that are passed to ``fn`` at each steps. One can opt to exclude variable used in ``fn`` from this list as long as they are part of the computational graph, though for clarity we encourage not to do so.\n\nn_steps ``n_steps`` is the number of steps to iterate given as an int or Theano scalar (> 0). If any of the input sequences do not have enough elements, scan will raise an error. If n_steps is not provided, ``scan`` will figure out the amount of steps it should run given its input sequences.\n\nsave_every_N ``save_every_N`` is the number of steps to go without storing the computations of ``scan`` (ie they will have to be recomputed during the gradient computation).\n\npadding If the length of the sequences is not a multiple of ``save_every_N``, the sequences will be zero padded to make this version of ``scan`` work properly, but will also result in a memory copy. It can be avoided by setting ``padding`` to False, but you need to make sure the length of the sequences is a multple of ``save_every_N``.\n##### Returns\n* **tuple\n    Tuple of the form ``(outputs, updates)`` as in **: func\n\n* ****: func\n\n', 'stemmed comments': ['must', 'return', 'specifi', 'valu', 'recurr', 'outputs_info', 'theano', 'list', 'version', 'If', 'sequenc', 'form', 'In', 'gradient', 'prohibit', 'nonrecurr', 'step', 'function', 'element', 'multpl', 'singlyrecurr', 'need', 'inform', 'graph', 'figur', 'properli', 'allow', 'rais', 'work', 'set', 'error', 'n_step', 'fals', 'dictionari', 'length', 'result', 'No', 'comput', 'number', 'oper', 'sure', 'It', 'huge', 'run', 'also', 'scalar', 'multipl', 'less', 'opt', 'output', 'save_every_n', 'encourag', 'time', 'func', 'store', 'pad', 'ie', 'state', 'input', 'the', 'provid', 'updat', 'pass', 'ever', 'thi', 'note', 'divid', 'given', 'tupl', 'initi', 'go', 'zero', 'last', 'variabl', 'involv', 'though', 'one', 'respect', 'part', 'timestep', '>', 'paramet', 'document', 'everi', 'use', 'recomput', 'forward', 'scan', 'argument', 'avoid', 'non_sequ', '~theanoscan', 'copi', 'clariti', 'describ', 'current', 'amount', 'make', 'exclud', 'long', 'intermedi', 'without', 'onli', 'all', '0', 'see', 'memori', 'remaind', 'restrict', 'int', 'fn', 'iter', 'assumpt', 'enough']}"
104,"{'func name': 'profile_printer', 'comments': '', 'stemmed comments': []}"
105,"{'func name': 'scan_merge_inouts', 'comments': '', 'stemmed comments': []}"
106,"{'func name': 'try_reload', 'comments': '', 'stemmed comments': []}"
107,"{'func name': 'inner_fct', 'comments': '', 'stemmed comments': []}"
108,"{'func name': 'inner_fct', 'comments': '', 'stemmed comments': []}"
109,"{'func name': 'forced_replace', 'comments': ""Check all internal values of the graph that compute the variable ``out`` for occurrences of values identical with ``x``. If such occurrences are encountered then they are replaced with variable ``y``.\n\nParameters ---------- out : Theano Variable x : Theano Variable y : Theano Variable\n\nExamples -------- out := sigmoid(wu)*(1-sigmoid(wu)) x := sigmoid(wu) forced_replace(out, x, y) := y*(1-y)\n\nNote ---- When it find a match, it don't continue on the corresponding inputs.\n"", 'stemmed comments': ['graph', 'continu', 'ident', 'paramet', 'encount', 'x', 'correspond', 'forced_replac', 'valu', 'input', 'exampl', 'find', 'wu', 'theano', 'nt', 'match', 'If', 'note', '1sigmoid', 'when', 'comput', 'check', 'intern', '1y', '=', 'sigmoid', 'occurr', 'variabl', 'replac']}"
110,"{'func name': 'foldr', 'comments': ""Similar behaviour as haskell' foldr.\n\nParameters ---------- fn The function that ``foldr`` applies at each iteration step (see ``scan`` for more info). sequences List of sequences over which ``foldr`` iterates (see ``scan`` for more info). outputs_info List of dictionaries describing the outputs of reduce (see ``scan`` for more info). non_sequences List of arguments passed to `fn`. ``foldr`` will not iterate over these arguments (see ``scan`` for more info). mode See ``scan``. name See ``scan``.\n"", 'stemmed comments': ['foldr', 'behaviour', 'similar', 'paramet', 'reduc', 'scan', 'argument', 'the', 'outputs_info', 'non_sequ', 'list', 'dictionari', 'haskel', 'info', 'pass', 'sequenc', 'mode', 'describ', 'function', 'step', 'appli', 'see', 'name', 'fn', 'iter', 'output']}"
111,"{'func name': 'scan', 'comments': 'This function constructs and applies a Scan op to the provided arguments.\n\nParameters ---------- fn ``fn`` is a function that describes the operations involved in one step of ``scan``. ``fn`` should construct variables describing the output of one iteration step. It should expect as input theano variables representing all the slices of the input sequences and previous values of the outputs, as well as all other arguments given to scan as ``non_sequences``. The order in which scan passes these variables to ``fn``\n\nis the following :\n\n* all time slices of the first sequence * all time slices of the second sequence * ... * all time slices of the last sequence * all past slices of the first output * all past slices of the second output * ... * all past slices of the last output * all other arguments (the list given as `non_sequences` to scan)\n\nThe order of the sequences is the same as the one in the list `sequences` given to scan. The order of the outputs is the same as the order of ``outputs_info``. For any sequence or output the order of the time slices is the same as the one in which they have been given as taps. For example if one writes the following :\n\n.. code-block:: python\n\nscan(fn, sequences = [ dict(input= Sequence1, taps = [-3,2,-1]) , Sequence2 , dict(input =\n\nSequence3, taps = 3) ] , outputs_info = [ dict(initial =\n\nOutput1, taps = [-3,-5]) , dict(initial = Output2, taps = None) , Output3 ] , non_sequences = [ Argument1, Argument2])\n\n``fn`` should expect the following arguments in this given order:\n\n#. ``Sequence1[t-3]`` #. ``Sequence1[t+2]`` #. ``Sequence1[t-1]`` #. ``Sequence2[t]`` #. ``Sequence3[t+3]`` #. ``Output1[t-3]`` #. ``Output1[t-5]`` #. ``Output3[t-1]`` #. ``Argument1`` #. ``Argument2``\n\nThe list of ``non_sequences`` can also contain shared variables used in the function, though ``scan`` is able to figure those out on its own so they can be skipped. For the clarity of the code we recommend though to provide them to scan. To some extend ``scan`` can also figure out other ``non sequences`` (not shared) even if not passed to scan (but used by `fn`). A simple example of this would be :\n\n.. code-block:: python\n\nimport theano.tensor as TT W\n\n = TT.matrix() W_2 = W**2 def f(x): return TT.dot(x,W_2)\n\nThe function is expected to return two things. One is a list of outputs ordered in the same order as ``outputs_info``, with the difference that there should be only one output variable per output initial state (even if no tap value is used). Secondly `fn` should return an update dictionary (that tells how to update any shared variable after each iteration step). The dictionary can optionally be given as a list of tuples. There is no constraint on the order of these two list, ``fn`` can return either ``(outputs_list, update_dictionary)`` or ``(update_dictionary, outputs_list)`` or just one of the two (in case the other is empty).\n\nTo use ``scan`` as a while loop, the user needs to change the function ``fn`` such that also a stopping condition is returned. To do so, he/she needs to wrap the condition in an ``until`` class. The condition should be returned as a third element, for example:\n\n.. code-block:: python\n\n... return [y1_t, y2_t], {x:x+1}, theano.scan_module.until(x < 50)\n\nNote that a number of steps (considered in here as the maximum number of steps ) is still required even though a condition is passed (and it is used to allocate memory if needed). = ):\n\nsequences ``sequences`` is the list of Theano variables or dictionaries describing the sequences ``scan`` has to iterate over. If a sequence is given as wrapped in a dictionary, then a set of optional information can be provided about the sequence. The dictionary should have the following keys:\n\n* ``input`` (*mandatory*) -- Theano variable representing the sequence.\n\n* ``taps`` -- Temporal taps of the sequence required by ``fn``. They are provided as a list of integers, where a value ``k`` impiles that at iteration step ``t`` scan will pass to ``fn`` the slice ``t+k``. Default value is ``[0]``\n\nAny Theano variable in the list ``sequences`` is automatically wrapped into a dictionary where ``taps`` is set to ``[0]``\n\noutputs_info ``outputs_info`` is the list of Theano variables or dictionaries describing the initial state of the outputs computed recurrently. When this initial states are given as dictionary optional information can be provided about the output corresponding to these initial states. The dictionary should have the following keys:\n\n* ``initial`` -- Theano variable that represents the initial state of a given output. In case the output is not computed recursively (think of a map) and does not require an initial state this field can be skipped. Given that (only) the previous time step of the output is used by ``fn``, the initial state **should have the same shape** as the output and **should not involve a downcast** of the data type of the output. If multiple time taps are used, the initial state should have one extra dimension that should cover all the possible taps. For example if we use ``-5``, ``-2`` and ``-1`` as past taps, at step 0, ``fn`` will require (by an abuse of notation) ``output[-5]``, ``output[-2]`` and ``output[-1]``. This will be given by the initial state, which in this case should have the shape (5,)+output.shape. If this variable containing the initial state is called ``init_y`` then ``init_y[0]`` *corresponds to* ``output[-5]``. ``init_y[1]`` *correponds to* ``output[-4]``, ``init_y[2]`` corresponds to ``output[-3]``, ``init_y[3]`` coresponds to ``output[-2]``, ``init_y[4]`` corresponds to ``output[-1]``. While this order might seem strange, it comes natural from splitting an array at a given point. Assume that we have a array ``x``, and we choose ``k`` to be time step ``0``. Then our initial state would be ``x[:k]``, while the output will be ``x[k:]``. Looking at this split, elements in ``x[:k]`` are ordered exactly like those in ``init_y``. * ``taps`` -- Temporal taps of the output that will be pass to ``fn``. They are provided as a list of *negative* integers, where a value ``k`` implies that at iteration step ``t`` scan will pass to ``fn`` the slice ``t+k``.\n\n``scan`` will follow this logic if partial information is given:\n\n* If an output is not wrapped in a dictionary, ``scan`` will wrap it in one assuming that you use only the last step of the output (i.e. it makes your tap value list equal to [-1]). * If you wrap an output in a dictionary and you do not provide any taps but you provide an initial state it will assume that you are using only a tap value of -1. * If you wrap an output in a dictionary but you do not provide any initial state, it assumes that you are not using any form of taps. * If you provide a ``None`` instead of a variable or a empty dictionary ``scan`` assumes that you will not use any taps for this output (like for example in case of a map)\n\nIf ``outputs_info`` is an empty list or None, ``scan`` assumes that no tap is used for any of the outputs. If information is provided just for a subset of the outputs an exception is raised (because there is no convention on how scan should map the provided information to the outputs of ``fn``)\n\nnon_sequences ``non_sequences`` is the list of arguments that are passed to ``fn`` at each steps. One can opt to exclude variable used in ``fn`` from this list as long as they are part of the computational graph, though for clarity we encourage not to do so.\n\nn_steps ``n_steps`` is the number of steps to iterate given as an int or Theano scalar. If any of the input sequences do not have enough elements, scan will raise an error. If the *value is 0* the outputs will have *0 rows*. If n_steps is not provided, ``scan`` will figure out the amount of steps it should run given its input sequences. ``n_steps`` < 0 is not supported anymore.\n\ntruncate_gradient ``truncate_gradient`` is the number of steps to use in truncated BPTT.\n\nIf you compute gradients through a scan op, they are computed using backpropagation through time. By providing a different value then -1, you choose to use truncated BPTT instead of classical BPTT, where you go for only ``truncate_gradient`` number of steps back in time.\n\ngo_backwards ``go_backwards`` is a flag indicating if ``scan`` should go backwards through the sequences. If you think of each sequence as indexed by time, making this flag True would mean that ``scan`` goes back in time, namely that for any sequence it starts from the end and goes towards 0.\n\nname When profiling ``scan``, it is crucial to provide a name for any instance of ``scan``. The profiler will produce an overall profile of your code as well as profiles for the computation of one step of each instance of ``scan``. The ``name`` of the instance appears in those profiles and can greatly help to disambiguate information.\n\nmode It is recommended to leave this argument to None, especially when profiling ``scan`` (otherwise the results are not going to be accurate). If you prefer the computations of one step of ``scan`` to be done differently then the entire function, you can use this parameter to describe how the computations in this loop are done (see ``theano.function`` for details about possible values and their meaning).\n\nprofile Flag or string. If true, or different from the empty string, a profile object will be created and attached to the inner graph of scan. In case ``profile`` is True, the profile object will have the name of the scan instance, otherwise it will have the passed string. Profile object collect (and print) information only when running the inner graph with the new cvm linker ( with default modes, other linkers this argument is useless)\n\nallow_gc Set the value of allow gc for the internal graph of scan.\n\nIf set to None, this will use the value of config.scan.allow_gc.\n\nThe full scan behavior related to allocation is determined by this value and the Theano flag allow_gc. If the flag allow_gc is True (default) and this scan parameter allow_gc is False (default), then we let scan allocate all intermediate memory on the first iteration, those are not garbage collected them during that first iteration (this is determined by the scan allow_gc). This speed up allocation of the following iteration. But we free all those temp allocation at the end of all iterations (this is what the Theano flag allow_gc mean).\n\nIf you use preallocate and this scan is on GPU, the speed up from the scan allow_gc is small. If you are missing memory, disable the scan allow_gc could help you run graph that request much memory.\n\nstrict If true, all the shared variables used in ``fn`` must be provided as a part of ``non_sequences`` or ``sequences``.\n\nreturn_list If True, will always return a list, even if there is only 1 output.\n##### Returns\n', 'stemmed comments': ['back', 'order', 'natur', 'well', 'valu', 'subset', 'consid', 'If', 'form', 'overal', 'k', '{', 'inner', 'element', 'still', 'relat', 'previou', 'truncat', 'requir', 'accur', 'rais', 'help', '32', 'init_i', 'map', 'error', 'n_step', 'code', 'result', 'free', 'entir', 'integ', 'number', 'abus', 'chang', 'attach', 'skip', 'contain', 'To', 't1', 'crucial', 'input', 'configscanallow_gc', 'the', 'dict', 'let', 'construct', 'A', 'look', 'true', 'thi', 'sequence2', 'produc', 'By', 'small', 'then', '=', 'TT', 'call', 'though', 'differ', 'outputs_list', 'output1', 'miss', 'paramet', 'argument1', 'appear', 'expect', 'leav', 'user', 'array', 'collect', 'non_sequ', '1', 'empti', 'long', 'string', '0', 'memori', 'disabl', 'must', 'tell', 'think', 'object', 'garbag', 'update_dictionari', 'recurr', 'creat', 'sequenc', 'In', 'correpond', 'ani', 'would', 'ttdot', 'data', 'goe', 'anymor', 'useless', 'backward', 'set', 'fals', 'gpu', 'while', 'non', 'repres', 'opt', 'index', 'shape', 'encourag', 'dimens', 'like', 'seem', 'corespond', 'provid', 'impli', 'partial', 'given', 'except', '50', '[', 'appli', 'go', 'none', 'python', 'field', 'variabl', 'cover', 'cvm', 'equal', 'second', 'input=', 'row', 'y2_t', 'per', 'clariti', 'describ', 'maximum', 'output3', 'return_list', 'past', 'class', 'end', 'profil', 'option', 'truncate_gradi', 'iter', 'enough', 'automat', 'exampl', 'linker', 'come', 'when', 'third', 'step', 'function', 'share', 'y1_t', 'secondli', 'need', 'inform', 'default', 'graph', 'figur', 'loop', 'simpl', 'allow', 'codeblock', 'bptt', 'notat', 'he/sh', '}', '<', 'comput', 'strict', 'strang', 'extend', '5', 'run', 'also', 'ttmatrix', 'multipl', 'theanotensor', 'output', 'instanc', 'support', 'write', 'ie', 'state', 't2', 'recurs', 'pass', 'op', 'prefer', 'two', 'output2', 'slice', 'extra', 'intern', 'last', 'f', 'mode', 'go_backward', 'involv', '2', 'sequence3', 'one', 'outputshap', 'recommend', 'alway', 'scan', 'argument', 'behavior', 'impil', 'toward', 'choos', 'make', 'amount', 'exclud', 'intermedi', 'first', 'see', 'neg', '3', 'logic', 'convent', 'fn', 'they', 'request', 'return', 'might', 'point', 'outputs_info', 'theano', 'list', 't3', 'classic', 'much', 'mean', 'otherwis', 'gradient', 'new', 'determin', 'for', 'flag', 'wrap', 'theanofunct', 'backpropag', '4', 'follow', 'disambigu', 'assum', 'x1', 'done', 'print', 'dictionari', 'theanoscan_moduleuntil', 'condit', 'import', 'prealloc', 'oper', 'It', 'speed', 'tap', ']', 'exactli', 'scalar', 'stop', 'abl', 'detail', 'downcast', 'alloc', 'tempor', 'time', 'tk', 'gc', 'temp', 'there', 'allow_gc', 'key', 'thing', 'updat', 'especi', 'note', 'tupl', 'initi', 'name', 'full', 'instead', 'possibl', 'sequence1', 'greatli', 'case', 'part', 'def', 'correspond', 'even', 'use', 'x', 'mandatori', 'argument2', 'indic', 'W', 'could', 'split', 'w_2', 'start', 'type', 'either', 'constraint', 't5', 'int', 'but']}"
112,"{'func name': 'key_to_cmp', 'comments': 'comparator function based on ""key"" function\n\n\n', 'stemmed comments': ['function', 'key', 'compar', 'base']}"
113,"{'func name': 'do_setup', 'comments': '', 'stemmed comments': []}"
114,"{'func name': 'randomstate_constructor', 'comments': 'SharedVariable Constructor for RandomState.\n\n\n', 'stemmed comments': ['randomst', 'constructor', 'sharedvari']}"
115,"{'func name': 'generic_constructor', 'comments': 'SharedVariable Constructor.\n\n\n', 'stemmed comments': ['constructor', 'sharedvari']}"
116,"{'func name': 'shared', 'comments': 'SharedVariable constructor for scalar values. Default: int64 or float64.\n\nNotes ----- We implement this using 0-d tensors for now.\n', 'stemmed comments': ['implement', 'default', 'note', 'valu', 'int64', '0d', 'tensor', 'use', 'constructor', 'scalar', 'We', 'sharedvari', 'float64']}"
117,"{'func name': 'sparse_constructor', 'comments': 'SharedVariable Constructor for SparseType.\n\nwriteme\n', 'stemmed comments': ['constructor', 'writem', 'sparsetyp', 'sharedvari']}"
118,"{'func name': 'scalar_constructor', 'comments': 'SharedVariable constructor for scalar values. Default: int64 or float64.\n\nNotes ----- We implement this using 0-d tensors for now.\n\nWe ignore the borrow parameter as we convert ``value`` to an ndarray (this is a new object). This respects the semantic of borrow, as it is a hint to Theano that we can reuse it.\n', 'stemmed comments': ['default', 'respect', 'tensor', 'ignor', 'paramet', 'object', 'use', 'ndarray', 'float64', 'valu', 'convert', 'borrow', 'theano', 'hint', 'sharedvari', 'thi', 'note', '0d', 'new', 'We', 'implement', 'int64', 'semant', 'constructor', 'reus', 'scalar']}"
119,"{'func name': 'local_1msigmoid', 'comments': '1-sigm(x) -> sigm(-x)\n\n\n', 'stemmed comments': ['>', 'sigm', '1sigm', 'x']}"
120,"{'func name': 'kron', 'comments': 'Kronecker product.\n\nSame as scipy.linalg.kron(a, b).\n\nParameters ---------- a: array_like b: array_like\n##### Returns\n', 'stemmed comments': ['array_lik', 'kroneck', 'b', 'return', 'same', 'paramet', 'scipylinalgkron', 'product']}"
121,"{'func name': 'topk_and_argtopk', 'comments': 'Returns the results of both topk() and argtopk() in one Op.\n\nSee the respective documentation for details.\n##### Returns\n* **tuple**: (values, indices)\n\n', 'stemmed comments': ['one', 'detail', 'valu', 'see', 'argtopk', 'indic', 'respect', 'return', 'tupl', 'document', 'Op', 'result', 'topk']}"
122,"{'func name': 'local_gpua_topkop', 'comments': '', 'stemmed comments': []}"
123,"{'func name': 'max_pool', 'comments': 'Implements a max pooling layer\n\nTakes as input a 2D tensor of shape batch_size x img_size and performs max pooling.\n\nMax pooling downsamples by taking the max value in a given area, here defined by maxpoolshp. Outputs a 2D tensor of shape batch_size x output_size.\n\n:param images: 2D tensor containing images on which to apply convolution. Assumed to be of shape batch_size x img_size :param imgshp: tuple containing image dimensions :param maxpoolshp: tuple containing shape of area to max pool over\n\n:return: out1, symbolic result (2D tensor) :return: out2, logical shape of the output\n', 'stemmed comments': ['defin', 'shape', 'contain', 'imgshp', 'dimens', 'tensor', 'return', 'take', 'x', 'max', 'assum', 'out2', 'param', 'valu', '2D', 'input', 'convolut', 'result', 'maxpoolshp', 'layer', 'symbol', 'output_s', 'given', 'tupl', 'pool', 'img_siz', 'appli', 'implement', 'imag', 'area', 'perform', 'batch_siz', 'out1', 'downsampl', 'logic', 'output']}"
124,"{'func name': 'speed_multilayer_conv', 'comments': '', 'stemmed comments': []}"
125,"{'func name': 'strip_leading_white_space', 'comments': '', 'stemmed comments': []}"
126,"{'func name': 'take', 'comments': '', 'stemmed comments': []}"
127,"{'func name': 'check_and_convert_boolean_masks', 'comments': 'This function checks if the boolean mask arrays in the index have the right shape and converts them to index arrays by calling nonzero. For each boolean mask, we check if the mask has the same shape as the input. This is enforced in NumPy 0.13.0 and newer, but not by earlier versions. If the size is not the same, this method raises an IndexError.\n\n\n', 'stemmed comments': ['index', 'shape', 'enforc', 'earlier', 'rais', 'nonzero', 'input', 'convert', 'array', 'version', 'mask', 'If', 'thi', 'method', 'right', '0130', 'check', 'function', 'boolean', 'for', 'newer', 'numpi', 'size', 'call', 'indexerror']}"
128,"{'func name': 'test_jacobian_disconnected_inputs', 'comments': '', 'stemmed comments': []}"
129,"{'func name': 'test_constant_shapes', 'comments': '', 'stemmed comments': []}"
130,"{'func name': 'test_gputri', 'comments': '', 'stemmed comments': []}"
131,"{'func name': 'test_multivar_grad', 'comments': '', 'stemmed comments': []}"
132,"{'func name': 'test_constant', 'comments': '', 'stemmed comments': []}"
133,"{'func name': 'structure_function', 'comments': 'Decorator to structure a function which apply on dense matrix.\n\nHere, the inputs of the function must be dense matrix. The sparse pattern is determined by finding the zeros.\n\n:param index: The index of the parameter from which the function must be structured.\n\n:return: The structured function for its `index` parameter.\n', 'stemmed comments': ['must', 'index', 'decor', 'return', 'paramet', 'structur', 'matrix', 'input', 'find', 'the', 'dens', 'pattern', 'here', 'function', 'determin', 'appli', 'zero', 'spars', 'param']}"
134,"{'func name': 'random_lil', 'comments': '', 'stemmed comments': []}"
135,"{'func name': 'test_symbolic_slice', 'comments': '', 'stemmed comments': []}"
136,"{'func name': 'skip_if_blas_ldflags_empty', 'comments': '', 'stemmed comments': []}"
137,"{'func name': 'test_gemv_dot_strides', 'comments': '', 'stemmed comments': []}"
138,"{'func name': 'matrixmultiply', 'comments': '', 'stemmed comments': []}"
139,"{'func name': 'test_batch_normalization_broadcastable', 'comments': '', 'stemmed comments': []}"
140,"{'func name': 'test_shared_input_output', 'comments': '', 'stemmed comments': []}"
141,"{'func name': 'test_cgpukernelbase', 'comments': '', 'stemmed comments': []}"
142,"{'func name': 'test_flag_detection', 'comments': '', 'stemmed comments': []}"
143,"{'func name': 'test_short_platform', 'comments': '', 'stemmed comments': []}"
144,"{'func name': 'test_conv3d', 'comments': '', 'stemmed comments': []}"
145,"{'func name': 'test_broadcast_grad', 'comments': '', 'stemmed comments': []}"
146,"{'func name': 'setup_grad_case', 'comments': '', 'stemmed comments': []}"
147,"{'func name': 'test_baddestroymap_c', 'comments': '', 'stemmed comments': []}"
148,"{'func name': 'test_multiple_inplace', 'comments': '', 'stemmed comments': []}"
149,"{'func name': 'test_determinism_1', 'comments': '', 'stemmed comments': []}"
150,"{'func name': 'test_opt_f16_prec32', 'comments': '', 'stemmed comments': []}"
151,"{'func name': 'speed_reduce10', 'comments': '', 'stemmed comments': []}"
152,"{'func name': 'test_not_implemented_elemwise_grad', 'comments': '', 'stemmed comments': []}"
153,"{'func name': 'test_to_one_hot', 'comments': '', 'stemmed comments': []}"
154,"{'func name': 'check_all_files', 'comments': 'List all .py files under dir_path (theano path), check if they follow flake8 format, save all the error-formatted files into theano_filelist.txt. This function is used for generating the ""whitelist_flake8"" in this file.\n\n\n', 'stemmed comments': ['py', 'whitelist_flake8', 'gener', 'follow', 'file', 'theano', 'list', 'errorformat', 'theano_filelisttxt', 'use', 'check', 'path', 'flake8', 'function', 'save', 'thi', 'format', 'dir_path']}"
155,"{'func name': 'test_sync_update', 'comments': '', 'stemmed comments': []}"
156,"{'func name': 'test_function_dump', 'comments': '', 'stemmed comments': []}"
157,"{'func name': 'test_merge_opt_runtime', 'comments': '', 'stemmed comments': []}"
158,"{'func name': 'test_undefined_grad_opt', 'comments': '', 'stemmed comments': []}"
159,"{'func name': 'test_graph_opt_caching', 'comments': '', 'stemmed comments': []}"
160,"{'func name': 'prenode', 'comments': '', 'stemmed comments': []}"
161,"{'func name': 'more_complex_test', 'comments': '', 'stemmed comments': []}"
162,"{'func name': 'test_lower_triangular_and_cholesky_grad', 'comments': '', 'stemmed comments': []}"
163,"{'func name': 'test_matrix_inverse_solve', 'comments': '', 'stemmed comments': []}"
164,"{'func name': 'test_container_deepcopy', 'comments': '', 'stemmed comments': []}"
165,"{'func name': 'test_may_share_memory', 'comments': '', 'stemmed comments': []}"
166,"{'func name': 'test_merge_with_weird_eq', 'comments': '', 'stemmed comments': []}"
167,"{'func name': 'test_nan_guard_mode', 'comments': '', 'stemmed comments': []}"
168,"{'func name': 'test_deepcopied_type_filter', 'comments': '', 'stemmed comments': []}"
169,"{'func name': 'test_including', 'comments': '', 'stemmed comments': []}"
170,"{'func name': 'test_not_inplace', 'comments': '', 'stemmed comments': []}"
171,"{'func name': 'test_mpi_schedule', 'comments': '', 'stemmed comments': []}"
172,"{'func name': 'test_multinomial_dtypes', 'comments': '', 'stemmed comments': []}"
173,"{'func name': 'test_unpickle_legacy_op', 'comments': '', 'stemmed comments': []}"
174,"{'func name': 'test_NanGuardMode', 'comments': '', 'stemmed comments': []}"
175,"{'func name': 'test_trace', 'comments': '', 'stemmed comments': []}"
176,"{'func name': 'softmax_unittest_template', 'comments': '', 'stemmed comments': []}"
177,"{'func name': 'test_confusion_matrix', 'comments': '', 'stemmed comments': []}"
178,"{'func name': 'test_debug_error_message', 'comments': '', 'stemmed comments': []}"
179,"{'func name': 'test_shape_i_hash', 'comments': '', 'stemmed comments': []}"
180,"{'func name': 'test_local_dimshuffle_subtensor', 'comments': '', 'stemmed comments': []}"
181,"{'func name': 'test_sd_csc', 'comments': '', 'stemmed comments': []}"
182,"{'func name': 'test_crossentropycategorical1hot_lifter', 'comments': '', 'stemmed comments': []}"
183,"{'func name': 'test_constant_folding', 'comments': '', 'stemmed comments': []}"
184,"{'func name': 'test_blocksparse_inplace_outer_opt', 'comments': '', 'stemmed comments': []}"
185,"{'func name': 'test_local_log_sum_exp3', 'comments': '', 'stemmed comments': []}"
186,"{'func name': 'test_pre_constant_merge_slice', 'comments': '', 'stemmed comments': []}"
187,"{'func name': 'test_dump_load', 'comments': '', 'stemmed comments': []}"
188,"{'func name': 'data_of', 'comments': '', 'stemmed comments': []}"
189,"{'func name': 'test_pickle_unpickle_without_reoptimization', 'comments': '', 'stemmed comments': []}"
190,"{'func name': 'test_unpickle_gpuarray_as_numpy_ndarray_flag2', 'comments': '', 'stemmed comments': []}"
191,"{'func name': 'test_pool3d', 'comments': '', 'stemmed comments': []}"
192,"{'func name': 'test_subtensor', 'comments': '', 'stemmed comments': []}"
193,"{'func name': 'test_record_mode_bad', 'comments': '', 'stemmed comments': []}"
194,"{'func name': 'check_if_gpu_reduce_not_in_graph', 'comments': '', 'stemmed comments': []}"
195,"{'func name': 'test_target_parameter', 'comments': '', 'stemmed comments': []}"
196,"{'func name': 'test_cpu_target_with_shared_variable', 'comments': '', 'stemmed comments': []}"
197,"{'func name': 'test_equal_compuations', 'comments': '', 'stemmed comments': []}"
198,"{'func name': 'test_mintap_onestep', 'comments': '', 'stemmed comments': []}"
199,"{'func name': 'test_posort', 'comments': '', 'stemmed comments': []}"
200,"{'func name': 'test_scalar_shared_options', 'comments': '', 'stemmed comments': []}"
201,"{'func name': 'test_expm_grad_3', 'comments': '', 'stemmed comments': []}"
202,"{'func name': 'test_argsort_grad', 'comments': '', 'stemmed comments': []}"
203,"{'func name': 'test_adv_subtensor', 'comments': '', 'stemmed comments': []}"
204,"{'func name': 'inputs', 'comments': '', 'stemmed comments': []}"
205,"{'func name': 'test_none_Constant', 'comments': '', 'stemmed comments': []}"
206,"{'func name': 'test_sparse_type', 'comments': '', 'stemmed comments': []}"
207,"{'func name': 'test_set_value_non_contiguous', 'comments': '', 'stemmed comments': []}"
208,"{'func name': 'rand_ranged_matrix', 'comments': '', 'stemmed comments': []}"
209,"{'func name': 'test_cdata', 'comments': '', 'stemmed comments': []}"
210,"{'func name': 'test_hash_from_ndarray', 'comments': '', 'stemmed comments': []}"
211,"{'func name': 'test_stack_trace', 'comments': '', 'stemmed comments': []}"
212,"{'func name': 'test_hash_from_sparse', 'comments': '', 'stemmed comments': []}"
213,"{'func name': 'test_None_dimShuffle_replace', 'comments': '', 'stemmed comments': []}"
214,"{'func name': 'test_no_recycling', 'comments': '', 'stemmed comments': []}"
215,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
216,"{'func name': 'main', 'comments': '', 'stemmed comments': []}"
217,"{'func name': 'as_int_none_variable', 'comments': '', 'stemmed comments': []}"
218,"{'func name': 'GpuArray_pickler', 'comments': '', 'stemmed comments': []}"
219,"{'func name': 'values_eq_approx_always_true', 'comments': '', 'stemmed comments': []}"
220,"{'func name': '_is_sparse', 'comments': 'Returns ------- boolean True iff x is a L{scipy.sparse.spmatrix} (and not a L{numpy.ndarray}).\n\n\n', 'stemmed comments': ['L', 'numpyndarray', 'iff', 'scipysparsespmatrix', 'return', '}', '{', 'x', 'true', 'boolean']}"
221,"{'func name': 'unify', 'comments': '', 'stemmed comments': []}"
222,"{'func name': 'assertFailure_fast', 'comments': ""A Decorator to handle the test cases that are failing when THEANO_FLAGS =cycle_detection='fast'.\n\n\n"", 'stemmed comments': ['=cycle_detection=fast', 'theano_flag', 'decor', 'case', 'test', 'A', 'handl', 'fail']}"
223,"{'func name': 'shape_of_variables', 'comments': ""Compute the numeric shape of all intermediate variables given input shapes.\n\nParameters ---------- fgraph The theano.FunctionGraph in question. input_shapes : dict A dict mapping input to shape.\n##### Returns\n* **shapes **: dict\n    A dict mapping variable to shape\n\n* **.. warning**: \n\n* **>>> y = x[512**: ]; y.name = 'y'\n\n* **>>> d = shape_of_variables(fgraph, {x**: (1024, 1024)})\n\n"", 'stemmed comments': ['fgraph', 'shape', 'return', '>', 'paramet', ';', 'numer', 'x', 'theanofunctiongraph', 'yname', 'map', 'input', 'the', 'dict', 'A', 'y', '1024', 'input_shap', 'intermedi', 'comput', '{', '}', 'given', 'shape_of_vari', 'question', '[', 'warn', '=', '512', ']', 'variabl']}"
224,"{'func name': 'hash_from_file', 'comments': 'Return the SHA256 hash of a file.\n\n\n', 'stemmed comments': ['file', 'sha256', 'return', 'hash']}"
225,"{'func name': 'hash_from_sparse', 'comments': '', 'stemmed comments': []}"
226,"{'func name': 'equal_slices', 'comments': '', 'stemmed comments': []}"
227,"{'func name': 'scan_setup_py', 'comments': ""Validate the contents of setup.py against Versioneer's expectations.\n\n\n"", 'stemmed comments': ['setuppi', 'expect', 'valid', 'version', 'content', 's']}"
228,"{'func name': 'calculate_reallocate_info', 'comments': 'WRITEME : explain the parameters\n\n\n', 'stemmed comments': ['explain', 'paramet', 'writem']}"
229,"{'func name': 'output_subprocess_Popen', 'comments': 'Calls subprocess_Popen, returning the output, error and exit code in a tuple.\n\n\n', 'stemmed comments': ['error', 'return', 'exit', 'tupl', 'subprocess_popen', 'code', 'call', 'output']}"
