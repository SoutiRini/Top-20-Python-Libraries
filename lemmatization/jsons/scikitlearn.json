{"function":
[{
    "source file": "validation.py",
    "line number": "1342",
    "func name": "_check_fit_params",
    "func arg": "(X, fit_params, indices)",
    "comments": "Check and validate the parameters passed during `fit`.\n\nParameters ---------- X : array-like of shape (n_samples, n_features) Data array.\n\nfit_params : dict Dictionary containing the parameters passed at fit.\n\nindices : array-like of shape (n_samples,), default=None Indices to be selected if the parameter has the same size as `X`.\n##### Returns\n* **fit_params_validated **: dict\n    Validated parameters. We ensure that the values support indexing.\n\n"
},{
    "source file": "text.py",
    "line number": "1297",
    "func name": "_make_int_array",
    "func arg": "()",
    "comments": "Construct an array.array of a type suitable for scipy.sparse indices.\n\n\n"
},{
    "source file": "test_weight_boosting.py",
    "line number": "574",
    "func name": "test_adaboost_negative_weight_error",
    "func arg": "(model, X, y)",
    "comments": ""
},{
    "source file": "test_warm_start.py",
    "line number": "162",
    "func name": "test_random_seeds_warm_start",
    "func arg": "(GradientBoosting, X, y, rng_type)",
    "comments": ""
},{
    "source file": "test_voting.py",
    "line number": "541",
    "func name": "test_voting_verbose",
    "func arg": "(estimator, capsys)",
    "comments": ""
},{
    "source file": "test_variance_threshold.py",
    "line number": "51",
    "func name": "test_variance_nan",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_validation2.py",
    "line number": "1200",
    "func name": "test_check_sparse_pandas_sp_format",
    "func arg": "(sp_format)",
    "comments": ""
},{
    "source file": "test_validation1.py",
    "line number": "1762",
    "func name": "test_score",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_utils.py",
    "line number": "691",
    "func name": "test_to_object_array",
    "func arg": "(sequence)",
    "comments": ""
},{
    "source file": "test_unsupervised.py",
    "line number": "222",
    "func name": "test_davies_bouldin_score",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_truncated_svd.py",
    "line number": "176",
    "func name": "test_truncated_svd_eq_pca",
    "func arg": "(X_sparse)",
    "comments": ""
},{
    "source file": "test_tree.py",
    "line number": "1956",
    "func name": "test_X_idx_sorted_deprecated",
    "func arg": "(TreeEstimator)",
    "comments": ""
},{
    "source file": "test_theil_sen.py",
    "line number": "267",
    "func name": "test_less_samples_than_features",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_text.py",
    "line number": "1336",
    "func name": "test_n_features_in",
    "func arg": "(Vectorizer, X)",
    "comments": ""
},{
    "source file": "test_testing.py",
    "line number": "634",
    "func name": "test_convert_container",
    "func arg": "(constructor_name, container_type)",
    "comments": ""
},{
    "source file": "test_target.py",
    "line number": "320",
    "func name": "test_transform_target_regressor_route_pipeline",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_t_sne.py",
    "line number": "946",
    "func name": "test_tsne_n_jobs",
    "func arg": "(method)",
    "comments": "Make sure that the n_jobs parameter doesn't impact the output\n\n\n"
},{
    "source file": "test_svmlight_format.py",
    "line number": "519",
    "func name": "test_load_with_offsets_error",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_svm.py",
    "line number": "1254",
    "func name": "test_custom_kernel_not_array_input",
    "func arg": "(Estimator)",
    "comments": "Test using a custom kernel that is not fed with array-like for floats\n\n\n"
},{
    "source file": "test_supervised.py",
    "line number": "348",
    "func name": "test_mutual_info_score_positive_constant_label",
    "func arg": "(labels_true, labels_pred)",
    "comments": ""
},{
    "source file": "test_stochastic_optimizers.py",
    "line number": "80",
    "func name": "test_adam_optimizer",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_stats.py",
    "line number": "64",
    "func name": "test_weighted_percentile_2d",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_stacking.py",
    "line number": "502",
    "func name": "test_stacking_without_n_features_in",
    "func arg": "(make_dataset, Stacking, Estimator)",
    "comments": ""
},{
    "source file": "test_splitting.py",
    "line number": "402",
    "func name": "test_splitting_missing_values",
    "func arg": "(X_binned, all_gradients, has_missing_values, n_bins_non_missing, expected_split_on_nan, expected_bin_idx, expected_go_to_left)",
    "comments": ""
},{
    "source file": "test_split.py",
    "line number": "1617",
    "func name": "test_random_state_shuffle_false",
    "func arg": "(Klass)",
    "comments": ""
},{
    "source file": "test_spectral.py",
    "line number": "233",
    "func name": "test_n_components",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_spectral_embedding.py",
    "line number": "330",
    "func name": "test_spectral_embedding_first_eigen_vector",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_sparsefuncs.py",
    "line number": "607",
    "func name": "test_csr_row_norms",
    "func arg": "(dtype)",
    "comments": ""
},{
    "source file": "test_sparse.py",
    "line number": "355",
    "func name": "test_consistent_proba",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_sparse_pca.py",
    "line number": "194",
    "func name": "test_spca_n_components_",
    "func arg": "(SPCA, n_components)",
    "comments": ""
},{
    "source file": "test_sparse_coordinate_descent.py",
    "line number": "293",
    "func name": "test_sparse_enet_coordinate_descent",
    "func arg": "()",
    "comments": "Test that a warning is issued if model does not converge\n\n\n"
},{
    "source file": "test_show_versions.py",
    "line number": "31",
    "func name": "test_show_versions",
    "func arg": "(capsys)",
    "comments": ""
},{
    "source file": "test_shortest_path.py",
    "line number": "89",
    "func name": "test_dijkstra_bug_fix",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_sgd.py",
    "line number": "1582",
    "func name": "test_SGDClassifier_fit_for_all_backends",
    "func arg": "(backend)",
    "comments": ""
},{
    "source file": "test_seq_dataset.py",
    "line number": "140",
    "func name": "test_buffer_dtype_mismatch_error",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_search.py",
    "line number": "1845",
    "func name": "test_scalar_fit_param_compat",
    "func arg": "(SearchCV, param_search)",
    "comments": ""
},{
    "source file": "test_score_objects.py",
    "line number": "744",
    "func name": "test_multiclass_roc_no_proba_scorer_errors",
    "func arg": "(scorer_name)",
    "comments": ""
},{
    "source file": "test_samples_generator.py",
    "line number": "545",
    "func name": "test_make_circles_unbalanced",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_sag.py",
    "line number": "831",
    "func name": "test_sag_classifier_raises_error",
    "func arg": "(solver)",
    "comments": ""
},{
    "source file": "test_robust_covariance.py",
    "line number": "139",
    "func name": "test_mcd_increasing_det_warning",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_ridge.py",
    "line number": "1315",
    "func name": "test_ridge_sag_with_X_fortran",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_rfe.py",
    "line number": "488",
    "func name": "test_multioutput",
    "func arg": "(ClsRFE)",
    "comments": ""
},{
    "source file": "test_reingold_tilford.py",
    "line number": "25",
    "func name": "test_buchheim",
    "func arg": "(tree, n_nodes)",
    "comments": ""
},{
    "source file": "test_regression.py",
    "line number": "328",
    "func name": "test_mean_absolute_percentage_error",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_rcv1.py",
    "line number": "13",
    "func name": "test_fetch_rcv1",
    "func arg": "(fetch_rcv1_fxt)",
    "comments": ""
},{
    "source file": "test_rbm.py",
    "line number": "211",
    "func name": "test_convergence_dtype_consistency",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_ransac.py",
    "line number": "500",
    "func name": "test_ransac_final_model_fit_sample_weight",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_ranking.py",
    "line number": "1449",
    "func name": "test_partial_roc_auc_score",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_random.py",
    "line number": "185",
    "func name": "test_our_rand_r",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_random_projection.py",
    "line number": "344",
    "func name": "test_works_with_sparse_data",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_quad_tree.py",
    "line number": "103",
    "func name": "test_summarize",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_predictor.py",
    "line number": "49",
    "func name": "test_infinite_values_and_thresholds",
    "func arg": "(threshold, expected_predictions)",
    "comments": ""
},{
    "source file": "test_pprint.py",
    "line number": "543",
    "func name": "test_kwargs_in_init",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_pls.py",
    "line number": "428",
    "func name": "test_pls_scaling",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_plot_roc_curve.py",
    "line number": "186",
    "func name": "test_plot_roc_curve_pos_label",
    "func arg": "(pyplot, response_method)",
    "comments": ""
},{
    "source file": "test_plot_precision_recall.py",
    "line number": "199",
    "func name": "test_plot_precision_recall_pos_label",
    "func arg": "(pyplot, response_method)",
    "comments": ""
},{
    "source file": "test_plot_partial_dependence.py",
    "line number": "498",
    "func name": "test_plot_partial_dependence_multiclass_error",
    "func arg": "(pyplot, params, err_msg)",
    "comments": ""
},{
    "source file": "test_plot_confusion_matrix.py",
    "line number": "308",
    "func name": "test_default_labels",
    "func arg": "(pyplot, display_labels, expected_labels)",
    "comments": ""
},{
    "source file": "test_pipeline.py",
    "line number": "1202",
    "func name": "test_feature_union_fit_params",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_permutation_importance.py",
    "line number": "335",
    "func name": "test_permutation_importance_large_memmaped_data",
    "func arg": "(input_type)",
    "comments": ""
},{
    "source file": "test_perceptron.py",
    "line number": "66",
    "func name": "test_undefined_methods",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_pca.py",
    "line number": "631",
    "func name": "test_assess_dimesion_rank_one",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_passive_aggressive.py",
    "line number": "273",
    "func name": "test_passive_aggressive_deprecated_attr",
    "func arg": "(klass)",
    "comments": ""
},{
    "source file": "test_partial_dependence.py",
    "line number": "708",
    "func name": "test_warning_for_kind_legacy",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_pairwise.py",
    "line number": "1260",
    "func name": "test_pairwise_distances_data_derived_params",
    "func arg": "(n_jobs, metric, dist_function, y_is_x)",
    "comments": ""
},{
    "source file": "test_optimize.py",
    "line number": "9",
    "func name": "test_newton_cg",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_optics.py",
    "line number": "420",
    "func name": "test_precomputed_dists",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_openml.py",
    "line number": "1248",
    "func name": "test_convert_arff_data_type",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_online_lda.py",
    "line number": "398",
    "func name": "test_verbosity",
    "func arg": "(verbose, evaluate_every, expected_lines, expected_perplexities)",
    "comments": ""
},{
    "source file": "test_omp.py",
    "line number": "221",
    "func name": "test_omp_reaches_least_squares",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_olivetti_faces.py",
    "line number": "13",
    "func name": "test_olivetti_faces",
    "func arg": "(fetch_olivetti_faces_fxt)",
    "comments": ""
},{
    "source file": "test_nmf.py",
    "line number": "542",
    "func name": "test_nmf_custom_init_dtype_error",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_neighbors.py",
    "line number": "1687",
    "func name": "test_auto_algorithm",
    "func arg": "(X, metric, metric_params, expected_algo)",
    "comments": ""
},{
    "source file": "test_neighbors_tree.py",
    "line number": "258",
    "func name": "test_pickle",
    "func arg": "(Cls, metric, protocol)",
    "comments": ""
},{
    "source file": "test_neighbors_pipeline.py",
    "line number": "182",
    "func name": "test_kneighbors_regressor",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_nearest_centroid.py",
    "line number": "140",
    "func name": "test_manhattan_metric",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_nca.py",
    "line number": "526",
    "func name": "test_parameters_valid_types",
    "func arg": "(param, value)",
    "comments": ""
},{
    "source file": "test_naive_bayes.py",
    "line number": "815",
    "func name": "test_check_accuracy_on_digits",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_mutual_info.py",
    "line number": "176",
    "func name": "test_mutual_info_options",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_murmurhash.py",
    "line number": "68",
    "func name": "test_uniform_distribution",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_multioutput.py",
    "line number": "584",
    "func name": "test_regressor_chain_w_fit_params",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_multiclass2.py",
    "line number": "399",
    "func name": "test_ovr_decision_function",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_multiclass1.py",
    "line number": "765",
    "func name": "test_pairwise_cross_val_score",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_monotonic_contraints.py",
    "line number": "273",
    "func name": "test_bounded_value_min_gain_to_split",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_mocking.py",
    "line number": "163",
    "func name": "test_checking_classifier_methods_to_check",
    "func arg": "(iris, methods_to_check, predict_method)",
    "comments": ""
},{
    "source file": "test_mlp.py",
    "line number": "759",
    "func name": "test_mlp_param_dtypes",
    "func arg": "(dtype, Estimator)",
    "comments": ""
},{
    "source file": "test_mixture.py",
    "line number": "16",
    "func name": "test_gaussian_mixture_n_iter",
    "func arg": "(estimator)",
    "comments": ""
},{
    "source file": "test_metaestimators2.py",
    "line number": "67",
    "func name": "test_if_delegate_has_method",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_metaestimators1.py",
    "line number": "48",
    "func name": "test_metaestimator_delegation",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_mean_shift.py",
    "line number": "175",
    "func name": "test_mean_shift_zero_bandwidth",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_mds.py",
    "line number": "58",
    "func name": "test_MDS",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_loss.py",
    "line number": "312",
    "func name": "test_init_gradient_and_hessians_sample_weight",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_logistic.py",
    "line number": "1802",
    "func name": "test_scores_attribute_layout_elasticnet",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_lof.py",
    "line number": "219",
    "func name": "test_predicted_outlier_number",
    "func arg": "(expected_outliers)",
    "comments": ""
},{
    "source file": "test_locally_linear.py",
    "line number": "140",
    "func name": "test_integer_input",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_link.py",
    "line number": "37",
    "func name": "test_link_derivative",
    "func arg": "(Link)",
    "comments": ""
},{
    "source file": "test_lfw.py",
    "line number": "172",
    "func name": "test_load_fake_lfw_pairs",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_least_angle.py",
    "line number": "759",
    "func name": "test_X_none_gram_not_none",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_label.py",
    "line number": "610",
    "func name": "test_inverse_binarize_multiclass",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_label_propagation.py",
    "line number": "172",
    "func name": "test_predict_sparse_callable_kernel",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_knn.py",
    "line number": "639",
    "func name": "test_knn_tags",
    "func arg": "(na, allow_nan)",
    "comments": ""
},{
    "source file": "test_kernels.py",
    "line number": "359",
    "func name": "test_rational_quadratic_kernel",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_kernel_ridge.py",
    "line number": "78",
    "func name": "test_kernel_ridge_multi_output",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_kernel_pca.py",
    "line number": "290",
    "func name": "test_kernel_pca_inverse_transform",
    "func arg": "(kernel)",
    "comments": ""
},{
    "source file": "test_kernel_approximation.py",
    "line number": "270",
    "func name": "test_nystroem_precomputed_kernel",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_kde.py",
    "line number": "242",
    "func name": "test_check_is_fitted",
    "func arg": "(method)",
    "comments": ""
},{
    "source file": "test_kddcup99.py",
    "line number": "43",
    "func name": "test_shuffle",
    "func arg": "(fetch_kddcup99_fxt)",
    "comments": ""
},{
    "source file": "test_k_means.py",
    "line number": "1097",
    "func name": "test_minibatch_kmeans_wrong_params",
    "func arg": "(param, match)",
    "comments": ""
},{
    "source file": "test_isotonic.py",
    "line number": "563",
    "func name": "test_isotonic_2darray_more_than_1_feature",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_isomap.py",
    "line number": "179",
    "func name": "test_sparse_input",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_init.py",
    "line number": "15",
    "func name": "test_import_skl",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_incremental_pca.py",
    "line number": "365",
    "func name": "test_incremental_pca_partial_fit_float_division",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_impute.py",
    "line number": "1438",
    "func name": "test_simple_imputation_inverse_transform_exceptions",
    "func arg": "(missing_value)",
    "comments": ""
},{
    "source file": "test_image.py",
    "line number": "329",
    "func name": "test_width_patch",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_iforest.py",
    "line number": "315",
    "func name": "test_iforest_with_uniform_data",
    "func arg": "()",
    "comments": "Test whether iforest predicts inliers when using uniform data\n\n\n"
},{
    "source file": "test_huber.py",
    "line number": "206",
    "func name": "test_huber_bool",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_histogram.py",
    "line number": "145",
    "func name": "test_hist_subtraction",
    "func arg": "(constant_hessian)",
    "comments": ""
},{
    "source file": "test_hierarchical.py",
    "line number": "758",
    "func name": "test_invalid_shape_precomputed_dist_matrix",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_grower.py",
    "line number": "360",
    "func name": "test_split_on_nan_with_infinite_values",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_graphical_lasso.py",
    "line number": "182",
    "func name": "test_graphical_lasso_cv_scores",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_graph.py",
    "line number": "60",
    "func name": "test_explicit_diagonal",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_gradient_boosting2.py",
    "line number": "760",
    "func name": "test_staged_predict",
    "func arg": "(HistGradientBoosting, X, y)",
    "comments": ""
},{
    "source file": "test_gradient_boosting1.py",
    "line number": "1302",
    "func name": "test_gbr_degenerate_feature_importances",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_gradient_boosting_loss_functions.py",
    "line number": "295",
    "func name": "test_lad_equals_quantile_50",
    "func arg": "(seed)",
    "comments": ""
},{
    "source file": "test_gpr.py",
    "line number": "454",
    "func name": "test_K_inv_reset",
    "func arg": "(kernel)",
    "comments": ""
},{
    "source file": "test_gpc.py",
    "line number": "172",
    "func name": "test_multi_class_n_jobs",
    "func arg": "(kernel)",
    "comments": ""
},{
    "source file": "test_glm.py",
    "line number": "430",
    "func name": "test_tags",
    "func arg": "(estimator, value)",
    "comments": ""
},{
    "source file": "test_glm_distribution.py",
    "line number": "93",
    "func name": "test_deviance_derivative",
    "func arg": "(family)",
    "comments": "Test deviance derivative for different families.\n\n\n"
},{
    "source file": "test_gaussian_mixture.py",
    "line number": "1026",
    "func name": "test_init",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_function_transformer.py",
    "line number": "155",
    "func name": "test_function_transformer_frame",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_from_model.py",
    "line number": "384",
    "func name": "test_importance_getter",
    "func arg": "(estimator, importance_getter)",
    "comments": ""
},{
    "source file": "test_forest.py",
    "line number": "1352",
    "func name": "test_little_tree_with_small_max_samples",
    "func arg": "(ForestClass)",
    "comments": ""
},{
    "source file": "test_fixes.py",
    "line number": "89",
    "func name": "test_masked_array_deprecated",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_feature_select.py",
    "line number": "644",
    "func name": "test_mutual_info_regression",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_feature_hasher.py",
    "line number": "160",
    "func name": "test_hash_collisions",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_feature_agglomeration.py",
    "line number": "11",
    "func name": "test_feature_agglomeration",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_fastica.py",
    "line number": "288",
    "func name": "test_fastica_output_shape",
    "func arg": "(whiten, return_X_mean, return_n_iter)",
    "comments": ""
},{
    "source file": "test_fast_dict.py",
    "line number": "26",
    "func name": "test_int_float_dict_argmin",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_factor_analysis.py",
    "line number": "22",
    "func name": "test_factor_analysis",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_extmath.py",
    "line number": "701",
    "func name": "test_safe_sparse_dot_dense_output",
    "func arg": "(dense_output)",
    "comments": ""
},{
    "source file": "test_export.py",
    "line number": "464",
    "func name": "test_not_fitted_tree",
    "func arg": "(pyplot)",
    "comments": ""
},{
    "source file": "test_estimator_html_repr.py",
    "line number": "261",
    "func name": "test_one_estimator_print_change_only",
    "func arg": "(print_changed_only)",
    "comments": ""
},{
    "source file": "test_estimator_checks.py",
    "line number": "633",
    "func name": "test_xfail_ignored_in_check_estimator",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_encoders.py",
    "line number": "694",
    "func name": "test_encoders_does_not_support_none_values",
    "func arg": "(Encoder)",
    "comments": ""
},{
    "source file": "test_encode.py",
    "line number": "65",
    "func name": "test_check_unknown",
    "func arg": "(values, uniques, expected_diff, expected_mask)",
    "comments": ""
},{
    "source file": "test_enable_iterative_imputer.py",
    "line number": "8",
    "func name": "test_imports_strategies",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_enable_hist_gradient_boosting.py",
    "line number": "8",
    "func name": "test_imports_strategies",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_elliptic_envelope.py",
    "line number": "36",
    "func name": "test_score_samples",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_dummy.py",
    "line number": "760",
    "func name": "test_n_features_in_",
    "func arg": "(Dummy)",
    "comments": ""
},{
    "source file": "test_docstring_parameters.py",
    "line number": "174",
    "func name": "test_fit_docstring_attributes",
    "func arg": "(name, Estimator)",
    "comments": ""
},{
    "source file": "test_dist_metrics.py",
    "line number": "191",
    "func name": "test_input_data_size",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_discriminant_analysis.py",
    "line number": "480",
    "func name": "test_raises_value_error_on_same_number_of_classes_and_samples",
    "func arg": "(solver)",
    "comments": "Tests that if the number of samples equals the number of classes, a ValueError is raised.\n\n\n"
},{
    "source file": "test_discretization.py",
    "line number": "316",
    "func name": "test_32_equal_64",
    "func arg": "(input_dtype, encode)",
    "comments": ""
},{
    "source file": "test_dict_vectorizer.py",
    "line number": "115",
    "func name": "test_n_features_in",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_dict_learning.py",
    "line number": "572",
    "func name": "test_sparse_coder_n_features_in",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_deprecation.py",
    "line number": "58",
    "func name": "test_pickle",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_dbscan.py",
    "line number": "401",
    "func name": "test_dbscan_precomputed_metric_with_initial_rows_zero",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_data.py",
    "line number": "2534",
    "func name": "test_minmax_scaler_clip",
    "func arg": "(feature_range)",
    "comments": ""
},{
    "source file": "test_cython_blas.py",
    "line number": "214",
    "func name": "test_gemm",
    "func arg": "(dtype, opA, transA, opB, transB, order)",
    "comments": ""
},{
    "source file": "test_covtype.py",
    "line number": "45",
    "func name": "test_pandas_dependency_message",
    "func arg": "(fetch_covtype_fxt, hide_available_pandas)",
    "comments": ""
},{
    "source file": "test_covariance.py",
    "line number": "236",
    "func name": "test_oas",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_coordinate_descent.py",
    "line number": "1202",
    "func name": "test_linear_models_cv_fit_for_all_backends",
    "func arg": "(backend, estimator)",
    "comments": ""
},{
    "source file": "test_config.py",
    "line number": "62",
    "func name": "test_set_config",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_compare_lightgbm.py",
    "line number": "155",
    "func name": "test_same_predictions_multiclass_classification",
    "func arg": "(seed, min_samples_leaf, n_samples, max_leaf_nodes)",
    "comments": ""
},{
    "source file": "test_common6.py",
    "line number": "150",
    "func name": "test_missing_value_pandas_na_support",
    "func arg": "(est, func)",
    "comments": ""
},{
    "source file": "test_common5.py",
    "line number": "1390",
    "func name": "test_thresholded_metric_permutation_invariance",
    "func arg": "(name)",
    "comments": ""
},{
    "source file": "test_common4.py",
    "line number": "199",
    "func name": "test_inf_nan_input",
    "func arg": "(metric_name, metric_func)",
    "comments": ""
},{
    "source file": "test_common3.py",
    "line number": "93",
    "func name": "test_imputers_pandas_na_integer_array_support",
    "func arg": "(imputer, add_indicator)",
    "comments": ""
},{
    "source file": "test_common2.py",
    "line number": "167",
    "func name": "test_ensemble_heterogeneous_estimators_all_dropped",
    "func arg": "(X, y, estimator)",
    "comments": ""
},{
    "source file": "test_common1.py",
    "line number": "122",
    "func name": "test_common_check_pandas_dependency",
    "func arg": "(name, dataset_func)",
    "comments": ""
},{
    "source file": "test_common.py",
    "line number": "197",
    "func name": "test_class_support_removed",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_column_transformer.py",
    "line number": "1358",
    "func name": "test_feature_names_empty_columns",
    "func arg": "(empty_col)",
    "comments": ""
},{
    "source file": "test_classification.py",
    "line number": "2266",
    "func name": "test_balanced_accuracy_score",
    "func arg": "(y_true, y_pred)",
    "comments": ""
},{
    "source file": "test_class_weight.py",
    "line number": "261",
    "func name": "test_compute_sample_weight_more_than_32",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_chi2.py",
    "line number": "85",
    "func name": "test_chisquare",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_check_build.py",
    "line number": "13",
    "func name": "test_raise_build_error",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_california_housing.py",
    "line number": "30",
    "func name": "test_pandas_dependency_message",
    "func arg": "(fetch_california_housing_fxt, hide_available_pandas)",
    "comments": ""
},{
    "source file": "test_calibration.py",
    "line number": "419",
    "func name": "test_calibration_attributes",
    "func arg": "(clf, cv)",
    "comments": ""
},{
    "source file": "test_build.py",
    "line number": "9",
    "func name": "test_openmp_parallelism_enabled",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_bounds.py",
    "line number": "143",
    "func name": "test_newrand_bounded_rand_int_limits",
    "func arg": "(range_)",
    "comments": "Test that `bounded_rand_int_wrap` is defined for unsigned 32bits ints\n\n\n"
},{
    "source file": "test_birch.py",
    "line number": "164",
    "func name": "test_birch_n_clusters_long_int",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_binning.py",
    "line number": "303",
    "func name": "test_infinite_values",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_bicluster2.py",
    "line number": "39",
    "func name": "test_consensus_score_issue2445",
    "func arg": "()",
    "comments": "Different number of biclusters in A and B\n\n\n"
},{
    "source file": "test_bicluster1.py",
    "line number": "269",
    "func name": "test_n_jobs_deprecated",
    "func arg": "(klass, n_jobs)",
    "comments": ""
},{
    "source file": "test_bayesian_mixture.py",
    "line number": "463",
    "func name": "test_bayesian_mixture_predict_predict_proba",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_bayes.py",
    "line number": "253",
    "func name": "test_update_sigma",
    "func arg": "(seed)",
    "comments": ""
},{
    "source file": "test_base6.py",
    "line number": "23",
    "func name": "test_log_loss_1_prob_finite",
    "func arg": "(y_true, y_prob)",
    "comments": ""
},{
    "source file": "test_base5.py",
    "line number": "488",
    "func name": "test_fused_types_make_dataset",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_base4.py",
    "line number": "41",
    "func name": "test_base_imputer_not_transform",
    "func arg": "(data)",
    "comments": ""
},{
    "source file": "test_base3.py",
    "line number": "115",
    "func name": "test_get_support",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_base2.py",
    "line number": "78",
    "func name": "test_set_random_states",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_base1.py",
    "line number": "231",
    "func name": "test_bunch_dir",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_base.py",
    "line number": "530",
    "func name": "test_repr_html_wraps",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_ball_tree.py",
    "line number": "59",
    "func name": "test_query_haversine",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_bagging.py",
    "line number": "890",
    "func name": "test_bagging_get_estimators_indices",
    "func arg": "()",
    "comments": ""
},{
    "source file": "test_affinity_propagation.py",
    "line number": "222",
    "func name": "test_affinity_propagation_convergence_warning_dense_sparse",
    "func arg": "(centers)",
    "comments": "Non-regression, see #13334\n\n\n"
},{
    "source file": "test_20news.py",
    "line number": "83",
    "func name": "test_20news_normalization",
    "func arg": "(fetch_20newsgroups_vectorized_fxt)",
    "comments": ""
},{
    "source file": "stats.py",
    "line number": "7",
    "func name": "_weighted_percentile",
    "func arg": "(array, sample_weight, percentile)",
    "comments": "Compute weighted percentile\n\nComputes lower weighted percentile. If `array` is a 2D array, the `percentile` is computed along the axis 0.\n\n.. versionchanged:: 0.24 Accepts 2D `array`.\n\nParameters ---------- array : 1D or 2D array Values to take the weighted percentile of.\n\nsample_weight: 1D or 2D array Weights for each value in `array`. Must be same shape as `array` or of shape `(array.shape[0],)`.\n\npercentile: int, default=50 Percentile to compute. Must be value between 0 and 100.\n##### Returns\n* **percentile **: int if `array` 1D, ndarray if `array` 2D\n    Weighted percentile.\n\n"
},{
    "source file": "sparsefuncs.py",
    "line number": "519",
    "func name": "csc_median_axis_0",
    "func arg": "(X)",
    "comments": "Find the median across axis 0 of a CSC matrix. It is equivalent to doing np.median(X, axis=0).\n\nParameters ---------- X : CSC sparse matrix, shape (n_samples, n_features) Input data.\n##### Returns\n* **median **: ndarray, shape (n_features,)\n    Median.\n\n"
},{
    "source file": "setup17.py",
    "line number": "7",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "setup16.py",
    "line number": "5",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "setup15.py",
    "line number": "7",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "setup14.py",
    "line number": "6",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "setup13.py",
    "line number": "4",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "setup12.py",
    "line number": "4",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "setup11.py",
    "line number": "6",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "setup10.py",
    "line number": "6",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "setup9.py",
    "line number": "7",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "setup8.py",
    "line number": "7",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "setup7.py",
    "line number": "5",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "setup6.py",
    "line number": "4",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "setup5.py",
    "line number": "6",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "setup4.py",
    "line number": "7",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "setup3.py",
    "line number": "8",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "setup2.py",
    "line number": "7",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "setup1.py",
    "line number": "7",
    "func name": "configuration",
    "func arg": "(parent_package, top_path)",
    "comments": ""
},{
    "source file": "roc_curve.py",
    "line number": "130",
    "func name": "plot_roc_curve",
    "func arg": "(estimator, X, y, **kwargs)",
    "comments": "Plot Receiver operating characteristic (ROC) curve.\n\nExtra keyword arguments will be passed to matplotlib's `plot`.\n\nRead more in the :ref:`User Guide <visualizations>`.\n\nParameters ---------- estimator : estimator instance Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline` in which the last estimator is a classifier.\n\nX : {array-like, sparse matrix} of shape (n_samples, n_features) Input values.\n\ny : array-like of shape (n_samples,) Target values.\n\nsample_weight : array-like of shape (n_samples,), default=None Sample weights.\n\ndrop_intermediate : boolean, default=True Whether to drop some suboptimal thresholds which would not appear on a plotted ROC curve. This is useful in order to create lighter ROC curves.\n\nresponse_method : {'predict_proba', 'decision_function', 'auto'}\n\n\n\n default='auto' Specifies whether to use :term:`predict_proba` or :term:`decision_function` as the target response. If set to 'auto', :term:`predict_proba` is tried first and if it does not exist :term:`decision_function` is tried next.\n\nname : str, default=None Name of ROC Curve for labeling. If `None`, use the name of the estimator.\n\nax : matplotlib axes, default=None Axes object to plot on. If `None`, a new figure and axes is created.\n\npos_label : str or int, default=None The class considered as the positive class when computing the roc auc metrics. By default, `estimators.classes_[1]` is considered as the positive class.\n\n.. versionadded:: 0.24\n##### Returns\n* **display **: \n\n* **roc_auc_score **: Compute the area under the ROC curve\n\n* **roc_curve **: Compute Receiver operating characteristic (ROC) curve\n\n* **>>> import matplotlib.pyplot as plt  # doctest**: +SKIP\n\n* **>>> metrics.plot_roc_curve(clf, X_test, y_test)  # doctest**: +SKIP\n\n* **>>> plt.show()                                   # doctest**: +SKIP\n\n"
},{
    "source file": "random.py",
    "line number": "14",
    "func name": "_random_choice_csc",
    "func arg": "(n_samples, classes, class_probability, random_state)",
    "comments": "Generate a sparse random matrix given column class distributions\n\nParameters ---------- n_samples : int, Number of samples to draw in each column.\n\nclasses : list of size n_outputs of arrays of size (n_classes,) List of classes for each column.\n\nclass_probability : list of size n_outputs of arrays of\n\n\n\n\n\n\n\n shape (n_classes,), default=None Class distribution of each column. If None, uniform distribution is assumed.\n\nrandom_state : int, RandomState instance, default=None Controls the randomness of the sampled classes. See :term:`Glossary <random_state>`.\n##### Returns\n* **random_matrix **: sparse csc matrix of size (n_samples, n_outputs)\n\n"
},{
    "source file": "random_projection.py",
    "line number": "196",
    "func name": "_sparse_random_matrix",
    "func arg": "(n_components, n_features, density, random_state)",
    "comments": "Generalized Achlioptas random sparse matrix for random projection\n\nSetting density to 1 / 3 will yield the original matrix by Dimitris Achlioptas while setting a lower value will yield the generalization by Ping Li et al.\n\nIf we note :math:`s = 1 / density`, the components of the random matrix are drawn from:\n\n- -sqrt(s) / sqrt(n_components)\n\n with probability 1 / 2s -\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwith probability 1\n\n- 1 / s\n\n- +sqrt(s) / sqrt(n_components)\n\n with probability 1 / 2s\n\nRead more in the :ref:`User Guide <sparse_random_matrix>`.\n\nParameters ---------- n_components : int, Dimensionality of the target projection space.\n\nn_features : int, Dimensionality of the original source space.\n\ndensity : float or 'auto', default='auto' Ratio of non-zero component in the random projection matrix in the range `(0, 1]`\n\nIf density = 'auto', the value is set to the minimum density as recommended by Ping Li et al.: 1 / sqrt(n_features).\n\nUse density = 1 / 3.0 if you want to reproduce the results from Achlioptas, 2001.\n\nrandom_state : int or RandomState instance, default=None Controls the pseudo random number generator used to generate the matrix at fit time. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.\n##### Returns\n* **components **: array or CSR matrix with shape [n_components, n_features]\n    The generated Gaussian random matrix.\n\n* **.. [1] Ping Li, T. Hastie and K. W. Church, 2006,\n       \"Very Sparse Random Projections\".\n       https**: //web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf\n\n* **.. [2] D. Achlioptas, 2001, \"Database-friendly random projections\",\n       http**: //www.cs.ucsc.edu/~optas/papers/jl.pdf\n\n"
},{
    "source file": "precision_recall_curve.py",
    "line number": "137",
    "func name": "plot_precision_recall_curve",
    "func arg": "(estimator, X, y, **kwargs)",
    "comments": "Plot Precision Recall Curve for binary classifiers.\n\nExtra keyword arguments will be passed to matplotlib's `plot`.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nParameters ---------- estimator : estimator instance Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline` in which the last estimator is a classifier.\n\nX : {array-like, sparse matrix} of shape (n_samples, n_features) Input values.\n\ny : array-like of shape (n_samples,) Binary target values.\n\nsample_weight : array-like of shape (n_samples,), default=None Sample weights.\n\nresponse_method : {'predict_proba', 'decision_function', 'auto'},\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n default='auto' Specifies whether to use :term:`predict_proba` or :term:`decision_function` as the target response. If set to 'auto', :term:`predict_proba` is tried first and if it does not exist :term:`decision_function` is tried next.\n\nname : str, default=None Name for labeling curve. If `None`, the name of the estimator is used.\n\nax : matplotlib axes, default=None Axes object to plot on. If `None`, a new figure and axes is created.\n\npos_label : str or int, default=None The class considered as the positive class when computing the precision and recall metrics. By default, `estimators.classes_[1]` is considered as the positive class.\n\n.. versionadded:: 0.24\n\n**kwargs : dict Keyword arguments to be passed to matplotlib's `plot`.\n##### Returns\n* **display **: \n\n* **precision_recall_curve **: Compute precision-recall pairs for different probability thresholds\n\n"
},{
    "source file": "pre_build_helpers.py",
    "line number": "88",
    "func name": "basic_check_build",
    "func arg": "()",
    "comments": "Check basic compilation and linking of C code\n\n\n"
},{
    "source file": "pipeline.py",
    "line number": "1016",
    "func name": "make_union",
    "func arg": "()",
    "comments": "Construct a FeatureUnion from the given transformers.\n\nThis is a shorthand for the FeatureUnion constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting.\n\nParameters ---------- *transformers : list of estimators\n\nn_jobs : int, default=None Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.\n\n.. versionchanged:: v0.20 `n_jobs` default changed from 1 to None\n\nverbose : bool, default=False If True, the time elapsed while fitting each transformer will be printed as it is completed.\n##### Returns\n* **f **: FeatureUnion\n\n* **sklearn.pipeline.FeatureUnion **: Class for concatenating the results\n    of multiple transformer objects.\n\n"
},{
    "source file": "partial_dependence.py",
    "line number": "20",
    "func name": "plot_partial_dependence",
    "func arg": "(estimator, X, features)",
    "comments": "Partial dependence (PD) and individual conditional expectation (ICE) plots.\n\nPartial dependence plots, individual conditional expectation plots or an overlay of both of them can be plotted by setting the ``kind`` parameter. The ``len(features)`` plots are arranged in a grid with ``n_cols`` columns. Two-way partial dependence plots are plotted as contour plots. The deciles of the feature values will be shown with tick marks on the x-axes for one-way plots, and on both axes for two-way plots.\n\nRead more in the :ref:`User Guide <partial_dependence>`.\n\n.. note::\n\n:func:`plot_partial_dependence` does not support using the same axes with multiple calls. To plot the the partial dependence for multiple estimators, please pass the axes created by the first call to the second call::\n\n>>> from sklearn.inspection import plot_partial_dependence >>> from sklearn.datasets import make_friedman1 >>> from sklearn.linear_model import LinearRegression >>> from sklearn.ensemble import RandomForestRegressor >>> X, y = make_friedman1() >>> est1 = LinearRegression().fit(X, y) >>> est2 = RandomForestRegressor().fit(X, y) >>> disp1 = plot_partial_dependence(est1, X, ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n [1, 2])\n\n# doctest: +SKIP >>> disp2 = plot_partial_dependence(est2, X, [1, 2], ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ax=disp1.axes_)\n\n# doctest: +SKIP\n\n.. warning::\n\nFor :class:`~sklearn.ensemble.GradientBoostingClassifier` and :class:`~sklearn.ensemble.GradientBoostingRegressor`, the `'recursion'` method (used by default) will not account for the `init` predictor of the boosting process. In practice, this will produce the same values as `'brute'` up to a constant offset in the target response, provided that `init` is a constant estimator (which is the default). However, if `init` is not a constant estimator, the partial dependence values are incorrect for `'recursion'` because the offset will be sample-dependent. It is preferable to use the `'brute'` method. Note that this only applies to :class:`~sklearn.ensemble.GradientBoostingClassifier` and :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.\n\nParameters ---------- estimator : BaseEstimator A fitted estimator object implementing :term:`predict`, :term:`predict_proba`, or :term:`decision_function`. Multioutput-multiclass classifiers are not supported.\n\nX : {array-like or dataframe} of shape (n_samples, n_features) ``X`` is used to generate a grid of values for the target ``features`` (where the partial dependence will be evaluated), and also to generate values for the complement features when the `method` is `'brute'`.\n\nfeatures : list of {int, str, pair of int, pair of str} The target features for which to create the PDPs. If `features[i]` is an integer or a string, a one-way PDP is created; if `features[i]` is a tuple, a two-way PDP is created (only supported with `kind='average'`). Each tuple must be of size 2. if any entry is a string, then it must be in ``feature_names``.\n\nfeature_names : array-like of shape (n_features,), dtype=str, default=None Name of each feature; `feature_names[i]` holds the name of the feature with index `i`. By default, the name of the feature corresponds to their numerical index for NumPy array and their column name for pandas dataframe.\n\ntarget : int, default=None\n\n- In a multiclass setting, specifies the class for which the PDPs should be computed. Note that for binary classification, the positive class (index 1) is always used.\n\n- In a multioutput setting, specifies the task for which the PDPs should be computed.\n\nIgnored in binary classification or classical regression settings.\n\nresponse_method : {'auto', 'predict_proba', 'decision_function'},\n\n\n\n\n\n\n\n\n\n\n\n default='auto' Specifies whether to use :term:`predict_proba` or :term:`decision_function` as the target response. For regressors this parameter is ignored and the response is always the output of :term:`predict`. By default, :term:`predict_proba` is tried first and we revert to :term:`decision_function` if it doesn't exist. If ``method`` is `'recursion'`, the response is always the output of :term:`decision_function`.\n\nn_cols : int, default=3 The maximum number of columns in the grid plot. Only active when `ax` is a single axis or `None`.\n\ngrid_resolution : int, default=100 The number of equally spaced points on the axes of the plots, for each target feature.\n\npercentiles : tuple of float, default=(0.05, 0.95) The lower and upper percentile used to create the extreme values for the PDP axes. Must be in [0, 1].\n\nmethod : str, default='auto' The method used to calculate the averaged predictions:\n\n- `'recursion'` is only supported for some tree-based estimators (namely :class:`~sklearn.ensemble.GradientBoostingClassifier`, :class:`~sklearn.ensemble.GradientBoostingRegressor`, :class:`~sklearn.ensemble.HistGradientBoostingClassifier`, :class:`~sklearn.ensemble.HistGradientBoostingRegressor`, :class:`~sklearn.tree.DecisionTreeRegressor`, :class:`~sklearn.ensemble.RandomForestRegressor` but is more efficient in terms of speed. With this method, the target response of a classifier is always the decision function, not the predicted probabilities. Since the `'recursion'` method implicitely computes the average of the ICEs by design, it is not compatible with ICE and thus `kind` must be `'average'`.\n\n- `'brute'` is supported for any estimator, but is more computationally intensive.\n\n- `'auto'`: the `'recursion'` is used for estimators that support it, and `'brute'` is used otherwise.\n\nPlease see :ref:`this note <pdp_method_differences>` for differences between the `'brute'` and `'recursion'` method.\n\nn_jobs : int, default=None The number of CPUs to use to compute the partial dependences. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.\n\nverbose : int, default=0 Verbose output during PD computations.\n\nline_kw : dict, default=None Dict with keywords passed to the ``matplotlib.pyplot.plot`` call. For one-way partial dependence plots.\n\ncontour_kw : dict, default=None Dict with keywords passed to the ``matplotlib.pyplot.contourf`` call. For two-way partial dependence plots.\n\nax : Matplotlib axes or array-like of Matplotlib axes, default=None\n\n- If a single axis is passed in, it is treated as a bounding axes and a grid of partial dependence plots will be drawn within these bounds. The `n_cols` parameter controls the number of columns in the grid.\n\n- If an array-like of axes are passed in, the partial dependence plots will be drawn directly into these axes.\n\n- If `None`, a figure and a bounding axes is created and treated as the single axes case.\n\n.. versionadded:: 0.22\n\nkind : {'average', 'individual', 'both'}, default='average' Whether to plot the partial dependence averaged across all the samples in the dataset or one line per sample or both.\n\n- ``kind='average'`` results in the traditional PD plot;\n\n- ``kind='individual'`` results in the ICE plot.\n\nNote that the fast ``method='recursion'`` option is only available for ``kind='average'``. Plotting individual dependencies requires using the slower ``method='brute'`` option.\n\n.. versionadded:: 0.24\n\nsubsample : float, int or None, default=1000 Sampling for ICE curves when `kind` is 'individual' or 'both'. If `float`, should be between 0.0 and 1.0 and represent the proportion of the dataset to be used to plot ICE curves. If `int`, represents the absolute number samples to use.\n\nNote that the full dataset is still used to calculate averaged partial dependence when `kind='both'`.\n\n.. versionadded:: 0.24\n##### Returns\n* **display**: \n\n* **>>> plot_partial_dependence(clf, X, [0, (0, 1)]) #doctest**: +SKIP\n\n* **sklearn.inspection.partial_dependence**: Return raw partial\n  dependence values\n\n"
},{
    "source file": "pairwise.py",
    "line number": "1851",
    "func name": "pairwise_kernels",
    "func arg": "(X, Y, metric, **kwds)",
    "comments": "Compute the kernel between arrays X and optional array Y.\n\nThis method takes either a vector array or a kernel matrix, and returns a kernel matrix. If the input is a vector array, the kernels are computed. If the input is a kernel matrix, it is returned instead.\n\nThis method provides a safe way to take a kernel matrix as input, while preserving compatibility with many other algorithms that take a vector array.\n\nIf Y is given (default is None), then the returned matrix is the pairwise kernel between the arrays from both X and Y.\n\nValid values for metric are: ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf', 'laplacian', 'sigmoid', 'cosine']\n\nRead more in the :ref:`User Guide <metrics>`.\n\nParameters ---------- X : array [n_samples_a, n_samples_a] if metric == \"precomputed\", or,\n\n\n\n\n\n\n\n\n\n\n\n\n\n[n_samples_a, n_features] otherwise Array of pairwise kernels between samples, or a feature array.\n\nY : array of shape (n_samples_b, n_features), default=None A second feature array only if X has shape [n_samples_a, n_features].\n\nmetric : str or callable, default=\"linear\" The metric to use when calculating kernel between instances in a feature array. If metric is a string, it must be one of the metrics in pairwise.PAIRWISE_KERNEL_FUNCTIONS. If metric is \"precomputed\", X is assumed to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two rows from X as input and return the corresponding kernel value as a single number. This means that callables from :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on matrices, not single samples. Use the string identifying the kernel instead.\n\nfilter_params : bool, default=False Whether to filter invalid parameters or not.\n\nn_jobs : int, default=None The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel.\n\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.\n\n**kwds : optional keyword parameters Any further parameters are passed directly to the kernel function.\n##### Returns\n* **K **: array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b]\n    A kernel matrix K such that K_{i, j} is the kernel between the\n    ith and jth vectors of the given matrix X, if Y is None.\n    If Y is not None, then K_{i, j} is the kernel between the ith array\n    from X and the jth array from Y.\n\n"
},{
    "source file": "optimize.py",
    "line number": "207",
    "func name": "_check_optimize_result",
    "func arg": "(solver, result, max_iter, extra_warning_msg)",
    "comments": "Check the OptimizeResult for successful convergence\n\nParameters ---------- solver : str Solver name. Currently only `lbfgs` is supported.\n\nresult : OptimizeResult Result of the scipy.optimize.minimize function.\n\nmax_iter : int, default=None Expected maximum number of iterations.\n\nextra_warning_msg : str, default=None Extra warning message.\n##### Returns\n* **n_iter **: int\n   Number of iterations.\n\n"
},{
    "source file": "openmp_helpers.py",
    "line number": "47",
    "func name": "check_openmp_support",
    "func arg": "()",
    "comments": "Check whether OpenMP test code can be compiled and run\n\n\n"
},{
    "source file": "multioutput.py",
    "line number": "44",
    "func name": "_partial_fit_estimator",
    "func arg": "(estimator, X, y, classes, sample_weight, first_time)",
    "comments": ""
},{
    "source file": "multiclass2.py",
    "line number": "407",
    "func name": "_ovr_decision_function",
    "func arg": "(predictions, confidences, n_classes)",
    "comments": "Compute a continuous, tie-breaking OvR decision function from OvO.\n\nIt is important to include a continuous value, not only votes, to make computing AUC or calibration meaningful.\n\nParameters ---------- predictions : array-like, shape (n_samples, n_classifiers) Predicted classes for each binary classifier.\n\nconfidences : array-like, shape (n_samples, n_classifiers) Decision functions or predicted probabilities for positive class for each binary classifier.\n\nn_classes : int Number of classes. n_classifiers must be ``n_classes * (n_classes\n\n- 1 ) / 2``\n"
},{
    "source file": "multiclass1.py",
    "line number": "491",
    "func name": "_partial_fit_ovo_binary",
    "func arg": "(estimator, X, y, i, j)",
    "comments": "Partially fit a single binary estimator(one-vs-one).\n\n\n"
},{
    "source file": "metaestimators.py",
    "line number": "148",
    "func name": "_safe_split",
    "func arg": "(estimator, X, y, indices, train_indices)",
    "comments": "Create subset of dataset and properly handle kernels.\n\nSlice X, y according to indices for cross-validation, but take care of precomputed kernel-matrices or pairwise affinities / distances.\n\nIf ``estimator._pairwise is True``, X needs to be square and we slice rows and columns. If ``train_indices`` is not None, we slice rows using ``indices`` (assumed the test set) and columns using ``train_indices``, indicating the training set.\n\nLabels y will always be indexed only along the first axis.\n\nParameters ---------- estimator : object Estimator to determine whether we should slice only rows or rows and columns.\n\nX : array-like, sparse matrix or iterable Data to be indexed. If ``estimator._pairwise is True``, this needs to be a square array-like or sparse matrix.\n\ny : array-like, sparse matrix or iterable Targets to be indexed.\n\nindices : array of int Rows to select from X and y. If ``estimator._pairwise is True`` and ``train_indices is None`` then ``indices`` will also be used to slice columns.\n\ntrain_indices : array of int or None, default=None If ``estimator._pairwise is True`` and ``train_indices is not None``, then ``train_indices`` will be use to slice the columns of X.\n##### Returns\n* **X_subset **: array-like, sparse matrix or list\n    Indexed data.\n\n* **y_subset **: array-like, sparse matrix or list\n    Indexed targets.\n\n"
},{
    "source file": "kernels.py",
    "line number": "2119",
    "func name": "_approx_fprime",
    "func arg": "(xk, f, epsilon, args)",
    "comments": ""
},{
    "source file": "isotonic.py",
    "line number": "80",
    "func name": "isotonic_regression",
    "func arg": "(y)",
    "comments": "Solve the isotonic regression model.\n\nRead more in the :ref:`User Guide <isotonic>`.\n\nParameters ---------- y : array-like of shape (n_samples,) The data.\n\nsample_weight : array-like of shape (n_samples,), default=None Weights on each point of the regression. If None, weight is set to 1 (equal weights).\n\ny_min : float, default=None Lower bound on the lowest predicted value (the minimum value may still be higher). If not set, defaults to -inf.\n\ny_max : float, default=None Upper bound on the highest predicted value (the maximum may still be lower). If not set, defaults to +inf.\n\nincreasing : bool, default=True Whether to compute ``y_`` is increasing (if set to True) or decreasing (if set to False)\n##### Returns\n* **y_ **: list of floats\n    Isotonic fit of y.\n\n"
},{
    "source file": "image.py",
    "line number": "405",
    "func name": "reconstruct_from_patches_2d",
    "func arg": "(patches, image_size)",
    "comments": "Reconstruct the image from all of its patches.\n\nPatches are assumed to overlap and the image is constructed by filling in the patches from left to right, top to bottom, averaging the overlapping regions.\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.\n\nParameters ---------- patches : ndarray of shape (n_patches, patch_height, patch_width) or\n\n\n\n\n\n\n\n (n_patches, patch_height, patch_width, n_channels) The complete set of patches. If the patches contain colour information, channels are indexed along the last dimension: RGB patches would have `n_channels=3`.\n\nimage_size : tuple of int (image_height, image_width) or\n\n\n\n\n\n\n\n (image_height, image_width, n_channels) The size of the image that will be reconstructed.\n##### Returns\n* **image **: ndarray of shape image_size\n    The reconstructed image.\n\n"
},{
    "source file": "grower.py",
    "line number": "524",
    "func name": "_fill_predictor_node_array",
    "func arg": "(predictor_nodes, grower_node, bin_thresholds, n_bins_non_missing, next_free_idx)",
    "comments": "Helper used in make_predictor to set the TreePredictor fields.\n\n\n"
},{
    "source file": "graph.py",
    "line number": "23",
    "func name": "single_source_shortest_path_length",
    "func arg": "(graph, source)",
    "comments": "Return the shortest path length from source to all reachable nodes.\n\n\n##### Returns\n* **graph **: sparse matrix or 2D array (preferably LIL matrix)\n    Adjacency matrix of the graph\n\n* **source **: integer\n   Starting node for path\n\n* **cutoff **: integer, optional\n    Depth to stop the search - only\n    paths of length <= cutoff are returned.\n\n"
},{
    "source file": "glm.py",
    "line number": "38",
    "func name": "_y_pred_deviance_derivative",
    "func arg": "(coef, X, y, weights, family, link)",
    "comments": "Compute y_pred and the derivative of the deviance w.r.t coef.\n\n\n"
},{
    "source file": "fixes.py",
    "line number": "165",
    "func name": "_take_along_axis",
    "func arg": "(arr, indices, axis)",
    "comments": "Implements a simplified version of np.take_along_axis if numpy version < 1.15\n\n\n"
},{
    "source file": "extmath.py",
    "line number": "788",
    "func name": "stable_cumsum",
    "func arg": "(arr, axis, rtol, atol)",
    "comments": "Use high precision for cumsum and check that final value matches sum\n\nParameters ---------- arr : array-like To be cumulatively summed as flat axis : int, optional Axis along which the cumulative sum is computed. The default (None) is to compute the cumsum over the flattened array. rtol : float Relative tolerance, see ``np.allclose`` atol : float Absolute tolerance, see ``np.allclose``\n"
},{
    "source file": "estimator_checks.py",
    "line number": "2961",
    "func name": "check_requires_y_none",
    "func arg": "(name, estimator_orig)",
    "comments": ""
},{
    "source file": "discriminant_analysis.py",
    "line number": "96",
    "func name": "_class_cov",
    "func arg": "(X, y, priors, shrinkage)",
    "comments": "Compute weighted within-class covariance matrix.\n\nThe per-class covariance are weighted by the class priors.\n\nParameters ---------- X : array-like of shape (n_samples, n_features) Input data.\n\ny : array-like of shape (n_samples,) or (n_samples, n_targets) Target values.\n\npriors : array-like of shape (n_classes,) Class priors.\n\nshrinkage : 'auto' or float, default=None Shrinkage parameter, possible values:\n\n- None: no shrinkage (default).\n\n- 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n\n- float between 0 and 1: fixed shrinkage parameter.\n##### Returns\n* **cov **: array-like of shape (n_features, n_features)\n    Weighted within-class covariance matrix\n\n"
},{
    "source file": "deprecation.py",
    "line number": "115",
    "func name": "_is_deprecated",
    "func arg": "(func)",
    "comments": "Helper to check if func is wrapped by our deprecated decorator\n\n\n"
},{
    "source file": "confusion_matrix.py",
    "line number": "158",
    "func name": "plot_confusion_matrix",
    "func arg": "(estimator, X, y_true)",
    "comments": "Plot Confusion Matrix.\n\nRead more in the :ref:`User Guide <confusion_matrix>`.\n\nParameters ---------- estimator : estimator instance Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline` in which the last estimator is a classifier.\n\nX : {array-like, sparse matrix} of shape (n_samples, n_features) Input values.\n\ny : array-like of shape (n_samples,) Target values.\n\nlabels : array-like of shape (n_classes,), default=None List of labels to index the matrix. This may be used to reorder or select a subset of labels. If `None` is given, those that appear at least once in `y_true` or `y_pred` are used in sorted order.\n\nsample_weight : array-like of shape (n_samples,), default=None Sample weights.\n\nnormalize : {'true', 'pred', 'all'}, default=None Normalizes confusion matrix over the true (rows), predicted (columns) conditions or all the population. If None, confusion matrix will not be normalized.\n\ndisplay_labels : array-like of shape (n_classes,), default=None Target names used for plotting. By default, `labels` will be used if it is defined, otherwise the unique labels of `y_true` and `y_pred` will be used.\n\ninclude_values : bool, default=True Includes values in confusion matrix.\n\nxticks_rotation : {'vertical', 'horizontal'} or float,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n default='horizontal' Rotation of xtick labels.\n\nvalues_format : str, default=None Format specification for values in confusion matrix. If `None`, the format specification is 'd' or '.2g' whichever is shorter.\n\ncmap : str or matplotlib Colormap, default='viridis' Colormap recognized by matplotlib.\n\nax : matplotlib Axes, default=None Axes object to plot on. If `None`, a new figure and axes is created.\n\ncolorbar : bool, default=True Whether or not to add a colorbar to the plot.\n\n.. versionadded:: 0.24\n##### Returns\n* **display **: \n\n* **confusion_matrix **: Compute confusion matrix to evaluate the accuracy of a classification\n\n* **>>> import matplotlib.pyplot as plt  # doctest**: +SKIP\n\n* **>>> plot_confusion_matrix(clf, X_test, y_test)  # doctest**: +SKIP\n\n* **>>> plt.show()  # doctest**: +SKIP\n\n"
},{
    "source file": "conftest3.py",
    "line number": "5",
    "func name": "pytest_ignore_collect",
    "func arg": "(path, config)",
    "comments": ""
},{
    "source file": "conftest2.py",
    "line number": "29",
    "func name": "pytest_runtest_setup",
    "func arg": "(item)",
    "comments": "Set the number of openmp threads based on the number of workers xdist is using to prevent oversubscription.\n\nParameters ---------- item : pytest item item to be processed\n"
},{
    "source file": "conftest1.py",
    "line number": "66",
    "func name": "hide_available_pandas",
    "func arg": "(monkeypatch)",
    "comments": "Pretend pandas was not installed.\n\n\n"
},{
    "source file": "conftest.py",
    "line number": "7",
    "func name": "print_changed_only_false",
    "func arg": "()",
    "comments": ""
},{
    "source file": "common1.py",
    "line number": "12",
    "func name": "generate_clustered_data",
    "func arg": "(seed, n_clusters, n_features, n_samples_per_cluster, std)",
    "comments": ""
},{
    "source file": "class_weight.py",
    "line number": "76",
    "func name": "compute_sample_weight",
    "func arg": "(class_weight, y)",
    "comments": "Estimate sample weights by class for unbalanced datasets.\n\nParameters ---------- class_weight : dict, list of dicts, \"balanced\", or None, optional Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n\nNote that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}].\n\nThe \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data: ``n_samples / (n_classes * np.bincount(y))``.\n\nFor multi-output, the weights of each column of y will be multiplied.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs) Array of original class labels per sample.\n\nindices : array-like, shape (n_subsample,), or None Array of indices to be used in a subsample. Can be of length less than n_samples in the case of a subsample, or equal to n_samples in the case of a bootstrap subsample with repeated indices. If None, the sample weight will be calculated over the full sample. Only \"balanced\" is supported for class_weight if this is provided.\n##### Returns\n* **sample_weight_vect **: ndarray, shape (n_samples,)\n    Array with sample weights as applied to the original y\n\n"
},{
    "source file": "calibration.py",
    "line number": "588",
    "func name": "calibration_curve",
    "func arg": "(y_true, y_prob)",
    "comments": "Compute true and predicted probabilities for a calibration curve.\n\nThe method assumes the inputs come from a binary classifier, and discretize the [0, 1] interval into bins.\n\nCalibration curves may also be referred to as reliability diagrams.\n\nRead more in the :ref:`User Guide <calibration>`.\n\nParameters ---------- y_true : array-like of shape (n_samples,) True targets.\n\ny_prob : array-like of shape (n_samples,) Probabilities of the positive class.\n\nnormalize : bool, default=False Whether y_prob needs to be normalized into the [0, 1] interval, i.e. is not a proper probability. If True, the smallest value in y_prob is linearly mapped onto 0 and the largest one onto 1.\n\nn_bins : int, default=5 Number of bins to discretize the [0, 1] interval. A bigger number requires more data. Bins with no samples (i.e. without corresponding values in `y_prob`) will not be returned, thus the returned arrays may have less than `n_bins` values.\n\nstrategy : {'uniform', 'quantile'}, default='uniform' Strategy used to define the widths of the bins.\n\nuniform The bins have identical widths. quantile The bins have the same number of samples and depend on `y_prob`.\n##### Returns\n* **prob_true **: ndarray of shape (n_bins,) or smaller\n    The proportion of samples whose class is the positive class, in each\n    bin (fraction of positives).\n\n* **prob_pred **: ndarray of shape (n_bins,) or smaller\n    The mean predicted probability in each bin.\n\n"
},{
    "source file": "binning.py",
    "line number": "19",
    "func name": "_find_binning_thresholds",
    "func arg": "(data, max_bins, subsample, random_state)",
    "comments": "Extract feature-wise quantiles from numerical data.\n\nMissing values are ignored for finding the thresholds.\n\nParameters ---------- data : array-like, shape (n_samples, n_features) The data to bin. max_bins: int The maximum number of bins to use for non-missing values. If for a given feature the number of unique values is less than ``max_bins``, then those unique values will be used to compute the bin thresholds, instead of the quantiles. subsample : int or None If ``n_samples > subsample``, then ``sub_samples`` samples will be randomly chosen to compute the quantiles. If ``None``, the whole data is used. random_state: int, RandomState instance or None Pseudo-random number generator to control the random sub-sampling. Pass an int for reproducible output across multiple function calls. See :term: `Glossary <random_state>`.\n\nReturn ------ binning_thresholds: list of arrays For each feature, stores the increasing numeric values that can be used to separate the bins. Thus ``len(binning_thresholds) == n_features``.\n"
},{
    "source file": "base2.py",
    "line number": "48",
    "func name": "_get_response",
    "func arg": "(X, estimator, response_method, pos_label)",
    "comments": "Return response and positive label.\n\nParameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) Input values.\n\nestimator : estimator instance Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline` in which the last estimator is a classifier.\n\nresponse_method: {'auto', 'predict_proba', 'decision_function'} Specifies whether to use :term:`predict_proba` or :term:`decision_function` as the target response. If set to 'auto', :term:`predict_proba` is tried first and if it does not exist :term:`decision_function` is tried next.\n\npos_label : str or int, default=None The class considered as the positive class when computing the metrics. By default, `estimators.classes_[1]` is considered as the positive class.\n##### Returns\n* **y_pred**: ndarray of shape (n_samples,)\n    Target scores calculated from the provided response_method\n    and pos_label.\n\n* **pos_label**: str or int\n    The class considered as the positive class when computing\n    the metrics.\n\n"
},{
    "source file": "base1.py",
    "line number": "790",
    "func name": "is_outlier_detector",
    "func arg": "(estimator)",
    "comments": "Return True if the given estimator is (probably) an outlier detector.\n\nParameters ---------- estimator : object Estimator object to test.\n##### Returns\n* **out **: bool\n    True if estimator is an outlier detector and False otherwise.\n\n"
},{
    "source file": "_weight_boosting.py",
    "line number": "265",
    "func name": "_samme_proba",
    "func arg": "(estimator, n_classes, X)",
    "comments": "Calculate algorithm 4, step 2, equation c) of Zhu et al [1].\n\nReferences ---------- .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n"
},{
    "source file": "_validation.py",
    "line number": "1543",
    "func name": "_aggregate_score_dicts",
    "func arg": "(scores)",
    "comments": "Aggregate the list of dict to dict of np ndarray\n\nThe aggregated output of _aggregate_score_dicts will be a list of dict of form [{'prec': 0.1, 'acc':1.0}, {'prec': 0.1, 'acc':1.0}, ...] Convert it to a dict of array {'prec': np.array([0.1 ...]), ...}\n\nParameters ----------\n\nscores : list of dict List of dicts of the scores for all scorers. This is a flat list, assumed originally to be of row major order.\n\nExample -------\n\n>>> scores = [{'a': 1, 'b':10}, {'a': 2, 'b':2}, {'a': 3, 'b':3}, ...\n\n\n\n\n\n\n\n\n\n {'a': 10, 'b': 10}]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n # doctest: +SKIP >>> _aggregate_score_dicts(scores)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# doctest: +SKIP {'a': array([1, 2, 3, 10]), 'b': array([10, 2, 3, 10])}\n"
},{
    "source file": "_unsupervised1.py",
    "line number": "303",
    "func name": "davies_bouldin_score",
    "func arg": "(X, labels)",
    "comments": "Computes the Davies-Bouldin score.\n\nThe score is defined as the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of within-cluster distances to between-cluster distances. Thus, clusters which are farther apart and less dispersed will result in a better score.\n\nThe minimum score is zero, with lower values indicating better clustering.\n\nRead more in the :ref:`User Guide <davies-bouldin_index>`.\n\n.. versionadded:: 0.20\n\nParameters ---------- X : array-like, shape (``n_samples``, ``n_features``) List of ``n_features``-dimensional data points. Each row corresponds to a single data point.\n\nlabels : array-like, shape (``n_samples``,) Predicted labels for each sample.\n##### Returns\n* **score**: float\n    The resulting Davies-Bouldin score.\n\n* **.. [1] Davies, David L.; Bouldin, Donald W. (1979).\n   `\"A Cluster Separation Measure\"\n   <https**: //ieeexplore.ieee.org/document/4766909>`__.\n   IEEE Transactions on Pattern Analysis and Machine Intelligence.\n   PAMI-1 (2)\n\n"
},{
    "source file": "_univariate_selection.py",
    "line number": "233",
    "func name": "f_regression",
    "func arg": "(X, y)",
    "comments": "Univariate linear regression tests.\n\nLinear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure.\n\nThis is done in 2 steps:\n\n1. The correlation between each regressor and the target is computed, that is, ((X[:, i]\n\n- mean(X[:, i])) * (y\n\n- mean_y)) / (std(X[:, i]) * std(y)). 2. It is converted to an F score then to a p-value.\n\nFor more on usage see the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters ---------- X : {array-like, sparse matrix}\n\nshape = (n_samples, n_features) The set of regressors that will be tested sequentially.\n\ny : array of shape(n_samples). The data matrix\n\ncenter : bool, default=True If true, X and y will be centered.\n##### Returns\n* **F **: array, shape=(n_features,)\n    F values of features.\n\n* **pval **: array, shape=(n_features,)\n    p-values of F-scores.\n\n* **mutual_info_regression**: Mutual information for a continuous target.\n\n* **f_classif**: ANOVA F-value between label/feature for classification tasks.\n\n* **chi2**: Chi-squared stats of non-negative features for classification tasks.\n\n* **SelectKBest**: Select features based on the k highest scores.\n\n* **SelectFpr**: Select features based on a false positive rate test.\n\n* **SelectFdr**: Select features based on an estimated false discovery rate.\n\n* **SelectFwe**: Select features based on family-wise error rate.\n\n* **SelectPercentile**: Select features based on percentile of the highest\n    scores.\n\n"
},{
    "source file": "_twenty_newsgroups.py",
    "line number": "328",
    "func name": "fetch_20newsgroups_vectorized",
    "func arg": "()",
    "comments": "Load the 20 newsgroups dataset and vectorize it into token counts (classification).\n\nDownload it if necessary.\n\nThis is a convenience function; the transformation is done using the default settings for :class:`~sklearn.feature_extraction.text.CountVectorizer`. For more advanced usage (stopword filtering, n-gram extraction, etc.), combine fetch_20newsgroups with a custom :class:`~sklearn.feature_extraction.text.CountVectorizer`, :class:`~sklearn.feature_extraction.text.HashingVectorizer`, :class:`~sklearn.feature_extraction.text.TfidfTransformer` or :class:`~sklearn.feature_extraction.text.TfidfVectorizer`.\n\nThe resulting counts are normalized using :func:`sklearn.preprocessing.normalize` unless normalize is set to False.\n\n=================\n\n ========== Classes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 20 Samples total\n\n\n\n\n\n\n\n\n\n\n\n18846 Dimensionality\n\n\n\n\n\n\n\n\n\n130107 Features\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreal =================\n\n ==========\n\nRead more in the :ref:`User Guide <20newsgroups_dataset>`.\n\nParameters ---------- subset : {'train', 'test', 'all'}, default='train' Select the dataset to load: 'train' for the training set, 'test' for the test set, 'all' for both, with shuffled ordering.\n\nremove : tuple, default=() May contain any subset of ('headers', 'footers', 'quotes'). Each of these are kinds of text that will be detected and removed from the newsgroup posts, preventing classifiers from overfitting on metadata.\n\n'headers' removes newsgroup headers, 'footers' removes blocks at the ends of posts that look like signatures, and 'quotes' removes lines that appear to be quoting another post.\n\ndata_home : str, default=None Specify an download and cache folder for the datasets. If None, all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\ndownload_if_missing : bool, default=True If False, raise an IOError if the data is not locally available instead of trying to download the data from the source site.\n\nreturn_X_y : bool, default=False If True, returns ``(data.data, data.target)`` instead of a Bunch object.\n\n.. versionadded:: 0.20\n\nnormalize : bool, default=True If True, normalizes each document's feature vector to unit norm using :func:`sklearn.preprocessing.normalize`.\n\n.. versionadded:: 0.22\n##### Returns\n* **bunch **: \n\n* **(data, target) **: tuple if ``return_X_y`` is True\n    .. versionadded\n\n"
},{
    "source file": "_theil_sen.py",
    "line number": "151",
    "func name": "_lstsq",
    "func arg": "(X, y, indices, fit_intercept)",
    "comments": "Least Squares Estimator for TheilSenRegressor class.\n\nThis function calculates the least squares method on a subset of rows of X and y defined by the indices array. Optionally, an intercept column is added if intercept is set to true.\n\nParameters ---------- X : array-like of shape (n_samples, n_features) Design matrix, where n_samples is the number of samples and n_features is the number of features.\n\ny : array, shape = [n_samples] Target vector, where n_samples is the number of samples.\n\nindices : array, shape = [n_subpopulation, n_subsamples] Indices of all subsamples with respect to the chosen subpopulation.\n\nfit_intercept : bool Fit intercept or not.\n##### Returns\n* **weights **: array, shape = [n_subpopulation, n_features + intercept]\n    Solution matrix of n_subpopulation solved least square problems.\n\n"
},{
    "source file": "_testing.py",
    "line number": "752",
    "func name": "_convert_container",
    "func arg": "(container, constructor_name, columns_name)",
    "comments": ""
},{
    "source file": "_t_sne.py",
    "line number": "403",
    "func name": "trustworthiness",
    "func arg": "(X, X_embedded)",
    "comments": "Expresses to what extent the local structure is retained.\n\nThe trustworthiness is within [0, 1]. It is defined as\n\n.. math::\n\nT(k) = 1\n\n- \\frac{2},{nk (2n\n\n- 3k\n\n- 1)} \\sum^n_{i=1} \\sum_{j \\in \\mathcal{N}_{i}^{k}} \\max(0, (r(i, j)\n\n- k))\n\nwhere for each sample i, :math:`\\mathcal{N}_{i}^{k}` are its k nearest neighbors in the output space, and every sample j is its :math:`r(i, j)`-th nearest neighbor in the input space. In other words, any unexpected nearest neighbors in the output space are penalised in proportion to their rank in the input space.\n\n* \"Neighborhood Preservation in Nonlinear Projection Methods: An Experimental Study\" J. Venna, S. Kaski * \"Learning a Parametric Embedding by Preserving Local Structure\" L.J.P. van der Maaten\n\nParameters ---------- X : array, shape (n_samples, n_features) or (n_samples, n_samples) If the metric is 'precomputed' X must be a square distance matrix. Otherwise it contains a sample per row.\n\nX_embedded : array, shape (n_samples, n_components) Embedding of the training data in low-dimensional space.\n\nn_neighbors : int, default=5 Number of neighbors k that will be considered.\n\nmetric : string, or callable, default='euclidean' Which metric to use for computing pairwise distances between samples from the original input space. If metric is 'precomputed', X must be a matrix of pairwise distances or squared distances. Otherwise, see the documentation of argument metric in sklearn.pairwise.pairwise_distances for a list of available metrics.\n\n.. versionadded:: 0.20\n##### Returns\n* **trustworthiness **: float\n    Trustworthiness of the low-dimensional embedding.\n\n"
},{
    "source file": "_svmlight_format_io.py",
    "line number": "392",
    "func name": "dump_svmlight_file",
    "func arg": "(X, y, f)",
    "comments": "Dump the dataset in svmlight / libsvm file format.\n\nThis format is a text-based format, with one sample per line. It does not store zero valued features hence is suitable for sparse dataset.\n\nThe first element of each line can be used to store a target variable to predict.\n\nParameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vectors, where n_samples is the number of samples and n_features is the number of features.\n\ny : {array-like, sparse matrix}, shape = [n_samples (, n_labels)] Target values. Class labels must be an integer or float, or array-like objects of integer or float for multilabel classifications.\n\nf : string or file-like in binary mode If string, specifies the path that will contain the data. If file-like, data will be written to f. f should be opened in binary mode.\n\nzero_based : boolean, optional Whether column indices should be written zero-based (True) or one-based (False).\n\ncomment : string, optional Comment to insert at the top of the file. This should be either a Unicode string, which will be encoded as UTF-8, or an ASCII byte string. If a comment is given, then it will be preceded by one that identifies the file as having been dumped by scikit-learn. Note that not all tools grok comments in SVMlight files.\n\nquery_id : array-like of shape (n_samples,) Array containing pairwise preference constraints (qid in svmlight format).\n\nmultilabel : boolean, optional Samples may have several labels each (see https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n\n.. versionadded:: 0.17 parameter *multilabel* to support multilabel datasets.\n"
},{
    "source file": "_supervised.py",
    "line number": "960",
    "func name": "entropy",
    "func arg": "(labels)",
    "comments": "Calculates the entropy for a labeling.\n\nParameters ---------- labels : int array, shape = [n_samples] The labels\n\nNotes ----- The logarithm used is the natural logarithm (base-e).\n"
},{
    "source file": "_stochastic_gradient.py",
    "line number": "354",
    "func name": "fit_binary",
    "func arg": "(est, i, X, y, alpha, C, learning_rate, max_iter, pos_weight, neg_weight, sample_weight, validation_mask, random_state)",
    "comments": "Fit a single binary classifier.\n\nThe i'th class is considered the \"positive\" class.\n\nParameters ---------- est : Estimator object The estimator to fit\n\ni : int Index of the positive class\n\nX : numpy array or sparse matrix of shape [n_samples,n_features] Training data\n\ny : numpy array of shape [n_samples, ] Target values\n\nalpha : float The regularization parameter\n\nC : float Maximum step size for passive aggressive\n\nlearning_rate : string The learning rate. Accepted values are 'constant', 'optimal', 'invscaling', 'pa1' and 'pa2'.\n\nmax_iter : int The maximum number of iterations (epochs)\n\npos_weight : float The weight of the positive class\n\nneg_weight : float The weight of the negative class\n\nsample_weight : numpy array of shape [n_samples, ] The weight of each sample\n\nvalidation_mask : numpy array of shape [n_samples, ], default=None Precomputed validation mask in case _fit_binary is called in the context of a one-vs-rest reduction.\n\nrandom_state : int, RandomState instance, default=None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.\n"
},{
    "source file": "_split.py",
    "line number": "2205",
    "func name": "_build_repr",
    "func arg": "()",
    "comments": ""
},{
    "source file": "_spectral.py",
    "line number": "161",
    "func name": "spectral_clustering",
    "func arg": "(affinity)",
    "comments": "Apply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance, when clusters are nested circles on the 2D plane.\n\nIf affinity is the adjacency matrix of a graph, this method can be used to find normalized graph cuts.\n\nRead more in the :ref:`User Guide <spectral_clustering>`.\n\nParameters ---------- affinity : {array-like, sparse matrix} of shape (n_samples, n_samples) The affinity matrix describing the relationship of the samples to embed. **Must be symmetric**.\n\nPossible examples:\n\n- adjacency matrix of a graph,\n\n- heat kernel of the pairwise distance matrix of the samples,\n\n- symmetric k-nearest neighbours connectivity matrix of the samples.\n\nn_clusters : int, default=None Number of clusters to extract.\n\nn_components : int, default=n_clusters Number of eigen vectors to use for the spectral embedding\n\neigen_solver : {None, 'arpack', 'lobpcg', or 'amg'} The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities. If None, then ``'arpack'`` is used.\n\nrandom_state : int, RandomState instance, default=None A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when eigen_solver == 'amg' and by the K-Means initialization. Use an int to make the randomness deterministic. See :term:`Glossary <random_state>`.\n\nn_init : int, default=10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.\n\neigen_tol : float, default=0.0 Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack eigen_solver.\n\nassign_labels : {'kmeans', 'discretize'}, default='kmeans' The strategy to use to assign labels in the embedding space.\n\nThere are two ways to assign labels after the laplacian embedding.\n\nk-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization. See the 'Multiclass spectral clustering' paper referenced below for more details on the discretization approach.\n##### Returns\n* **labels **: array of integers, shape\n\n* **- Normalized cuts and image segmentation, 2000\n  Jianbo Shi, Jitendra Malik\n  http**: //citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324\n\n* **- A Tutorial on Spectral Clustering, 2007\n  Ulrike von Luxburg\n  http**: //citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n\n* **- Multiclass spectral clustering, 2003\n  Stella X. Yu, Jianbo Shi\n  https**: //www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf\n\n* **This algorithm solves the normalized cut for k=2**: it is a\n\n"
},{
    "source file": "_spectral_embedding.py",
    "line number": "137",
    "func name": "spectral_embedding",
    "func arg": "(adjacency)",
    "comments": "Project the sample on the first eigenvectors of the graph Laplacian.\n\nThe adjacency matrix is used to compute a normalized graph Laplacian whose spectrum (especially the eigenvectors associated to the smallest eigenvalues) has an interpretation in terms of minimal number of cuts necessary to split the graph into comparably sized components.\n\nThis embedding can also 'work' even if the ``adjacency`` variable is not strictly the adjacency matrix of a graph but more generally an affinity or similarity matrix between samples (for instance the heat kernel of a euclidean distance matrix or a k-NN matrix).\n\nHowever care must taken to always make the affinity matrix symmetric so that the eigenvector decomposition works as expected.\n\nNote : Laplacian Eigenmaps is the actual algorithm implemented here.\n\nRead more in the :ref:`User Guide <spectral_embedding>`.\n\nParameters ---------- adjacency : array-like or sparse graph, shape: (n_samples, n_samples) The adjacency matrix of the graph to embed.\n\nn_components : integer, optional, default 8 The dimension of the projection subspace.\n\neigen_solver : {None, 'arpack', 'lobpcg', or 'amg'}, default None The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities. If None, then ``'arpack'`` is used.\n\nrandom_state : int, RandomState instance, default=None Determines the random number generator used for the initialization of the lobpcg eigenvectors decomposition when ``solver`` == 'amg'. Pass an int for reproducible results across multiple function calls. See :term: `Glossary <random_state>`.\n\neigen_tol : float, optional, default=0.0 Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack eigen_solver.\n\nnorm_laplacian : bool, optional, default=True If True, then compute normalized Laplacian.\n\ndrop_first : bool, optional, default=True Whether to drop the first eigenvector. For spectral embedding, this should be True as the first eigenvector should be constant vector for connected graph, but for spectral clustering, this should be kept as False to retain the first eigenvector.\n##### Returns\n* **embedding **: array, shape=(n_samples, n_components)\n    The reduced samples.\n\n* *** https**: //en.wikipedia.org/wiki/LOBPCG\n\n* *** Toward the Optimal Preconditioned Eigensolver**: Locally Optimal\n  Block Preconditioned Conjugate Gradient Method\n  Andrew V. Knyazev\n  https\n\n"
},{
    "source file": "_species_distributions.py",
    "line number": "142",
    "func name": "fetch_species_distributions",
    "func arg": "()",
    "comments": "Loader for species distribution dataset from Phillips et. al. (2006)\n\nRead more in the :ref:`User Guide <datasets>`.\n\nParameters ---------- data_home : str, default=None Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\ndownload_if_missing : bool, default=True If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site.\n##### Returns\n* **data **: \n\n* *** `\"Maximum entropy modeling of species geographic distributions\"\n  <http**: //rob.schapire.net/papers/ecolmod.pdf>`_\n  S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling,\n  190\n\n* **The two species are**: \n\n* **- `\"Bradypus variegatus\"\n  <http**: //www.iucnredlist.org/details/3038/0>`_ ,\n  the Brown-throated Sloth.\n\n* **- `\"Microryzomys minutus\"\n  <http**: //www.iucnredlist.org/details/13408/0>`_ ,\n  also known as the Forest Small Rice Rat, a rodent that lives in Peru,\n  Colombia, Ecuador, Peru, and Venezuela.\n\n* **- For an example of using this dataset with scikit-learn, see\n  **: ref\n\n"
},{
    "source file": "_shrunk_covariance.py",
    "line number": "435",
    "func name": "oas",
    "func arg": "(X)",
    "comments": "Estimate covariance with the Oracle Approximating Shrinkage algorithm.\n\nParameters ---------- X : array-like of shape (n_samples, n_features) Data from which to compute the covariance estimate.\n\nassume_centered : bool, default=False If True, data will not be centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data will be centered before computation.\n##### Returns\n* **shrunk_cov **: array-like of shape (n_features, n_features)\n    Shrunk covariance.\n\n* **shrinkage **: float\n    Coefficient in the convex combination used for the computation\n    of the shrunk estimate.\n\n* **The regularised (shrunk) covariance is**: \n\n* **to the one given in the article. See **: class\n\n"
},{
    "source file": "_show_versions.py",
    "line number": "76",
    "func name": "show_versions",
    "func arg": "()",
    "comments": "Print useful debugging information\"\n\n.. versionadded:: 0.20\n"
},{
    "source file": "_search.py",
    "line number": "379",
    "func name": "_check_param_grid",
    "func arg": "(param_grid)",
    "comments": ""
},{
    "source file": "_scorer.py",
    "line number": "522",
    "func name": "make_scorer",
    "func arg": "(score_func, **kwargs)",
    "comments": "Make a scorer from a performance metric or loss function.\n\nThis factory function wraps scoring functions for use in GridSearchCV and cross_val_score. It takes a score function, such as ``accuracy_score``, ``mean_squared_error``, ``adjusted_rand_index`` or ``average_precision`` and returns a callable that scores an estimator's output.\n\nRead more in the :ref:`User Guide <scoring>`.\n\nParameters ---------- score_func : callable, Score function (or loss function) with signature ``score_func(y, y_pred, **kwargs)``.\n\ngreater_is_better : boolean, default=True Whether score_func is a score function (default), meaning high is good, or a loss function, meaning low is good. In the latter case, the scorer object will sign-flip the outcome of the score_func.\n\nneeds_proba : boolean, default=False Whether score_func requires predict_proba to get probability estimates out of a classifier.\n\nIf True, for binary `y_true`, the score function is supposed to accept a 1D `y_pred` (i.e., probability of the positive class, shape `(n_samples,)`).\n\nneeds_threshold : boolean, default=False Whether score_func takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method.\n\nIf True, for binary `y_true`, the score function is supposed to accept a 1D `y_pred` (i.e., probability of the positive class or the decision function, shape `(n_samples,)`).\n\nFor example ``average_precision`` or the area under the roc curve can not be computed using discrete predictions alone.\n\n**kwargs : additional arguments Additional parameters to be passed to score_func.\n##### Returns\n* **scorer **: callable\n    Callable object that returns a scalar score; greater is better.\n\n* **>>> grid = GridSearchCV(LinearSVC(), param_grid={'C'**: [1, 10]},\n\n* **function is supposed to accept the output of **: term\n\n* **output of **: term\n\n"
},{
    "source file": "_samples_generator.py",
    "line number": "1676",
    "func name": "make_checkerboard",
    "func arg": "(shape, n_clusters)",
    "comments": "Generate an array with block checkerboard structure for biclustering.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n\nParameters ---------- shape : tuple (n_rows, n_cols) The shape of the result.\n\nn_clusters : int or array-like (n_row_clusters, n_column_clusters) The number of row and column clusters.\n\nnoise : float, default=0.0 The standard deviation of the gaussian noise.\n\nminval : int, default=10 Minimum value of a bicluster.\n\nmaxval : int, default=100 Maximum value of a bicluster.\n\nshuffle : bool, default=True Shuffle the samples.\n\nrandom_state : int or RandomState instance, default=None Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.\n##### Returns\n* **X **: array of shape `shape`\n    The generated array.\n\n* **rows **: array of shape (n_clusters, X.shape[0],)\n    The indicators for cluster membership of each row.\n\n* **cols **: array of shape (n_clusters, X.shape[1],)\n    The indicators for cluster membership of each column.\n\n* **.. [1] Kluger, Y., Basri, R., Chang, J. T., & Gerstein, M. (2003).\n    Spectral biclustering of microarray data**: coclustering genes\n    and conditions. Genome research, 13(4), 703-716.\n\n"
},{
    "source file": "_sag.py",
    "line number": "89",
    "func name": "sag_solver",
    "func arg": "(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)",
    "comments": "SAG solver for Ridge and LogisticRegression\n\nSAG stands for Stochastic Average Gradient: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a constant learning rate.\n\nIMPORTANT NOTE: 'sag' solver converges faster on columns that are on the same scale. You can normalize the data by using sklearn.preprocessing.StandardScaler on your data before passing it to the fit method.\n\nThis implementation works with data represented as dense numpy arrays or sparse scipy arrays of floating point values for the features. It will fit the data according to squared loss or log loss.\n\nThe regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using the squared euclidean norm L2.\n\n.. versionadded:: 0.17\n\nParameters ---------- X : {array-like, sparse matrix}, shape (n_samples, n_features) Training data\n\ny : numpy array, shape (n_samples,) Target values. With loss='multinomial', y must be label encoded (see preprocessing.LabelEncoder).\n\nsample_weight : array-like, shape (n_samples,), optional Weights applied to individual samples (1. for unweighted).\n\nloss : 'log' | 'squared' | 'multinomial' Loss function that will be optimized: -'log' is the binary logistic loss, as used in LogisticRegression. -'squared' is the squared loss, as used in Ridge. -'multinomial' is the multinomial logistic loss, as used in LogisticRegression.\n\n.. versionadded:: 0.18 *loss='multinomial'*\n\nalpha : float, optional L2 regularization term in the objective function ``(0.5 * alpha * || W ||_F^2)``. Defaults to 1.\n\nbeta : float, optional L1 regularization term in the objective function ``(beta * || W ||_1)``. Only applied if ``is_saga`` is set to True. Defaults to 0.\n\nmax_iter : int, optional The max number of passes over the training data if the stopping criteria is not reached. Defaults to 1000.\n\ntol : double, optional The stopping criteria for the weights. The iterations will stop when max(change in weights) / max(weights) < tol. Defaults to .001\n\nverbose : integer, optional The verbosity level.\n\nrandom_state : int, RandomState instance, default=None Used when shuffling the data. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.\n\ncheck_input : bool, default True If False, the input arrays X and y will not be checked.\n\nmax_squared_sum : float, default None Maximum squared sum of X over samples. If None, it will be computed, going through all the samples. The value should be precomputed to speed up cross validation.\n\nwarm_start_mem : dict, optional The initialization parameters used for warm starting. Warm starting is currently used in LogisticRegression but not in Ridge. It contains:\n\n- 'coef': the weight vector, with the intercept in last line if the intercept is fitted.\n\n- 'gradient_memory': the scalar gradient for all seen samples.\n\n- 'sum_gradient': the sum of gradient over all seen samples, for each feature.\n\n- 'intercept_sum_gradient': the sum of gradient over all seen samples, for the intercept.\n\n- 'seen': array of boolean describing the seen samples.\n\n- 'num_seen': the number of seen samples.\n\nis_saga : boolean, optional Whether to use the SAGA algorithm or the SAG algorithm. SAGA behaves better in the first epochs, and allow for l1 regularisation.\n##### Returns\n* **coef_ **: array, shape (n_features)\n    Weight vector.\n\n* **n_iter_ **: int\n    The number of full pass on all samples.\n\n* **warm_start_mem **: dict\n    Contains a 'coef' key with the fitted result, and possibly the\n    fitted intercept at the end of the array. Contains also other keys\n    used for warm starting.\n\n* **https**: //arxiv.org/abs/1407.0202\n\n* **SAGA**: A Fast Incremental Gradient Method With Support\n\n"
},{
    "source file": "_robust_covariance.py",
    "line number": "310",
    "func name": "fast_mcd",
    "func arg": "(X, support_fraction, cov_computation_method, random_state)",
    "comments": "Estimates the Minimum Covariance Determinant matrix.\n\nRead more in the :ref:`User Guide <robust_covariance>`.\n\nParameters ---------- X : array-like of shape (n_samples, n_features) The data matrix, with p features and n samples.\n\nsupport_fraction : float, default=None The proportion of points to be included in the support of the raw MCD estimate. Default is `None`, which implies that the minimum value of `support_fraction` will be used within the algorithm: `(n_sample + n_features + 1) / 2`. This parameter must be in the range (0, 1).\n\ncov_computation_method : callable,\n\n\n\n\n\n\n\n\n\n\n\n default=:func:`sklearn.covariance.empirical_covariance` The function which will be used to compute the covariance. Must return an array of shape (n_features, n_features).\n\nrandom_state : int or RandomState instance, default=None Determines the pseudo random number generator for shuffling the data. Pass an int for reproducible results across multiple function calls. See :term: `Glossary <random_state>`.\n##### Returns\n* **location **: ndarray of shape (n_features,)\n    Robust location of the data.\n\n* **covariance **: ndarray of shape (n_features, n_features)\n    Robust covariance of the features.\n\n* **support **: ndarray of shape (n_samples,), dtype=bool\n    A mask of the observations that have been used to compute\n    the robust location and covariance estimates of the data set.\n\n"
},{
    "source file": "_ridge.py",
    "line number": "969",
    "func name": "_find_smallest_angle",
    "func arg": "(query, vectors)",
    "comments": "Find the column of vectors that is most aligned with the query.\n\nBoth query and the columns of vectors must have their l2 norm equal to 1.\n\nParameters ---------- query : ndarray of shape (n_samples,) Normalized query vector.\n\nvectors : ndarray of shape (n_samples, n_features) Vectors to which we compare query, as columns. Must be normalized.\n"
},{
    "source file": "_rfe.py",
    "line number": "28",
    "func name": "_rfe_single_fit",
    "func arg": "(rfe, estimator, X, y, train, test, scorer)",
    "comments": "Return the score for a fit across one fold.\n\n\n"
},{
    "source file": "_reingold_tilford.py",
    "line number": "168",
    "func name": "second_walk",
    "func arg": "(v, m, depth, min)",
    "comments": ""
},{
    "source file": "_regression1.py",
    "line number": "865",
    "func name": "mean_gamma_deviance",
    "func arg": "(y_true, y_pred)",
    "comments": "Mean Gamma deviance regression loss.\n\nGamma deviance is equivalent to the Tweedie deviance with the power parameter `power=2`. It is invariant to scaling of the target variable, and measures relative errors.\n\nRead more in the :ref:`User Guide <mean_tweedie_deviance>`.\n\nParameters ---------- y_true : array-like of shape (n_samples,) Ground truth (correct) target values. Requires y_true > 0.\n\ny_pred : array-like of shape (n_samples,) Estimated target values. Requires y_pred > 0.\n\nsample_weight : array-like of shape (n_samples,), default=None Sample weights.\n##### Returns\n* **loss **: float\n    A non-negative floating point value (the best value is 0.0).\n\n"
},{
    "source file": "_rcv1.py",
    "line number": "285",
    "func name": "_find_permutation",
    "func arg": "(a, b)",
    "comments": "find the permutation from a to b\n\n\n"
},{
    "source file": "_ransac.py",
    "line number": "23",
    "func name": "_dynamic_max_trials",
    "func arg": "(n_inliers, n_samples, min_samples, probability)",
    "comments": "Determine number trials such that at least one outlier-free subset is sampled for the given inlier/outlier ratio.\n\nParameters ---------- n_inliers : int Number of inliers in the data.\n\nn_samples : int Total number of samples in the data.\n\nmin_samples : int Minimum number of samples chosen randomly from original data.\n\nprobability : float Probability (confidence) that one outlier-free sample is generated.\n##### Returns\n* **trials **: int\n    Number of trials.\n\n"
},{
    "source file": "_ranking.py",
    "line number": "1344",
    "func name": "ndcg_score",
    "func arg": "(y_true, y_score)",
    "comments": "Compute Normalized Discounted Cumulative Gain.\n\nSum the true scores ranked in the order induced by the predicted scores, after applying a logarithmic discount. Then divide by the best possible score (Ideal DCG, obtained for a perfect ranking) to obtain a score between 0 and 1.\n\nThis ranking metric yields a high value if true labels are ranked high by ``y_score``.\n\nParameters ---------- y_true : ndarray, shape (n_samples, n_labels) True targets of multilabel classification, or true scores of entities to be ranked.\n\ny_score : ndarray, shape (n_samples, n_labels) Target scores, can either be probability estimates, confidence values, or non-thresholded measure of decisions (as returned by \"decision_function\" on some classifiers).\n\nk : int, default=None Only consider the highest k scores in the ranking. If None, use all outputs.\n\nsample_weight : ndarray of shape (n_samples,),default=None Sample weights. If None, all samples are given the same weight.\n\nignore_ties : bool, default=False Assume that there are no ties in y_score (which is likely to be the case if y_score is continuous) for efficiency gains.\n##### Returns\n* **normalized_discounted_cumulative_gain **: float in [0., 1.]\n    The averaged NDCG scores for all samples.\n\n* **dcg_score **: Discounted Cumulative Gain (not normalized).\n\n* **<https**: //en.wikipedia.org/wiki/Discounted_cumulative_gain>`_\n\n* **>>> # we have groud-truth relevance of some answers to a query**: \n\n* **>>> # true relevance of our top predictions**: (10 / 10 + 5 / 10) / 2 = .75\n\n* **>>> # wrong results**: \n\n"
},{
    "source file": "_pprint.py",
    "line number": "335",
    "func name": "_safe_repr",
    "func arg": "(object, context, maxlevels, level, changed_only)",
    "comments": "Same as the builtin _safe_repr, with added support for Estimator objects.\n\n\n"
},{
    "source file": "_pls.py",
    "line number": "112",
    "func name": "_center_scale_xy",
    "func arg": "(X, Y, scale)",
    "comments": "Center X, Y and scale if the scale parameter==True\n\n\n##### Returns\n"
},{
    "source file": "_pilutil.py",
    "line number": "451",
    "func name": "imresize",
    "func arg": "(arr, size, interp, mode)",
    "comments": "Resize an image.\n\nThis function is only available if Python Imaging Library (PIL) is installed.\n\n.. warning::\n\nThis function uses `bytescale` under the hood to rescale images to use the full (0, 255) range if ``mode`` is one of ``None, 'L', 'P', 'l'``. It will also cast data for 2-D images to ``uint32`` for ``mode=None`` (which is the default).\n\nParameters ---------- arr : ndarray The array of image to be resized. size : int, float or tuple * int\n\n\n\n- Percentage of current size. * float\n\n- Fraction of current size. * tuple\n\n- Size of the output image (height, width).\n\ninterp : str, optional Interpolation to use for re-sizing ('nearest', 'lanczos', 'bilinear', 'bicubic' or 'cubic'). mode : str, optional The PIL image mode ('P', 'L', etc.) to convert `arr` before resizing. If ``mode=None`` (the default), 2-D images will be treated like ``mode='L'``, i.e. casting to long integer.\n\nFor 3-D and 4-D arrays, `mode` will be set to ``'RGB'`` and ``'RGBA'`` respectively.\n##### Returns\n* **imresize **: ndarray\n    The resized array of image.\n\n* **toimage **: Implicitly used to convert `arr` according to `mode`.\n\n* **scipy.ndimage.zoom **: More generic implementation that does not use PIL.\n\n"
},{
    "source file": "_permutation_importance.py",
    "line number": "42",
    "func name": "permutation_importance",
    "func arg": "(estimator, X, y)",
    "comments": "Permutation importance for feature evaluation [BRE]_.\n\nThe :term:`estimator` is required to be a fitted estimator. `X` can be the data set used to train the estimator or a hold-out set. The permutation importance of a feature is calculated as follows. First, a baseline metric, defined by :term:`scoring`, is evaluated on a (potentially different) dataset defined by the `X`. Next, a feature column from the validation set is permuted and the metric is evaluated again. The permutation importance is defined to be the difference between the baseline metric and metric from permutating the feature column.\n\nRead more in the :ref:`User Guide <permutation_importance>`.\n\nParameters ---------- estimator : object An estimator that has already been :term:`fitted` and is compatible with :term:`scorer`.\n\nX : ndarray or DataFrame, shape (n_samples, n_features) Data on which permutation importance will be computed.\n\ny : array-like or None, shape (n_samples, ) or (n_samples, n_classes) Targets for supervised or `None` for unsupervised.\n\nscoring : string, callable or None, default=None Scorer to use. It can be a single string (see :ref:`scoring_parameter`) or a callable (see :ref:`scoring`). If None, the estimator's default scorer is used.\n\nn_repeats : int, default=5 Number of times to permute a feature.\n\nn_jobs : int or None, default=None Number of jobs to run in parallel. The computation is done by computing permutation score for each columns and parallelized over the columns. `None` means 1 unless in a :obj:`joblib.parallel_backend` context. `-1` means using all processors. See :term:`Glossary <n_jobs>` for more details.\n\nrandom_state : int, RandomState instance, default=None Pseudo-random number generator to control the permutations of each feature. Pass an int to get reproducible results across function calls. See :term: `Glossary <random_state>`.\n##### Returns\n* **result **: \n\n* **.. [BRE] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32,\n         2001. https**: //doi.org/10.1023/A\n\n"
},{
    "source file": "_pca.py",
    "line number": "100",
    "func name": "_infer_dimension",
    "func arg": "(spectrum, n_samples)",
    "comments": "Infers the dimension of a dataset with a given spectrum.\n\nThe returned value will be in [1, n_features\n\n- 1].\n"
},{
    "source file": "_partial_dependence.py",
    "line number": "207",
    "func name": "partial_dependence",
    "func arg": "(estimator, X, features)",
    "comments": "Partial dependence of ``features``.\n\nPartial dependence of a feature (or a set of features) corresponds to the average response of an estimator for each possible value of the feature.\n\nRead more in the :ref:`User Guide <partial_dependence>`.\n\n.. warning::\n\nFor :class:`~sklearn.ensemble.GradientBoostingClassifier` and :class:`~sklearn.ensemble.GradientBoostingRegressor`, the `'recursion'` method (used by default) will not account for the `init` predictor of the boosting process. In practice, this will produce the same values as `'brute'` up to a constant offset in the target response, provided that `init` is a constant estimator (which is the default). However, if `init` is not a constant estimator, the partial dependence values are incorrect for `'recursion'` because the offset will be sample-dependent. It is preferable to use the `'brute'` method. Note that this only applies to :class:`~sklearn.ensemble.GradientBoostingClassifier` and :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.\n\nParameters ---------- estimator : BaseEstimator A fitted estimator object implementing :term:`predict`, :term:`predict_proba`, or :term:`decision_function`. Multioutput-multiclass classifiers are not supported.\n\nX : {array-like or dataframe} of shape (n_samples, n_features) ``X`` is used to generate a grid of values for the target ``features`` (where the partial dependence will be evaluated), and also to generate values for the complement features when the `method` is 'brute'.\n\nfeatures : array-like of {int, str} The feature (e.g. `[0]`) or pair of interacting features (e.g. `[(0, 1)]`) for which the partial dependency should be computed.\n\nresponse_method : {'auto', 'predict_proba', 'decision_function'},\n\n\n\n\n\n\n\n\n\n\n\n default='auto' Specifies whether to use :term:`predict_proba` or :term:`decision_function` as the target response. For regressors this parameter is ignored and the response is always the output of :term:`predict`. By default, :term:`predict_proba` is tried first and we revert to :term:`decision_function` if it doesn't exist. If ``method`` is 'recursion', the response is always the output of :term:`decision_function`.\n\npercentiles : tuple of float, default=(0.05, 0.95) The lower and upper percentile used to create the extreme values for the grid. Must be in [0, 1].\n\ngrid_resolution : int, default=100 The number of equally spaced points on the grid, for each target feature.\n\nmethod : {'auto', 'recursion', 'brute'}, default='auto' The method used to calculate the averaged predictions:\n\n- `'recursion'` is only supported for some tree-based estimators (namely :class:`~sklearn.ensemble.GradientBoostingClassifier`, :class:`~sklearn.ensemble.GradientBoostingRegressor`, :class:`~sklearn.ensemble.HistGradientBoostingClassifier`, :class:`~sklearn.ensemble.HistGradientBoostingRegressor`, :class:`~sklearn.tree.DecisionTreeRegressor`, :class:`~sklearn.ensemble.RandomForestRegressor`, ) when `kind='average'`. This is more efficient in terms of speed. With this method, the target response of a classifier is always the decision function, not the predicted probabilities. Since the `'recursion'` method implicitely computes the average of the Individual Conditional Expectation (ICE) by design, it is not compatible with ICE and thus `kind` must be `'average'`.\n\n- `'brute'` is supported for any estimator, but is more computationally intensive.\n\n- `'auto'`: the `'recursion'` is used for estimators that support it, and `'brute'` is used otherwise.\n\nPlease see :ref:`this note <pdp_method_differences>` for differences between the `'brute'` and `'recursion'` method.\n\nkind : {'legacy', 'average', 'individual', 'both'}, default='legacy' Whether to return the partial dependence averaged across all the samples in the dataset or one line per sample or both. See Returns below.\n\nNote that the fast `method='recursion'` option is only available for `kind='average'`. Plotting individual dependencies requires using the slower `method='brute'` option.\n\n.. versionadded:: 0.24 .. deprecated:: 0.24 `kind='legacy'` is deprecated and will be removed in version 0.26. `kind='average'` will be the new default. It is intended to migrate from the ndarray output to :class:`~sklearn.utils.Bunch` output.\n##### Returns\n* **predictions **: ndarray or\n\n* **values **: seq of 1d ndarrays\n    The values with which the grid has been created. The generated grid\n    is a cartesian product of the arrays in ``values``. ``len(values) ==\n    len(features)``. The size of each array ``values[j]`` is either\n    ``grid_resolution``, or the number of unique values in ``X[\n\n* **...                    grid_resolution=2) # doctest**: +SKIP\n\n* **sklearn.inspection.plot_partial_dependence**: Plot partial dependence\n\n"
},{
    "source file": "_optics.py",
    "line number": "902",
    "func name": "_extract_xi_labels",
    "func arg": "(ordering, clusters)",
    "comments": "Extracts the labels from the clusters returned by `_xi_cluster`. We rely on the fact that clusters are stored with the smaller clusters coming before the larger ones.\n\nParameters ---------- ordering : array-like of shape (n_samples,) The ordering of points calculated by OPTICS\n\nclusters : array-like of shape (n_clusters, 2) List of clusters i.e. (start, end) tuples, as returned by `_xi_cluster`.\n##### Returns\n* **labels **: ndarray of shape (n_samples,)\n\n"
},{
    "source file": "_openml.py",
    "line number": "690",
    "func name": "fetch_openml",
    "func arg": "(name)",
    "comments": "Fetch dataset from openml by name or dataset id.\n\nDatasets are uniquely identified by either an integer ID or by a combination of name and version (i.e. there might be multiple versions of the 'iris' dataset). Please give either name or data_id (not both). In case a name is given, a version can also be provided.\n\nRead more in the :ref:`User Guide <openml>`.\n\n.. versionadded:: 0.20\n\n.. note:: EXPERIMENTAL\n\nThe API is experimental (particularly the return value structure), and might have small backward-incompatible changes in future releases.\n\nParameters ---------- name : str, default=None String identifier of the dataset. Note that OpenML can have multiple datasets with the same name.\n\nversion : int or 'active', default='active' Version of the dataset. Can only be provided if also ``name`` is given. If 'active' the oldest version that's still active is used. Since there may be more than one active version of a dataset, and those versions may fundamentally be different from one another, setting an exact version is highly recommended.\n\ndata_id : int, default=None OpenML ID of the dataset. The most specific way of retrieving a dataset. If data_id is not given, name (and potential version) are used to obtain a dataset.\n\ndata_home : str, default=None Specify another download and cache folder for the data sets. By default all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\ntarget_column : str, list or None, default='default-target' Specify the column name in the data to use as target. If 'default-target', the standard target column a stored on the server is used. If ``None``, all columns are returned as data and the target is ``None``. If list (of strings), all columns with these names are returned as multi-target (Note: not all scikit-learn classifiers can handle all types of multi-output combinations)\n\ncache : bool, default=True Whether to cache downloaded datasets using joblib.\n\nreturn_X_y : bool, default=False If True, returns ``(data, target)`` instead of a Bunch object. See below for more information about the `data` and `target` objects.\n\nas_frame : bool or 'auto', default=False If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric, string or categorical). The target is a pandas DataFrame or Series depending on the number of target_columns. The Bunch will contain a ``frame`` attribute with the target and the data. If ``return_X_y`` is True, then ``(data, target)`` will be pandas DataFrames or Series as describe above. If as_frame is 'auto', the data and target will be converted to DataFrame or Series as if as_frame is set to True, unless the dataset is stored in sparse format.\n##### Returns\n* **data **: \n\n* **(data, target) **: tuple if ``return_X_y`` is True\n    .. note\n\n"
},{
    "source file": "_omp.py",
    "line number": "684",
    "func name": "_omp_path_residues",
    "func arg": "(X_train, y_train, X_test, y_test, copy, fit_intercept, normalize, max_iter)",
    "comments": "Compute the residues on left-out data for a full LARS path\n\nParameters ---------- X_train : array, shape (n_samples, n_features) The data to fit the LARS on\n\ny_train : array, shape (n_samples) The target variable to fit LARS on\n\nX_test : array, shape (n_samples, n_features) The data to compute the residues on\n\ny_test : array, shape (n_samples) The target variable to compute the residues on\n\ncopy : boolean, optional Whether X_train, X_test, y_train and y_test should be copied.\n\nIf False, they may be overwritten.\n\nfit_intercept : boolean whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).\n\nnormalize : boolean, optional, default True This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``.\n\nmax_iter : integer, optional Maximum numbers of iterations to perform, therefore maximum features to include. 100 by default.\n##### Returns\n* **residues **: array, shape (n_samples, max_features)\n    Residues of the prediction on the test data\n\n"
},{
    "source file": "_olivetti_faces.py",
    "line number": "40",
    "func name": "fetch_olivetti_faces",
    "func arg": "()",
    "comments": "Load the Olivetti faces data-set from AT&T (classification).\n\nDownload it if necessary.\n\n=================\n\n ===================== Classes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n40 Samples total\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 400 Dimensionality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 4096 Features\n\n\n\n\n\n\n\n\n\n\n\nreal, between 0 and 1 =================\n\n =====================\n\nRead more in the :ref:`User Guide <olivetti_faces_dataset>`.\n\nParameters ---------- data_home : str, default=None Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\nshuffle : bool, default=False If True the order of the dataset is shuffled to avoid having images of the same person grouped.\n\nrandom_state : int, RandomState instance or None, default=0 Determines random number generation for dataset shuffling. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.\n\ndownload_if_missing : bool, default=True If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site.\n\nreturn_X_y : bool, default=False If True, returns `(data, target)` instead of a `Bunch` object. See below for more information about the `data` and `target` object.\n\n.. versionadded:: 0.22\n##### Returns\n* **data **: \n\n* **(data, target) **: tuple if `return_X_y=True`\n    .. versionadded\n\n"
},{
    "source file": "_nmf.py",
    "line number": "845",
    "func name": "non_negative_factorization",
    "func arg": "(X, W, H, n_components)",
    "comments": "Compute Non-negative Matrix Factorization (NMF)\n\nFind two non-negative matrices (W, H) whose product approximates the non- negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction.\n\nThe objective function is::\n\n0.5 * ||X\n\n- WH||_Fro^2 + alpha * l1_ratio * ||vec(W)||_1 + alpha * l1_ratio * ||vec(H)||_1 + 0.5 * alpha * (1\n\n- l1_ratio) * ||W||_Fro^2 + 0.5 * alpha * (1\n\n- l1_ratio) * ||H||_Fro^2\n\nWhere::\n\n||A||_Fro^2 = \\sum_{i,j} A_{ij}^2 (Frobenius norm) ||vec(A)||_1 = \\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)\n\nFor multiplicative-update ('mu') solver, the Frobenius norm (0.5 * ||X\n\n- WH||_Fro^2) can be changed into another beta-divergence loss, by changing the beta_loss parameter.\n\nThe objective function is minimized with an alternating minimization of W and H. If H is given and update_H=False, it solves for W only.\n\nParameters ---------- X : array-like, shape (n_samples, n_features) Constant matrix.\n\nW : array-like, shape (n_samples, n_components) If init='custom', it is used as initial guess for the solution.\n\nH : array-like, shape (n_components, n_features) If init='custom', it is used as initial guess for the solution. If update_H=False, it is used as a constant, to solve for W only.\n\nn_components : integer Number of components, if n_components is not set all features are kept.\n\ninit : None | 'random' | 'nndsvd' | 'nndsvda' | 'nndsvdar' | 'custom' Method used to initialize the procedure. Default: None.\n\nValid options:\n\n- None: 'nndsvd' if n_components < n_features, otherwise 'random'.\n\n- 'random': non-negative random matrices, scaled with: sqrt(X.mean() / n_components)\n\n- 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD) initialization (better for sparseness)\n\n- 'nndsvda': NNDSVD with zeros filled with the average of X (better when sparsity is not desired)\n\n- 'nndsvdar': NNDSVD with zeros filled with small random values (generally faster, less accurate alternative to NNDSVDa for when sparsity is not desired)\n\n- 'custom': use custom matrices W and H if `update_H=True`. If `update_H=False`, then only custom matrix H is used.\n\n.. versionchanged:: 0.23 The default value of `init` changed from 'random' to None in 0.23.\n\nupdate_H : boolean, default: True Set to True, both W and H will be estimated from initial guesses. Set to False, only W will be estimated.\n\nsolver : 'cd' | 'mu' Numerical solver to use:\n\n- 'cd' is a Coordinate Descent solver that uses Fast Hierarchical Alternating Least Squares (Fast HALS).\n\n- 'mu' is a Multiplicative Update solver.\n\n.. versionadded:: 0.17 Coordinate Descent solver.\n\n.. versionadded:: 0.19 Multiplicative Update solver.\n\nbeta_loss : float or string, default 'frobenius' String must be in {'frobenius', 'kullback-leibler', 'itakura-saito'}. Beta divergence to be minimized, measuring the distance between X and the dot product WH. Note that values different from 'frobenius' (or 2) and 'kullback-leibler' (or 1) lead to significantly slower fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input matrix X cannot contain zeros. Used only in 'mu' solver.\n\n.. versionadded:: 0.19\n\ntol : float, default: 1e-4 Tolerance of the stopping condition.\n\nmax_iter : integer, default: 200 Maximum number of iterations before timing out.\n\nalpha : double, default: 0. Constant that multiplies the regularization terms.\n\nl1_ratio : double, default: 0. The regularization mixing parameter, with 0 <= l1_ratio <= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\nregularization : 'both' | 'components' | 'transformation' | None Select whether the regularization affects the components (H), the transformation (W), both or none of them.\n\nrandom_state : int, RandomState instance, default=None Used for NMF initialisation (when ``init`` == 'nndsvdar' or 'random'), and in Coordinate Descent. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.\n\nverbose : integer, default: 0 The verbosity level.\n\nshuffle : boolean, default: False If true, randomize the order of coordinates in the CD solver.\n##### Returns\n* **W **: array-like, shape (n_samples, n_components)\n    Solution to the non-negative least squares problem.\n\n* **H **: array-like, shape (n_components, n_features)\n    Solution to the non-negative least squares problem.\n\n* **n_iter **: int\n    Actual number of iterations.\n\n* **computer sciences 92.3**: 708-721, 2009.\n\n"
},{
    "source file": "_mutual_info.py",
    "line number": "375",
    "func name": "mutual_info_classif",
    "func arg": "(X, y)",
    "comments": "Estimate mutual information for a discrete target variable.\n\nMutual information (MI) [1]_ between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in [2]_ and [3]_. Both methods are based on the idea originally proposed in [4]_.\n\nIt can be used for univariate features selection, read more in the :ref:`User Guide <univariate_feature_selection>`.\n\nParameters ---------- X : array-like or sparse matrix, shape (n_samples, n_features) Feature matrix.\n\ny : array-like of shape (n_samples,) Target vector.\n\ndiscrete_features : {'auto', bool, array-like}, default='auto' If bool, then determines whether to consider all features discrete or continuous. If array, then it should be either a boolean mask with shape (n_features,) or array with indices of discrete features. If 'auto', it is assigned to False for dense `X` and to True for sparse `X`.\n\nn_neighbors : int, default=3 Number of neighbors to use for MI estimation for continuous variables, see [2]_ and [3]_. Higher values reduce variance of the estimation, but could introduce a bias.\n\ncopy : bool, default=True Whether to make a copy of the given data. If set to False, the initial data will be overwritten.\n\nrandom_state : int, RandomState instance or None, default=None Determines random number generation for adding small noise to continuous variables in order to remove repeated values. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.\n##### Returns\n* **mi **: ndarray, shape (n_features,)\n    Estimated mutual information between each feature and the target.\n\n* **.. [1] `Mutual Information\n       <https**: //en.wikipedia.org/wiki/Mutual_information>`_\n       on Wikipedia.\n\n* **.. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy\n       of a Random Vector**: , Probl. Peredachi Inf., 23\n\n"
},{
    "source file": "_multilayer_perceptron.py",
    "line number": "37",
    "func name": "_pack",
    "func arg": "(coefs_, intercepts_)",
    "comments": "Pack the parameters into a single vector.\n\n\n"
},{
    "source file": "_mean_shift.py",
    "line number": "192",
    "func name": "get_bin_seeds",
    "func arg": "(X, bin_size, min_bin_freq)",
    "comments": "Finds seeds for mean_shift.\n\nFinds seeds by first binning data onto a grid whose lines are spaced bin_size apart, and then choosing those bins with at least min_bin_freq points.\n\nParameters ----------\n\nX : array-like of shape (n_samples, n_features) Input points, the same points that will be used in mean_shift.\n\nbin_size : float Controls the coarseness of the binning. Smaller values lead to more seeding (which is computationally more expensive). If you're not sure how to set this, set it to the value of the bandwidth used in clustering.mean_shift.\n\nmin_bin_freq : int, default=1 Only bins with at least min_bin_freq will be selected as seeds. Raising this value decreases the number of seeds found, which makes mean_shift computationally cheaper.\n##### Returns\n* **bin_seeds **: array-like of shape (n_samples, n_features)\n    Points used as initial kernel positions in clustering.mean_shift.\n\n"
},{
    "source file": "_mds.py",
    "line number": "134",
    "func name": "smacof",
    "func arg": "(dissimilarities)",
    "comments": "Computes multidimensional scaling using the SMACOF algorithm.\n\nThe SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a multidimensional scaling algorithm which minimizes an objective function (the *stress*) using a majorization technique. Stress majorization, also known as the Guttman Transform, guarantees a monotone convergence of stress, and is more powerful than traditional techniques such as gradient descent.\n\nThe SMACOF algorithm for metric MDS can summarized by the following steps:\n\n1. Set an initial start configuration, randomly or not. 2. Compute the stress 3. Compute the Guttman Transform 4. Iterate 2 and 3 until convergence.\n\nThe nonmetric algorithm adds a monotonic regression step before computing the stress.\n\nParameters ---------- dissimilarities : ndarray of shape (n_samples, n_samples) Pairwise dissimilarities between the points. Must be symmetric.\n\nmetric : boolean, optional, default: True Compute metric or nonmetric SMACOF algorithm.\n\nn_components : int, default=2 Number of dimensions in which to immerse the dissimilarities. If an ``init`` array is provided, this option is overridden and the shape of ``init`` is used to determine the dimensionality of the embedding space.\n\ninit : ndarray of shape (n_samples, n_components), default=None Starting configuration of the embedding to initialize the algorithm. By default, the algorithm is initialized with a randomly chosen array.\n\nn_init : int, default=8 Number of times the SMACOF algorithm will be run with different initializations. The final results will be the best output of the runs, determined by the run with the smallest final stress. If ``init`` is provided, this option is overridden and a single run is performed.\n\nn_jobs : int, default=None The number of jobs to use for the computation. If multiple initializations are used (``n_init``), each run of the algorithm is computed in parallel.\n\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.\n\nmax_iter : int, default=300 Maximum number of iterations of the SMACOF algorithm for a single run.\n\nverbose : int, default=0 Level of verbosity.\n\neps : float, default=1e-3 Relative tolerance with respect to stress at which to declare convergence.\n\nrandom_state : int or RandomState instance, default=None Determines the random number generator used to initialize the centers. Pass an int for reproducible results across multiple function calls. See :term: `Glossary <random_state>`.\n\nreturn_n_iter : bool, default=False Whether or not to return the number of iterations.\n##### Returns\n* **X **: ndarray of shape (n_samples, n_components)\n    Coordinates of the points in a ``n_components``-space.\n\n* **stress **: float\n    The final value of the stress (sum of squared distance of the\n    disparities and the distances for all constrained points).\n\n* **n_iter **: int\n    The number of iterations corresponding to the best stress. Returned\n    only if ``return_n_iter`` is set to ``True``.\n\n* **\"Nonmetric multidimensional scaling**: a numerical method\" Kruskal, J.\n\n"
},{
    "source file": "_mask.py",
    "line number": "7",
    "func name": "_get_mask",
    "func arg": "(X, value_to_mask)",
    "comments": "Compute the boolean mask X == value_to_mask.\n\n\n"
},{
    "source file": "_logistic.py",
    "line number": "822",
    "func name": "_log_reg_scoring_path",
    "func arg": "(X, y, train, test, pos_class, Cs, scoring, fit_intercept, max_iter, tol, class_weight, verbose, solver, penalty, dual, intercept_scaling, multi_class, random_state, max_squared_sum, sample_weight, l1_ratio)",
    "comments": "Computes scores across logistic_regression_path\n\nParameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) Training data.\n\ny : array-like of shape (n_samples,) or (n_samples, n_targets) Target labels.\n\ntrain : list of indices The indices of the train set.\n\ntest : list of indices The indices of the test set.\n\npos_class : int, default=None The class with respect to which we perform a one-vs-all fit. If None, then it is assumed that the given problem is binary.\n\nCs : int or list of floats, default=10 Each of the values in Cs describes the inverse of regularization strength. If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4. If not provided, then a fixed set of values for Cs are used.\n\nscoring : callable, default=None A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. For a list of scoring functions that can be used, look at :mod:`sklearn.metrics`. The default scoring option used is accuracy_score.\n\nfit_intercept : bool, default=False If False, then the bias term is set to zero. Else the last term of each coef_ gives us the intercept.\n\nmax_iter : int, default=100 Maximum number of iterations for the solver.\n\ntol : float, default=1e-4 Tolerance for stopping criteria.\n\nclass_weight : dict or 'balanced', default=None Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.\n\nThe \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``\n\nNote that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.\n\nverbose : int, default=0 For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.\n\nsolver : {'lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'},\n\n\n\n\n\n\n\n\n\n\n\n default='lbfgs' Decides which solver to use.\n\npenalty : {'l1', 'l2', 'elasticnet'}, default='l2' Used to specify the norm used in the penalization. The 'newton-cg', 'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is only supported by the 'saga' solver.\n\ndual : bool, default=False Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features.\n\nintercept_scaling : float, default=1. Useful only when the solver 'liblinear' is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a \"synthetic\" feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.\n\nmulti_class : {'auto', 'ovr', 'multinomial'}, default='auto' If the option chosen is 'ovr', then a binary problem is fit for each label. For 'multinomial' the loss minimised is the multinomial loss fit across the entire probability distribution, *even when the data is binary*. 'multinomial' is unavailable when solver='liblinear'.\n\nrandom_state : int, RandomState instance, default=None Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the data. See :term:`Glossary <random_state>` for details.\n\nmax_squared_sum : float, default=None Maximum squared sum of X over samples. Used only in SAG solver. If None, it will be computed, going through all the samples. The value should be precomputed to speed up cross validation.\n\nsample_weight : array-like of shape(n_samples,), default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.\n\nl1_ratio : float, default=None The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination of L1 and L2.\n##### Returns\n* **coefs **: ndarray of shape (n_cs, n_features) or (n_cs, n_features + 1)\n    List of coefficients for the Logistic Regression model. If\n    fit_intercept is set to True then the second dimension will be\n    n_features + 1, where the last item represents the intercept.\n\n* **Cs **: ndarray\n    Grid of Cs used for cross-validation.\n\n* **scores **: ndarray of shape (n_cs,)\n    Scores obtained for each Cs.\n\n* **n_iter **: ndarray of shape(n_cs,)\n    Actual number of iteration for each Cs.\n\n"
},{
    "source file": "_locally_linear.py",
    "line number": "189",
    "func name": "locally_linear_embedding",
    "func arg": "(X)",
    "comments": "Perform a Locally Linear Embedding analysis on the data.\n\nRead more in the :ref:`User Guide <locally_linear_embedding>`.\n\nParameters ---------- X : {array-like, NearestNeighbors} Sample data, shape = (n_samples, n_features), in the form of a numpy array or a NearestNeighbors object.\n\nn_neighbors : int number of neighbors to consider for each point.\n\nn_components : int number of coordinates for the manifold.\n\nreg : float, default=1e-3 regularization constant, multiplies the trace of the local covariance matrix of the distances.\n\neigen_solver : {'auto', 'arpack', 'dense'}, default='auto' auto : algorithm will attempt to choose the best method for input data\n\narpack : use arnoldi iteration in shift-invert mode. For this method, M may be a dense matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable for some problems.\n\nIt is best to try several random seeds in order to check results.\n\ndense\n\n: use standard dense matrix operations for the eigenvalue decomposition.\n\nFor this method, M must be an array or matrix type.\n\nThis method should be avoided for large problems.\n\ntol : float, default=1e-6 Tolerance for 'arpack' method Not used if eigen_solver=='dense'.\n\nmax_iter : int, default=100 maximum number of iterations for the arpack solver.\n\nmethod : {'standard', 'hessian', 'modified', 'ltsa'}, default='standard' standard : use the standard locally linear embedding algorithm. see reference [1]_ hessian\n\n: use the Hessian eigenmap method.\n\nThis method requires n_neighbors > n_components * (1 + (n_components + 1) / 2. see reference [2]_ modified : use the modified locally linear embedding algorithm. see reference [3]_ ltsa\n\n\n\n : use local tangent space alignment algorithm see reference [4]_\n\nhessian_tol : float, default=1e-4 Tolerance for Hessian eigenmapping method. Only used if method == 'hessian'\n\nmodified_tol : float, default=1e-12 Tolerance for modified LLE method. Only used if method == 'modified'\n\nrandom_state : int, RandomState instance, default=None Determines the random number generator when ``solver`` == 'arpack'. Pass an int for reproducible results across multiple function calls. See :term: `Glossary <random_state>`.\n\nn_jobs : int or None, default=None The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.\n##### Returns\n* **Y **: array-like, shape [n_samples, n_components]\n    Embedding vectors.\n\n* **squared_error **: float\n    Reconstruction error for the embedding vectors. Equivalent to\n    ``norm(Y - W Y, 'fro')**2``, where W are the reconstruction weights.\n\n* **.. [1] Roweis, S. & Saul, L. Nonlinear dimensionality reduction\n    by locally linear embedding.  Science 290**: 2323 (2000).\n\n* **.. [2] Donoho, D. & Grimes, C. Hessian eigenmaps**: Locally\n    linear embedding techniques for high-dimensional data.\n    Proc Natl Acad Sci U S A.  100\n\n* **.. [3] Zhang, Z. & Wang, J. MLLE**: Modified Locally Linear\n    Embedding Using Multiple Weights.\n    http\n\n* **.. [4] Zhang, Z. & Zha, H. Principal manifolds and nonlinear\n    dimensionality reduction via tangent space alignment.\n    Journal of Shanghai Univ.  8**: 406 (2004)\n\n"
},{
    "source file": "_lobpcg.py",
    "line number": "143",
    "func name": "lobpcg",
    "func arg": "(A, X, B, M, Y, tol, maxiter, largest, verbosityLevel, retLambdaHistory, retResidualNormsHistory)",
    "comments": "Locally Optimal Block Preconditioned Conjugate Gradient Method (LOBPCG)\n\nLOBPCG is a preconditioned eigensolver for large symmetric positive definite (SPD) generalized eigenproblems.\n\nParameters ---------- A : {sparse matrix, dense matrix, LinearOperator} The symmetric linear operator of the problem, usually a sparse matrix.\n\nOften called the \"stiffness matrix\". X : ndarray, float32 or float64 Initial approximation to the ``k`` eigenvectors (non-sparse). If `A` has ``shape=(n,n)`` then `X` should have shape ``shape=(n,k)``. B : {dense matrix, sparse matrix, LinearOperator}, optional The right hand side operator in a generalized eigenproblem. By default, ``B = Identity``.\n\nOften called the \"mass matrix\". M : {dense matrix, sparse matrix, LinearOperator}, optional Preconditioner to `A`; by default ``M = Identity``. `M` should approximate the inverse of `A`. Y : ndarray, float32 or float64, optional n-by-sizeY matrix of constraints (non-sparse), sizeY < n The iterations will be performed in the B-orthogonal complement of the column-space of Y. Y must be full rank. tol : scalar, optional Solver tolerance (stopping criterion). The default is ``tol=n*sqrt(eps)``. maxiter : int, optional Maximum number of iterations.\n\nThe default is ``maxiter=min(n, 20)``. largest : bool, optional When True, solve for the largest eigenvalues, otherwise the smallest. verbosityLevel : int, optional Controls solver output.\n\nThe default is ``verbosityLevel=0``. retLambdaHistory : bool, optional Whether to return eigenvalue history.\n\nDefault is False. retResidualNormsHistory : bool, optional Whether to return history of residual norms.\n\nDefault is False.\n##### Returns\n* **w **: ndarray\n    Array of ``k`` eigenvalues\n\n* **v **: ndarray\n    An array of ``k`` eigenvectors.  `v` has the same shape as `X`.\n\n* **lambdas **: list of ndarray, optional\n    The eigenvalue history, if `retLambdaHistory` is True.\n\n* **rnorms **: list of ndarray, optional\n    The history of residual norms, if `retResidualNormsHistory` is True.\n\n* **https**: //arxiv.org/abs/0705.2626\n\n* **The convergence speed depends basically on two factors**: \n\n* **.. [1] A. V. Knyazev (2001),\n       Toward the Optimal Preconditioned Eigensolver**: Locally Optimal\n       Block Preconditioned Conjugate Gradient Method.\n       SIAM Journal on Scientific Computing 23, no. 2,\n       pp. 517-541. http\n\n* **.. [2] A. V. Knyazev, I. Lashuk, M. E. Argentati, and E. Ovchinnikov\n       (2007), Block Locally Optimal Preconditioned Eigenvalue Xolvers\n       (BLOPEX) in hypre and PETSc. https**: //arxiv.org/abs/0705.2626\n\n* **.. [3] A. V. Knyazev's C and MATLAB implementations**: https\n\n* **Constraints**: \n\n* **Preconditioner in the inverse of A in this example**: \n\n* **The preconditiner must be defined by a function**: \n\n* **>>> def precond( x )**: \n\n* **The preconditioner function is passed to lobpcg as a `LinearOperator`**: \n\n* **Let us now solve the eigenvalue problem for the matrix A**: \n\n"
},{
    "source file": "_lfw.py",
    "line number": "391",
    "func name": "fetch_lfw_pairs",
    "func arg": "()",
    "comments": "Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).\n\nDownload it if necessary.\n\n=================\n\n ======================= Classes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 2 Samples total\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 13233 Dimensionality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 5828 Features\n\n\n\n\n\n\n\n\n\n\n\nreal, between 0 and 255 =================\n\n =======================\n\nIn the official `README.txt`_ this task is described as the \"Restricted\" task.\n\nAs I am not sure as to implement the \"Unrestricted\" variant correctly, I left it as unsupported for now.\n\n.. _`README.txt`: http://vis-www.cs.umass.edu/lfw/README.txt\n\nThe original images are 250 x 250 pixels, but the default slice and resize arguments reduce them to 62 x 47.\n\nRead more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.\n\nParameters ---------- subset : {'train', 'test', '10_folds'}, default='train' Select the dataset to load: 'train' for the development training set, 'test' for the development test set, and '10_folds' for the official evaluation set that is meant to be used with a 10-folds cross validation.\n\ndata_home : str, default=None Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\nfunneled : bool, default=True Download and use the funneled variant of the dataset.\n\nresize : float, default=0.5 Ratio used to resize the each face picture.\n\ncolor : bool, default=False Keep the 3 RGB channels instead of averaging them to a single gray level channel. If color is True the shape of the data has one more dimension than the shape with color = False.\n\nslice_ : tuple of slice, default=(slice(70, 195), slice(78, 172)) Provide a custom 2D slice (height, width) to extract the 'interesting' part of the jpeg files and avoid use statistical correlation from the background\n\ndownload_if_missing : bool, default=True If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site.\n##### Returns\n* **data **: \n\n"
},{
    "source file": "_least_angle.py",
    "line number": "1145",
    "func name": "_lars_path_residues",
    "func arg": "(X_train, y_train, X_test, y_test, Gram, copy, method, verbose, fit_intercept, normalize, max_iter, eps, positive)",
    "comments": "Compute the residues on left-out data for a full LARS path\n\nParameters ----------- X_train : array-like of shape (n_samples, n_features) The data to fit the LARS on\n\ny_train : array-like of shape (n_samples,) The target variable to fit LARS on\n\nX_test : array-like of shape (n_samples, n_features) The data to compute the residues on\n\ny_test : array-like of shape (n_samples,) The target variable to compute the residues on\n\nGram : None, 'auto' or array-like of shape (n_features, n_features),\n\n\n\n\n\n\n\n\n\n\n\n default=None Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram matrix is precomputed from the given X, if there are more samples than features\n\ncopy : bool, default=True Whether X_train, X_test, y_train and y_test should be copied; if False, they may be overwritten.\n\nmethod : {'lar' , 'lasso'}, default='lar' Specifies the returned model. Select ``'lar'`` for Least Angle Regression, ``'lasso'`` for the Lasso.\n\nverbose : bool or int, default=False Sets the amount of verbosity\n\nfit_intercept : bool, default=True whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).\n\npositive : bool, default=False Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. See reservations for using this option in combination with method 'lasso' for expected small values of alpha in the doc of LassoLarsCV and LassoLarsIC.\n\nnormalize : bool, default=True This parameter is ignored when ``fit_intercept`` is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``.\n\nmax_iter : int, default=500 Maximum number of iterations to perform.\n\neps : float, optional The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. By default, ``np.finfo(float).eps`` is used\n##### Returns\n* **alphas **: array-like of shape (n_alphas,)\n    Maximum of covariances (in absolute value) at each iteration.\n    ``n_alphas`` is either ``max_iter`` or ``n_features``, whichever\n    is smaller.\n\n* **active **: list\n    Indices of active variables at the end of the path.\n\n* **coefs **: array-like of shape (n_features, n_alphas)\n    Coefficients along the path\n\n* **residues **: array-like of shape (n_alphas, n_samples)\n    Residues of the prediction on the test data\n\n"
},{
    "source file": "_lda.py",
    "line number": "31",
    "func name": "_update_doc_distribution",
    "func arg": "(X, exp_topic_word_distr, doc_topic_prior, max_iters, mean_change_tol, cal_sstats, random_state)",
    "comments": "E-step: update document-topic distribution.\n\nParameters ---------- X : array-like or sparse matrix, shape=(n_samples, n_features) Document word matrix.\n\nexp_topic_word_distr : dense matrix, shape=(n_topics, n_features) Exponential value of expectation of log topic word distribution. In the literature, this is `exp(E[log(beta)])`.\n\ndoc_topic_prior : float Prior of document topic distribution `theta`.\n\nmax_iters : int Max number of iterations for updating document topic distribution in the E-step.\n\nmean_change_tol : float Stopping tolerance for updating document topic distribution in E-setp.\n\ncal_sstats : boolean Parameter that indicate to calculate sufficient statistics or not. Set `cal_sstats` to `True` when we need to run M-step.\n\nrandom_state : RandomState instance or None Parameter that indicate how to initialize document topic distribution. Set `random_state` to None will initialize document topic distribution to a constant number.\n##### Returns\n* **(doc_topic_distr, suff_stats) **: `doc_topic_distr` is unnormalized topic distribution for each document.\n    In the literature, this is `gamma`. we can calculate `E[log(theta)]`\n    from it.\n    `suff_stats` is expected sufficient statistics for the M-step.\n        When `cal_sstats == False`, this will be None.\n\n"
},{
    "source file": "_label.py",
    "line number": "614",
    "func name": "_inverse_binarize_thresholding",
    "func arg": "(y, output_type, classes, threshold)",
    "comments": "Inverse label binarization transformation using thresholding.\n\n\n"
},{
    "source file": "_kmeans.py",
    "line number": "1370",
    "func name": "_mini_batch_convergence",
    "func arg": "(model, iteration_idx, n_iter, tol, n_samples, centers_squared_diff, batch_inertia, context, verbose)",
    "comments": "Helper function to encapsulate the early stopping logic\n\n\n"
},{
    "source file": "_kddcup99.py",
    "line number": "310",
    "func name": "_mkdirp",
    "func arg": "(d)",
    "comments": "Ensure directory d exists (like mkdir -p on Unix) No guarantee that the directory is writable.\n\n\n"
},{
    "source file": "_iforest.py",
    "line number": "466",
    "func name": "_average_path_length",
    "func arg": "(n_samples_leaf)",
    "comments": "The average path length in a n_samples iTree, which is equal to the average path length of an unsuccessful BST search since the latter has the same structure as an isolation tree. Parameters ---------- n_samples_leaf : array-like of shape (n_samples,) The number of training samples in each test sample leaf, for each estimators.\n\n\n##### Returns\n* **average_path_length **: ndarray of shape (n_samples,)\n\n"
},{
    "source file": "_huber.py",
    "line number": "17",
    "func name": "_huber_loss_and_gradient",
    "func arg": "(w, X, y, epsilon, alpha, sample_weight)",
    "comments": "Returns the Huber loss and the gradient.\n\nParameters ---------- w : ndarray, shape (n_features + 1,) or (n_features + 2,) Feature vector. w[:n_features] gives the coefficients w[-1] gives the scale factor and if the intercept is fit w[-2] gives the intercept factor.\n\nX : ndarray, shape (n_samples, n_features) Input data.\n\ny : ndarray, shape (n_samples,) Target vector.\n\nepsilon : float Robustness of the Huber estimator.\n\nalpha : float Regularization parameter.\n\nsample_weight : ndarray, shape (n_samples,), optional Weight assigned to each sample.\n##### Returns\n"
},{
    "source file": "_hash.py",
    "line number": "23",
    "func name": "_iteritems",
    "func arg": "(d)",
    "comments": "Like d.iteritems, but accepts any collections.Mapping.\n\n\n"
},{
    "source file": "_graph.py",
    "line number": "118",
    "func name": "radius_neighbors_graph",
    "func arg": "(X, radius)",
    "comments": "Computes the (weighted) graph of Neighbors for points in X\n\nNeighborhoods are restricted the points at a distance lower than radius.\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.\n\nParameters ---------- X : array-like of shape (n_samples, n_features) or BallTree Sample data, in the form of a numpy array or a precomputed :class:`BallTree`.\n\nradius : float Radius of neighborhoods.\n\nmode : {'connectivity', 'distance'}, default='connectivity' Type of returned matrix: 'connectivity' will return the connectivity matrix with ones and zeros, and 'distance' will return the distances between neighbors according to the given metric.\n\nmetric : str, default='minkowski' The distance metric used to calculate the neighbors within a given radius for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is 'euclidean' ('minkowski' metric with the param equal to 2.)\n\np : int, default=2 Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nmetric_params : dict, default=None additional keyword arguments for the metric function.\n\ninclude_self : bool or 'auto', default=False Whether or not to mark each sample as the first nearest neighbor to itself. If 'auto', then True is used for mode='connectivity' and False for mode='distance'.\n\nn_jobs : int, default=None The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.\n##### Returns\n* **A **: sparse matrix of shape (n_samples, n_samples)\n    Graph where A[i, j] is assigned the weight of edge that connects\n    i to j. The matrix is of CSR format.\n\n"
},{
    "source file": "_graph_lasso.py",
    "line number": "412",
    "func name": "graphical_lasso_path",
    "func arg": "(X, alphas, cov_init, X_test, mode, tol, enet_tol, max_iter, verbose)",
    "comments": "l1-penalized covariance estimator along a path of decreasing alphas\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\nParameters ---------- X : ndarray of shape (n_samples, n_features) Data from which to compute the covariance estimate.\n\nalphas : array-like of shape (n_alphas,) The list of regularization parameters, decreasing order.\n\ncov_init : array of shape (n_features, n_features), default=None The initial guess for the covariance.\n\nX_test : array of shape (n_test_samples, n_features), default=None Optional test matrix to measure generalisation error.\n\nmode : {'cd', 'lars'}, default='cd' The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p > n. Elsewhere prefer cd which is more numerically stable.\n\ntol : float, default=1e-4 The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped. The tolerance must be a positive number.\n\nenet_tol : float, default=1e-4 The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode='cd'. The tolerance must be a positive number.\n\nmax_iter : int, default=100 The maximum number of iterations. This parameter should be a strictly positive integer.\n\nverbose : int or bool, default=False The higher the verbosity flag, the more information is printed during the fitting.\n##### Returns\n* **covariances_ **: list of shape (n_alphas,) of ndarray of shape             (n_features, n_features)\n    The estimated covariance matrices.\n\n* **precisions_ **: list of shape (n_alphas,) of ndarray of shape             (n_features, n_features)\n    The estimated (sparse) precision matrices.\n\n* **scores_ **: list of shape (n_alphas,), dtype=float\n    The generalisation error (log-likelihood) on the test data.\n    Returned only if test data is passed.\n\n"
},{
    "source file": "_gaussian_mixture.py",
    "line number": "380",
    "func name": "_estimate_log_gaussian_prob",
    "func arg": "(X, means, precisions_chol, covariance_type)",
    "comments": "Estimate the log Gaussian probability.\n\nParameters ---------- X : array-like of shape (n_samples, n_features)\n\nmeans : array-like of shape (n_components, n_features)\n\nprecisions_chol : array-like Cholesky decompositions of the precision matrices. 'full' : shape of (n_components, n_features, n_features) 'tied' : shape of (n_features, n_features) 'diag' : shape of (n_components, n_features) 'spherical' : shape of (n_components,)\n\ncovariance_type : {'full', 'tied', 'diag', 'spherical'}\n##### Returns\n* **log_prob **: array, shape (n_samples, n_components)\n\n"
},{
    "source file": "_function_transformer.py",
    "line number": "8",
    "func name": "_identity",
    "func arg": "(X)",
    "comments": "The identity function.\n\n\n"
},{
    "source file": "_from_model.py",
    "line number": "17",
    "func name": "_calculate_threshold",
    "func arg": "(estimator, importances, threshold)",
    "comments": "Interpret the threshold value\n\n\n"
},{
    "source file": "_forest.py",
    "line number": "459",
    "func name": "_accumulate_prediction",
    "func arg": "(predict, X, out, lock)",
    "comments": "This is a utility function for joblib's Parallel.\n\nIt can't go locally in ForestClassifier or ForestRegressor, because joblib complains that it cannot pickle it when placed there.\n"
},{
    "source file": "_fastica.py",
    "line number": "151",
    "func name": "fastica",
    "func arg": "(X, n_components)",
    "comments": "Perform Fast Independent Component Analysis.\n\nRead more in the :ref:`User Guide <ICA>`.\n\nParameters ---------- X : array-like, shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features.\n\nn_components : int, optional Number of components to extract. If None no dimension reduction is performed.\n\nalgorithm : {'parallel', 'deflation'}, optional Apply a parallel or deflational FASTICA algorithm.\n\nwhiten : boolean, optional If True perform an initial whitening of the data. If False, the data is assumed to have already been preprocessed: it should be centered, normed and white. Otherwise you will get incorrect results. In this case the parameter n_components will be ignored.\n\nfun : string or function, optional. Default: 'logcosh' The functional form of the G function used in the approximation to neg-entropy. Could be either 'logcosh', 'exp', or 'cube'. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. The derivative should be averaged along its last dimension. Example:\n\ndef my_g(x): return x ** 3, np.mean(3 * x ** 2, axis=-1)\n\nfun_args : dictionary, optional Arguments to send to the functional form. If empty or None and if fun='logcosh', fun_args will take value {'alpha' : 1.0}\n\nmax_iter : int, optional Maximum number of iterations to perform.\n\ntol : float, optional A positive scalar giving the tolerance at which the un-mixing matrix is considered to have converged.\n\nw_init : (n_components, n_components) array, optional Initial un-mixing array of dimension (n.comp,n.comp). If None (default) then an array of normal r.v.'s is used.\n\nrandom_state : int, RandomState instance, default=None Used to initialize ``w_init`` when not specified, with a normal distribution. Pass an int, for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.\n\nreturn_X_mean : bool, optional If True, X_mean is returned too.\n\ncompute_sources : bool, optional If False, sources are not computed, but only the rotation matrix. This can save memory when working with big data. Defaults to True.\n\nreturn_n_iter : bool, optional Whether or not to return the number of iterations.\n##### Returns\n* **K **: array, shape (n_components, n_features) | None.\n    If whiten is 'True', K is the pre-whitening matrix that projects data\n    onto the first n_components principal components. If whiten is 'False',\n    K is 'None'.\n\n* **W **: array, shape (n_components, n_components)\n    The square matrix that unmixes the data after whitening.\n    The mixing matrix is the pseudo-inverse of matrix ``W K``\n    if K is not None, else it is the inverse of W.\n\n* **S **: array, shape (n_samples, n_components) | None\n    Estimated source matrix\n\n* **X_mean **: array, shape (n_features, )\n    The mean over features. Returned only if return_X_mean is True.\n\n* **n_iter **: int\n    If the algorithm is \"deflation\", n_iter is the\n    maximum number of iterations run across all components. Else\n    they are just the number of iterations taken to converge. This is\n    returned only when return_n_iter is set to `True`.\n\n* **Implemented using FastICA**: \n\n* ***A. Hyvarinen and E. Oja, Independent Component Analysis**: \n\n"
},{
    "source file": "_factor_analysis.py",
    "line number": "392",
    "func name": "_ortho_rotation",
    "func arg": "(components, method, tol, max_iter)",
    "comments": "Return rotated components.\n\n\n"
},{
    "source file": "_export.py",
    "line number": "819",
    "func name": "export_text",
    "func arg": "(decision_tree)",
    "comments": "Build a text report showing the rules of a decision tree.\n\nNote that backwards compatibility may not be supported.\n\nParameters ---------- decision_tree : object The decision tree estimator to be exported. It can be an instance of DecisionTreeClassifier or DecisionTreeRegressor.\n\nfeature_names : list of str, default=None A list of length n_features containing the feature names. If None generic names will be used (\"feature_0\", \"feature_1\", ...).\n\nmax_depth : int, default=10 Only the first max_depth levels of the tree are exported. Truncated branches will be marked with \"...\".\n\nspacing : int, default=3 Number of spaces between edges. The higher it is, the wider the result.\n\ndecimals : int, default=2 Number of decimal digits to display.\n\nshow_weights : bool, default=False If true the classification weights will be exported on each leaf. The classification weights are the number of samples each class.\n##### Returns\n* **report **: string\n    Text summary of all the rules in the decision tree.\n\n* **|   |--- class**: 0\n\n* **|   |   |--- class**: 2\n\n"
},{
    "source file": "_estimator_html_repr.py",
    "line number": "288",
    "func name": "estimator_html_repr",
    "func arg": "(estimator)",
    "comments": "Build a HTML representation of an estimator.\n\nRead more in the :ref:`User Guide <visualizing_composite_estimators>`.\n\nParameters ---------- estimator : estimator object The estimator to visualize.\n##### Returns\n* **html**: str\n    HTML representation of estimator.\n\n"
},{
    "source file": "_encode.py",
    "line number": "96",
    "func name": "_check_unknown",
    "func arg": "(values, known_values, return_mask)",
    "comments": "Helper function to check for unknowns in values to be encoded.\n\nUses pure python method for object dtype, and numpy method for all other dtypes.\n\nParameters ---------- values : array Values to check for unknowns. known_values : array Known values. Must be unique. return_mask : bool, default=False If True, return a mask of the same shape as `values` indicating the valid values.\n##### Returns\n* **diff **: list\n    The unique values present in `values` and not in `know_values`.\n\n* **valid_mask **: boolean array\n    Additionally returned if ``return_mask=True``.\n\n"
},{
    "source file": "_empirical_covariance.py",
    "line number": "52",
    "func name": "empirical_covariance",
    "func arg": "(X)",
    "comments": "Computes the Maximum likelihood covariance estimator\n\nParameters ---------- X : ndarray of shape (n_samples, n_features) Data from which to compute the covariance estimate\n\nassume_centered : bool, default=False If True, data will not be centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data will be centered before computation.\n##### Returns\n* **covariance **: ndarray of shape (n_features, n_features)\n    Empirical covariance (Maximum Likelihood Estimator).\n\n"
},{
    "source file": "_dict_vectorizer.py",
    "line number": "17",
    "func name": "_tosequence",
    "func arg": "(X)",
    "comments": "Turn X into a sequence or ndarray, avoiding a copy if possible.\n\n\n"
},{
    "source file": "_dict_learning.py",
    "line number": "624",
    "func name": "dict_learning_online",
    "func arg": "(X, n_components)",
    "comments": "Solves a dictionary learning matrix factorization problem online.\n\nFinds the best dictionary and the corresponding sparse code for approximating the data matrix X by solving::\n\n(U^*, V^*) = argmin 0.5 || X\n\n- U V ||_2^2 + alpha * || U ||_1 (U,V) with || V_k ||_2 = 1 for all\n\n0 <= k < n_components\n\nwhere V is the dictionary and U is the sparse code. This is accomplished by repeatedly iterating over mini-batches by slicing the input data.\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.\n\nParameters ---------- X : ndarray of shape (n_samples, n_features) Data matrix.\n\nn_components : int, default=2 Number of dictionary atoms to extract.\n\nalpha : float, default=1 Sparsity controlling parameter.\n\nn_iter : int, default=100 Number of mini-batch iterations to perform.\n\nreturn_code : bool, default = True Whether to also return the code U or just the dictionary V.\n\ndict_init : ndarray of shape (n_components, n_features), default = None Initial value for the dictionary for warm restart scenarios.\n\ncallback : callable, default=None callable that gets invoked every five iterations\n\nbatch_size : int, default=3 The number of samples to take in each batch.\n\nverbose : bool, default=False To control the verbosity of the procedure.\n\nshuffle : bool, default=True Whether to shuffle the data before splitting it in batches.\n\nn_jobs : int, default=None Number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.\n\nmethod : {'lars', 'cd'}, default='lars' lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse.\n\niter_offset : int, default=0 Number of previous iterations completed on the dictionary used for initialization.\n\nrandom_state : int or RandomState instance, default=None Used for initializing the dictionary when ``dict_init`` is not specified, randomly shuffling the data when ``shuffle`` is set to ``True``, and updating the dictionary. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.\n\nreturn_inner_stats : bool, default=False Return the inner statistics A (dictionary covariance) and B (data approximation). Useful to restart the algorithm in an online setting. If return_inner_stats is True, return_code is ignored\n\ninner_stats : tuple of (A, B) ndarrays, default=None Inner sufficient statistics that are kept by the algorithm. Passing them at initialization is useful in online settings, to avoid losing the history of the evolution. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix\n\nreturn_n_iter : bool, default=False Whether or not to return the number of iterations.\n\npositive_dict : bool, default=False Whether to enforce positivity when finding the dictionary.\n\n.. versionadded:: 0.20\n\npositive_code : bool, default=False Whether to enforce positivity when finding the code.\n\n.. versionadded:: 0.20\n\nmethod_max_iter : int, default=1000 Maximum number of iterations to perform when solving the lasso problem.\n\n.. versionadded:: 0.22\n##### Returns\n* **code **: ndarray of shape (n_samples, n_components),\n    the sparse code (only returned if `return_code=True`)\n\n* **dictionary **: ndarray of shape (n_components, n_features),\n    the solutions to the dictionary learning problem\n\n* **n_iter **: int\n    Number of iterations run. Returned only if `return_n_iter` is\n    set to `True`.\n\n"
},{
    "source file": "_dbscan.py",
    "line number": "24",
    "func name": "dbscan",
    "func arg": "(X, eps)",
    "comments": "Perform DBSCAN clustering from vector array or distance matrix.\n\nRead more in the :ref:`User Guide <dbscan>`.\n\nParameters ---------- X : {array-like, sparse (CSR) matrix} of shape (n_samples, n_features) or\n\n\n\n\n\n\n\n\n\n\n\n (n_samples, n_samples) A feature array, or array of distances between samples if ``metric='precomputed'``.\n\neps : float, default=0.5 The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is not a maximum bound on the distances of points within a cluster. This is the most important DBSCAN parameter to choose appropriately for your data set and distance function.\n\nmin_samples : int, default=5 The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself.\n\nmetric : str or callable, default='minkowski' The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by :func:`sklearn.metrics.pairwise_distances` for its metric parameter. If metric is \"precomputed\", X is assumed to be a distance matrix and must be square during fit. X may be a :term:`sparse graph <sparse graph>`, in which case only \"nonzero\" elements may be considered neighbors.\n\nmetric_params : dict, default=None Additional keyword arguments for the metric function.\n\n.. versionadded:: 0.19\n\nalgorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto' The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details.\n\nleaf_size : int, default=30 Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n\np : float, default=2 The power of the Minkowski metric to be used to calculate distance between points.\n\nsample_weight : array-like of shape (n_samples,), default=None Weight of each sample, such that a sample with a weight of at least ``min_samples`` is by itself a core sample; a sample with negative weight may inhibit its eps-neighbor from being core. Note that weights are absolute, and default to 1.\n\nn_jobs : int, default=None The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details. If precomputed distance are used, parallel execution is not available and thus n_jobs will have no effect.\n##### Returns\n* **core_samples **: ndarray of shape (n_core_samples,)\n    Indices of core samples.\n\n* **labels **: ndarray of shape (n_samples,)\n    Cluster labels for each point.  Noisy samples are given the label -1.\n\n* **For an example, see **: ref\n\n* ****: func\n\n* **In**: Proceedings of the 2nd International Conference on Knowledge Discovery\n\n* **DBSCAN revisited, revisited**: why and how you should (still) use DBSCAN.\n\n"
},{
    "source file": "_data.py",
    "line number": "3261",
    "func name": "power_transform",
    "func arg": "(X, method)",
    "comments": "Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired.\n\nCurrently, power_transform supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood.\n\nBox-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data.\n\nBy default, zero-mean, unit-variance normalization is applied to the transformed data.\n\nRead more in the :ref:`User Guide <preprocessing_transformer>`.\n\n Parameters ---------- X : array-like of shape (n_samples, n_features) The data to be transformed using a power transformation.\n\nmethod : {'yeo-johnson', 'box-cox'}, default='yeo-johnson' The power transform method. Available methods are:\n\n- 'yeo-johnson' [1]_, works with positive and negative values\n\n- 'box-cox' [2]_, only works with strictly positive values\n\n.. versionchanged:: 0.23 The default value of the `method` parameter changed from 'box-cox' to 'yeo-johnson' in 0.23.\n\nstandardize : bool, default=True Set to True to apply zero-mean, unit-variance normalization to the transformed output.\n\ncopy : bool, default=True Set to False to perform inplace computation during transformation.\n##### Returns\n* **X_trans **: ndarray of shape (n_samples, n_features)\n    The transformed data.\n\n* **.. warning**: \n\n* **PowerTransformer **: Equivalent transformation with the\n    ``Transformer`` API (e.g. as part of a preprocessing\n\n* **quantile_transform **: Maps data to a standard normal distribution with\n    the parameter `output_distribution='normal'`.\n\n* **NaNs are treated as missing values**: disregarded in ``fit``, and maintained\n\n* **see **: ref\n\n"
},{
    "source file": "_covtype.py",
    "line number": "63",
    "func name": "fetch_covtype",
    "func arg": "()",
    "comments": "Load the covertype dataset (classification).\n\nDownload it if necessary.\n\n=================\n\n ============ Classes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7 Samples total\n\n\n\n\n\n\n\n\n\n\n\n 581012 Dimensionality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n54 Features\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n int =================\n\n ============\n\nRead more in the :ref:`User Guide <covtype_dataset>`.\n\nParameters ---------- data_home : str, default=None Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\ndownload_if_missing : bool, default=True If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site.\n\nrandom_state : int or RandomState instance, default=None Determines random number generation for dataset shuffling. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.\n\nshuffle : bool, default=False Whether to shuffle dataset.\n\nreturn_X_y : bool, default=False If True, returns ``(data.data, data.target)`` instead of a Bunch object.\n\n.. versionadded:: 0.20\n\nas_frame : bool, default=False If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric). The target is a pandas DataFrame or Series depending on the number of target columns. If `return_X_y` is True, then (`data`, `target`) will be pandas DataFrames or Series as described below.\n\n.. versionadded:: 0.24\n##### Returns\n* **dataset **: \n\n* **(data, target) **: tuple if ``return_X_y`` is True\n    .. versionadded\n\n"
},{
    "source file": "_coordinate_descent.py",
    "line number": "1040",
    "func name": "_path_residuals",
    "func arg": "(X, y, train, test, path, path_params, alphas, l1_ratio, X_order, dtype)",
    "comments": "Returns the MSE for the models computed by 'path'\n\nParameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) Training data.\n\ny : array-like of shape (n_samples,) or (n_samples, n_targets) Target values\n\ntrain : list of indices The indices of the train set\n\ntest : list of indices The indices of the test set\n\npath : callable function returning a list of models on the path. See enet_path for an example of signature\n\npath_params : dictionary Parameters passed to the path function\n\nalphas : array-like, default=None Array of float that is used for cross-validation. If not provided, computed using 'path'.\n\nl1_ratio : float, default=1 float between 0 and 1 passed to ElasticNet (scaling between l1 and l2 penalties). For ``l1_ratio = 0`` the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty. For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2.\n\nX_order : {'F', 'C'}, default=None The order of the arrays expected by the path function to avoid memory copies\n\ndtype : a numpy dtype, default=None The dtype of the arrays expected by the path function to avoid memory copies\n"
},{
    "source file": "_config.py",
    "line number": "86",
    "func name": "config_context",
    "func arg": "(**new_config)",
    "comments": "Context manager for global scikit-learn configuration\n\nParameters ---------- assume_finite : bool, optional If True, validation for finiteness will be skipped, saving time, but leading to potential crashes. If False, validation for finiteness will be performed, avoiding error.\n\nGlobal default: False.\n\nworking_memory : int, optional If set, scikit-learn will attempt to limit the size of temporary arrays to this number of MiB (per job when parallelised), often saving both computation time and memory on expensive operations that can be performed in chunks. Global default: 1024.\n\nprint_changed_only : bool, optional If True, only the parameters that were set to non-default values will be printed when printing an estimator. For example, ``print(SVC())`` while True will only print 'SVC()', but would print 'SVC(C=1.0, cache_size=200, ...)' with all the non-changed parameters when False. Default is True.\n\n.. versionchanged:: 0.23 Default changed from False to True.\n\ndisplay : {'text', 'diagram'}, optional If 'diagram', estimators will be displayed as a diagram in a Jupyter lab or notebook context. If 'text', estimators will be displayed as text. Default is 'text'.\n\n.. versionadded:: 0.23\n\nNotes ----- All settings, not just those presently modified, will be returned to their previous values when the context manager is exited. This is not thread-safe.\n\nExamples -------- >>> import sklearn >>> from sklearn.utils.validation import assert_all_finite >>> with sklearn.config_context(assume_finite=True): ...\n\n\n\n assert_all_finite([float('nan')]) >>> with sklearn.config_context(assume_finite=True): ...\n\n\n\n with sklearn.config_context(assume_finite=False): ...\n\n\n\n\n\n\n\n assert_all_finite([float('nan')]) Traceback (most recent call last): ... ValueError: Input contains NaN, ...\n\nSee Also -------- set_config: Set global scikit-learn configuration get_config: Retrieve current values of the global configuration\n"
},{
    "source file": "_column_transformer.py",
    "line number": "778",
    "func name": "_is_negative_indexing",
    "func arg": "(key)",
    "comments": ""
},{
    "source file": "_classification1.py",
    "line number": "2373",
    "func name": "brier_score_loss",
    "func arg": "(y_true, y_prob)",
    "comments": "Compute the Brier score.\n\nThe smaller the Brier score, the better, hence the naming with \"loss\". Across all items in a set N predictions, the Brier score measures the mean squared difference between (1) the predicted probability assigned to the possible outcomes for item i, and (2) the actual outcome. Therefore, the lower the Brier score is for a set of predictions, the better the predictions are calibrated. Note that the Brier score always takes on a value between zero and one, since this is the largest possible difference between a predicted probability (which must be between zero and one) and the actual outcome (which can take on values of only 0 and 1). The Brier loss is composed of refinement loss and calibration loss. The Brier score is appropriate for binary and categorical outcomes that can be structured as true or false, but is inappropriate for ordinal variables which can take on three or more values (this is because the Brier score assumes that all possible outcomes are equivalently \"distant\" from one another). Which label is considered to be the positive label is controlled via the parameter pos_label, which defaults to 1. Read more in the :ref:`User Guide <calibration>`.\n\nParameters ---------- y_true : array, shape (n_samples,) True targets.\n\ny_prob : array, shape (n_samples,) Probabilities of the positive class.\n\nsample_weight : array-like of shape (n_samples,), default=None Sample weights.\n\npos_label : int or str, default=None Label of the positive class. Defaults to the greater label unless y_true is all 0 or all -1 in which case pos_label defaults to 1.\n##### Returns\n* **score **: float\n    Brier score\n\n* **.. [1] `Wikipedia entry for the Brier score.\n        <https**: //en.wikipedia.org/wiki/Brier_score>`_\n\n"
},{
    "source file": "_california_housing.py",
    "line number": "54",
    "func name": "fetch_california_housing",
    "func arg": "()",
    "comments": "Load the California housing dataset (regression).\n\n==============\n\n ============== Samples total\n\n\n\n\n\n\n\n\n\n\n\n 20640 Dimensionality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8 Features\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n real Target\n\n\n\n\n\n\n\n\n\n real 0.15\n\n- 5. ==============\n\n ==============\n\nRead more in the :ref:`User Guide <california_housing_dataset>`.\n\nParameters ---------- data_home : str, default=None Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\ndownload_if_missing : bool, default=True If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site.\n\n return_X_y : bool, default=False. If True, returns ``(data.data, data.target)`` instead of a Bunch object.\n\n.. versionadded:: 0.20\n\nas_frame : bool, default=False If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric, string or categorical). The target is a pandas DataFrame or Series depending on the number of target_columns.\n\n.. versionadded:: 0.23\n##### Returns\n* **dataset **: \n\n* **(data, target) **: tuple if ``return_X_y`` is True\n    .. versionadded\n\n"
},{
    "source file": "_bounds.py",
    "line number": "14",
    "func name": "l1_min_c",
    "func arg": "(X, y)",
    "comments": "Return the lowest bound for C such that for C in (l1_min_C, infinity) the model is guaranteed not to be empty. This applies to l1 penalized classifiers, such as LinearSVC with penalty='l1' and linear_model.LogisticRegression with penalty='l1'.\n\nThis value is valid if class_weight parameter in fit() is not set.\n\nParameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vector, where n_samples in the number of samples and n_features is the number of features.\n\ny : array-like of shape (n_samples,) Target vector relative to X.\n\nloss : {'squared_hinge', 'log'}, default='squared_hinge' Specifies the loss function. With 'squared_hinge' it is the squared hinge loss (a.k.a. L2 loss). With 'log' it is the loss of logistic regression models.\n\nfit_intercept : bool, default=True Specifies if the intercept should be fitted by the model. It must match the fit() method parameter.\n\nintercept_scaling : float, default=1.0 when fit_intercept is True, instance vector x becomes [x, intercept_scaling], i.e. a \"synthetic\" feature with constant value equals to intercept_scaling is appended to the instance vector. It must match the fit() method parameter.\n##### Returns\n* **l1_min_c **: float\n    minimum value for C\n\n"
},{
    "source file": "_birch.py",
    "line number": "40",
    "func name": "_split_node",
    "func arg": "(node, threshold, branching_factor)",
    "comments": "The node has to be split if there is no place for a new subcluster in the node. 1. Two empty nodes and two empty subclusters are initialized. 2. The pair of distant subclusters are found. 3. The properties of the empty subclusters and nodes are updated according to the nearest distance between the subclusters to the pair of distant subclusters. 4. The two nodes are set as children to the two subclusters.\n\n\n"
},{
    "source file": "_bicluster2.py",
    "line number": "49",
    "func name": "consensus_score",
    "func arg": "(a, b)",
    "comments": "The similarity of two sets of biclusters.\n\nSimilarity between individual biclusters is computed. Then the best matching between sets is found using the Hungarian algorithm. The final score is the sum of similarities divided by the size of the larger set.\n\nRead more in the :ref:`User Guide <biclustering>`.\n\nParameters ---------- a : (rows, columns) Tuple of row and column indicators for a set of biclusters.\n\nb : (rows, columns) Another set of biclusters like ``a``.\n\nsimilarity : 'jaccard' or callable, default='jaccard' May be the string \"jaccard\" to use the Jaccard coefficient, or any function that takes four arguments, each of which is a 1d indicator vector: (a_rows, a_columns, b_rows, b_columns).\n\nReferences ----------\n\n* Hochreiter, Bodenhofer, et. al., 2010. `FABIA: factor analysis for bicluster acquisition <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/>`__.\n"
},{
    "source file": "_bicluster1.py",
    "line number": "72",
    "func name": "_log_normalize",
    "func arg": "(X)",
    "comments": "Normalize ``X`` according to Kluger's log-interactions scheme.\n\n\n"
},{
    "source file": "_bayesian_mixture.py",
    "line number": "38",
    "func name": "_log_wishart_norm",
    "func arg": "(degrees_of_freedom, log_det_precisions_chol, n_features)",
    "comments": "Compute the log of the Wishart distribution normalization term.\n\nParameters ---------- degrees_of_freedom : array-like of shape (n_components,) The number of degrees of freedom on the covariance Wishart distributions.\n\nlog_det_precision_chol : array-like of shape (n_components,) The determinant of the precision matrix for each component.\n\nn_features : int The number of features.\n\nReturn ------ log_wishart_norm : array-like of shape (n_components,) The log normalization of the Wishart distribution.\n"
},{
    "source file": "_base9.py",
    "line number": "826",
    "func name": "_fit_liblinear",
    "func arg": "(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)",
    "comments": "Used by Logistic Regression (and CV) and LinearSVC/LinearSVR.\n\nPreprocessing is done in this function before supplying it to liblinear.\n\nParameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vector, where n_samples in the number of samples and n_features is the number of features.\n\ny : array-like of shape (n_samples,) Target vector relative to X\n\nC : float Inverse of cross-validation parameter. Lower the C, the more the penalization.\n\nfit_intercept : bool Whether or not to fit the intercept, that is to add a intercept term to the decision function.\n\nintercept_scaling : float LibLinear internally penalizes the intercept and this term is subject to regularization just like the other terms of the feature vector. In order to avoid this, one should increase the intercept_scaling. such that the feature vector becomes [x, intercept_scaling].\n\nclass_weight : dict or 'balanced', default=None Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n\nThe \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``\n\npenalty : {'l1', 'l2'} The norm of the penalty used in regularization.\n\ndual : bool Dual or primal formulation,\n\nverbose : int Set verbose to any positive number for verbosity.\n\nmax_iter : int Number of iterations.\n\ntol : float Stopping condition.\n\nrandom_state : int or RandomState instance, default=None Controls the pseudo random number generation for shuffling the data. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.\n\nmulti_class : {'ovr', 'crammer_singer'}, default='ovr' `ovr` trains n_classes one-vs-rest classifiers, while `crammer_singer` optimizes a joint objective over all classes. While `crammer_singer` is interesting from an theoretical perspective as it is consistent it is seldom used in practice and rarely leads to better accuracy and is more expensive to compute. If `crammer_singer` is chosen, the options loss, penalty and dual will be ignored.\n\nloss : {'logistic_regression', 'hinge', 'squared_hinge',\n\n\n\n\n\n\n\n\n\n\n\n 'epsilon_insensitive', 'squared_epsilon_insensitive},\n\n\n\n\n\n\n\n\n\n\n\n default='logistic_regression' The loss function used to fit the model.\n\nepsilon : float, default=0.1 Epsilon parameter in the epsilon-insensitive loss function. Note that the value of this parameter depends on the scale of the target variable y. If unsure, set epsilon=0.\n\nsample_weight : array-like of shape (n_samples,), default=None Weights assigned to each sample.\n##### Returns\n* **coef_ **: ndarray of shape (n_features, n_features + 1)\n    The coefficient vector got by minimizing the objective function.\n\n* **intercept_ **: float\n    The intercept term added to the vector.\n\n* **n_iter_ **: int\n    Maximum number of iterations run across all classes.\n\n"
},{
    "source file": "_base8.py",
    "line number": "201",
    "func name": "binary_log_loss",
    "func arg": "(y_true, y_prob)",
    "comments": "Compute binary logistic loss for classification.\n\nThis is identical to log_loss in binary classification case, but is kept for its use in multilabel case.\n\nParameters ---------- y_true : array-like or label indicator matrix Ground truth (correct) labels.\n\ny_prob : array-like of float, shape = (n_samples, 1) Predicted probabilities, as returned by a classifier's predict_proba method.\n##### Returns\n* **loss **: float\n    The degree to which the samples are correctly predicted.\n\n"
},{
    "source file": "_base7.py",
    "line number": "785",
    "func name": "_tree_query_radius_parallel_helper",
    "func arg": "(tree, **kwargs)",
    "comments": "Helper for the Parallel calls in RadiusNeighborsMixin.radius_neighbors\n\nThe Cython method tree.query_radius is not directly picklable by cloudpickle under PyPy.\n"
},{
    "source file": "_base6.py",
    "line number": "39",
    "func name": "_check_X",
    "func arg": "(X, n_components, n_features, ensure_min_samples)",
    "comments": "Check the input data X.\n\nParameters ---------- X : array-like of shape (n_samples, n_features)\n\nn_components : int\n##### Returns\n* **X **: array, shape (n_samples, n_features)\n\n"
},{
    "source file": "_base5.py",
    "line number": "556",
    "func name": "_pre_fit",
    "func arg": "(X, y, Xy, precompute, normalize, fit_intercept, copy, check_input, sample_weight)",
    "comments": "Aux function used at beginning of fit in linear models\n\nParameters ---------- order : 'F', 'C' or None, default=None Whether X and y will be forced to be fortran or c-style. Only relevant if sample_weight is not None.\n"
},{
    "source file": "_base4.py",
    "line number": "134",
    "func name": "_average_multiclass_ovo_score",
    "func arg": "(binary_metric, y_true, y_score, average)",
    "comments": "Average one-versus-one scores for multiclass classification.\n\nUses the binary metric for one-vs-one multiclass classification, where the score is computed according to the Hand & Till (2001) algorithm.\n\nParameters ---------- binary_metric : callable The binary metric function to use that accepts the following as input y_true_target : array, shape = [n_samples_target] Some sub-array of y_true for a pair of classes designated positive and negative in the one-vs-one scheme. y_score_target : array, shape = [n_samples_target] Scores corresponding to the probability estimates of a sample belonging to the designated positive class label\n\ny_true : array-like of shape (n_samples,) True multiclass labels.\n\ny_score : array-like of shape (n_samples, n_classes) Target scores corresponding to probability estimates of a sample belonging to a particular class\n\naverage : {'macro', 'weighted'}, default='macro' Determines the type of averaging performed on the pairwise binary metric scores ``'macro'``: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account. Classes are assumed to be uniformly distributed. ``'weighted'``: Calculate metrics for each label, taking into account the prevalence of the classes.\n##### Returns\n* **score **: float\n    Average of the pairwise binary metric scores\n\n"
},{
    "source file": "_base3.py",
    "line number": "175",
    "func name": "_partition_estimators",
    "func arg": "(n_estimators, n_jobs)",
    "comments": "Private function used to partition estimators between jobs.\n\n\n"
},{
    "source file": "_base1.py",
    "line number": "1157",
    "func name": "_fetch_remote",
    "func arg": "(remote, dirname)",
    "comments": "Helper function to download a remote dataset into path\n\nFetch a dataset pointed by remote's url, save into path using remote's filename and ensure its integrity based on the SHA256 Checksum of the downloaded file.\n\nParameters ---------- remote : RemoteFileMetadata Named tuple containing remote dataset meta information: url, filename and checksum\n\ndirname : string Directory to save the file to.\n##### Returns\n* **file_path**: string\n    Full path of the created file.\n\n"
},{
    "source file": "_base.py",
    "line number": "31",
    "func name": "_most_frequent",
    "func arg": "(array, extra_value, n_repeat)",
    "comments": "Compute the most frequent value in a 1d array extended with [extra_value] * n_repeat, where extra_value is assumed to be not part of the array.\n\n\n"
},{
    "source file": "_bagging.py",
    "line number": "179",
    "func name": "_parallel_predict_regression",
    "func arg": "(estimators, estimators_features, X)",
    "comments": "Private function used to compute predictions within a job.\n\n\n"
},{
    "source file": "_arff.py",
    "line number": "1117",
    "func name": "dumps",
    "func arg": "(obj)",
    "comments": "Serialize an object representing the ARFF document, returning a string.\n\nparam obj: a dictionary. :return: a string with the ARFF document.\n"
},{
    "source file": "_agglomerative.py",
    "line number": "631",
    "func name": "_hc_cut",
    "func arg": "(n_clusters, children, n_leaves)",
    "comments": "Function cutting the ward tree for a given number of clusters.\n\nParameters ---------- n_clusters : int or ndarray The number of clusters to form.\n\nchildren : ndarray of shape (n_nodes-1, 2) The children of each non-leaf node. Values less than `n_samples` correspond to leaves of the tree which are the original samples. A node `i` greater than or equal to `n_samples` is a non-leaf node and has children `children_[i\n\n- n_samples]`. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node `n_samples + i`\n\nn_leaves : int Number of leaves of the tree.\n##### Returns\n* **labels **: array [n_samples]\n    cluster labels for each point\n\n"
},{
    "source file": "_affinity_propagation.py",
    "line number": "34",
    "func name": "affinity_propagation",
    "func arg": "(S)",
    "comments": "Perform Affinity Propagation Clustering of data\n\nRead more in the :ref:`User Guide <affinity_propagation>`.\n\nParameters ----------\n\nS : array-like of shape (n_samples, n_samples) Matrix of similarities between points\n\npreference : array-like of shape (n_samples,) or float, default=None Preferences for each point\n\n- points with larger values of preferences are more likely to be chosen as exemplars. The number of exemplars, i.e. of clusters, is influenced by the input preferences value. If the preferences are not passed as arguments, they will be set to the median of the input similarities (resulting in a moderate number of clusters). For a smaller amount of clusters, this can be set to the minimum value of the similarities.\n\nconvergence_iter : int, default=15 Number of iterations with no change in the number of estimated clusters that stops the convergence.\n\nmax_iter : int, default=200 Maximum number of iterations\n\ndamping : float, default=0.5 Damping factor between 0.5 and 1.\n\ncopy : bool, default=True If copy is False, the affinity matrix is modified inplace by the algorithm, for memory efficiency\n\nverbose : bool, default=False The verbosity level\n\nreturn_n_iter : bool, default=False Whether or not to return the number of iterations.\n\nrandom_state : int or RandomState instance, default=0 Pseudo-random number generator to control the starting state. Use an int for reproducible results across function calls. See the :term:`Glossary <random_state>`.\n\n.. versionadded:: 0.23 this parameter was previously hardcoded as 0.\n##### Returns\n* **cluster_centers_indices **: ndarray of shape (n_clusters,)\n    index of clusters centers\n\n* **labels **: ndarray of shape (n_samples,)\n    cluster labels for each point\n\n* **n_iter **: int\n    number of iterations run. Returned only if `return_n_iter` is\n    set to True.\n\n* **For an example, see **: ref\n\n"
}]
}