{
    "source file": "gradcheck.py",
    "line number": "404",
    "func name": "gradgradcheck",
    "func arg": "(func, inputs, grad_outputs, eps, atol, rtol, gen_non_contig_grad_outputs, raise_exception, nondet_tol, check_undefined_grad)",
    "comments": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t. tensors in :attr:`inputs` and :attr:`grad_outputs` that are of floating point or complex type and with ``requires_grad=True``.\n\nThis function checks that backpropagating through the gradients computed to the given :attr:`grad_outputs` are correct.\n\nThe check between numerical and analytical gradients uses :func:`~torch.allclose`.\n\n.. note:: The default values are designed for :attr:`input` and :attr:`grad_outputs` of double precision. This check will likely fail if they are of less precision, e.g., ``FloatTensor``.\n\n.. warning:: If any checked tensor in :attr:`input` and :attr:`grad_outputs` has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from :func:`torch.expand`), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.\n##### Args\n* **func (function)**: a Python function that takes Tensor inputs and returns\n    a Tensor or a tuple of Tensors\n\n* **inputs (tuple of Tensor or Tensor)**: inputs to the function\n\n* **grad_outputs (tuple of Tensor or Tensor, optional)**: The gradients with\n    respect to the function's outputs.\n\n* **eps (float, optional)**: perturbation for finite differences\n\n* **atol (float, optional)**: absolute tolerance\n\n* **rtol (float, optional)**: relative tolerance\n\n* **gen_non_contig_grad_outputs (bool, optional)**: if\n\n* **raise_exception (bool, optional)**: indicating whether to raise an exception if\n    the check fails. The exception gives more information about the\n    exact nature of the failure. This is helpful when debugging gradchecks.\n\n* **nondet_tol (float, optional)**: tolerance for non-determinism. When running\n    identical inputs through the differentiation, the results must either match\n    exactly (default, 0.0) or be within this tolerance. Note that a small amount\n    of nondeterminism in the gradient will lead to larger inaccuracies in\n    the second derivative.\n\n* **check_undefined_grad (bool, options)**: if True, check if undefined output grads\n    are supported and treated as zeros\n\n##### Returns\n"
}