{
    "source file": "_shgo.py",
    "line number": "18",
    "func name": "shgo",
    "func arg": "(func, bounds, args, constraints, n, iters, callback, minimizer_kwargs, options, sampling_method)",
    "comments": "Finds the global minimum of a function using SHG optimization.\n\nSHGO stands for \"simplicial homology global optimization\".\n\nParameters ---------- func : callable The objective function to be minimized.\n\nMust be in the form ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array and ``args`` is a tuple of any additional fixed parameters needed to completely specify the function. bounds : sequence Bounds for variables.\n\n``(min, max)`` pairs for each element in ``x``, defining the lower and upper bounds for the optimizing argument of `func`. It is required to have ``len(bounds) == len(x)``. ``len(bounds)`` is used to determine the number of parameters in ``x``. Use ``None`` for one of min or max when there is no bound in that direction. By default bounds are ``(None, None)``. args : tuple, optional Any additional fixed parameters needed to completely specify the objective function. constraints : dict or sequence of dict, optional Constraints definition. Function(s) ``R**n`` in the form::\n\ng(x) >= 0 applied as g : R^n -> R^m h(x) == 0 applied as h : R^n -> R^p\n\nEach constraint is defined in a dictionary with fields:\n\ntype : str Constraint type: 'eq' for equality, 'ineq' for inequality. fun : callable The function defining the constraint. jac : callable, optional The Jacobian of `fun` (only for SLSQP). args : sequence, optional Extra arguments to be passed to the function and Jacobian.\n\nEquality constraint means that the constraint function result is to be zero whereas inequality means that it is to be non-negative. Note that COBYLA only supports inequality constraints.\n\n.. note::\n\nOnly the COBYLA and SLSQP local minimize methods currently support constraint arguments. If the ``constraints`` sequence used in the local optimization problem is not defined in ``minimizer_kwargs`` and a constrained method is used then the global ``constraints`` will be used. (Defining a ``constraints`` sequence in ``minimizer_kwargs`` means that ``constraints`` will not be added so if equality constraints and so forth need to be added then the inequality functions in ``constraints`` need to be added to ``minimizer_kwargs`` too).\n\nn : int, optional Number of sampling points used in the construction of the simplicial complex. Note that this argument is only used for ``sobol`` and other arbitrary `sampling_methods`. iters : int, optional Number of iterations used in the construction of the simplicial complex. callback : callable, optional Called after each iteration, as ``callback(xk)``, where ``xk`` is the current parameter vector. minimizer_kwargs : dict, optional Extra keyword arguments to be passed to the minimizer ``scipy.optimize.minimize`` Some important options could be:\n\n* method : str The minimization method (e.g. ``SLSQP``). * args : tuple Extra arguments passed to the objective function (``func``) and its derivatives (Jacobian, Hessian). * options : dict, optional Note that by default the tolerance is specified as ``{ftol: 1e-12}``\n\noptions : dict, optional A dictionary of solver options. Many of the options specified for the global routine are also passed to the scipy.optimize.minimize routine. The options that are also passed to the local routine are marked with \"(L)\".\n\nStopping criteria, the algorithm will terminate if any of the specified criteria are met. However, the default algorithm does not require any to be specified:\n\n* maxfev : int (L) Maximum number of function evaluations in the feasible domain. (Note only methods that support this option will terminate the routine at precisely exact specified value. Otherwise the criterion will only terminate during a global iteration) * f_min Specify the minimum objective function value, if it is known. * f_tol : float Precision goal for the value of f in the stopping criterion. Note that the global routine will also terminate if a sampling point in the global routine is within this tolerance. * maxiter : int Maximum number of iterations to perform. * maxev : int Maximum number of sampling evaluations to perform (includes searching in infeasible points). * maxtime : float Maximum processing runtime allowed * minhgrd : int Minimum homology group rank differential. The homology group of the objective function is calculated (approximately) during every iteration. The rank of this group has a one-to-one correspondence with the number of locally convex subdomains in the objective function (after adequate sampling points each of these subdomains contain a unique global minimum). If the difference in the hgr is 0 between iterations for ``maxhgrd`` specified iterations the algorithm will terminate.\n\nObjective function knowledge:\n\n* symmetry : bool Specify True if the objective function contains symmetric variables. The search space (and therefore performance) is decreased by O(n!).\n\n* jac : bool or callable, optional Jacobian (gradient) of objective function. Only for CG, BFGS, Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg. If ``jac`` is a boolean and is True, ``fun`` is assumed to return the gradient along with the objective function. If False, the gradient will be estimated numerically. ``jac`` can also be a callable returning the gradient of the objective. In this case, it must accept the same arguments as ``fun``. (Passed to `scipy.optimize.minmize` automatically)\n\n* hess, hessp : callable, optional Hessian (matrix of second-order derivatives) of objective function or Hessian of objective function times an arbitrary vector p. Only for Newton-CG, dogleg, trust-ncg. Only one of ``hessp`` or ``hess`` needs to be given. If ``hess`` is provided, then ``hessp`` will be ignored. If neither ``hess`` nor ``hessp`` is provided, then the Hessian product will be approximated using finite differences on ``jac``. ``hessp`` must compute the Hessian times an arbitrary vector. (Passed to `scipy.optimize.minmize` automatically)\n\nAlgorithm settings:\n\n* minimize_every_iter : bool If True then promising global sampling points will be passed to a local minimization routine every iteration. If False then only the final minimizer pool will be run. Defaults to False. * local_iter : int Only evaluate a few of the best minimizer pool candidates every iteration. If False all potential points are passed to the local minimization routine. * infty_constraints: bool If True then any sampling points generated which are outside will the feasible domain will be saved and given an objective function value of ``inf``. If False then these points will be discarded. Using this functionality could lead to higher performance with respect to function evaluations before the global minimum is found, specifying False will use less memory at the cost of a slight decrease in performance. Defaults to True.\n\nFeedback:\n\n* disp : bool (L) Set to True to print convergence messages.\n\nsampling_method : str or function, optional Current built in sampling method options are ``sobol`` and ``simplicial``. The default ``simplicial`` uses less memory and provides the theoretical guarantee of convergence to the global minimum in finite time. The ``sobol`` method is faster in terms of sampling point generation at the cost of higher memory resources and the loss of guaranteed convergence. It is more appropriate for most \"easier\" problems where the convergence is relatively fast. User defined sampling functions must accept two arguments of ``n`` sampling points of dimension ``dim`` per call and output an array of sampling points with shape `n x dim`.\n##### Returns\n* **res **: OptimizeResult\n    The optimization result represented as a `OptimizeResult` object.\n    Important attributes are\n\n* **In general, the optimization problems are of the form**: \n\n* **described at https**: //web.maths.unsw.edu.au/~fkuo/sobol/ translated to\n\n* **.. [4] Hoch, W and Schittkowski, K (1981) \"Test examples for nonlinear\n       programming codes\", Lecture Notes in Economics and Mathematical\n       Systems, 187. Springer-Verlag, New York.\n       http**: //www.ai7.uni-bayreuth.de/test_problem_coll.pdf\n\n* **.. [5] Wales, DJ (2015) \"Perspective**: Insight into reaction coordinates and\n       dynamics from the potential energy landscape\",\n       Journal of Chemical Physics, 142(13), 2015.\n\n* **First consider the problem of minimizing the Rosenbrock function, `rosen`**: \n\n* **(https**: //en.wikipedia.org/wiki/Test_functions_for_optimization)\n\n* **>>> def eggholder(x)**: \n\n* **input 30 initial sampling points of the Sobol sequence**: \n\n* **can be called using**: \n\n* **following example from Hock and Schittkowski problem 73 (cattle-feed) [4]_**: \n\n* **The approximate answer given in [4]_ is**: \n\n* **>>> def f(x)**: # (cattle-feed)\n\n* **>>> def g1(x)**: \n\n* **>>> def g2(x)**: \n\n* **>>> def h1(x)**: \n\n* **>>> cons = ({'type'**: 'ineq', 'fun'\n\n* **...         {'type'**: 'eq', 'fun'\n\n* **>>> res\n     fun**: 29.894378159142136\n    funl\n\n"
}