{
    "source file": "least_squares.py",
    "line number": "240",
    "func name": "least_squares",
    "func arg": "(fun, x0, jac, bounds, method, ftol, xtol, gtol, x_scale, loss, f_scale, diff_step, tr_solver, tr_options, jac_sparsity, max_nfev, verbose, args, kwargs)",
    "comments": "Solve a nonlinear least-squares problem with bounds on the variables.\n\nGiven the residuals f(x) (an m-D real function of n real variables) and the loss function rho(s) (a scalar function), `least_squares` finds a local minimum of the cost function F(x)::\n\nminimize F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m\n\n- 1) subject to lb <= x <= ub\n\nThe purpose of the loss function rho(s) is to reduce the influence of outliers on the solution.\n\nParameters ---------- fun : callable Function which computes the vector of residuals, with the signature ``fun(x, *args, **kwargs)``, i.e., the minimization proceeds with respect to its first argument. The argument ``x`` passed to this function is an ndarray of shape (n,) (never a scalar, even for n=1). It must allocate and return a 1-D array_like of shape (m,) or a scalar. If the argument ``x`` is complex or the function ``fun`` returns complex residuals, it must be wrapped in a real function of real arguments, as shown at the end of the Examples section. x0 : array_like with shape (n,) or float Initial guess on independent variables. If float, it will be treated as a 1-D array with one element. jac : {'2-point', '3-point', 'cs', callable}, optional Method of computing the Jacobian matrix (an m-by-n matrix, where element (i, j) is the partial derivative of f[i] with respect to x[j]). The keywords select a finite difference scheme for numerical estimation. The scheme '3-point' is more accurate, but requires twice as many operations as '2-point' (default). The scheme 'cs' uses complex steps, and while potentially the most accurate, it is applicable only when `fun` correctly handles complex inputs and can be analytically continued to the complex plane. Method 'lm' always uses the '2-point' scheme. If callable, it is used as ``jac(x, *args, **kwargs)`` and should return a good approximation (or the exact value) for the Jacobian as an array_like (np.atleast_2d is applied), a sparse matrix (csr_matrix preferred for performance) or a `scipy.sparse.linalg.LinearOperator`. bounds : 2-tuple of array_like, optional Lower and upper bounds on independent variables. Defaults to no bounds. Each array must match the size of `x0` or be a scalar, in the latter case a bound will be the same for all variables. Use ``np.inf`` with an appropriate sign to disable bounds on all or some variables. method : {'trf', 'dogbox', 'lm'}, optional Algorithm to perform minimization.\n\n* 'trf' : Trust Region Reflective algorithm, particularly suitable for large sparse problems with bounds. Generally robust method. * 'dogbox' : dogleg algorithm with rectangular trust regions, typical use case is small problems with bounds. Not recommended for problems with rank-deficient Jacobian. * 'lm' : Levenberg-Marquardt algorithm as implemented in MINPACK. Doesn't handle bounds and sparse Jacobians. Usually the most efficient method for small unconstrained problems.\n\nDefault is 'trf'. See Notes for more information. ftol : float or None, optional Tolerance for termination by the change of the cost function. Default is 1e-8. The optimization process is stopped when ``dF < ftol * F``, and there was an adequate agreement between a local quadratic model and the true model in the last step. If None, the termination by this condition is disabled. xtol : float or None, optional Tolerance for termination by the change of the independent variables. Default is 1e-8. The exact condition depends on the `method` used:\n\n* For 'trf' and 'dogbox' : ``norm(dx) < xtol * (xtol + norm(x))``. * For 'lm' : ``Delta < xtol * norm(xs)``, where ``Delta`` is a trust-region radius and ``xs`` is the value of ``x`` scaled according to `x_scale` parameter (see below).\n\nIf None, the termination by this condition is disabled. gtol : float or None, optional Tolerance for termination by the norm of the gradient. Default is 1e-8. The exact condition depends on a `method` used:\n\n* For 'trf' : ``norm(g_scaled, ord=np.inf) < gtol``, where ``g_scaled`` is the value of the gradient scaled to account for the presence of the bounds [STIR]_. * For 'dogbox' : ``norm(g_free, ord=np.inf) < gtol``, where ``g_free`` is the gradient with respect to the variables which are not in the optimal state on the boundary. * For 'lm' : the maximum absolute value of the cosine of angles between columns of the Jacobian and the residual vector is less than `gtol`, or the residual vector is zero.\n\nIf None, the termination by this condition is disabled. x_scale : array_like or 'jac', optional Characteristic scale of each variable. Setting `x_scale` is equivalent to reformulating the problem in scaled variables ``xs = x / x_scale``. An alternative view is that the size of a trust region along jth dimension is proportional to ``x_scale[j]``. Improved convergence may be achieved by setting `x_scale` such that a step of a given size along any of the scaled variables has a similar effect on the cost function. If set to 'jac', the scale is iteratively updated using the inverse norms of the columns of the Jacobian matrix (as described in [JJMore]_). loss : str or callable, optional Determines the loss function. The following keyword values are allowed:\n\n* 'linear' (default) : ``rho(z) = z``. Gives a standard least-squares problem. * 'soft_l1' : ``rho(z) = 2 * ((1 + z)**0.5\n\n- 1)``. The smooth approximation of l1 (absolute value) loss. Usually a good choice for robust least squares. * 'huber' : ``rho(z) = z if z <= 1 else 2*z**0.5\n\n- 1``. Works similarly to 'soft_l1'. * 'cauchy' : ``rho(z) = ln(1 + z)``. Severely weakens outliers influence, but may cause difficulties in optimization process. * 'arctan' : ``rho(z) = arctan(z)``. Limits a maximum loss on a single residual, has properties similar to 'cauchy'.\n\nIf callable, it must take a 1-D ndarray ``z=f**2`` and return an array_like with shape (3, m) where row 0 contains function values, row 1 contains first derivatives and row 2 contains second derivatives. Method 'lm' supports only 'linear' loss. f_scale : float, optional Value of soft margin between inlier and outlier residuals, default is 1.0. The loss function is evaluated as follows ``rho_(f**2) = C**2 * rho(f**2 / C**2)``, where ``C`` is `f_scale`, and ``rho`` is determined by `loss` parameter. This parameter has no effect with ``loss='linear'``, but for other `loss` values it is of crucial importance. max_nfev : None or int, optional Maximum number of function evaluations before the termination. If None (default), the value is chosen automatically:\n\n* For 'trf' and 'dogbox' : 100 * n. * For 'lm' :\n\n100 * n if `jac` is callable and 100 * n * (n + 1) otherwise (because 'lm' counts function calls in Jacobian estimation).\n\ndiff_step : None or array_like, optional Determines the relative step size for the finite difference approximation of the Jacobian. The actual step is computed as ``x * diff_step``. If None (default), then `diff_step` is taken to be a conventional \"optimal\" power of machine epsilon for the finite difference scheme used [NR]_. tr_solver : {None, 'exact', 'lsmr'}, optional Method for solving trust-region subproblems, relevant only for 'trf' and 'dogbox' methods.\n\n* 'exact' is suitable for not very large problems with dense Jacobian matrices. The computational complexity per iteration is comparable to a singular value decomposition of the Jacobian matrix. * 'lsmr' is suitable for problems with sparse and large Jacobian matrices. It uses the iterative procedure `scipy.sparse.linalg.lsmr` for finding a solution of a linear least-squares problem and only requires matrix-vector product evaluations.\n\nIf None (default), the solver is chosen based on the type of Jacobian returned on the first iteration. tr_options : dict, optional Keyword options passed to trust-region solver.\n\n* ``tr_solver='exact'``: `tr_options` are ignored. * ``tr_solver='lsmr'``: options for `scipy.sparse.linalg.lsmr`. Additionally,\n\n``method='trf'`` supports\n\n'regularize' option (bool, default is True), which adds a regularization term to the normal equation, which improves convergence if the Jacobian is rank-deficient [Byrd]_ (eq. 3.4).\n\njac_sparsity : {None, array_like, sparse matrix}, optional Defines the sparsity structure of the Jacobian matrix for finite difference estimation, its shape must be (m, n). If the Jacobian has only few non-zero elements in *each* row, providing the sparsity structure will greatly speed up the computations [Curtis]_. A zero entry means that a corresponding element in the Jacobian is identically zero. If provided, forces the use of 'lsmr' trust-region solver. If None (default), then dense differencing will be used. Has no effect for 'lm' method. verbose : {0, 1, 2}, optional Level of algorithm's verbosity:\n\n* 0 (default) : work silently. * 1 : display a termination report. * 2 : display progress during iterations (not supported by 'lm' method).\n\nargs, kwargs : tuple and dict, optional Additional arguments passed to `fun` and `jac`. Both empty by default. The calling signature is ``fun(x, *args, **kwargs)`` and the same for `jac`.\n##### Returns\n* **`OptimizeResult` with the following fields defined**: \n\n* **x **: ndarray, shape (n,)\n    Solution found.\n\n* **cost **: float\n    Value of the cost function at the solution.\n\n* **fun **: ndarray, shape (m,)\n    Vector of residuals at the solution.\n\n* **jac **: ndarray, sparse matrix or LinearOperator, shape (m, n)\n    Modified Jacobian matrix at the solution, in the sense that J^T J\n    is a Gauss-Newton approximation of the Hessian of the cost function.\n    The type is the same as the one used by the algorithm.\n\n* **grad **: ndarray, shape (m,)\n    Gradient of the cost function at the solution.\n\n* **optimality **: float\n    First-order optimality measure. In unconstrained problems, it is always\n    the uniform norm of the gradient. In constrained problems, it is the\n    quantity which was compared with `gtol` during iterations.\n\n* **active_mask **: ndarray of int, shape (n,)\n    Each component shows whether a corresponding constraint is active\n    (that is, whether a variable is at the bound)\n\n* **nfev **: int\n    Number of function evaluations done. Methods 'trf' and 'dogbox' do not\n    count function calls for numerical Jacobian approximation, as opposed\n    to 'lm' method.\n\n* **njev **: int or None\n    Number of Jacobian evaluations done. If numerical Jacobian\n    approximation is used in 'lm' method, it is set to None.\n\n* **status **: int\n    The reason for algorithm termination\n\n* **message **: str\n    Verbal description of the termination reason.\n\n* **success **: bool\n    True if one of the convergence criteria is satisfied (`status` > 0).\n\n* **leastsq **: A legacy wrapper for the MINPACK implementation of the\n          Levenberg-Marquadt algorithm.\n\n* **curve_fit **: Least-squares minimization applied to a curve-fitting problem.\n\n* **.. versionadded**: \n\n* **.. [JJMore] J. J. More, \"The Levenberg-Marquardt Algorithm**: Implementation\n            and Theory,\" Numerical Analysis, ed. G. A. Watson, Lecture\n            Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.\n\n* **.. [BA] B. Triggs et. al., \"Bundle Adjustment - A Modern Synthesis\",\n        Proceedings of the International Workshop on Vision Algorithms**: Theory and Practice, pp. 298-372, 1999.\n\n* **>>> def fun_rosenbrock(x)**: \n\n* **We also provide the analytic Jacobian**: \n\n* **>>> def jac_rosenbrock(x)**: \n\n* **Putting this all together, we see that the new solution lies on the bound**: \n\n* **variables**: \n\n* **>>> def fun_broyden(x)**: \n\n* **...     f[1**: ] -= x[\n\n* **...     f[**: -1] -= 2 * x[1\n\n* **>>> def sparsity_broyden(n)**: \n\n* **outliers, define the model parameters, and generate data**: \n\n* **>>> def gen_data(t, a, b, c, noise=0, n_outliers=0, random_state=0)**: \n\n* **>>> def fun(x, t, y)**: \n\n* **Compute a standard least-squares solution**: \n\n* **following function**: \n\n* **>>> def f(z)**: \n\n* **by simply handling the real and imaginary parts as independent variables**: \n\n* **>>> def f_wrap(x)**: \n\n* **variables we optimize a 2m-D real function of 2n real variables**: \n\n"
}