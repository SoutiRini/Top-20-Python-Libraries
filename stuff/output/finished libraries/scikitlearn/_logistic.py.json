{
    "source file": "_logistic.py",
    "line number": "822",
    "func name": "_log_reg_scoring_path",
    "func arg": "(X, y, train, test, pos_class, Cs, scoring, fit_intercept, max_iter, tol, class_weight, verbose, solver, penalty, dual, intercept_scaling, multi_class, random_state, max_squared_sum, sample_weight, l1_ratio)",
    "comments": "Computes scores across logistic_regression_path\n\nParameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) Training data.\n\ny : array-like of shape (n_samples,) or (n_samples, n_targets) Target labels.\n\ntrain : list of indices The indices of the train set.\n\ntest : list of indices The indices of the test set.\n\npos_class : int, default=None The class with respect to which we perform a one-vs-all fit. If None, then it is assumed that the given problem is binary.\n\nCs : int or list of floats, default=10 Each of the values in Cs describes the inverse of regularization strength. If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4. If not provided, then a fixed set of values for Cs are used.\n\nscoring : callable, default=None A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. For a list of scoring functions that can be used, look at :mod:`sklearn.metrics`. The default scoring option used is accuracy_score.\n\nfit_intercept : bool, default=False If False, then the bias term is set to zero. Else the last term of each coef_ gives us the intercept.\n\nmax_iter : int, default=100 Maximum number of iterations for the solver.\n\ntol : float, default=1e-4 Tolerance for stopping criteria.\n\nclass_weight : dict or 'balanced', default=None Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.\n\nThe \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``\n\nNote that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.\n\nverbose : int, default=0 For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.\n\nsolver : {'lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'},\n\n\n\n\n\n\n\n\n\n\n\n default='lbfgs' Decides which solver to use.\n\npenalty : {'l1', 'l2', 'elasticnet'}, default='l2' Used to specify the norm used in the penalization. The 'newton-cg', 'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is only supported by the 'saga' solver.\n\ndual : bool, default=False Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features.\n\nintercept_scaling : float, default=1. Useful only when the solver 'liblinear' is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a \"synthetic\" feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.\n\nmulti_class : {'auto', 'ovr', 'multinomial'}, default='auto' If the option chosen is 'ovr', then a binary problem is fit for each label. For 'multinomial' the loss minimised is the multinomial loss fit across the entire probability distribution, *even when the data is binary*. 'multinomial' is unavailable when solver='liblinear'.\n\nrandom_state : int, RandomState instance, default=None Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the data. See :term:`Glossary <random_state>` for details.\n\nmax_squared_sum : float, default=None Maximum squared sum of X over samples. Used only in SAG solver. If None, it will be computed, going through all the samples. The value should be precomputed to speed up cross validation.\n\nsample_weight : array-like of shape(n_samples,), default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.\n\nl1_ratio : float, default=None The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination of L1 and L2.\n##### Returns\n* **coefs **: ndarray of shape (n_cs, n_features) or (n_cs, n_features + 1)\n    List of coefficients for the Logistic Regression model. If\n    fit_intercept is set to True then the second dimension will be\n    n_features + 1, where the last item represents the intercept.\n\n* **Cs **: ndarray\n    Grid of Cs used for cross-validation.\n\n* **scores **: ndarray of shape (n_cs,)\n    Scores obtained for each Cs.\n\n* **n_iter **: ndarray of shape(n_cs,)\n    Actual number of iteration for each Cs.\n\n"
}