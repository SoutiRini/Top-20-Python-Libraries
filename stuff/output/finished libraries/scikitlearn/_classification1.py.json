{
    "source file": "_classification1.py",
    "line number": "2373",
    "func name": "brier_score_loss",
    "func arg": "(y_true, y_prob)",
    "comments": "Compute the Brier score.\n\nThe smaller the Brier score, the better, hence the naming with \"loss\". Across all items in a set N predictions, the Brier score measures the mean squared difference between (1) the predicted probability assigned to the possible outcomes for item i, and (2) the actual outcome. Therefore, the lower the Brier score is for a set of predictions, the better the predictions are calibrated. Note that the Brier score always takes on a value between zero and one, since this is the largest possible difference between a predicted probability (which must be between zero and one) and the actual outcome (which can take on values of only 0 and 1). The Brier loss is composed of refinement loss and calibration loss. The Brier score is appropriate for binary and categorical outcomes that can be structured as true or false, but is inappropriate for ordinal variables which can take on three or more values (this is because the Brier score assumes that all possible outcomes are equivalently \"distant\" from one another). Which label is considered to be the positive label is controlled via the parameter pos_label, which defaults to 1. Read more in the :ref:`User Guide <calibration>`.\n\nParameters ---------- y_true : array, shape (n_samples,) True targets.\n\ny_prob : array, shape (n_samples,) Probabilities of the positive class.\n\nsample_weight : array-like of shape (n_samples,), default=None Sample weights.\n\npos_label : int or str, default=None Label of the positive class. Defaults to the greater label unless y_true is all 0 or all -1 in which case pos_label defaults to 1.\n##### Returns\n* **score **: float\n    Brier score\n\n* **.. [1] `Wikipedia entry for the Brier score.\n        <https**: //en.wikipedia.org/wiki/Brier_score>`_\n\n"
}