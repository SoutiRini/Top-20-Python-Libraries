{
    "source file": "_base9.py",
    "line number": "826",
    "func name": "_fit_liblinear",
    "func arg": "(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)",
    "comments": "Used by Logistic Regression (and CV) and LinearSVC/LinearSVR.\n\nPreprocessing is done in this function before supplying it to liblinear.\n\nParameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vector, where n_samples in the number of samples and n_features is the number of features.\n\ny : array-like of shape (n_samples,) Target vector relative to X\n\nC : float Inverse of cross-validation parameter. Lower the C, the more the penalization.\n\nfit_intercept : bool Whether or not to fit the intercept, that is to add a intercept term to the decision function.\n\nintercept_scaling : float LibLinear internally penalizes the intercept and this term is subject to regularization just like the other terms of the feature vector. In order to avoid this, one should increase the intercept_scaling. such that the feature vector becomes [x, intercept_scaling].\n\nclass_weight : dict or 'balanced', default=None Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n\nThe \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``\n\npenalty : {'l1', 'l2'} The norm of the penalty used in regularization.\n\ndual : bool Dual or primal formulation,\n\nverbose : int Set verbose to any positive number for verbosity.\n\nmax_iter : int Number of iterations.\n\ntol : float Stopping condition.\n\nrandom_state : int or RandomState instance, default=None Controls the pseudo random number generation for shuffling the data. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.\n\nmulti_class : {'ovr', 'crammer_singer'}, default='ovr' `ovr` trains n_classes one-vs-rest classifiers, while `crammer_singer` optimizes a joint objective over all classes. While `crammer_singer` is interesting from an theoretical perspective as it is consistent it is seldom used in practice and rarely leads to better accuracy and is more expensive to compute. If `crammer_singer` is chosen, the options loss, penalty and dual will be ignored.\n\nloss : {'logistic_regression', 'hinge', 'squared_hinge',\n\n\n\n\n\n\n\n\n\n\n\n 'epsilon_insensitive', 'squared_epsilon_insensitive},\n\n\n\n\n\n\n\n\n\n\n\n default='logistic_regression' The loss function used to fit the model.\n\nepsilon : float, default=0.1 Epsilon parameter in the epsilon-insensitive loss function. Note that the value of this parameter depends on the scale of the target variable y. If unsure, set epsilon=0.\n\nsample_weight : array-like of shape (n_samples,), default=None Weights assigned to each sample.\n##### Returns\n* **coef_ **: ndarray of shape (n_features, n_features + 1)\n    The coefficient vector got by minimizing the objective function.\n\n* **intercept_ **: float\n    The intercept term added to the vector.\n\n* **n_iter_ **: int\n    Maximum number of iterations run across all classes.\n\n"
}