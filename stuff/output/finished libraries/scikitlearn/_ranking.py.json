{
    "source file": "_ranking.py",
    "line number": "1344",
    "func name": "ndcg_score",
    "func arg": "(y_true, y_score)",
    "comments": "Compute Normalized Discounted Cumulative Gain.\n\nSum the true scores ranked in the order induced by the predicted scores, after applying a logarithmic discount. Then divide by the best possible score (Ideal DCG, obtained for a perfect ranking) to obtain a score between 0 and 1.\n\nThis ranking metric yields a high value if true labels are ranked high by ``y_score``.\n\nParameters ---------- y_true : ndarray, shape (n_samples, n_labels) True targets of multilabel classification, or true scores of entities to be ranked.\n\ny_score : ndarray, shape (n_samples, n_labels) Target scores, can either be probability estimates, confidence values, or non-thresholded measure of decisions (as returned by \"decision_function\" on some classifiers).\n\nk : int, default=None Only consider the highest k scores in the ranking. If None, use all outputs.\n\nsample_weight : ndarray of shape (n_samples,),default=None Sample weights. If None, all samples are given the same weight.\n\nignore_ties : bool, default=False Assume that there are no ties in y_score (which is likely to be the case if y_score is continuous) for efficiency gains.\n##### Returns\n* **normalized_discounted_cumulative_gain **: float in [0., 1.]\n    The averaged NDCG scores for all samples.\n\n* **dcg_score **: Discounted Cumulative Gain (not normalized).\n\n* **<https**: //en.wikipedia.org/wiki/Discounted_cumulative_gain>`_\n\n* **>>> # we have groud-truth relevance of some answers to a query**: \n\n* **>>> # true relevance of our top predictions**: (10 / 10 + 5 / 10) / 2 = .75\n\n* **>>> # wrong results**: \n\n"
}