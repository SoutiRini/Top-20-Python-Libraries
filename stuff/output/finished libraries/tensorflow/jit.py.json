{
    "source file": "jit.py",
    "line number": "42",
    "func name": "experimental_jit_scope",
    "func arg": "(compile_ops, separate_compiled_gradients)",
    "comments": "Enable or disable JIT compilation of operators within the scope.\n\nNOTE: This is an experimental feature.\n\nThe compilation is a hint and only supported on a best-effort basis.\n\nExample usage:\n\n```python with tf.xla.experimental.jit_scope(): c = tf.matmul(a, b)\n\n# compiled with tf.xla.experimental.jit_scope(compile_ops=False): d = tf.matmul(a, c)\n\n# not compiled with tf.xla.experimental.jit_scope( compile_ops=lambda node_def: 'matmul' in node_def.op.lower()): e = tf.matmul(a, b) + d\n\n# matmul is compiled, the addition is not. ```\n\nExample of `separate_compiled_gradients`:\n\n```python # In the example below, the computations for f, g and h will all be compiled # in separate scopes. with tf.xla.experimental.jit_scope( separate_compiled_gradients=True): f = tf.matmul(a, b) g = tf.gradients([f], [a, b], name='mygrads1') h = tf.gradients([f], [a, b], name='mygrads2') ```\n##### Args\n* **compile_ops**: Whether to enable or disable compilation in the scope.\n  Either a Python bool, or a callable that accepts the parameter\n  `node_def` and returns a python bool.\n\n* **separate_compiled_gradients**: If true put each gradient subgraph into a\n  separate compilation scope. This gives fine-grained control over which\n  portions of the graph will be compiled as a single unit. Compiling\n  gradients separately may yield better performance for some graphs.\n  The scope is named based on the scope of the forward computation as well\n  as the name of the gradients. As a result, the gradients will be compiled\n  in a scope that is separate from both the forward computation, and from\n  other gradients.\n\n"
}