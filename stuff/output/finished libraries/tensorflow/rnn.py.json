{
    "source file": "rnn.py",
    "line number": "279",
    "func name": "bidirectional_dynamic_rnn",
    "func arg": "(cell_fw, cell_bw, inputs, sequence_length, initial_state_fw, initial_state_bw, dtype, parallel_iterations, swap_memory, time_major, scope)",
    "comments": "Creates a dynamic version of bidirectional recurrent neural network.\n\nTakes input and builds independent forward and backward RNNs. The input_size of forward and backward cell must match. The initial state for both directions is zero by default (but can be set optionally) and no intermediate states are ever returned -- the network is fully unrolled for the given (passed in) length(s) of the sequence(s) or completely unrolled if length(s) is not given.\n##### Args\n* **cell_fw**: An instance of RNNCell, to be used for forward direction.\n\n* **cell_bw**: An instance of RNNCell, to be used for backward direction.\n\n* **inputs**: The RNN inputs.\n  If time_major == False (default), this must be a tensor of shape\n\n* **sequence_length**: (optional) An int32/int64 vector, size `[batch_size]`,\n  containing the actual lengths for each of the sequences in the batch. If\n  not provided, all batch entries are assumed to be full sequences; and time\n  reversal is applied from time `0` to `max_time` for each sequence.\n\n* **initial_state_fw**: (optional) An initial state for the forward RNN. This must\n  be a tensor of appropriate type and shape `[batch_size,\n  cell_fw.state_size]`. If `cell_fw.state_size` is a tuple, this should be a\n  tuple of tensors having shapes `[batch_size, s] for s in\n  cell_fw.state_size`.\n\n* **initial_state_bw**: (optional) Same as for `initial_state_fw`, but using the\n  corresponding properties of `cell_bw`.\n\n* **dtype**: (optional) The data type for the initial states and expected output.\n  Required if initial_states are not provided or RNN states have a\n  heterogeneous dtype.\n\n* **parallel_iterations**: (Default\n\n* **swap_memory**: Transparently swap the tensors produced in forward inference\n  but needed for back prop from GPU to CPU.  This allows training RNNs which\n  would typically not fit on a single GPU, with very minimal (or no)\n  performance penalty.\n\n* **time_major**: The shape format of the `inputs` and `outputs` Tensors. If true,\n  these `Tensors` must be shaped `[max_time, batch_size, depth]`. If false,\n  these `Tensors` must be shaped `[batch_size, max_time, depth]`. Using\n  `time_major = True` is a bit more efficient because it avoids transposes\n  at the beginning and end of the RNN calculation.  However, most TensorFlow\n  data is batch-major, so by default this function accepts input and emits\n  output in batch-major form.\n\n* **scope**: VariableScope for the created subgraph; defaults to\n  \"bidirectional_rnn\"\n\n##### Returns\n* **A tuple (outputs, output_states) where**: outputs\n\n"
}