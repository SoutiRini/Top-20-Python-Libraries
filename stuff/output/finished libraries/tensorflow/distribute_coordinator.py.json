{
    "source file": "distribute_coordinator.py",
    "line number": "631",
    "func name": "run_distribute_coordinator",
    "func arg": "(worker_fn, strategy, eval_fn, eval_strategy, mode, cluster_spec, task_type, task_id, session_config, rpc_layer)",
    "comments": "Runs the coordinator for distributed TensorFlow.\n\nThis function runs a split coordinator for distributed TensorFlow in its default mode, i.e the STANDALONE_CLIENT mode. Given a `cluster_spec` specifying server addresses and their roles in a cluster, this coordinator will figure out how to set them up, give the underlying function the right targets for master sessions via a scope object and coordinate their training. The cluster consisting of standard servers needs to be brought up either with the standard server binary or with a binary running distribute coordinator with `task_type` set to non-client type which will then turn into standard servers.\n\nIn addition to be the distribute coordinator, this is also the source of configurations for each job in the distributed training. As there are multiple ways to configure a distributed TensorFlow cluster, its context object provides these configurations so that users or higher-level APIs don't have to figure out the configuration for each job by themselves.\n\nIn the between-graph replicated training, this coordinator will create multiple threads and each calls the `worker_fn` which is supposed to create its own graph and connect to one worker master given by its context object. In the in-graph replicated training, it has only one thread calling this `worker_fn`.\n\nAnother mode is the INDEPENDENT_WORKER mode where each server runs a distribute coordinator which will start a standard server and optionally runs `worker_fn` depending whether it is between-graph training or in-graph replicated training.\n\nThe `strategy` object is expected to be a DistributionStrategy object which has implemented methods needed by distributed coordinator such as `configure(session_config, cluster_spec, task_type, task_id)` which configures the strategy object for a specific task and `experimental_should_init` property which instructs the distribute coordinator whether to run init ops for a task. The distribute coordinator will make a copy of the `strategy` object, call its `configure` method and pass it to `worker_fn` as an argument.\n\nThe `worker_fn` defines the training logic and is called under its own worker context which can be accessed to via `get_current_worker_context`. A worker context provides access to configurations for each task, e.g. the task_type, task_id, master target and so on. Since `worker_fn` will be called in a thread and possibly multiple times, caller should be careful when it accesses global data. For example, it is unsafe to define flags in a `worker_fn` or to define different environment variables for different `worker_fn`s.\n\nThe `worker_fn` for the between-graph replication is defined as if there is only one worker corresponding to the `worker_fn` and possibly ps jobs. For example, when training with parameter servers, it assigns variables to parameter servers and all other operations to that worker. In the in-graph replication case, the `worker_fn` has to define operations for all worker jobs. Using a distribution strategy can simplify the `worker_fn` by not having to worry about the replication and device assignment of variables and operations.\n\nThis method is intended to be invoked by high-level APIs so that users don't have to explicitly call it to run this coordinator. For those who don't use high-level APIs, to change a program to use this coordinator, wrap everything in a the program after global data definitions such as commandline flag definition into the `worker_fn` and get task-specific configurations from the worker context.\n\nThe `cluster_spec` can be either passed by the argument or parsed from the \"TF_CONFIG\" environment variable. Example of a TF_CONFIG: ``` cluster = {'chief': ['host0:2222'], 'ps': ['host1:2222', 'host2:2222'], 'worker': ['host3:2222', 'host4:2222', 'host5:2222']} os.environ['TF_CONFIG'] = json.dumps({'cluster': cluster}) ```\n\nIf `cluster_spec` is not given in any format, it becomes local training and this coordinator will connect to a local session.\n\nFor evaluation, if \"evaluator\" exists in the cluster_spec, a separate thread will be created to call `eval_fn` with its `task_type` set to \"evaluator\". If `eval_fn` is not defined, fall back to `worker_fn`. This implies that evaluation will be done on a single machine if there is an \"evaluator\" task. If \"evaluator\" doesn't exist in the cluster_spec, it entirely depends on the `worker_fn` for how to do evaluation.\n##### Args\n* **worker_fn**: the function to be called. The function should accept a\n  `strategy` object and will be given access to a context object via a\n  context manager scope.\n\n* **strategy**: a DistributionStrategy object specifying whether it should\n  run between-graph replicated training or not, whether to run init ops,\n  etc. This object will also be configured given `session_config`,\n  `cluster_spec`, `task_type` and `task_id`.\n\n* **eval_fn**: optional function for \"evaluator\" task. If `eval_fn` is not passed\n  in but a \"evaluator\" task is found in the `cluster_spec`, the `worker_fn`\n  will be used for this task.\n\n* **eval_strategy**: optional DistributionStrategy object for \"evaluator\" task.\n\n* **mode**: in which mode this distribute coordinator runs.\n\n* **cluster_spec**: a dict, ClusterDef or ClusterSpec specifying servers and roles\n  in a cluster. If not set or empty, fall back to local training.\n\n* **task_type**: the current task type, optional if this is a client.\n\n* **task_id**: the current task id, optional if this is a client.\n\n* **session_config**: an optional `tf.compat.v1.ConfigProto` object which will be\n  passed to `strategy`'s `configure` method and used to create a session.\n\n* **rpc_layer**: optional string, the protocol for RPC, e.g. \"grpc\".\n\n##### Returns\n"
}