{
    "source file": "state_ops.py",
    "line number": "821",
    "func name": "batch_scatter_update",
    "func arg": "(ref, indices, updates, use_locking, name)",
    "comments": "Generalization of `tf.compat.v1.scatter_update` to axis different than 0.\n\nAnalogous to `batch_gather`. This assumes that `ref`, `indices` and `updates` have a series of leading dimensions that are the same for all of them, and the updates are performed on the last dimension of indices. In other words, the dimensions should be the following:\n\n`num_prefix_dims = indices.ndims\n\n- 1` `batch_dim = num_prefix_dims + 1` `updates.shape = indices.shape + var.shape[batch_dim:]`\n\nwhere\n\n`updates.shape[:num_prefix_dims]` `== indices.shape[:num_prefix_dims]` `== var.shape[:num_prefix_dims]`\n\nAnd the operation performed can be expressed as:\n\n`var[i_1, ..., i_n, indices[i_1, ..., i_n, j]] = updates[i_1, ..., i_n, j]`\n\nWhen indices is a 1D tensor, this operation is equivalent to `tf.compat.v1.scatter_update`.\n\nTo avoid this operation there would be 2 alternatives: 1) Reshaping the variable by merging the first `ndims` dimensions. However, this is not possible because `tf.reshape` returns a Tensor, which we cannot use `tf.compat.v1.scatter_update` on. 2) Looping over the first `ndims` of the variable and using `tf.compat.v1.scatter_update` on the subtensors that result of slicing the first dimension. This is a valid option for `ndims = 1`, but less efficient than this implementation.\n\nSee also `tf.compat.v1.scatter_update` and `tf.compat.v1.scatter_nd_update`.\n##### Args\n* **ref**: `Variable` to scatter onto.\n\n* **indices**: Tensor containing indices as described above.\n\n* **updates**: Tensor of updates to apply to `ref`.\n\n* **use_locking**: Boolean indicating whether to lock the writing operation.\n\n* **name**: Optional scope name string.\n\n##### Returns\n"
}