{
    "source file": "control_flow_ops2.py",
    "line number": "336",
    "func name": "vectorized_map",
    "func arg": "(fn, elems, fallback_to_while_loop)",
    "comments": "Parallel map on the list of tensors unpacked from `elems` on dimension 0.\n\nThis method works similar to `tf.map_fn` but is optimized to run much faster, possibly with a much larger memory footprint. The speedups are obtained by vectorization (see [Auto-Vectorizing TensorFlow Graphs: Jacobians, Auto-Batching and Beyond](https://arxiv.org/pdf/1903.04243.pdf)). The idea behind vectorization is to semantically launch all the invocations of `fn` in parallel and fuse corresponding operations across all these invocations. This fusion is done statically at graph generation time and the generated code is often similar in performance to a manually fused version.\n\nBecause `tf.vectorized_map` fully parallelizes the batch, this method will generally be significantly faster than using `tf.map_fn`, especially in eager mode. However this is an experimental feature and currently has a lot of limitations:\n\n- There should be no data dependency between the different semantic invocations of `fn`, i.e. it should be safe to map the elements of the inputs in any order.\n\n- Stateful kernels may mostly not be supported since these often imply a data dependency. We do support a limited set of such stateful kernels though (like RandomFoo, Variable operations like reads, etc).\n\n- `fn` has limited support for control flow operations.\n\n- `fn` should return nested structure of Tensors or Operations. However if an Operation is returned, it should have zero outputs.\n\n- The shape and dtype of any intermediate or output tensors in the computation of `fn` should not depend on the input to `fn`.\n\nExamples: ```python def outer_product(a): return tf.tensordot(a, a, 0)\n\nbatch_size = 100 a = tf.ones((batch_size, 32, 32)) c = tf.vectorized_map(outer_product, a) assert c.shape == (batch_size, 32, 32, 32, 32) ```\n\n```python # Computing per-example gradients\n\nbatch_size = 10 num_features = 32 layer = tf.keras.layers.Dense(1)\n\ndef model_fn(arg): with tf.GradientTape() as g: inp, label = arg inp = tf.expand_dims(inp, 0) label = tf.expand_dims(label, 0) prediction = layer(inp) loss = tf.nn.l2_loss(label\n\n- prediction) return g.gradient(loss, (layer.kernel, layer.bias))\n\ninputs = tf.random.uniform([batch_size, num_features]) labels = tf.random.uniform([batch_size, 1]) per_example_gradients = tf.vectorized_map(model_fn, (inputs, labels)) assert per_example_gradients[0].shape == (batch_size, num_features, 1) assert per_example_gradients[1].shape == (batch_size, 1) ```\n##### Args\n* **fn**: The callable to be performed. It accepts one argument, which will have\n  the same (possibly nested) structure as `elems`, and returns a possibly\n  nested structure of Tensors and Operations, which may be different than\n  the structure of `elems`.\n\n* **elems**: A tensor or (possibly nested) sequence of tensors, each of which will\n  be unpacked along their first dimension. The nested sequence of the\n  resulting slices will be mapped over by `fn`.\n\n* **fallback_to_while_loop**: If true, on failing to vectorize an operation,\n  the unsupported op is wrapped in a tf.while_loop to execute the map\n  iterations. Note that this fallback only happens for unsupported ops and\n  other parts of `fn` are still vectorized. If false, on encountering an\n  unsupported op, a ValueError is thrown. Note that the fallbacks can result\n  in slowdowns since vectorization often yields speedup of one to two orders\n  of magnitude.\n\n##### Returns\n"
}