{
    "source file": "nn_impl.py",
    "line number": "2272",
    "func name": "sampled_softmax_loss",
    "func arg": "(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, remove_accidental_hits, partition_strategy, name, seed)",
    "comments": "Computes and returns the sampled softmax training loss.\n\nThis is a faster way to train a softmax classifier over a huge number of classes.\n\nThis operation is for training only.\n\nIt is generally an underestimate of the full softmax loss.\n\nA common use case is to use this method for training, and calculate the full softmax loss for evaluation or inference. In this case, you must set `partition_strategy=\"div\"` for the two losses to be consistent, as in the following example:\n\n```python if mode == \"train\": loss = tf.nn.sampled_softmax_loss( weights=weights, biases=biases, labels=labels, inputs=inputs, ..., partition_strategy=\"div\") elif mode == \"eval\": logits = tf.matmul(inputs, tf.transpose(weights)) logits = tf.nn.bias_add(logits, biases) labels_one_hot = tf.one_hot(labels, n_classes) loss = tf.nn.softmax_cross_entropy_with_logits( labels=labels_one_hot, logits=logits) ```\n\nSee our Candidate Sampling Algorithms Reference ([pdf](https://www.tensorflow.org/extras/candidate_sampling.pdf)). Also see Section 3 of (Jean et al., 2014) for the math.\n##### Args\n* **weights**: A `Tensor` of shape `[num_classes, dim]`, or a list of `Tensor`\n    objects whose concatenation along dimension 0 has shape\n    [num_classes, dim].  The (possibly-sharded) class embeddings.\n\n* **biases**: A `Tensor` of shape `[num_classes]`.  The class biases.\n\n* **labels**: A `Tensor` of type `int64` and shape `[batch_size,\n    num_true]`. The target classes.  Note that this format differs from\n    the `labels` argument of `nn.softmax_cross_entropy_with_logits`.\n\n* **inputs**: A `Tensor` of shape `[batch_size, dim]`.  The forward\n    activations of the input network.\n\n* **num_sampled**: An `int`.  The number of classes to randomly sample per batch.\n\n* **num_classes**: An `int`. The number of possible classes.\n\n* **num_true**: An `int`.  The number of target classes per training example.\n\n* **sampled_values**: a tuple of (`sampled_candidates`, `true_expected_count`,\n    `sampled_expected_count`) returned by a `*_candidate_sampler` function.\n    (if None, we default to `log_uniform_candidate_sampler`)\n\n* **remove_accidental_hits**: A `bool`.  whether to remove \"accidental hits\"\n    where a sampled class equals one of the target classes.  Default is\n    True.\n\n* **partition_strategy**: A string specifying the partitioning strategy, relevant\n    if `len(weights) > 1`. Currently `\"div\"` and `\"mod\"` are supported.\n    Default is `\"mod\"`. See `tf.nn.embedding_lookup` for more details.\n\n* **name**: A name for the operation (optional).\n\n* **seed**: random seed for candidate sampling. Default to None, which doesn't set\n    the op-level random seed for candidate sampling.\n\n##### Returns\n* **ferences**: \n\n* **On Using Very Large Target Vocabulary for Neural Machine Translation**: [Jean et al., 2014]\n  (https\n\n"
}