{
    "source file": "multi_gpu_utils.py",
    "line number": "42",
    "func name": "multi_gpu_model",
    "func arg": "(model, gpus, cpu_merge, cpu_relocation)",
    "comments": "Replicates a model on different GPUs.\n\nSpecifically, this function implements single-machine multi-GPU data parallelism. It works in the following way:\n\n- Divide the model's input(s) into multiple sub-batches.\n\n- Apply a model copy on each sub-batch. Every model copy is executed on a dedicated GPU.\n\n- Concatenate the results (on CPU) into one big batch.\n\nE.g. if your `batch_size` is 64 and you use `gpus=2`, then we will divide the input into 2 sub-batches of 32 samples, process each sub-batch on one GPU, then return the full batch of 64 processed samples.\n\nThis induces quasi-linear speedup on up to 8 GPUs.\n\nThis function is only available with the TensorFlow backend for the time being.\n\nArguments: model: A Keras model instance. To avoid OOM errors, this model could have been built on CPU, for instance (see usage example below). gpus: Integer >= 2, number of on GPUs on which to create model replicas. cpu_merge: A boolean value to identify whether to force merging model weights under the scope of the CPU or not. cpu_relocation: A boolean value to identify whether to create the model's weights under the scope of the CPU. If the model is not defined under any preceding device scope, you can still rescue it by activating this option.\n##### Returns\n* **ple 1**: Training models with weights merge on CPU\n\n* **with tf.device('/cpu**: 0')\n\n* **# Save model via the template model (which shares the same weights)**: \n\n* **ple 2**: Training models with weights merge on CPU using cpu_relocation\n\n* **ython\n ..\n # Not needed to change the device scope for model definition**: model = Xception(weights=None, ..)\n try\n\n* **ple 3**: Training models with weights merge on GPU (recommended for NV-link)\n\n"
}