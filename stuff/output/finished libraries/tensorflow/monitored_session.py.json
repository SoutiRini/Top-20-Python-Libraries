{
    "source file": "monitored_session.py",
    "line number": "434",
    "func name": "MonitoredTrainingSession",
    "func arg": "(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir, save_graph_def)",
    "comments": "Creates a `MonitoredSession` for training.\n\nFor a chief, this utility sets proper session initializer/restorer. It also creates hooks related to checkpoint and summary saving. For workers, this utility sets proper session creator which waits for the chief to initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for more information.\n##### Args\n* **master**: `String` the TensorFlow master to use.\n\n* **is_chief**: If `True`, it will take care of initialization and recovery the\n  underlying TensorFlow session. If `False`, it will wait on a chief to\n  initialize or recover the TensorFlow session.\n\n* **checkpoint_dir**: A string.  Optional path to a directory where to restore\n  variables.\n\n* **scaffold**: A `Scaffold` used for gathering or building supportive ops. If not\n  specified, a default one is created. It's used to finalize the graph.\n\n* **hooks**: Optional list of `SessionRunHook` objects.\n\n* **chief_only_hooks**: list of `SessionRunHook` objects. Activate these hooks if\n  `is_chief==True`, ignore otherwise.\n\n* **save_checkpoint_secs**: The frequency, in seconds, that a checkpoint is saved\n  using a default checkpoint saver. If both `save_checkpoint_steps` and\n  `save_checkpoint_secs` are set to `None`, then the default checkpoint\n  saver isn't used. If both are provided, then only `save_checkpoint_secs`\n  is used. Default 600.\n\n* **save_summaries_steps**: The frequency, in number of global steps, that the\n  summaries are written to disk using a default summary saver. If both\n  `save_summaries_steps` and `save_summaries_secs` are set to `None`, then\n  the default summary saver isn't used. Default 100.\n\n* **save_summaries_secs**: The frequency, in secs, that the summaries are written\n  to disk using a default summary saver.  If both `save_summaries_steps` and\n  `save_summaries_secs` are set to `None`, then the default summary saver\n  isn't used. Default not enabled.\n\n* **config**: an instance of `tf.compat.v1.ConfigProto` proto used to configure\n  the session. It's the `config` argument of constructor of\n  `tf.compat.v1.Session`.\n\n* **stop_grace_period_secs**: Number of seconds given to threads to stop after\n  `close()` has been called.\n\n* **log_step_count_steps**: The frequency, in number of global steps, that the\n  global step/sec is logged.\n\n* **max_wait_secs**: Maximum time workers should wait for the session to become\n  available. This should be kept relatively short to help detect incorrect\n  code, but sometimes may need to be increased if the chief takes a while to\n  start up.\n\n* **save_checkpoint_steps**: The frequency, in number of global steps, that a\n  checkpoint is saved using a default checkpoint saver. If both\n  `save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then\n  the default checkpoint saver isn't used. If both are provided, then only\n  `save_checkpoint_secs` is used. Default not enabled.\n\n* **summary_dir**: A string.  Optional path to a directory where to save\n  summaries. If None, checkpoint_dir is used instead.\n\n* **save_graph_def**: Whether to save the GraphDef and MetaGraphDef to\n  `checkpoint_dir`. The GraphDef is saved after the session is created as\n  `graph.pbtxt`. MetaGraphDefs are saved out for every checkpoint as\n  `model.ckpt-*.meta`.\n\n##### Returns\n"
}