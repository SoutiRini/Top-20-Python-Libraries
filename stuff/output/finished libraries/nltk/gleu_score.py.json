{
    "source file": "gleu_score.py",
    "line number": "87",
    "func name": "corpus_gleu",
    "func arg": "(list_of_references, hypotheses, min_len, max_len)",
    "comments": "Calculate a single corpus-level GLEU score (aka. system-level GLEU) for all the hypotheses and their respective references.\n\nInstead of averaging the sentence level GLEU scores (i.e. macro-average precision), Wu et al. (2016) sum up the matching tokens and the max of hypothesis and reference tokens for each sentence, then compute using the aggregate values.\n\nFrom Mike Schuster (via email): \"For the corpus, we just add up the two statistics n_match and n_all = max(n_all_output, n_all_target) for all sentences, then calculate gleu_score = n_match / n_all, so it is not just a mean of the sentence gleu scores (in our case, longer sentences count more, which I think makes sense as they are more difficult to translate).\"\n\n>>> hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', ...\n\n\n\n\n\n\n\n 'ensures', 'that', 'the', 'military', 'always', ...\n\n\n\n\n\n\n\n 'obeys', 'the', 'commands', 'of', 'the', 'party'] >>> ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that', ...\n\n\n\n\n\n\n\n\n\n'ensures', 'that', 'the', 'military', 'will', 'forever', ...\n\n\n\n\n\n\n\n\n\n'heed', 'Party', 'commands'] >>> ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which', ...\n\n\n\n\n\n\n\n\n\n'guarantees', 'the', 'military', 'forces', 'always', ...\n\n\n\n\n\n\n\n\n\n'being', 'under', 'the', 'command', 'of', 'the', 'Party'] >>> ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the', ...\n\n\n\n\n\n\n\n\n\n'army', 'always', 'to', 'heed', 'the', 'directions', ...\n\n\n\n\n\n\n\n\n\n'of', 'the', 'party']\n\n>>> hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was', ...\n\n\n\n\n\n\n\n 'interested', 'in', 'world', 'history'] >>> ref2a = ['he', 'was', 'interested', 'in', 'world', 'history', ...\n\n\n\n\n\n\n\n\n\n'because', 'he', 'read', 'the', 'book']\n\n>>> list_of_references = [[ref1a, ref1b, ref1c], [ref2a]] >>> hypotheses = [hyp1, hyp2] >>> corpus_gleu(list_of_references, hypotheses) # doctest: +ELLIPSIS 0.5673...\n\nThe example below show that corpus_gleu() is different from averaging sentence_gleu() for hypotheses\n\n>>> score1 = sentence_gleu([ref1a], hyp1) >>> score2 = sentence_gleu([ref2a], hyp2) >>> (score1 + score2) / 2 # doctest: +ELLIPSIS 0.6144...\n\n:param list_of_references: a list of reference sentences, w.r.t. hypotheses :type list_of_references: list(list(list(str))) :param hypotheses: a list of hypothesis sentences :type hypotheses: list(list(str)) :param min_len: The minimum order of n-gram this function should extract. :type min_len: int :param max_len: The maximum order of n-gram this function should extract. :type max_len: int :return: The corpus-level GLEU score. :rtype: float\n"
}