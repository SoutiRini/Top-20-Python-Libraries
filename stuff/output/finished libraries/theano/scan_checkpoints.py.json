{
    "source file": "scan_checkpoints.py",
    "line number": "7",
    "func name": "scan_checkpoints",
    "func arg": "(fn, sequences, outputs_info, non_sequences, name, n_steps, save_every_N, padding)",
    "comments": "Scan function that uses less memory, but is more restrictive.\n\nIn :func:`~theano.scan`, if you compute the gradient of the output with respect to the input, you will have to store the intermediate results at each time step, which can be prohibitively huge. This function allows to do ``save_every_N`` steps of forward computations without storing the intermediate results, and to recompute them during the gradient computation.\n\nNotes ----- Current assumptions:\n\n* Every sequence has the same length. * If ``n_steps`` is specified, it has the same value as the length of any sequence. * The value of ``save_every_N`` divides the number of steps the scan will run without remainder. * Only singly-recurrent and non-recurrent outputs are used. No multiple recurrences. * Only the last timestep of any output will ever be used.\n\nParameters ---------- fn ``fn`` is a function that describes the operations involved in one step of ``scan``. See the documentation of :func:`~theano.scan` for more information.\n\nsequences ``sequences`` is the list of Theano variables or dictionaries describing the sequences ``scan`` has to iterate over. All sequences must be the same length in this version of ``scan``.\n\noutputs_info ``outputs_info`` is the list of Theano variables or dictionaries describing the initial state of the outputs computed recurrently.\n\nnon_sequences ``non_sequences`` is the list of arguments that are passed to ``fn`` at each steps. One can opt to exclude variable used in ``fn`` from this list as long as they are part of the computational graph, though for clarity we encourage not to do so.\n\nn_steps ``n_steps`` is the number of steps to iterate given as an int or Theano scalar (> 0). If any of the input sequences do not have enough elements, scan will raise an error. If n_steps is not provided, ``scan`` will figure out the amount of steps it should run given its input sequences.\n\nsave_every_N ``save_every_N`` is the number of steps to go without storing the computations of ``scan`` (ie they will have to be recomputed during the gradient computation).\n\npadding If the length of the sequences is not a multiple of ``save_every_N``, the sequences will be zero padded to make this version of ``scan`` work properly, but will also result in a memory copy. It can be avoided by setting ``padding`` to False, but you need to make sure the length of the sequences is a multple of ``save_every_N``.\n##### Returns\n* **tuple\n    Tuple of the form ``(outputs, updates)`` as in **: func\n\n* ****: func\n\n"
}