{
    "source file": "kernel_codegen.py",
    "line number": "303",
    "func name": "inline_softmax_fixed_shared",
    "func arg": "(N, buf, x, stride_x, load_x, sm, sm_stride, write_sm, threadPos, threadCount, b, stride_b, load_b, dtype)",
    "comments": "Generate code to perform softmax with a fixed amount of shared memory.\n\nOn entry, `buf` is assumed to be empty.\n\nOn exit, `buf[0]` contains the softmax, `buf2` contains un-normalized softmax.\n\nParameters ---------- N Length of the buffer, atleast waprSize(32). buf A shared memory buffer of size warpSize * sizeof(dtype). x A ptr to the gpu memory where the row is stored. stride_x The stride between each element in x. load_x Wrapper to read from x. sm A ptr to the gpu memory to store the result. sm_stride The stride between each sm element. write_sm Wrapper before writing to sm. threadPos Index of executing thread. threadCount Number of executing threads. b Optional, pointer to the bias. stride_b Optional, the stride of b if b is provided. load_b Optional, wrapper to read from b if b is provided. dtype Optional, the dtype of the softmax's output if not float32.\n\nNotes ----- `buf` should be in gpu shared memory, we access it many times.\n\nWe use tx as an int variable in a loop.\n"
}